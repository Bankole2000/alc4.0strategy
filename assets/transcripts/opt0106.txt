Foundations for Cloud Architecture
by James Bannan

Looking at taking advantage of cloud computing? This course teaches you all about understanding cloud computing - what it is, what it can do you for, and how to architect it for the best results.

There is no one size fits all in cloud computing. In this course, Foundations for Cloud Architecture, you'll dive into a detailed approach to understanding cloud technology, and how you can best utilize it. First, you'll explore the differences between public, private, and hybrid cloud computing, the business requirements they fill, and their architectural caveats. Next, you'll learn about the practical variations in public cloud service offerings. Finally, you'll take a look into how you and your business can benefit from advanced cloud services. When you're finished with this course, you'll have a better understanding of the major cloud computing concepts and services to confidently make your own architectural choices.

Course author
Author: James Bannan	
James Bannan
James Bannan is a consultant based in Melbourne, Australia. He has worked in the IT industry for over 18 years and has worked in a variety of roles, from helpdesk to systems administration and now...

Course info
Level
Beginner
Rating
4.4 stars with 162 raters(162)
My rating
null stars

Duration
2h 36m
Released
18 Oct 2017
Share course

Defining Cloud Architecture
Centralized and De-centralized Datacenters
Hello, and welcome to this Pluralsight course on Cloud Architecture Foundations. My name is James Bannan. I'm a Microsoft MVP specializing in Azure architecture, and I work as a consultant and a cloud architect in Melbourne, Australia. The first module in this course is all about defining cloud architecture because like any technical framework, cloud computing comes complete with a wealth of concepts and components, which can make the path to the cloud more like a confused stumble through thick fog, so we'll use this module to examine what it is we actually mean when we talk about cloud computing, by looking at the following fundamental concepts which are core to understanding cloud computing architecture and services, the history of evolution of datacenters, centralized versus de-centralized computing, and centralized versus de-centralized datacenters, the impact of virtualization on computing in general, and datacenters in particular, how both the commoditization of hardware and resource abstraction have changed the way we think about an architect computing at scale, and how the evolution of datacenter computing has enabled a service-driven business model. Let's start by taking a look at the concepts of datacenters and how they have evolved over the years. Datacenters have been around for many years, and the concept is a fundamental component of enterprise computing. Mainframe computers, which required specialized rooms to store them in, have been around since the 1960s, although arguably the first datacenter was located at Bletchley park in the early 1940s, where the hand-built automated number crunchers played a pivotal role in the United Kingdom's code breaking efforts during the Second World War. For decades, the only functional datacenter model was that of the centralized datacenter. By this, we mean that each company and enterprise which needed internal computing resources, built and maintained their own mainframe systems, and ensured that the systems were fed sufficient power, kept cool, were backed up, and actively managed throughout their lifecycle. With the emergence of client-server computing in the 1970s, 1980s, and 1990s, it became possible for businesses and home consumers to purchase inexpensive, yet relatively powerful systems, which were capable of serving requests, much in the same way as traditional mainframes. Fewer computing resources were spread across a much larger number of physical machines, hence the de-centralized datacenter. With the rise of internet connectivity from the mid 1990s through to the early 2000s, the de-centralized datacenter model meant that businesses could provide a head office experience to remote offices, and eventually to workers in the field, or those working from home. As internet connections became faster and more reliable, except in Australia where we all have to take turns on the internet, it became possible for businesses to start bringing that remote, de-centralized infrastructure back to the main datacenter at head office. After all, if the remote site can connect back to head office with enough bandwidth, why spend the money purchasing and maintaining additional hardware. However, it's important to realize that the concept of a centralized datacenter is only centralized from the point of view of individual businesses. Zoom out far enough, and you can see that all these centralized datacenters dotted across every city in every country in the world is, from a global perspective, still massively de-centralized.

Virtualization and Commoditization
Virtualization has been around for much longer than most of us realize. Really, it's been around in one form or another for almost as long as mainframes. However, when we think of virtualization, most of us think about the surge in hardware virtualization on commodity server systems, which occurred in the 2000s. It also led to rapid and barely-controlled service brawl, but that's a nightmare for another bedtime story. Jump forward a few more years, and now we're working with containers. Virtualization has moved beyond providing a virtual hardware environment, and can now provide a virtual operating system kernel. For most of the history of datacenters, the capability of systems to scale has been limited by practical physical constraints, such as the amounts of data you could fit onto the storage media available at the time, the amount of data you could push from one system to the next using whatever networking infrastructure and protocols were available, the amounts of calculations a processor could perform, or the speed at which a system's internal components could communicate with each other, and many more constraints besides. However, in the past decade or so, there has been a distinct blurring between what constitutes enterprise-grade and consumer-grade, and this change has been driven by the constant improvement in the quality and capabilities, and the price of consumer-grade hardware. Let's look at an example. In order to provide robust storage to support datacenter workloads, a business would traditionally invest in expensive, dedicated storage arrays. Servers which needed to make use of this storage, whether for hosting virtual machines, storing data, or supporting high-performance databases, were presented with preconfigured storage volumes as ______. The servers acted as pure consumers of the storage, which was controlled and presented by the array. Fast forward to today and we're now at the point where each physical server has the power and capability to act as a fully redundant storage array in its own right, simply using commodity disks, working in conjunction with other identical systems in the same datacenter across a high-speed networking back plane, and enterprises can now build their own highly-scalable storage fabric. This ability to reduce the reliance upon specialist hardware solutions and build out one's own solutions using the native capabilities of commodity physical server hardware has been absolutely critical in the evolution of centralized computing within datacenters and the unlocking of global-scale centralized computing, which lies at the heart of cloud computing.

Service-driven Architecture
Consider the case of an individual who has been tasked with implementing a new application, which their company has purchased. In times past, our hero would have gone to the company's IT department asking for this new application to be provisioned, and the chances were pretty good that they would get sucked into the maelstrom of server specifications, vendor contracts, and procurement times. Three months later, they might actually have something up and running, but only if they were very, very lucky. This was because each application needed its own resources, and those resources could only be provided by physical hardware. Given that providing accurate resource and performance specifications for any given application has always been difficult, and remains so, and combined with the capital expenditure-driven approach to datacenter expansion, which we have already discussed, what was the inevitable outcome? Why, that servers were ordered and provisioned and were generally massively over specked for their intended purpose. After all, working within this mindset, it's actually easier to spend too much on a server once than it is to buy a more reasonably-priced server, and then go out and have to buy another one in six months' time. Virtualization changed this, as we were able to fit more resources onto the physical kit. So the hero of our story now can simply ask for a new virtual machine to run this application. Of course, they still don't know the resource requirements, so they will probably end up consuming more resources than necessary. But still, there's been some improvement in the process. In the past ten years, this paradigm has been under considerable pressure by service-driven architecture. The thinking goes something like this, the person requesting to run an application doesn't care about the underlying infrastructure. They're never going to be responsible for it, and they don't want their own projects to be held back because of it, and they really don't want to have to submit a request to a different department to have the necessary resources and access allocated. It's a very inefficient process. IT department doesn't really, or really doesn't want to get involved with the minutia of each and every application which runs on the systems for which they are responsible. They aren't the application owners, but they end up having to get so deeply involved in resource allocation and platform troubleshooting, that they end up becoming the defacto owners. The IT department would quite like the application owners to be able to provision what they want without having to go through them to make it happen. But, only as long as critical procedures, such as adherence to security practices, are complied with. Convenience cannot equate to noncompliance. Using service-driven architecture, the IT department can abstract the underlying datacenter infrastructure in such a way that fundamental resources like compute, networking, and storage are presented as a predefined and bundled service, which can be consumed by a customer like our application owner. After all, customers aren't necessarily only external to our organization. The IT department can offer these services in such a way that all the underlying critical processes, such as security, are still adhered to. In fact, the service consumer cannot bypass these processes because they are baked into the service offering and, ideally, the consumer is not even aware of them. The application owner, or the service customer, has access to an identity-driven, self-service mechanism, like a web portal, which allows them to choose the services which they would like to run. Upon making a selection, an automated business process is engaged, which provisions the application using the service mechanisms supplied by the IT department. The application owner gets the outcome they need without having to engage directly with the IT department, and the IT department keep providing services and supporting the underlying infrastructure without needing to be directly engaged with whatever is running on it. It may sound like a story which is too good to be true, but the reality is that many businesses are either looking at, or have actually deployed such internal solutions, whereas for other global businesses, like cloud computing providers, this is exactly how their business model works. Technological evolution now allows for massively centralize computing on a world-wide scale where consumers can request and pay for what they actually need rather than what they think they might need in 12 months' time. So, let's do a quick recap on what we've covered in this module. We've examined some of the core concepts of cloud computing, specifically centralized and de-centralized computing, and centralized and de-centralized datacenters. We then looked at the impact of virtualization in our datacenters, and how the nature of virtualization has changed such that we can focus more and more on the services we want to access, while achieving a greater and greater concentration of services on existing physical hardware. We looked at how the never-ending improvements in physical hardware have blurred the lines between the traditional spheres of enterprise-grade and consumer-grade, bringing us to the point where robust architecture can be scaled across the world using off-the-shelf commodity hardware peripherals. And finally, we looked at the impact of the decades of improvement where abstraction from the underlying infrastructure into service-driven mindset has enabled genuine self-service of complex IT solutions, which is the essential foundation of cloud computing. Now let's move onto the next module where we will delve deeper into the different types of cloud computing, public, private, and hybrid, and what makes them different, and what business needs they fulfill.

Understanding Types of Cloud Computing
Private Cloud Computing
Hello, and welcome to this module in the Pluralsight course, Cloud Architecture Foundations. This module is all about understanding the different types of cloud-computing implementations, which you might encounter or consider in your own business. As we have already established, the nature of cloud computing is one which makes heavy use of virtualization and commodity hardware to provide an abstracted self-service user experience to customers or end users. The ultimate consumers of a cloud-computing service do not need to be external to your business. They could easily represent different internal business units such as finance, HR, or application and development. Therefore, cloud computing is all about where you place the resources which serve your customers, and what the capabilities of the platform are, and as such, cloud computing tends to come in three main flavors, private cloud, which incorporates external hosted services, public cloud, and hybrid cloud, which, as the name suggests, is a fusion of public and private cloud. Let's delve into each type in more detail. Private cloud computing can be defined in a number of ways. One of the least helpful involves changing the sign on your server room from server room to private cloud. As we've already established, cloud computing consists of a service-driven framework and automated processes, which result in a specific, yet flexible consumer outcome. You can't run up a traditional server, install some VMs, and claim to have a private cloud. There is a particular mindset which must come into play and be implemented correctly. But let's say that you've invested heavily in automation on top of abstracted hardware, what makes your environment a private cloud? In its simplest form, a private cloud is a cloud-computing environment which completely leverages on-premises infrastructure and services. As with a traditional mainframe, or client-server model, the business is responsible for the purchase configuration and maintenance of the hardware which supports the entire environment, including provisioning sufficient power and cooling and lifecycle management systems, such as protection, recovery, disaster recovery, and archival platforms. Communication between the various systems within such an environment, will almost invariably occur over networking infrastructure which the business owns and maintains directly, such as a private internal network, or which has been specially provisioned, such as dedicated fiber optic connection between buildings. As such, no part of the overall solution needs to traverse the public internet in order to function, and therefore we can say that this environment is a private cloud. Of course, things are never as clear cut as this example makes out. Let's look at a couple of different scenarios. A business has built a private-cloud environment, which consists of services which are spread between a couple of datacenters. As neither datacenter has the capacity to run everything in a fully centralized manner, the datacenters are physically distant enough from each other so that a dedicated network connection is either not feasible or prohibitively expensive. So, connectivity between the two datacenters is achieved using locally available public internet connections with a secured private virtual network tunnel connecting the two end points. Is this still a private cloud? The answer is yes, because whilst there is a component of shared infrastructure, it is being used as a means to provide private services which are not exposed publicly, and the business is still essentially in control of the entire solution. Next scenario. A business has built an on-premises private cloud, but the data center in which the infrastructure is housed is old and no longer fit for purpose. Ongoing maintenance costs are too high, and the business is unwilling in invest in building a completely new datacenter. So the decision is taken to partner with a hosting provider, which will provide dedicated and isolated rack space for servers, power, cooling, and a private network connection to the business's head office. The business is still responsible for the installation, configuration, and maintenance of its computing infrastructure, but can now decommission its old hardware. Is this still a private cloud? Again, the answer is yes because whilst the overall solution makes use of infrastructure which is owned and operated by a third party, the business still has responsibility over, and control of the physical infrastructure which makes up the core of the service platform. Additionally, the services offered by the hosting provider are physically isolated from any other services which it may be providing to other customers, and access to those services is dedicated and physically secure. So the definition encompasses a number of potential scenarios, but if the business is ultimately responsible for the purchase, configuration, management and maintenance of the physical computing infrastructure, then you're certainly dealing with a private cloud. Now ask why you might prefer, or require this form of cloud computing. Private-cloud computing is generally favored by very large organizations which have a pre-existing significant investment in on-premises computing and dedicated datacenters, regardless of the provider. Many such organizations have long-lived legacy applications which cannot be easily re-platformed or relocated, yet these applications still perform critical business functions. Some such businesses also have extremely data sovereignty and security compliance requirements, which are imposed by industry or government regulation, or both, and having invested a significant amount over many years to achieve certification, the business is unwilling to go through the process again simply to move to a new external platform, regardless of the perceived benefits. Such businesses also tend to have very conservative corporate risk profiles and keeping all data and systems within its own direct sphere of control is a mandatory part of satisfying internal risk audits. It's also worth pointing out that despite public cloud computing still being the noisy cool kid in the playground, the vast majority of datacenter implementations are still on-premises, although it has been rebranded and been provided with a makeover with automation and business-focused processes, private cloud still represents traditional on-premises computing, which has decades of maturity behind it. Don't expect to see it vanish any time soon.

Public Cloud Computing
On the other side of private cloud sits public cloud. At its core, a public cloud provides services which a business can consume on as as-required basis, or a subscription-based model. These services can be consumed quickly, don't require significant upfront investment, and don't require the customer to bring any physical hardware to the party. So thinking back to our earlier example of a business making use of the services of a hosting provider, without the purchase, installation, and configuration of physical systems, the business doesn't have a computing platform. The hosting provider offers some services, which go into the overall solution, but on their own, it is not enough. Public cloud providers offer complete solutions aimed at a wide variety of business and technical requirements which don't require customers to bring along their own hardware in order to make the solution useable. Accessing public cloud services is almost invariably done via the public internet, although the traffic always securely encrypted. There are exceptions to this, as we shortly see, but in general it holds true. The principle behind public cloud computing is that you, as the customer, access the resource or the service that you need rather than having to build up and maintain a supporting infrastructure just so that you can get the outcome you're looking for. Let's look at a few examples which serve to highlight what public cloud computing is. Some of your developers need virtual machines to act as a sandboxed environment so that they can build new applications on a full operating system without worrying about breaking things. Your own private cloud doesn't have the capacity to provide all the virtual machines or the network isolation needed. So for you to provide the services required, you'd have to purchase additional hardware. Alternatively, the developers can go to a public cloud provider and provision the virtual machines needed, pay for what they use, and decommission the resources once they have finished with them. They get what they need, and neither you nor the business needs to finance additional investment in order to make it occur. Of course, sometimes people forget to turn public cloud VMs off, or even to tell the finance department what they've done, but that's another issue. Let's look at another example. One of the data analysts in your company needs a new SQL database. Your environment has sufficient capacity, but the process is that IT needs to allocate resources and provision a new virtual machine. The analyst then has to log on and manually install and configure SQL Server software, after requesting the installation medium. From the analyst's perspective, this is painful. All they were after was a database, and yet they have had to jump through multiple hoops to get the thing that they actually need. From the business's point of view, it's also quite painful. A data analyst is an expensive resource, and they've had to waste significant amounts of time performing non-core tasks simply so that they can get on with their job. Far better for the analyst to leverage a public cloud provider and provision a new database directly, not a virtual machine, and not a database server, but rather the thing they specifically need, a database. Yes, there will be a virtual machine and a database server somewhere in the solution, but the data analyst doesn't need to worry about any of that. It's all been abstracted away, allowing them to get on with things. Last scenario. You start up a new company with 10 employees. You all need access to email, shared calendaring, collaboration, and document management, but you're based in a managed office workspace with no server-room facilities or wired networking, and none of you have the time or the inclination to devote to building and maintaining on-premises systems just so that you can run a business. After all, even if you have the technical capability, why bother? Using a subscription-based payment model, you can quickly sigh up to a business service which give you and your employees access to everything needed to enable day-to-day operating requirements and collaboration. It's important to keep in mind that behind the scenes of all the scenario's discussed, there is still a physical infrastructure running somewhere. There are still virtual machines and mail servers, and database servers, and disk arrays, and networking. But all of these supporting services are completely abstracted from the service consumer. The public cloud provider maintains these systems behind the scenes as a means to provide a wide variety of services, which can be consumed easily, but there is no escape from an ultimate dependency on physical systems running somewhere. It may look like magic, but the magic really lies in the public cloud provider, making it all seem like magic in the first place. Public cloud services come in all shapes and sizes as we will see, but we do see many instances of business turning to public cloud in order to meet specific needs, such as, supporting projects or work tasks which require agility and high levels of automation, providing sandbox environments to cater for isolated testing, providing compute resources or services to geographically dispersed offices or field workers, enabling existing enterprises to slowly move away from, and eventually decommission, on-premises datacenters, providing and end-to-end collaboration and management platform to born-in-the-cloud businesses. Business investment in public cloud might not be as wide spread as private cloud, but there is no doubt that public cloud practices are having a significant impact on the way in which senior level executives think about their own IT services and operations.

Hybrid Cloud Computing
Finally, let's take a look at hybrid cloud. It probably won't come as any surprise that a hybrid cloud is a combination of private cloud and public cloud working together. What may come as a surprise is that for businesses which have invested in public cloud services, the hybrid cloud implementation is certainly the most common. Why would this be? The reason is that for most businesses, investments in public cloud does not, and in the vast majority of cases cannot, involve a wholesale migration to the cloud. Some businesses do talk about going 100% cloud, but the reality is that this is rarely practical or feasible, especially in the short term. Before we look at precisely why this is, let's take a moment to explore how we define a hybrid cloud implementation. Hybrid cloud isn't simply a business which has services running in both an on-premises datacenter, as well as in one or more public cloud providers. By default, the services running in each environment are effectively isolated from each other and can be seen effectively as disparate datacenters which the business pays for, except that one of them is provided by a public cloud provider. To be considered a hybrid cloud, there needs to be some kind of interconnectivity between the on-premises and cloud-based environments, which enables a seamless transition from one to the other, and abstracts the service location away from the end user. Consider the following scenarios. A business has an on-premises database cluster, which serves a number of business-critical databases. The environment has been built for high availability, but not redundancy beyond the context of the datacenter itself. The business would like these databases to be available in the event of a datacenter outage. Rather than building a new datacenter or going to a hosting provider and provisioning physical servers, the business decides to make use of database services in a public cloud environment which supports replication from the on-premises database servers. A dedicated connection is made between the on-premises datacenter and public cloud provider to ensure that the database replication traffic does not traverse the public internet, even via a VPN. In this way, the public cloud environment effectively becomes an extension of the existing datacenter. In another scenario, a business has built an externally-facing web application using public cloud services in order to take advantage of service functionality, like auto scaling. However, the web application needs to talk to a back-end database, which is hosted on an on-premises database server. For specific reasons of regulatory compliance, company policy states that the database cannot be stored anywhere except within a physical on-premises system, so there needs to be some kind of connectivity between the web application frontend and the database backend. Building a VPN tunnel between the public cloud provider and the datacenter, the application developers can configure the web app to query the on-premises database directly without the traffic traversing the public internet or the business having to invest in a dedicated link. In both scenarios, a private link is created between an on-premises datacenter and the public cloud environment. This connectivity is at the heart of a hybrid cloud implementation, connecting the two environments in such a way as to make one the extension of the other. These scenarios also serve to highlight some of the reasons why a business cannot necessarily go all in to a public cloud environment. Where there is existing investment in on-premises infrastructure, there is usually an internal reluctance, especially from those responsible for finance, to simply move to a completely new platform until the current environment has had its full use, or in other words, the business's assets have been properly depreciated. Also, there may be regulatory reasons why some systems cannot be moved, or in other cases a pre-existing legacy application, or even an application which is not so legacy, will only run within the framework of older hardware and/or operating systems, which cannot be replicated by the cloud provider. So regardless of the business's enthusiasm to ditch on-premises computing and head for the cloud, the common reality is that cloud migrations tend to start life as a hybrid cloud implementation, with the existing datacenter and the cloud providers environment coexisting side by side. So let's do a quick recap on what we've covered in this module. We have looked at the three main types of cloud computing implementations, private cloud, public cloud, and hybrid cloud. We have looked at private cloud in detail and examined what it is about an on-premises implementation which makes it private, even when leveraging shared infrastructure provided by a third party. We looked at the nature of public cloud, why it is so attractive to many organizations, and some of the scenarios which it unlocks which cannot be easily met by traditional on-premises computing. And finally, we looked at what a hybrid cloud implementation is, and why a business might either want to or need to make use of both public and private cloud services simultaneously. Now let's move on to the next module in this course on Cloud Architecture Foundations, which is all about understanding the types of services on offer in a public cloud environment.

Understanding Public Cloud Services
Types of Public Cloud Computing
Hello, and welcome to this module in the Pluralsight course, Cloud Architecture Foundations. In this module, we're going to examine the main types of services available in public cloud, and which business requirements are met by each. Specifically, this module will look at Infrastructure as a Service, Platform as a Service, and Software as a Service, defining each and discussing the differences between them. As we have already seen earlier in the course, cloud computing, no matter what form it takes, is defined by a service-driven architecture where technology has evolved to a state whereby it can be offered out as a complete end-to-end service which can be quickly and easily consumed by end users and customers. This is why, when discussing public cloud, we tend to define the major blocks of offerings as service offerings, appending as a service to the end of each, signifying that the provider has made a certain infrastructure-based solution available as an easily consumable service. The same analogy can be made of pretty much any service-based industry out there. For example, your favorite café offers coffee as a service, even though it doesn't call it that, but you know, maybe it should. Services are therefore traditionally broken down into three main categories, Infrastructure as a Service, or IaaS. The public cloud provider offers raw computing resources in the form of one or more virtual machines. These are traditional datacenter resources, like CPU, RAM, disk-spaced storage, and networking. Platform as a Service, or PaaS. The public cloud provider offers direct access to an application-based service, such as a database server, or web application server. Rather than provision a new VM using IaaS and risk paying for CPU and memory which is never used, PaaS gives you access to the service you actually want. And finally, Software as a Service, SaaS. The public cloud provider gives you access to a pre-built software application, which you can configure within the boundaries set by the application, but where you are totally abstracted from the underlying infrastructure and application services which underpin the service itself. You want a corporate email system? You could spin up a VM and build the whole thing yourself, or you could just buy the service directly. As you can see from the diagram, the main difference between each of the different levels of service is principally the degree to which you as the customer are responsible for certain aspects of the overall solution, and the extent to which the service provider provides, and is responsible for, the services which underpin the end-to-end solution. Although it's not always the case, the general rule of thumb is that as you delegate more responsibility to the cloud service provider, the range of solutions which can be run on these services becomes narrower, but the cost of provisioning a complete solution also reduces as you are paying purely for the service you require as opposed to wasting resources. This is of course a very high-level overview of the breakdown of the different categories of public cloud services. There are a number of providers on the market and each offers services in a slightly different way. However, it should be very clear that the principles of transforming a traditional on-premises datacenter computing via the service-driven architecture model, is at the heart of public cloud computing. Let's delve into the first main category, Infrastructure as a Service.

Infrastructure-as-a-service
As the name implies, Infrastructure as a Service offerings, or IaaS offerings, provide virtual machines running on the cloud provider's own infrastructure. Most cloud providers offer a range of VMs with pre-built and pre-configured operating systems, which let you spin up a new VM quickly and get started on your operating system of choice, and some will also give you the ability to use their IaaS services, or you own, to supply your own customized operating system image for deployment. When you provision a VM using IaaS, you can usually choose from a range of predefined hardware specifications, such as how much CPU, RAM, and storage you want your new system to have access to. The size of VM you provision will determine how much you are charged because the service provider will usually allocate those resources to you exclusively, meaning that it can't allocate them to another customer. Therefore, you pay for what you request, which means that you do need to be careful and ensure that the resources you request meet your actual needs. Although, as we will see later in the course, making the wrong choice at the outset isn't necessarily a problem, as VM sizes can be changed during a VM's lifecycle. Unlike the virtualization environment in a private cloud, with a public cloud IaaS offering, the underlying physical computing resources are abstracted away from you, as is the virtualization management layer. This doesn't prevent you from performing basic functions against the VM directly, such as powering on, restarting, or powering off, but these commands are performed against an application programming interface, an API, which is authorized to submit the command to the underlying virtualization management layer. This abstraction also acts as a protective mechanism using identity management and role-based access control, RBAC, to ensure that only identities with the correct access can perform actions against provisioned services. Because you are abstracted away from the underlying compute and storage resources, this also means that you don't have to actively maintain the hardware which underpins your VM. For example, if you provisioned a new virtual machine with an operating system disk and a couple of data disks, these disks are actually virtual hard drives in whichever format your cloud provider utilizes. In a private cloud environment, you would be responsible for ensuring that the physical disks which support these virtual disks, were properly architectured for redundancy, such that one or more physical disk failures didn't result in the virtual disk being taken offline. In an IaaS environment, this is, or certainly should be, an implicit feature of the service. It's something you can't control, therefore it's not your responsibility, but rather that of the service provider. IaaS is often one of the first forays a customer makes into public cloud computing, as it requires little modification of existing on-premises applications, or at least there is a perception that little transformation is needed. For example, if you have a database running on an installation of SQL Server on a single Windows Server VM, then it's quite simple to replicate the same installation in service configuration on a Windows Server VM in a public cloud IaaS environment. There are of course a whole raft of things you can do, and should do, to make the provisioning of such services better than a mere replication of what you currently run on-premises, but the analogy holds true for more customers. IaaS has a lower barrier to entry, therefore it's usually the first public cloud service which customers running traditional on-premises applications invest in. There are, however, other reasons for choosing IaaS. A born-in-the-cloud business may find itself in a position where it need to support an application, or a technical process, which can only be run in a full virtual machine due to very specific configuration or middleware requirements. Of course there are very strong arguments against being lumbered with this kind of application in the first place, but let's run with the example. Whilst this business provisions all of its services using PaaS or SaaS, the application requirements are such that a VM is necessary, but rather than build and maintain its own on-premises infrastructure, the business can provide a VM using IaaS instead and control every aspect of the operating system, middleware, and configuration necessary to support the application. This type of application would be an excellent target for digital transformation, enabling the business to remove the need for a dedicated virtual machine and to run the application on a service platform, which is more streamlined and geared toward specific outcomes. With that in mind, let's take a closer look at Platform as a Service.

Platform-as-a-service and Software-as-a-service
Take the example of building a highly available database cluster, In order to do this in your own environment, you would need to ensure that there was sufficient underlying physical hardware, that the hardware was configured in such a way as to provide redundancy for the services running on them, and that the services themselves were configured to be redundant, and that the installation and configuration of the database services were optimized to provide a redundant architecture and single point of access in the event of a failover. Then, and only then, can you provision the service you wanted in the first place, a database. Platform as a Service allows you to bypass all of those prerequisites and directly provision the database. Of course all of the supporting resources and infrastructure are still there. Databases don't just materialize out of thin air after all, but the nature of PaaS means that the management and the provisioning of everything, from the application down, is completely abstracted away from you. As such, there are some things which you can't do, like control what software is installed on the underlying server, or make specific configuration changes to the supporting server operating system, but that's the compromise. You get the service you need at a lower rate than it would cost you to provision the same solution on IaaS, but then you are also locked out of making other choices, as you have delegated that to the service provider. PaaS offerings have traditionally, and continue to, revolve around application and database development and posting. PaaS providers are able to build and maintain clusters of web and database servers of different flavors and versions, and can then provide isolated environments to individual customers whilst serving those environments out from a shared infrastructural base. This allows the service provider to maximize the number of web application or database instances in a far more efficient way than an individual customer is likely to be able to achieve. This means that the service provider is able to provide the same service at a lower per-service cost, which is why the cost of a PaaS offering is almost lower than that of a comparable solution running on IaaS. Whilst IaaS tends to be the first foray into public cloud for many businesses, PaaS is often the first investment in public cloud for developers, both inside companies and as independent entities. After all, if the work you want to focus on is the development of a web application or a database model, why waste time provisioning and configuring a VM if you don't have to. In addition to generally having a lower per implementation cost than IaaS, because PaaS solutions require much less maintenance, almost none in fact, and configuration than IaaS-based solutions, they are extremely popular targets for digital transformation. This involves taking existing on-premises applications, which are currently tied to a full operating system within a physical or virtual system, and re-architecting it to work within a PaaS framework, thereby reducing ongoing costs, as well as unlocking greater capabilities for automation, scalability, and flexibility, with the idea in mind of abstracting further away from the underlying infrastructure to focus on the services we are specifically interested in. Let's take a look at Software as a Service, or SaaS. Software as a Service abstracts everything away from the service consumer in terms of the management and maintenance of the underlying resources, infrastructure, and services which support the end result, the software application itself. SaaS platforms are, ironically, the ones which are most familiar to individuals, as we use a wide variety of SaaS platforms every day, such as web-based email, streaming music, social media, and even this Pluralsight video which you're watching right now. These are all examples of SaaS platforms, but from the perspective of enterprise technology, SaaS needs to be treated a bit differently. The principle behind enterprise-grade SaaS is the same in that a business simply pays for the service it wants without worrying about the infrastructure or services which make it possible. The difference between SaaS products for consumers is that for businesses, the SaaS platform generally has to be able to either provide a secure isolated operating environment so that the business has the assurance that its data is not accessible by other users of the platform, or it needs to be able to integrate with existing services, whether on-premises, or running in IaaS or PaaS environments, or even in different SaaS platforms. Let's look at an example. The business you work for has decided to retire its on-premises email and collaboration systems because of the age of the underlying hardware, the cost of ongoing licensing, and the difficulty in hiring sufficiently skilled administrators. You have identified an appropriate SaaS offering which meets all of your business requirements in terms of functionality, administrative flexibility, and data sovereignty, but you also need to use your on-premises identity management system as the source of truth for the SaaS platform. In other words, you don't want to have to maintain two sets of user accounts and passwords for each user. The SaaS platform needs to support your existing user and password framework. This capability is generally beyond most SaaS offerings, which has been designed for non-enterprise consumers, simply because most consumers, except for some really hardcore technologists, don't build and maintain their own identity systems. So it's not a market which the SaaS providers need to cater for. However, businesses are a very different matter. Let's take another example. You have built and deployed a solution using both IaaS and PaaS public cloud services. Now, you want to enable health monitoring on log analytics across all of those platforms so that you have some insights as to what's occurring within your solution. You could provision more VMs, or PaaS services, and then deploy your own solution, but your public cloud provider offers these services as a SaaS solution. They need to, and do, integrate with your existing services. So these offerings aren't much good in isolation, and they are designed to be consumed in conjunction with other pre-existing services. But, rather than have to do everything yourself, you can enhance your existing solution deployments with additional services, again, just paying for what you need. So like IaaS and PaaS, SaaS offerings are extremely targeting, focusing on a very particular service or set of services, and are designed to provide a specific set of outcomes. They don't have the flexibility of IaaS or PaaS, but leveraging SaaS means that you free yourself from needing to build, configure, and maintain your own solutions to achieve the same outcomes. So let's do a recap on what we've covered in this module. We have examined the definitions of IaaS, PaaS, and SaaS, and seen at a high level what differentiates them. We looked at Infrastructure as a Service in more depth, seeing what elements have been abstracted away from the service consumer, and why you might want to leverage it. We studied Platform as a Service, and saw how it allows you to focus on the specific service which you are interested in, and how you no longer need to worry about any of the underlying infrastructure or operating system configuration. And finally, we looked at Software as a Service, looking at although we are very individually used to dealing with SaaS platforms, how SaaS for business needs to offer tight integration with existing systems in order to extend and enhance business services. Now let's move on to the next module in this course on Cloud Architecture Foundations, which is all about understanding Infrastructure as a Service in more depth, focusing specifically on compute, networking and storage, and how you need to approach IaaS from and architectural perspective.

Architecting Infrastructure-as-a-service
Right-sizing Virtual Machines
Welcome to this module in the Pluralsight course, Cloud Architecture Foundations. In this module, we're going to delve deeper into some of the architectural considerations when designing and deploying solutions based on Infrastructure as a Service in a public cloud environment. Specifically, we will look at the following aspects of architecting IaaS solutions, compute, how to right-size the amount of CPU and memory resources your VM is going to need. Storage, how to choose the right types of storage and avoid being stuck with underperforming VMs, and network, why we still need to worry about networking and how to ensure that you don't accidentally paint yourself into a corner. We are tackling this aspect of public cloud architecture in depth because of an irony which surrounds deploying cloud services, which is that the easier it is to deploy something, the easier it is to deploy something which doesn't suite your needs. I've seen many instances where businesses have ended up spending far too much money on public cloud services because they were able to quickly and easily spin up a number of VMs which were not correctly sized. Cloud service providers do their best to make that path to service consumption as smooth and as streamlined as possible, but they can't, and don't, second guess your choices. Are you sure you want a VM with 16 cores, 100 GB of RAM, and a dedicated GPU? Are you sure you're sure? Really, really sure? Yeah, that's not going to occur any time soon. The number and type of CPU cores allocated to your VM, as well as the amount of RAM, is probably one of the first most important choices you need to make when planning an IaaS deployment. This is because CPU and memory utilization are still the most expensive components of computing infrastructure, with the possible exception of GPUS. As such, it's easy to over spec your public cloud VM because the public cloud provider will happily supply you with whatever you ask for. It's up to you to determine whether or not you actually need it. When looking at VM sizes, a cloud provider will offer a range of reconfigured sizes, and it's more than likely that these sizes will be your only options. You're probably not going to be able to mix and match a CPU and memory to build your own custom VM sizes. There's a couple of reasons why this is so. The first is pragmatism from the perspective of the provider. They need to be able to plan and architect resource consumption within each of the datacenters, and this becomes extremely difficult to do if they never know what is going to be requested from one customer to the next. Being able to accurately plan and forecast consumption based upon pre-defined usage patterns allows the service provider to ensure that there is capacity available for when it's called upon, while optimizing the density and spread of VMs throughout the datacenter. The second reason is that it prevents customers from requesting oddly specked VMs, which can't be easily optimized. For example, a customer may decide that their business-critical application needs 8 processing cores, but only 4 gig of RAM. The likelihood of this being the case is incredibly slim, but if they were able to provision such a VM, they would either never get the benefit of all those processing cores, or the VM would be constantly running at maximum memory. Either outcome is a poor result for the customer, which may translate into the perception, incorrectly, that their cloud provider of choice isn't very good, but how do cloud providers cater for the inevitably wide range of customer choices and preferences? Usually providers break up their Infrastructure as a Service computing offerings into pre-defined sizes which are optimized for particular workloads, such as basic VMs with shared processing cores, or VMs with a very high core to memory ratio, VMs which have dedicated GPUS, or VMs which have massive amounts of memory for database and analytics workloads. Therefore, rather than have the customer choose from a vast array of VM sizes, and potentially end up choosing the wrong one, customers can instead choose first the appropriate workload, which is planned to run on the VM, and then choose a VM size from within a narrower range, understanding that all the VM sizes are likely to be optimized for their workload of choice. So, how do you make sure that you select a VM size which is appropriate for the workload you're looking to run in the cloud? There are a few tricks for narrowing in on the right choice. If the workload you wish to run in an IaaS VM is already running in your on-premises datacenter, make sure that you can collect the performance metrics from the physical system, or VM, on which the application is currently running. More data is better, but you should try and get at least two weeks' worth of performance data focusing on maximum and average CPU usage, and maximum and average memory usage. Average and maximum storage IOPS per disk is also very useful. This information will tell you about what you're application actually needs. If it's running on a VM with 4 cores and 16 GB of RAM, but the average CPU usage is about 10% and it never spikes above 25, then you can safely say that the application doesn't need access to so many processing cores, so there's no need to provision a cloud VM with the same specifications. And the same is true for RAM usage. Additionally, it's always worth drilling down into the CPU usage patterns for each core to see whether the application is actually making use of multiple cores. If not, than either the application can't use more than one core, so save yourself money and only provision one, or it's been incorrectly configured, and you should rectify that as soon as you can as you will be paying for multiple cores and not using them. If the workload you're planning to run in the cloud is one that you're not currently running on-premises, right-sizing can be a bit trickier as you don't have access to all that performance data. If you have access to an on-premises environment, it can be worth provisioning the application there first, simply to be able to gather performance metrics before you commit to purchasing a cloud-based VM. However, if this is not an option, then start off by researching as much as you can about the advertise resource requirements, although keep in mind that many software vendors will overstate the amount of resources needed for their application to run smoothly simply to avoid situations where the application performs poorly due to a lack of resources. Extend the research beyond official channels to industry professionals who had deployed the same application in the past, and have experience in the level of resource allocation it actually needs. If the application you are looking to deploy is particularly expensive and the stated resource requirements are high, thus resulting in an expensive VM, it can be worth engaging a professional services company with application experience to provide a detailed architecture along with sizing recommendations. But what occurs if you choose a VM size, provision your new system, and install the application, only to discover that all the work you did in attempting to right-size the VM has still landed you with a VM which is either too small or too large? The solution is scaling. Most cloud providers give you the option to either scale up or scale down any VM. This is called vertical elasticity and is a feature of public cloud IaaS. Basically, it allows you to increase or decrease the amount of allocated compute resources to any VM which has already been provisioned. This is done, usually, by reconfiguring the running VM to use a different predefined VM size, either larger or smaller. The cloud provider will handle all the changes and will communicate those changes to the installed operating system. Vertical elastic scaling almost always requires that the VM is restarted. This makes sense as from the operating systems point of view, you are effectively removing or adding physical hardware. Modern server operating systems are smart and flexible, but changing fundamental hardware resources on a live system is asking quite a lot. Once you're successfully up and running on an IaaS VM, gathering performance metrics remains of critical importance, as this is the data from which you can make informed choices. If it seems that the application workload on your new VM isn't performing as you think it should, don't jump straight into triggering a scale up until you've reviewed the performance logs. Poor system performance can be caused by issues elsewhere. And with that in mind, let's take a look at storage.

Choosing the Right Storage Services
Storage is easily one of the most overlooked aspects of architecting IaaS solutions because it's so easy to forget that it's there, and that there is so much of it. When provisioning a new VM, it's very easy to get lost in CPU and memory specifications and forget that provisioning the right kind of storage is just as important. For virtual machines, most cloud providers offer two types of storage on which to store virtual disks, or virtual disk types. Hard disk drive, or HDD-based storage, and solid state disk, or SSD-based storage. In exactly the same way that you can tell in your own home systems at the performance difference between a hard drive and an SSD, along with the associated price tag, the same is true for a cloud-based VM. SSD storage is terrific for workloads which have high IOPS requirements, whereas hard-drive-based storage is more appropriate for workloads which need lots of space, but lower IOPS. However, this is a generalization, and there are plenty of circumstances where you can get away with using hard-drive-based storage only and save a fair bit of money. For example, let's say that you are running a virtual machine, which is running a small database workload. Traditionally, databases can be pretty greedy in terms of IOPS, especially when they are not configured correctly, and in the case of you IaaS-based VM, the database files are sitting on a volume which is currently supported by a single virtual disk, on hard-disk-based storage. The single disk isn't providing sufficient IOPS, so the database performance is suboptimal as a result. You could go straight to an SSD-based storage solution, but instead, you can take advantage of the underlying resiliency of cloud storage, and attach multiple hard-disk-based virtual disks to the VM, and make use of a software RAID to increase the performance of the volume on which the database files are stored. In a traditional on-premises datacenter, software RAID isn't really recommended, as each of the underlying virtual disks are single points of failure, regardless of the supporting physical storage solution. However, in a cloud datacenter, virtual disk storage has much more resilience and fault tolerance. So we can look at leveraging the capabilities of the operating system to unlock different scenarios without having to jump straight into purchasing more expensive storage services. Beyond storage for virtual machines, public cloud providers offer storage for all sorts of uses, such as simple storage for files, repositories for container images, content delivery for websites, a source for gathering event logs from disparate systems, or even as a repository for long-term archival storage. Choosing the right kind of storage for each workload is important, as different services are geared toward specific business scenarios. For example, let's say you decide to provision storage for long-term archival purposes, but then later you provision a different service which makes regular calls to this archival storage and reads data from it. By its nature, long-term archiving means writing data to a nominated location and leaving it there for a long time, only accessing it very infrequently, if at all. The pricing structure for archival storage tends to reflect this. The cloud provider makes storage available at a lower price, which a business will be able to meet over a long time period, but only by assuming that the business will not want to make regular calls to access that data. From the cloud provider's perspective, it's more expensive to maintain a data storage service which is frequently accessed, and therefore accessing data in a long-term storage account is going to be more expensive each time than accessing data in a frequently accessed storage service. You now need to either stop your new service from making regular calls to the archival data, or you need to provision more appropriate storage and move the data there so that you're not being hit with a financial penalty. When architecting a storage solution, it's vital to work out exactly what your needs are. Things which you never worried about with on-premises storage will likely amount in nature of reads and writes, become very important for an IaaS-based solution.

Why Networking Still Matters
When architecting an IaaS-based solution, you need to take networking into account just in the same way as you do when designing something in you on-premises environment. Why do you need to bother with networking in the cloud at all? Well, regardless of where it sits, a virtual machine is still a virtual machine, and in order to communicate with anything other than itself, a VM needs to be attached to a network, which means that it still needs at least one network interface and some form of IP address allocation. Once you've established that a network is necessary, the next step is to decide what that network should look like. If you have to create a new virtual network, it can be tempting to go with whatever the cloud provider suggests by default, but remember that the provider doesn't know what you know, so the defaults may not suite your short-term or long-term needs. Most IaaS-based networks are still based on IPv4 address ranges. This means that you still need to plan out IP address ranges and subnets, and the choices you make depend upon your future plans for this new virtual network. If there is even the smallest chance that you might want to connect this virtual network to your on-premises network, then you should absolutely ensure that the address base of the virtual network does not overlap your on-premises network. Why is this important? Let's look at the following example. You have an on-premises network which uses an IPv4 address range of 172. 16. 0. 0/16. This means that every IP address from 172. 16. 0. 0 to 172. 16. 255. 255 is available for systems in your on-premises environment. That's a lot of IP addresses. You then provision a new virtual network in your public cloud environment to host a couple of new VMs. The environment is only ever going to small, so you decide to provision a much smaller virtual network using 172. 16. 100. 0/24. which means that IP addresses from 172. 16. 100. 0 to 172. 16. 100. 255 are available. You can see that the IP range of the new virtual network actually overlaps with your on-premises environment, but as long as the two networks are physically isolated, that isn't a problem. If you access your new VMs using their assigned public IP addresses, then the routing from the public IP address to the internal IPv4 address is handled by the provider, and you never need to worry about the internal IP addressing structure. In fact, you could have multiple virtual networks, all with exactly the same IP address range, as long as those virtual networks always remain isolated from each other. Later, you need to provision a hybrid cloud environment, which involves connecting your on-premises network to the cloud-based virtual network using a VPN. Now you have a problem because it is quite difficult to route network traffic between two hosts which reside on independent networks, but with overlapping IP ranges. It is possible to do when you control both ends of the connection, but the public cloud side of the connection is controlled by the provider, and in most cases it's not possible to configure that side of the solution in such a way that this scenario can be make to work. So if there is even the slightest chance that you'll want to connect a virtual network to another network in future, whether on-premises or another virtual network in the same provider, or even in a different provider, then endeavor to keep your IP address ranges unique. It shouldn't be too difficult as there are nearly 18 million IP addresses available for private allocation. In this scenario which we just examined, what could you do if you needed to route traffic between the two networks in order to access the VMs deployed in your public cloud environment? Probably the easiest approach would be to provision a new virtual network with an IP range which doesn't overlap with your on-premises network, and use the cloud provider's management tools to either move the VMs to the new network, or decommission and re-deploy them using the same underlying virtual hard disks so that no information is lost. Each public cloud provider offers different mechanisms for moving VMs around, so you won't be stuck for very long. Once the VMs are up and running in the new network, they should then receive new IP addresses, which can be reached from your on-premises network. We've reached the end of this module on Architecting for Infrastructure as a Service. Of course a single module on such a broad topic is always going to be very high level, and can't possibly address every possible scenario which you might encounter, but hopefully these discussions and scenarios have give you a better idea of what to look out for when planning and architecting Infrastructure as a Service solutions. Let's do a recap on what we've covered in this module. We looked at the importance of right-sizing your IaaS VMs in terms of the amount of CPU cores and memory, and some techniques by which you can get a better impression of the resource requirements of any application which you're looking to deploy onto an IaaS platform. We looked at the importance of understanding the different types of storage which might be on offer from a public cloud provider, and why it's important to choose the right storage to meet your business requirements so that you're not stuck with paying too much for the wrong service, or inflicting poor performance on your VMs unnecessarily. Finally, we looked at how networking is just as important in a cloud environment as it is for on-premises, why you need to pay careful attention to IP addressing, and what you can do if you find yourself trying to connect overlapping networks. Now let's move on to the next module in this course on Cloud Architecture Foundations, in which we will expand our understanding of architecting IaaS-based solutions by examining service resilience, or how to make sure that you're deploying robust workloads.

Architecting for Service Resilience
Infrastructure-as-a-service Resilience
Welcome to this module in the Pluralsight course, Cloud Architecture Foundations. Now that we have examined some of the high-level architectural decisions and choices which need to made when building out Infrastructure as a Service solutions, let's take a closer look at how to ensure that these solutions have been built with resilience in mind and why architectural resilience is something we even need to consider for public cloud resources. In particular, in this module we will look at the following aspects of architects in robust services. Resilience, ensuring that your Infrastructure-as-a-Service-based workload stays up no matter what's occurring under the hood. Geo-redundancy, making sure that even a datacenter outage isn't going to ruin your day, and self-healing, how your workloads can actually fix themselves. We're tackling this aspect of public cloud architecture because of a very common misconception around the public cloud services. Especially Infrastructure as a Service, which is that once you deploy a resource like a virtual machine into a public cloud environment, you don't need to worry about it anymore in terms of ensuring that the VM stays online and that the data is protected. This is, unfortunately, not true at all. Recall the image from earlier in the course, which discussed the different areas of responsibility which are shared between the customer and the service provider. The service provider will handle all the physical infrastructure up to and including the virtualization layer, but everything above that is the responsibility of the customer. The implications of this are that if there are any operating system patches to be applied, it's the customer's responsibility to apply them. If there is a problem with any application installed on the system, it's the customer's responsibility to remediate them, and if there's any important data on any of the disks attached to the VM, it's the customer's responsibility to ensure that it is protected appropriately. The provider makes sure that the underlying infrastructure, which supports your VM, is robust and stable, and if there are any issues, like disk failures, that these will not adversely impact your VM. Combined with active maintenance and monitoring allows the provider to offer a service-level agreement for the resource which you have provisioned. Let's say a 99% uptime guarantee. This sounds really good, and it is really, but when you crunch the numbers, a 99% uptime SLA means that you could reasonably expect your VM to offline for a total of 3. 65 days over the course of the year, although not all at once. Why is this? Faults occur and equipment fails, and cloud providers are not immune from this. Because they operate at such large scales, providers have to factor in a certain background rate of equipment and peripheral failure into their business models. The underlying infrastructure is designed to minimize the impact of any hardware faults, but outages cannot be prevented completely. Also, the cloud provider still has to maintain the operating systems which provide the virtualization layer. In the same way that patches and updates need to be applied to the operating system of your IaaS VM, so to for the supporting hosts. If a patch can be applied without needing to reboot the system, then terrific, but otherwise a reboot will be necessary and the VMs running on top will also need to be restarted for a short amount of time. You should therefore anticipate a certain amount of downtime for your IaaS VM. So with this in mind, what can you do about it?

Architecting for High Availability
Fortunately, the principle behind this solution is very much the same as if you were architecting the workload to run on-premises. Let's work through this by looking at a specific example. Imagine that the workload you are architecting consists of a database server which hosts a business-critical database. If you install everything onto one virtual machine, then if anything occurs to any component of the end-to-end solution, then the ultimate impact is that the database, which is the service your business customers actually care about, is no longer available. It's a strange way of looking at the problem, but consider it from the service consumer's point of view. They don't care whether a server has gone down. They don't care whether a hard drive failed. They do not care whether patches need to be applied to the operating system or not. They don't care whether the database server is experiencing a problem. They do care very much, that the database isn't available for them to do their work. So, as solution architects, we need to ensure that our efforts are focused on addressing the right outcome, namely, that we need to ensure that the service, in this case the database, is highly available and resilient. Yes we need to build resilience in multiple levels in order to ensure this, but these tasks are the means, not the end. Too often, those responsible for the technical aspects of a solution focus, not unreasonably, on the technical components, but this causes them to lose sight of the reason that this technology exists in the first place, and why we need to care about it at all. In this scenario, the first thing we do is ensure that we are using a database server platform, which can be architected for some form of high availability across multiple VMs. This can take several forms such as active/passive replication with failover, active/active synchronous replication with failover, or active/active asynchronous replication, again, with failover. The full solution which enables these HA scenarios will differ from product to product, and this is where an in-depth knowledge of the database solution is necessary. In the event of an outage on the nominated primary, assuming that there is such a solution role, there needs to be a system in place which informs the other database servers of the event, and allows another system to assume the role of the active primary, accepting incoming connections. By building multiple VMs, we can configure the VMs from the perspective of the cloud provider as a single service. The provider can then distribute the VMs so that they are separated from each other within the datacenter, so that if there is an outage due to a hardware fault or a host reboot, only one VM within the configured service should be impacted and the others keep on running. This robustness ensures that the database is still available, but this doesn't necessarily help the end users if they are still querying the offline database. Therefore, the method by which they need to communicate with their solution, usually a fully qualified and network domain name, also needs to be aware of the failover events. This is usually achieved by means of a network load balancer, either via a dedicated system in an on-premises environment, or by one or more native services offered by cloud providers. Using health probes, the load balancer can tell when one of the database servers, which it is actively monitoring, has gone offline, and redirects all incoming traffic to a specified alternative. In this way, from the customers point of view, although there has been an outage behind the scenes, and presumably technicians are running around trying to get it fixed, the core service is still available for consumption. This architectural model is the one which must be adhered to when building solutions using Infrastructure as a Service. Given that we know that any single VM running in a public cloud environment must incur some downtime, no matter how brief, to deploy a workload in a single VM is the same as accepting outages. Architect your solutions assuming outage, and you can actively mitigate against it and give your customers the best possible experience.

Understanding Geo-redundancy
The concepts we've looked at so far for making services more resilient only apply within the boundaries of a single public cloud datacenter. So what occurs when that entire datacenter is unavailable? We're not necessarily talking about earthquakes or meteor strikes, although public cloud services are certainly build with those possibilities in mind, but rather more mundane issues like a massive and sustained power outage or a major networking issue. In these situations, it doesn't matter how robust your solutions are if you can't actually access them. So this is where we can take advantage of the fact that in order to provide a robust service to their own customer, public cloud providers don't just have one datacenter, but rather multiple datacenters spread out around the world. Part of the reason for this is pragmatism. Customers in one geo-region would prefer to access a datacenter which is physical nearer to them for lower latency, but another benefit is that you can use the geographically dispersed nature of cloud datacenters to your own advantage. Let's take the example where we have architected a solution for an IaaS-based web application, which achieves a good level of robustness within a single datacenter by using an architectural pattern which consists of multiple IaaS instances, application and content delivered from source control, and network load balancing. Because the services on offer in one datacenter are generally the same as in the others, there are exceptions to this so this is something which needs to be factored in, it is possible to replicate this solution pattern across multiple datacenters. Using an external load balancing and routing service by the provider, we can direct customer traffic to the nearest datacenter to take advantage of low latency, but in the event that a datacenter is offline, for whatever reason, the external load balancer can automatically redirect traffic to another datacenter which is still online. The latency might be higher, but far better for the user experience to be slightly degraded then suffer a complete outage. However, not every workload can take advantage of geo-redundancy. Looking at our earlier example of a highly available database, if the database platform does not support data replication outside a certain tolerance, say four time zones, or it cannot support both synchronous and asynchronous replication, which would be needed to cope with the higher latency between datacenters, or data sovereignty restrictions means that you can't replicate data outside a geopolitical zone, then geo-redundancy isn't necessarily an option. Although some of these problems can be overcome by the fact that most providers maintain multiple datacenters in the same geopolitical region, for example, the U. S. To cater for these scenarios, even though redundancy across geopolitical regions may not be possible.

Elasticity and Self-healing
The scenarios which we have considered up to this point have all been based on fairly traditional VM architecture concepts. However, one of the things which is unique to public cloud environments, which we can take advantage of, is using horizontal elasticity to enable self-healing services. We've already touched upon elasticity in a scale up/scale down scenario. This is vertical elasticity. Horizontal elasticity describes scale-in and scale-out scenarios. Let's look at the example of our geo-redundant web application. We have architected for robustness within a single datacenter, and we have catered for the potential outage of an entire region, but we haven't catered for spikes in demand. Let's say that there is a surge in demand from users in the US. This may lead to the IaaS instances in the U. S. datacenter becoming saturated and the servers becoming unavailable. With geo-redundancy, the external load balancing service will flag that datacenter as offline and will redirect traffic to another one, where the process simple repeats. It's not a great outcome for the business or for customers. Using horizontal elasticity, we can allocate additional IaaS instances to the solution automatically to soak up the additional load, and then reduce the number of instances when demand drops. We only pay for the additional instances for the time that they are in existence. So there's no need to over commit investment in IaaS services. Pay for what you need, when you need it. That's the ultimate goal of cloud computing. In order to be able to provide horizontal elasticity, the IaaS instances need to be provisioned quickly, and to come online ready to work. There's no point in automatically provisioning new instances to meet demand if someone has to log onto each system and manually configure things. So the operating system image from which the VM is deployed, has to be flexible and independent. No messy post-deployment configuration. This means that we can do something rather cool. So let's walk through the logic. We build an operating system image which contains all the application code and configuration necessary to deploy a VM, which is ready to work. We define a scaling rule, which says that we want at least 2 instances running at any time. There's a multi-instance redundancy automatically taken care of. The provider provisions the two instances from this supplied image, then one of the instances goes offline because of a hardware fault, a host reboot, or because of some issue internal to the VM itself. Rather than have to manually fix or redeploy the virtual machine, the cloud provider realizes that the service which has been requested, namely that two instances should always be available, is not currently being met. Offline VM is erased, and a new one is automatically provisioned using the same image. The service remains online, and the users are none the wiser. Leveraging a combination of minimum number of instances and demand-based provisioning, you can architect a robust service which is self-healing in nature and which will scale out to meet spikes in demand. Unfortunately, this scenario isn't going to possible for every workload. It depends upon being able to provide an image which is ready to work. Some applications need complicated post-deployment configuration, which can't be baked into an image, and for these kinds of workloads, we need to leverage more traditional architectural patterns, which we discussed earlier in the module. So we've reached the end of this module on architecting resilient and robust Infrastructure as a Service solutions. Hopefully, you have received an idea of how the massively scaled nature of cloud computing unlocks a variety of options for making your workloads highly available and fault tolerant, but that one of the critical factors in any scenario is understanding the application or workload which you wish to deploy in depth. You can't architect a solution properly if you don't understand the intricacies of what it is you're going to deploy. Let's do a recap on what we've covered in this module. We looked at why outages occur even in public cloud datacenters, and why you need to factor them in. We looked at service resilience, what are some of the options you have for making your solutions more robust, and how to approach the problem from the customer's point of view. We examined making your solutions redundant across multiple datacenters, why you want to do this, what advantages it confers, and what some of the caveats are. And finally, we considered changing the traditional VM-based architectural patterns by replacing it with intelligent automated provisioning and auto-scaling to build a solution which can heal itself in the event of an outage. Now let's move on to the next module in this course on Cloud Architecture Foundations, in which we will leave behind Infrastructure as a Service, and take an in-depth look at architecting Platform as a Service solutions.

Architecting Platform-as-a-service
Approaching Digital Transformation
Welcome to this module in the Pluralsight course, Cloud Architecture Foundations. Up to this point, we have examined deploying public cloud services in a fair amount of depth, but we have concentrated primarily on Infrastructure as a Service. In this module, we're going to change things around by taking a much closer look at Platform as a Service, or PaaS. There are a wide range of PaaS offerings for different public cloud providers, and we're not going to be able to cover them all, or even a significant proportion of them in a single module, So in this module, we're going to focus on digital transformation, which is how to travel the road from IaaS to PaaS, and why it's important for any businesses, web services, which is all about running your web applications on PaaS, database services, or architecting your data for the cloud, and infrastructure development, building and shipping your services just like any other web application. To be perfectly transparent, the architectural considerations when building out a solution using Platform as a Service aren't so very different from IaaS. There are some differences of course, as we will see shortly, but on the whole what is good practice for one, is good practice for the other. What's important to look at is why you might want to use PaaS over IaaS, and what the benefits are. Consider many of the scenarios which we have already discussed. In each case, the solutions, which have been built and deployed, are all endeavoring to meet a certain business requirement by providing a service which users or customers or both are going to consume. We architect our solutions to provide the most resilience across the widest possible range of potential problem scenarios, thus ensuring that the service is as robust as we can possibly make it within the boundaries of affordability and return on investment. Consider again the diagram which shows the breakdown of responsibility between the customer and the cloud provider across the different public cloud offerings. PaaS reduces the amount of responsibility which we as a customer have. Just in the same way as we endeavor to architect IaaS solutions to allow the customer just to focus on the service which they care about, PaaS offers us, as cloud architects, exactly the same opportunity. If we want to provision a database service for our users and customers, why go to the trouble and expense of architecting and deploying and IaaS-based solution if we don't have to? If PaaS solution delivers the same outcome and meets our business requirements, then we should need a fairly compelling reason not to choose that option. It is generally true to say, not always, but generally, that for any given PaaS-based solution on offer, architecting and deploying the same service using IaaS is more complex and invariably more expensive. As we've already discussed earlier in the course, because PaaS solutions are focused on delivering a very specific outcome rather than a broad platform on which you can deploy whatever you'd like, cloud providers are able to make much more efficient use of the underlying resources which support PaaS offerings as opposed to IaaS. Therefore, they represent a lower cost to the cloud provider, and thus a lower cost to the customer, which is us. It is this less expensive, more efficient use of resources which is the true potential of cloud computing, and lies at the heart of digital transformation. Digital transformation is the process by which a business takes its existing VM-centric approach to solution delivery and moves it to a service-based approach, transforming existing applications and workloads to work within a PaaS framework, and thereby simplifying architecture, increasing delivery speeds, and reducing operating costs. Does that sound too good to be true? Well, plenty of businesses are engaging in digital transformation initiatives right now, but as always, the devil is in the details.

Architecting PaaS Web Solutions
PaaS-based web services enable you to provision a dedicated web hosting environment running on a particular operating system, Windows or Linux, with a certain amount of computing resources allocated. Yes, we still have to architect an appropriate amount of CPU and memory resources, but this is the nature of application architecture. Applications needs resources to run, and the cloud providers relies on us to let it know how much to allocate. However, despite using the operating system and resource capacity, we don't need to worry about configuring any of it. All of that is done by the provider. Once provisioned, the cloud provider will run our hosted web servers in a preconfigured environment with a number of configurable runtime options, such as the desired version of the. NET framework, if running on Windows, whether or not PHP should be enabled, and if so, which version, whether the architecture should be 32-bit or 64-bit, what the virtual document should be, and any virtual folders. Selecting any of these options does not require us to have direct access to the underlying operating system, which has been abstracted away from us. We make our architectural choices based purely on what the application needs in order to run. Architecting for redundancy within the datacenter is no longer a consideration, as this is built into the service fabric and geo-redundancy is achieved the same way as with an IaaS-based solution. Once the service is running in one datacenter, deployed to a second or third datacenter using the same deployment code and scripts and using external load balancing service to provide a unified frontend to end users and customers. For applications which are born in the cloud, which simply means that their development lifecycle starts off using the cloud platform services, this is a relatively straight-forward process. Select a PaaS service from a cloud provider, which offers the hosting configuration which your application requires, and deploy. But what about applications that started life on the premises and have been hosted on VMs with custom, and often very specific, configurations? This is where digital transformation starts to get harder. a PaaS-based web hosted environment represents a more restricted configuration than the equivalent environment running on IaaS. This is because when dealing with IaaS, which is a full operating system, you can install what you like, configure what you like, and include any and all dependencies which make your application work. A PaaS solution is more restricted by necessity because the cloud provider needs to be able to standardize the configuration in order to run an efficient service. Hence, the lower operating costs compared to IaaS. However, if your application can only run if a text file with a particular name exists in a particular location, or it requires an obscure dependency which has to be manually installed and configured, then these requirements are going to act as blockers to a PaaS transformation where the needs of the application must be balanced against the restriction of a standardized environment. At this point, you need to perform a cost benefit analysis, which measures the cost to the business of transforming the application to a cloud-ready state, against the benefit to the business of such a transformation. For example, if you have a business-critical web application currently running in an IaaS-based environment, which can be completely transformed to run on PaaS with one month's worth of effort by two developers, and as a result of the transformation, your developers can deploy changes into production much more rapidly, and the business can expand the application quickly into new markets, then the cost benefit clearly comes down on the side of investing in transformation. By comparison, another web application is also running in an IaaS-based environment, but this application is relatively old and is only being maintained to support a couple of legacy systems, all of which are going to be decommissioned in time. Clearly, even though the application might run more efficiently in a PaaS environment, the costs of transformation outweigh the benefits to the company, and it is a more sensible decision to maintain the application in its current state until it can be retired.

Exploring PaaS Database Solutions
The reason that database services are the most common form of PaaS offerings is because of web application services. PaaS-based web applications usually need to talk to a back-end repository of stateful data, and this tends to be a database of one flavor or another. In order to continue taking advantage of the various benefits of running a PaaS-based web application, it generally makes sense to run PaaS database services as well. The process of architecting a database service is, as with the web application, much simpler than designing the same solution on IaaS. Cloud service providers offer a range of database services, which are designed to support specific database types, such as Microsoft SQL, MySQL, or PostgreSQL. Depending on the size and scale of the underlying database service, multiple databases can be supported on the same service, although you can't mix and match database types. For example, you can't provision a single database service and use it to host both Microsoft SQL and MySQL databases. As with web applications, the database service is allocated a certain amount of physical resources depending on your requirements. Some cloud providers base this as if you were specifying a particular IaaS VM size, whereas other providers calculate resource and pricing tiers based on abstracted units of performance. Choosing the right pricing tier is dependent upon anticipated resource demands of your database, and this is where you'll need to consult heavily with a database administrator, unless of course you are the database administrator. Again, there is a different in approach to architecting PaaS database solutions depending on whether the database is being built for the first time in the cloud, or whether it already exists on-premises or in IaaS, in which case it is running on a dedicated database server instance with a specific configuration. This is because in order to provide an efficient service, PaaS database services often represent a subset of the functionality of a full VM-based installation. Some database rely on very specific server software configuration, or stored procedures, and these may not be supported in a more restricted PaaS environment. If the database is already running on-premises and you wish to transform it to PaaS, the same cost benefit analysis must be performed. Compare the cost of any re-architecture against the benefit to the organization. Depending on the source-database type, there are assessment tools which you can run against the database, which will flag some of the issues which might prevent a successful migration, and this can provide your database teams with sufficient information to assess the amount of effort needed to implement a full transformation.

Code-driven Infrastructure Development
As you migrate services to PaaS, or start building new PaaS-based solutions, one of the things which becomes clear is that the amount of configuration required over the equivalent solution built in PaaS, is dramatically reduced. Another thing which will become very apparent is that all cloud providers promote deployment, management, and configuration of their PaaS offerings using automation. Each provider has its own implementation and preferred methods, but all are very consistent in enabling programmatic interaction with its services. The implications of this when working primarily with PaaS is that you are more likely to already working with automated deployment mechanisms, especially with web applications. Developers work on application code, commit that code to a source control management system, and an automated continuous integration continuous deployment, CICD, system picks up those new code changes, runs them through a battery of automated tests and checks, and then finally deploys the code into production. Given that the underlying PaaS services which underpin our web and database applications, can themselves be completely defined as code, as cloud architects, we can start to take advantage of this build, test, deploy framework by defining all of our solutions as code. This has a number of advantages. It establishes a source of truth as to what our PaaS infrastructure should look like at any point in time, it eliminates the need for any kind of manual deployment or configuration, it promotes and enforces correct patterns of behavior in terms of change management, and it lowers the barriers to awareness between the application development and infrastructure operations teams, or DevOps. There are a number of popular open-source automation and deployment platforms out there, like Ansible, Jenkins, Terraform, and Spinnaker. I strongly encourage you to take a look at any or all of these, and start experimenting by using these products to deploy PaaS solutions to your cloud provider of choice. PaaS makes it much easier to treat our infrastructure solutions as code, which in turn encourages us to adopt a developer mindset and gain experience with developer tools. In this sense, PaaS has been one of the biggest disrupters to the traditional definition of an IT professional. IaaS allows us to continue doing what we already do, just in a new ecosystem, whereas PaaS encourages us to think completely differently. Don't be scared. This is exciting times. So, we've reached the end of this module on Architecting Platform as a Service solutions. We ended up discussing more about the implications of PaaS, and the way in which it is transforming our industry rather than specific scenarios because as we have seen, if you have a good understanding of IaaS solution architecture, then PaaS is much simpler, as much of the decisions have been abstracted away from you, and you end up focusing much more on the nature of the services you want rather than defining and deploying the infrastructure to support it. So, let's do a recap on what we've covered in this module. We looked at the nature of digital transformation, why a business might wish to look at deploying solutions on PaaS rather than IaaS. We looked at PaaS-based web applications, what we have to focus on when it comes to configuring them, and how to approach a transformation of an on-premises web application to PaaS. We considered database services and the importance of aligning your architectural tasks with input from your database administration team. And finally, we discussed the impacts of PaaS in terms of the promotion of code-driven infrastructure, and how approaching our solutions as code is one of the driving forces behind DevOps principles and the evolution of the IT professional. Now let's move on to the next module in this course on Cloud Architecture Foundations, in which we will delve into the world of microservice infrastructure by looking at how we plan for and architect containers.

Containers and Microservices
Docker Container Architecture
Welcome to this module in the Pluralsight course, Cloud Architecture Foundations. This module is all about how the impact of virtualization has enabled us to build applications in lightweight containers, and how this has allowed us to build complex, dynamic microservices in public cloud environments. The specific topic which we will focus on in this module are, Docker container architecture, public cloud container services, and container orchestration. Earlier in the course we looked at the impacts of virtualization on datacenters and computing in general. One of the outcomes of the evolution of virtualization is the virtualization of the operating system kernel, which enables us to run lightweight applications within their own isolated operating environments. Because each container is designed to be pausable and have a very minimal footprint, a container can be provisioned in only a couple of seconds as opposed to a virtual machines, which can take considerable longer. This makes containers extremely attractive as an application-hosting platform because you can make extremely efficient use of the underlying operating system, compute, and memory, and can rapidly provision services, allowing you to easily scale on demand. Using orchestration, you can create complex deployments of different containers working together and self-healing in the event of the problem. Containers are currently the ultimate throw-away commodity resource in enterprise and cloud computing. Because containers run on an abstraction of the operating system via a container engine, and because the container engine doesn't care whether the underlying operating system is running on a physical system or a virtual machine, containers can be crammed into clusters of virtual machines, thus making extremely efficient use of the underlying physical hardware, which makes them extremely attractive to public cloud providers. Although there are different ways of creating and running container images, the industry standard is Docker. Because Docker is ubiquitous with container image technology, all major platforms which offer a container engine service, support Docker-built images and orchestration platforms which make use of the Docker engine. Docker images are built using the Docker client, either on a local workstation using a hosted service, or as part of an automated CICD pipeline. The client is extremely lightweight, and works cross-platform. Images are defined in a Docker file, which describes all the settings and applications and services which need to be housed within the resulting container. Because the Docker file is static code, it can be, and really it should be, housed in a source-control repository, like GitHub, where it can be managed and version controlled. The resulting Docker container is then stored in a container repository. Docker has its own repository, Docker Hub, but public cloud providers also offer container repository services if you prefer using a private container within your cloud provider's environment. During a deployment, a call is made to one or more Docker engines to provision a new container based upon a particular container image and version tag. For example, Ubuntu:latest. If that version of the image is already available locally to the Docker engine, it simply uses the local copy. Otherwise, it pulls a copy from a nominated container registry. Because the images are incredibly lightweight, the new container is usually up and running within a second or two. Containers are designed to be run in a completely headless manner, meaning no requirement to log onto each container manually and interact with it in any way. Therefore, containers don't suite all workloads. For example, if your application requires a full UI to be available and monolithic management tools to be installed, then it's unlikely that the application can run as a container. However, if your application can run as a container, then it's well worth considering. Because containers used shared binaries and libraries from the underlying operating system, they don't need to be actively managed or patched. That's right, no patching. And if there's a problem with a container, you don't need to troubleshoot it. Just erase it and provision a new one. Containers are small, single-instance entities which aren't designed to be long lived, so you can spend more time on delivering a robust supporting environment and process automation. Additionally, the container is a self-contained entity, which means that it offers the same functionality running on a local development workstation, as it does in an enterprise-hosted environment, which reduces, or even eliminates the traditional developer problem of, but it worked on my workstation.

Public Cloud Container Services
If you're sold on the idea of using containers to deliver application services, the next thing to decide upon is how you're going to deliver a Docker engine on which to run your containers. You can of course build your own in your on-premises datacenter, but then you become responsible for the deployment, maintenance, and monitoring of the whole environment. In the same way that public cloud providers offer infrastructure, platform, and software services, They also offer container services. Instead of building your own, you simply provision a service, and start deploying containers. If you want to get up and running quickly with containers in a public cloud environment, most providers offer quick-start services which let's you start deploying containers within a few minutes. However, if you were going to need more complex infrastructure for your container, then it's worth taking some time to consider the various architectural considerations. Nothing is for free, and containers still require compute, memory, and storage resources to run, which means that virtual machines will have to be provisioned in order to support the overall solution. Additionally, depending on the container orchestration platform used, more on orchestration shortly, there may be additional VMs required. The cloud provider will usually handle the creating and configuration of all these VMs that you will need to provide details about the sizes which are appropriate, and keep in mind that this has a direct impact on running costs. As a general rule, only provision the minimal number of VMs you need to create a robust solution, but ensure that you adhere to the principles of infrastructure development, which we discussed earlier in the course, as this will allow you to easily make changes at a later stage. By default, containers and container services operate in an isolated network environment, only able to communicate with the outside world via firewall rules and load balances, which are controlled by the cloud provider and orchestration platform. This precludes them from communicating directly with other virtual networks, or your on-premises network using private channels. While it is possible to change the size and number of supporting VMs, it can be quite challenging to change network integration post deployment, especially if you didn't take IP addressing into consideration. Therefore, when planning to run public cloud container services, try and factor in future communication requirements at the outset.

Container Orchestration with Kubernetes
Now that you've got some container images and a place to run them, overall management becomes very important. After all, while containers are easy to provision, you don't want to have to deploy them one at a time. This is where container orchestration comes in. The analogy of an orchestra conductor is an accurate one. Imagine that the containers are the individual performers organized into discrete groups, or services, which provide a particular function. The violin section can be the frontend web tier, the rest of the strings the middle tier, the wind section can be the application cache, and the percussion section is the database tier. Leave them to their own devices, and result is chaos, but place them under active orchestration, and they coalesce into a coherent whole. There are a few container orchestration platforms available including Docker Swarm and DCOS. However, one of the most popular and widely supported is Kubernetes, so this is the platform we will focus on. At a very high level Kubernetes using a master, which consists of several components, which can all be located on a single machine, or spread across multiple clustered systems for high availability. The master is the conductor of our orchestrated solution. The master doesn't actually do any of the service hosting itself, but rather it receives instructions about what services are supposed to be running within the environment, and how those services are configured. The master also has the ability to communicate directly with the underlying cloud provider platform so that it can provision resources as necessary, such as externally-facing load balancers, and public IP addresses. The master then communicates with one or more worker nodes. These are single-instance VMs which actually run the container images. The master is aware at all times of how many nodes there are, and what their capabilities are, and depending on the underlying cloud platform, the master is able to automatically provision more nodes if more computing resources are needed. As an example, if the conductor of an orchestra was handed a musical score which required 30 musicians, but only 6 were available, the conductor would start rounding up more musicians to fulfill those specific requirements. Each worker node is configured with a basic service to enable bi-directional communications with the master and the outside world if needed. A service is then deployed to the master. A service consists of one or more pods, which are self-contained constructs, within which lie the container images. The master distributes the service to the worker nodes and orchestrates the deployment of pods across nodes, depending on the amount of available compute and memory. If necessary, the master can communicate directly with the underlying cloud provider platform to provision more VMs to act as nodes. From our perspective as cloud architects, container orchestration means that we don't need to worry about how our services are deployed or how they communicate with each other within the container service environment. As long as we have architected or configured this service from the outset with the correct initial VM sizing and network integration, we can focus entirely on the services, which services should be running, which services should be able to communicate with each other, and how best to deploy changes into production. Like a composer of music, we hand the finished work to our conductor, and leave it up to them to bring the orchestra together and produce what we have asked for. We focus on the desired end state and leave the mechanics to others. We've reached the end of this module on architecting container-driven microservices using public cloud providers, so let's do a recap on what we've covered in this module. We've looked at containers, what they are and what makes them so attractive to developers, architects, and businesses. We've examined Docker container architecture, what the underlying structure of a Docker image is, and how it runs on top of the Docker engine. We've then examined public cloud container services, what you need to consider when making use of public cloud provider services, to run your containers. And finally, we looked at container orchestration, why orchestration is important, and specifically, how Kubernetes architects the end-to-end solution. Now let's move on to the next module in this course on Cloud Architecture Foundations, in which we will move on from infrastructure-driven services, and look into the world of cloud-based identity.

Identity
Account Protection Strategies
Welcome to this module in the Pluralsight course, Cloud Architecture Foundations. This module is about the importance of architecting identity considerations into our public cloud solutions. In particular, we will focus on the importance and mechanisms of account protection, how to define a single source of identity, and making use of identity federation, and protecting our cloud-based assets using role-based access control. The fact that you have to protect your online accounts should go without saying, although it is a little scary sometimes at how few people actually do it. The consequences can be bad enough when the account in question is tied to your social medial accounts. But when the account is an administrator of your public cloud environment within which your production workloads are running, well, the consequences have business, financial, and possibly even criminal implications. Why is this? Let's say that the administrative account which was used to provision your public cloud environment is compromised. By default, this administrator account has access to absolutely everything within that environment. So someone who controls this account can access any resource or service deployed within the environment, and can block access to other accounts, preventing attempts to rectify the situation. VMs can be accessed, and data can be siphoned off. So, it might be a good idea to protect those accounts, don't you think? To be fair, account protection is important regardless of what the account is used for, it's just that the implications of a compromised cloud administrator account are so much more serious. However, here are a few tips for protecting any account with significant levels of access to anything really. The first one is to choose a good password, I mean, obviously. The use of a centralized password manager is quite common, but then you have to protect access to the password manager too. So selecting strong passwords is always an important consideration. If you need to remember the password, then this comic on XKCD explains some really good techniques for choosing a password which is both memorable and a lot stronger than you would think. As you can see in the image, plugging the two different passwords, which were mentioned in the XKCD comic into a password manager, shows that the more memorable password is indeed stronger, despite the lack of uppercase, special, or numeric characters. Secondly, multifactor authentication cannot be overestimated. A strong password, which is difficult to brute force, is very important of course, but it's never a good idea to base your defensive strategy on a single layer. So make sure that your cloud provider of choice supports multifactor authentication on all accounts, not just the important ones, and then make sure you actually implement it. Depending upon the capabilities of the identity provider, you should also look at implementing conditional access rules and fraud detection. Conditional access rules allow the identity provider to determine whether to challenge the authentication attempt against the accounts with stricter controls. For example, if the authentication attempt comes from an IP range which has been flagged as safe, then only one additional MFA challenge will be issues in addition to the password challenge. But if the IP range has not been flagged safe, then additional challenges can be issued, or the authentication attempt can be blocked entirely. By comparison, fraud detection looks for patterns of behavior which are suspicious in nature, like an authentication attempt coming from the U. S. and then another one coming from Europe 5 minutes later. These layers of protection don't offer a cast-iron guarantee that your admin account will never be breached. No system can guarantee 100% protection, but by applying multiple layers of defense, the likelihood of a breach becomes significantly less. However, the next thing to consider is how to limit the damage which these accounts can do in the event of a breach, because it is always important to assume that there will be a breach at some point, if it's not already occurred, and plans need to be in place to reduce the impact. When you provision a cloud environment, there is usually a one-to-one mapping between each environment and the admin account which was used to create it. As most partners require an initial account to create and access each environment, if you place all of your workloads into one environment, then if you assume that the admin account used to create the environment has, or potentially could be breached, then all of your cloud services are at risk. Another approach is to distribute your workloads and services across multiple environments, as long as you use different accounts for each environment, then if one account is breached, the damaged, or the blast radius, is significantly reduced. While this approach is a bit more cumbersome and require management of multiple public cloud environments, it does not impede the functionality of any workloads or services you may wish to deploy, and it offers a reasonably balance between security and convenience.

Sources of Identity
In addition to planning for account protection and breach mitigation, it's important to architect how the identities in your cloud environments will be managed. Most cloud providers don't assume that you have your own pre-existing identity platform, such as an on-premises identity system, which they should plug into. So they all offer an identity service, which, at the very minimum, meets the needs for managing any environments which you have provisioned with them. This approach is perfectly fine if you don't have your own source of identity truth, but what about it you do? If you have your own on-premises or IaaS-based identity platform, which you use to create and manage users, groups, permissions, and passwords throughout their lifecycle, then making use of a second completely separate identity platform becomes something of a management problem. Ideally, we want to avoid a situation where users have to manage and maintain separate accounts and passwords for different environments, or to be more specific, there will have to be multiple accounts, but we want to maintain a single source of truth for identity verification and authentication. This is where identity federation comes into our architectural plans. Most cloud providers will support a form of identity federation, such as Security Assertion Markup Language, SAML, or OpenID Connect, which enables the cloud provider to query a nominated identity source for account verification and authentication. Depending on the platform, the identity federation can also handle account creation requests on the cloud provider side. Let's look at an example, you have a pre-existing on-premises identity platform, which has been federated with your cloud provider. A new admin has been assigned permissions within the on-premises system, and the federation rules state that any account which has these permissions should be assigned to one or more roles in the cloud environment. However, at this stage there is no account assigned to the administrator in the cloud provider's identity platform. The administrator attempts to authenticate against the cloud provider, and because of the identity federation rules, the cloud provider redirects the authentication attempt back to the on-premises system. In turn, this system confirms that the new administrator is who they say they are, that they have the permissions required by the federation rules, that the password is correct. This authorization is sent back to the cloud provider, which then provisions a new account for the administrator and assigns the specific access roles and allows access. The next time the administrator goes to authenticate against the cloud provider, the provider knows that there is an account associated with this user account, but the authentication attempt will still be passed back to the on-premises identity system to ensure that the password is good. The cloud provider doesn't care about what the password actually is, only that the on-premises system is happy with it. This ensures that the administrator can manage their password according to on-premises password policies without needing to mend the password for their cloud account separately. This architectural pattern does assist greatly in the provisioning and management of passwords and access to public cloud environments, but it does place a larger onus of responsibility on the on-premises system. After all, if the cloud provider is going to redirect every authentication attempt back to the on-premises system, then that system has to be available, accessible, and in a healthy state. If you have a business need to architect on-premises federation in your environment, then make sure your identity platform supports high availability, it should anyway, and that any supporting federation services do the same. As a side conversation, many consumer applications and services are taking advantage of identity federation by using social media as a source of identity and authentication truth. This has many advantages for the service provider, as it enables them to implement identity-driven access and authorization without needing to build and support their own identity platform, which may be completely outside their core competency and business model. Rather than authenticating the user against their own platform, or an on-premises private and federated system, the service provider redirects the authentication attempt against a well-known social media platform. This is fine for services which are targeted for individual consumers, but should not be considered for enterprise systems like public cloud environments. With the enterprise services and workloads, the identity provider needs to be completely controlled either by the customer or by the cloud provider with whom the business has a commercial arrangement. Social media-based identity and authentication is certainly convenient and has its place, but in the context of enterprise cloud computing, the businesses are fixedly placing security in the hands of a third party and leaving the enforcement of any security policy up to each individual to perform. Needless to say, this is a situation to be avoided.

Role-based Access Control
Now that you have multiple layers of account protection, breach mitigation, and sources of identity and authentication sorted out, it's time to make sure that any accounts which can access your cloud environment is only able to perform the functions for which the person behind the account is responsible. This is role-based access control, or RBAC, which is a mechanism which allows you to adhere to the principles of JEA, or Just Enough Administration. For example, there's not much point putting in place all of the protective mechanisms which we have already discussed, if every account which is authorized to access your cloud environment, is automatically assigned full administrative privileges. Apart from dramatically increasing the risk of a potential breach, because now potentially dozens of accounts are attractive targets, not just one or two, it also massively increases the risk of an inexperience administrator doing something that they shouldn't through simple misadventure. Personally, I tend to try and thing well enough of people that given too much administrative privilege, they wouldn't act in a malicious manner, but the danger represented by a junior technician with full admin rights is terrify. And then of course there are the people who will act maliciously. Not great. So in the example of our junior administrator, the permissions assigned to them should translate into rights assigned within the cloud environment, which gives them sufficient rights to do the tasks for which they are responsible, and no more. There is an argument to be made for giving people read access across a wider section of the cloud environment so that they can see what else has been deployed in order to get a better picture of the overall environment, and to learn more about the platform, but not be able to change anything. Personally, I think this is a good idea. We are not using our back to try and silo people, but rather to limit the accidental damage they can do by giving them permissions beyond the agreement. So when considering role access, spend some time learning what kind of role-based controls your cloud provider offers, because each has a different approach to security and permissions, but whatever mechanism they use, make sure you architect permissions to roles and implement RBAC. So we've reached the end of this module on architecting security and permissions into our cloud environment. Let's do a recap on what we've covered. We have looked at the importance of account protection and some of the mechanisms by which you can implement a multi-layered defense against account breach. We also looked at using multiple accounts and multiple cloud environments to limit the damage caused by an account breach, which we must assume can and will occur. We examined the importance of using identity federation with our cloud provider to centrally control account creation, permissions, and authentication attempts. And finally, we considered the importance of role-based access control for protecting our assets and our administrators from themselves and from each other. Now let's move onto the next module in this course on Cloud Architecture Foundations, in which we will look at how we can maintain control, visibility, and security of our cloud environment as it expands rapidly.

Security and Management
Configuration Management
Welcome to this module in the Pluralsight course, Cloud Architecture Foundations. In this module, we're going to look at some of the mechanisms through which you can manage your public cloud environment effectively, as well as make use of native functionality to ensure that you are adhering to the strongest possible levels of security. In particular, this module will focus on configuration management, configuration and security compliance, encryption, log collection and analytics, and alerting and ChatOps. We have already discussed some aspects of cloud security in this course, specifically around identity-based protection mechanisms. protecting the accounts themselves, and using identity as a source of truth to protect your cloud environment and assets. This module is going to focus on the mechanisms by which you can protect cloud assets themselves, and gain much greater visibility over what's happening throughout your cloud environment, and how you can build confidence that the policies and protections which you put in place are actively strengthening your cloud infrastructure. The reason that these topics are so critical is due to a topic which we keep coming back to time and time again, namely, that public cloud providers are set up to streamline the consumption resources, and in order to do that, they make a number of assumptions about the configuration of any services which you may request. This includes configuration choices around security, diagnostics, and configuration management. It's not that the providers are tricking you into deploying insecure resources, it's simply the compromise between a streamlined deployment or high levels of security. Our responsibility, as cloud architects, is to know what the options are, and how best to implement them. The first mechanism we'll look at is configuration management. This is where you deploy resources and services according to a predetermined architectural pattern, which you have already verified and approved. Configuration management then enables you to update the architectural patterns centrally and have the updates rolled out across your cloud environments so that you can be confident that all of your resources and services are configured according to the most up-to-date architectural decisions. Let's explore this by means of an example. Let's say that you have deployed a number of virtual networks throughout your public cloud environments using the service template technology which your public cloud provider uses for automated deployments. The templates contain all of the information about how a virtual network is to be configured. For example, with network names, IP address ranges, and subnet details. The templates are parameterized so you can simply pass in some dynamic arrays for the different networks you wish to deploy, and the cloud provider's deployment APIs merge the data and handle the deployment. You use templates to deploy a 1. 0 version of your approved network configuration, making sure that the templates, and any associated scripts, are all stored in version control, then at a later stage, there is a requirement to add another subnet to all existing virtual networks in order to accommodate a new type of workload. Because you have invested in a programmatic deployment, you can update the templates to include the configuration details for the new subnet, update the deployment array properties accordingly, and increment the configuration version to 1. 1, perform another deployment, and the existing services are not deleted and recreated, which would be pretty bad for a virtual network which is already in use, but are rather simply updated to reflect the new configuration. Finally, using basic programmatic queries, you can easily determine which version of the template configuration your resources are using. Configuration management isn't simply a single tool or a piece of technology, it's far more of an operational approach to deploying and managing cloud infrastructure, and it's the cloud providers which make it possible. The previous example was focused on the provider's services specifically, but what if you are running multiple IaaS instances? Recall from previous modules that you, as the customer, are responsible for any configuration of the VM operating system. So what are your options, and why should you even care? Operating system and application-level configuration management is just as important as resource configuration because it's very easy for the configuration of an operating system, or application, to drift over time away from its original specification. This may not sound like a major problem, but given that the software configuration of any VM also incorporates things like the firewall, local user accounts, and security settings. It quickly becomes clear that configuration drift is something best to avoid. As the configuration of the operating system and applications is our responsibility, we need to ensure that we have a configuration management system in place. There are a wide range of products on the market, such as Chef, Puppet, Ansible, and SaltStack. Each has its own methods of implementing configuration management, and the choice of which to invest in will be dependent of business-specific considerations, such as which operating systems you need to support, and whether you prefer an agent-less system. Regardless of the products you decide upon, any good configuration management system will give you the ability to centrally store and define configuration templates for your entire environment. This allows you to control, to a very fine degree, how each VM in your cloud environment, or your on-premises environment too, if relevant, is configured, and whether or not the VM is drifting away from that approved configuration baseline. It also reduces the need for VM and application administrators to log on locally, and in fact it actively discourages it. Configuration management for both native cloud resources and IaaS operating systems and applications, all rely to a very great extent, on the principles of infrastructure development, which we discussed earlier in the course. Source control, automation, and CICD are critical tools in the effective management of your cloud-based assets.

Configuration and Security Compliance
Now that you have implemented an effective configuration management system throughout your cloud environment, how do you know whether or not the configurations which you have templated are actually being adhered to, or for that matter, how do you know whether your resources are actually secure? Depending upon the services offered by your cloud provider, you may be able to make use of a SaaS offering, which can document and track the resource configuration of your environment, allowing you to see visually how the environment is changing over time, and also flag any potential issues. Some cloud providers also offer configuration management recommendations, which will enable you pick up any obvious security gaps which can be quickly filled, such as VMs exposed to the internet via a common management port, or the lack of an active resource-level firewall. However, is it worth looking to implement such services if you have gone to the trouble of standardizing the configuration of your cloud resources? The answer is yes, for exactly the same reason that a business's financial accounts need to be checked by an external auditor and not the accountant who maintains them. Your company can have the best, most capable accountant known to finance, but you always bring in a third-party auditor, and this is the same principle we adhere to when considering configuration and security compliance for our cloud resources. We implement a dedicated configuration management platform to control our environment, but then use a different platform to verify that the results are as expected. Again, there are a number of tools on the market which can provide you with independent audits of your cloud environment, but one of the most comprehensive is an open-source tool called InSpec. You can embed this into an existing CICD framework and use it to write tests against a range of cloud provider services, as well as most major operating systems. If you're not sure what tests you should write for your IaaS instances operating systems, a great place to start are the benchmarks which are published and maintained by the Center for Internet Security, CIS. These describe in depth what settings should be configured to ensure that your operating systems are hardened to very specific predefined standards, and also provide a very comprehensive base from which to start writing your own custom tests. Automated compliance is, in many ways, the other half of the configuration management problem. You really need both solutions working well in order to have confidence in the configuration and security integrity of your cloud environment. When dealing with public cloud services, we tend to assume, correctly, that all data is encrypted in transit. After all, whenever we interact with a provider's services, we are always using HTTPS so the traffic is secure, and the same is true of internal traffic within the providers datacenters. But what about data at rest? Interestingly, most of the resources and services which we have looked at throughout the course, like VMs and VM disks, storage accounts and database services, all have the capability to be encrypted such that the data is encrypted at rest, but this options isn't necessarily automatically enabled. Why would this be the case? Cloud providers are able to offer encryption as a SaaS offering, making use of infrastructure internal to their own environments, like public key infrastructure, PKI, certificate authorities, and hardware security modules, HSMs, to securely store secrets and encryption certificates. However, these are separate services in their own right, and cloud providers don't automatically force you to subscribe to another service, just so that you can use the service which you were originally interested in. But now that you know data encryption at rest is available, leveraging the providers own services, it's absolutely something which you should consider. given the addition cost and configuration, it may not be worthwhile encrypting absolutely everything in your cloud environment. For example, if you wish to provision a test database, which will be relatively short lived, and will never hold any commercially sensitive data or data which could be used to identify individuals, then going through the process of encrypting it, it seems like overkill. The same is true for a storage account which is used to house some generic images, which are accessed by a web application service. Sure, encrypt it if you like, but it's not really necessary. However, encrypting virtual machines should be considered a priority because this acts as a significant barrier to data leakage in the event of an administrative account breach. If you don't encrypt your VMs and one of your admin accounts is compromised, one of the easiest ways in which an attacker can siphon off data is to simply make a copy of the virtual disks attached to your cloud VMs. They don't even need to launch an attack against the VM itself, just copy the disks somewhere else and crack them open to read what's inside. Encrypting the disks defeats this attack by ensuring that outside the context of the cloud environment, the disks cannot be accessed. The same is true of database encryption. By encrypting the database itself, you ensure that in the event of an account breach, the data might be copied outside your environment, but doing so renders it useless. Storage accounts can also be encrypted so that data is protected at rest, and depending on what you're planning on storing, this can be a very good idea. Storage which is used for long-term archiving should absolutely be encrypted, as should storage services which are used to house events or diagnostic logs, as these often contain information which would be extremely valuable to any attacker wishing to find out more about your cloud environment. So although there's not need to get completely paranoid and encrypt absolutely everything, given that cloud providers do offer robust encryption services, it's absolutely worth looking at what you are planning to deploy into the cloud, and protect the data appropriately.

Managing Encryption
We have already discussed the importance of collecting diagnostic performance metrics on IaaS-based VMs, but it's possible, and recommended, to go much deeper with diagnostic logging across your entire cloud environment. As with encryption, diagnostic and event logging is supported by all cloud providers on just about every resource and service you can possibly deploy, but it is rarely enabled by default. Again, this is because of the pragmatic balance between usability and management overhead, and also costs. Enabling diagnostic logging on all of your services can generate a lot of stored data over time, which you will be paying for, so cloud providers aren't going to automatically enable this scenario and have customers get stung for services they didn't know they were consuming. However, now you know, so you should definitely start enabling diagnostic logging on pretty much every service which supports it. Why do we care so much about logging? On the assumption that your cloud environment is going to be a long-lived entity, you absolutely want as much insight into what's happening across every resource and service which you're paying for. Logs not only provide rich data into the usage patterns and performance of your services, but they also enable you to audit security events, like authentication attempts, and internally event logs from the operating systems of IaaS VMs. This is invaluable data which can keep you on top of what's going on. The trick is how best to ingest and make sense of all that data. You're certainly not going to read through each and every log entry, or if you do you have way too much time on your hands. So we need a way of making sense of all of that data. There are plenty of products which provide a detailed log analysis and many of them can integrate directly with public cloud storage services. So, enable diagnostic logging in your environment, target one or more storage services to hold all this data, and don't forget to encrypt the data, and then configure an analysis service to ingest and process the data for you. Depending on the capabilities of your cloud provider, there may even be a SaaS offering which you can leverage instead of needing to invest in a third-party product. One of the advantages of using the cloud provider, if this is an option, is that major cloud providers also offer machine learning services, more on the shortly, and they are able to leverage this same service to perform complex data analysis, pattern recognition, and insights reporting. No matter the size of your cloud environment and the number of services you have configured, make sure that you embed diagnostics logging and analysis into your architectural patterns from the outset so that you are always confident that you're not missing anything and won't be caught by surprise. In an ideal world, the only kind of alert that you would want to receive is that there was a problem with a VM or cloud service, but that's it's been automatically fixed and you can get on with your day. This is possible of course, especially when dealing with automated configuration managements and compliance remediation. But sometimes things occur which can't be instantly fixed and need your attention. The question is, how do you want to be notified? The traditional way of receiving an alert in the event of a problem is an automated email, but this has proven over time to be less and less effective, partly because most of us have email overload and alert emails either get lost in the maelstrom of everyday correspondence, or because we simply don't read the emails. It happens and you've probably been there yourself. I certainly have. The day that you generate an email rule to automatically place alert emails into a dedicated folder, is the day that you've given up on ever actioning alert emails. This behavior is understandable. Alerts can seem very impersonal and the more you receive, the less important they seem, even though the reverse is true. Some businesses tie their alerting systems into support platforms, raising support tickets every time an automated alert is received. This certainly forces action on the alert, or at least it should do, and it's a good example of connecting a technical process with a business process. It's important to remember that while we work with machines, we are not machines ourselves. However, there is another option which has being driven by the evolution of infrastructure development. which we have already discussed, and serverless computing, more on that shortly. At a high level, cloud service providers offer the ability for you to create your own event-driven processes powered by the providers infrastructure. Let's look at an example. You and your team communicate internally using an online collaboration and messaging system. This system allows the creation of micro applications called bots, which can be triggered by a WebHook. These bots are capable of two-way communication. They can be triggered to talk directly to you, and you can talk back to them. In you cloud environment, the log analytics platform picks up on the fact that one of your database services has been running at maximum memory utilization for a predefined threshold, say 5 minutes. The platform raises an alert which triggers a call to the chatbots WebHook, and in your group chat window, the bot pops up to let you and the team know what's going on. You reply to the chatbot, which triggers it to submit a query back to the cloud provider platform to get some additional information about the database in question, and returns the data back to the chat window. This information shows you that the database has been incorrectly configured, scaling rules which were supposed to be triggered when there was a need for additional resources, have not been applied. and it turns out that this particular database was not deployed using the approved architectural template. You are able to get the information you need quickly and easily, all from within a communications platform geared towards human interaction. There is a loss of automation and sophisticated logic which underpins a solution like this, but the result is simple and elegant. As we deal with public cloud environments, which are continually increasing in scale and complexity, the requirement to be able to interact with these systems in ever simpler and more efficient ways will be increasingly more important. So we've reached the end of this module on achieving effective security, compliance, and management of our public cloud environment. Let's do a recap on what we've covered. We examined configuration management, why it is important and how you can use a combination of native cloud services and dedicated software platforms to configure your entire cloud environment from a central point. We looked at configuration and security compliance, specifically, why it's important to invest in, so that you can have confidence in your environment. We assessed encryption services looking at why they are not enabled by default, and how you should approach selectively enabling it on certain key cloud services. We delved into log collection and analytics, assessing why they are so important in order to gain complete insight into what's occurring throughout your cloud environment. And finally, we looked at altering and how leveraging cloud services allows us to make automated alerting interact with us in a way that is more useful and usable. Now let's move on to the next module in this course on Cloud Architecture Foundations, in which we will look at some of the more advanced services on offer in public cloud, including serverless computing and machine learning.

Advanced Cloud Services
Server-less Computing
Welcome to this module in the Pluralsight course, Cloud Architecture Foundations. In this module, we're going to look at some of the public cloud services which you can make use of, which definitely fall into the category of advanced services. There are plenty to choose from, but among the most popular, and the ones which we will focus on are, serverless computing, machine learning and big data, cognitive services, and Internet of Things. These services have either been traditionally only available to industries or institutions with access to significant amounts of dedicated computing power, like universities, the military, or governments, or they have only recently come into existence as a result of the continued scale and evolution of cloud services. We consider these to be advanced services because they are simply beyond the scope of most businesses to even consider attempting to produce their own equivalents. The size and scale at which public cloud providers operate, mean that they are able to invest in the development of these services both for their own use, for example, intelligent log analytics services, which leverage machine learning, and to make services available to customers like us. This means that we can easily consume services which would almost never be feasible to try and provision ourselves. We have already touched upon the concepts of serverless computing earlier in the course. The name is something of a misnomer because, as we have discussed in great depth already, there is no such thing as serverless computing. Somewhere, somehow, clusters of servers are busily whirring away to provide the services which we now refer to as serverless. What the name really describes is the extent to which the service consumer has been abstracted away from the underlying infrastructure. Serverless computing services allow you to run small chunks of code directly on the cloud provider's infrastructure. You don't need to request a certain amount of computing resources, or provision a VM. You don't need to buy an application service or request any kind of dedicated infrastructure. You simply write your code, store it on the cloud provider's platform, and when you run the code, you pay for however long it takes to run to completion. If your code finishes in 500 ms, then that's what you pay. Serverless code can be triggered by and associated WebHook, which is an HTTP endpoint, which can be used by any other application which is also able to use WebHooks. We discussed one example whereby a log analytics platform raised an alert, which triggered a chatbot via its WebHook, but the analytics platform could have also communicated to a serverless function stored on your cloud provider. That function could do anything, launch a new CICD pipeline to deploy more resources, launch a custom in-depth analysis of your cloud environment using a different analysis platform. These services are only really limited by the capabilities of the platforms and services you wish to communicate with, and your own imagination. In many ways, serverless computing acts as the glue which binds public cloud services together. As long as a service or platform can communicate via an API or WebHook, it's possible to use serverless computing to drive automation across platforms which have no native integration with each other. There are a couple of caveats with using serverless computing, although they are not major. The first is that serverless functions are code. That may seem obvious, but it means that there is no abstraction between you as the service consumer, and the code itself. If you want to make use of serverless computing, you will need to write some code, although most cloud providers support a range of languages, which you can choose from, so chances are good that you'll be able to find one with which you are most comfortable. Also, because you are built based upon runtime, it's very important to make sure that your functions are as streamlined and as efficient as possible. For example, if your function is written in PowerShell, don't waste time loading lots of additional PowerShell modules to make use of their embedded command set. If you're doing that, chances are that serverless computing isn't the right platform for what you're trying to achieve, or you need to learn how to code a bit better. And finally, some businesses do get carried away with the sophistication of serverless computing, and try running major applications completely as code-based microservices. You can certainly do that, and it's an impressive feat, but if your code is being run constantly due to the demand which your application environment needs to handle, you may quickly discover that it would've actually been more cost effective to provision a dedicated PaaS web application instead. So serverless computing is fantastic and has many, many applications, but just make sure that you go into it with your eyes open.

Machine Learning
We have already encountered machine learning in the context of log analytics. If we have been generating diagnostics and event logs for a significant period of time, there is simply too much data for a single person, or even a team of people to ingest and interpret in any meaningful manner. Machine learning is ultimately all about a very efficient pattern recognition, and allows you to ingest huge amounts of both structured and unstructured data, build algorithms which will query that data effectively and enable you to detect patterns and outcomes. Let's take an example. You have terabytes of customer data stretching back over 15 years of retail operations. You have a good idea about the sorts of things your customers like based upon the patterns of purchasing behavior, but the effectiveness of your marketing campaigns is very hit and miss. Some campaigns do extremely well, whereas others fail to make any impact. Using machine learning, you analyze your historical data against public government datasets, and query them to produce reports which demonstrate patterns of buying behavior linked to factors which you never thought to consider before, such as the relationship between the decision to buy, and short-term weather patterns, as opposed to longer-term seasonal weather patterns. This allows you to change your marketing strategy appropriately, understanding in much greater depth why the marketing campaigns which worked, worked the way in which they did, and why those that didn't, didn't. This kind of rich data is buried away in databases in most businesses, and until recently, there hasn't been an easy way to make use of it. While machine learning is very much geared towards data scientists and researchers, it is something which can be consumed by any business with an interest in business intelligence and data analytics, but this represents a major caveat with machine learning. In the same way that using serverless computing requires you to get stuck into code, machine learning requires you to understand data modeling and algorithms. If this is not your core competency, but you still want to make use of machine learning, it's recommended to engage the services of a business analyst to get you started. Machine learning is strongly linked with big data services. Big data is a major industry buzz word which gets managers very excited. It is another example of a service which has only been made possible by the evolution of cloud computing, simply because of its sheer scale. Big data refers to the volume of data which can be stored in a data-like service, the size of individual objects and files, and the number of objects which can be stored and efficiently queried. Depending on the capabilities of the cloud provider, a data-like service can easily store files which are over a petabyte in size each, that's 1, 024 terabytes, and it can store trillions of objects, which can be queried quickly and efficiently. To all practical extents, the amount of storage in a big data data lake is effectively unlimited, but you don't pay per gigabyte of storage as you do with a traditional cloud storage service, which is a good thing as few companies have that much money handy. The cloud-based data services are usually able to integrate with existing on-premises or cloud-based data platforms, making the functionality provided by data lake services effectively an extension of the capabilities of on-premises infrastructure. Cognitive services is where cloud computing starts to get just a little bit scary. We have already seen how machine learning is extremely effective at pattern recognition, but cognitive services takes that pattern recognition to the next level by ingesting, acting up, and interpreting data inputs, which is distinctly human in nature. Cognitive services provides functionality like facial recognition, looking at multiple images, recognizing the faces, and working out whether the people are the same in each picture, or emotion recognition, looking at those same images as before and assessing the strength of different emotions expressed in the facial features, or speaker recognition, enabling a service user to enroll their own voice as a means of identification. There are many more services including intelligent search across disparate data sources and text analytics. These services are constantly evolving, and in order to make use of them, all you need to do is register the service or services you are interested in, and query an API from your application. From the point of view of application developers, these services add a massive boost in functionality to your existing application framework, as you can incorporate advance and highly-sophisticated features, which you might never be able to create yourself. For example, if you create an application which uses multi-factor authentication to verify a user's identity, you can incorporate facial or voice recognition to increase the security of the product, all without having to write that functionality yourself. Additionally, as the service improves, so does the embedded functionality within your application. Cognitive services is often likened to the path to artificial intelligence because it makes use of massive amounts of data to provide functionality and return information to the user, which is usable and meaningful to an individual person, but the processing and data are far beyond what any single person can compute.

Cognitive Services
Finally, let's take a look at the Internet of Things, IoT. This is of significant interest to business because it enables them to invest in large numbers of small, independently operated devices, which support by directional communication with the cloud provider's IoT service. And IoT device can be literally almost anything, but from the point of view of the business, an IoT device could be something like a sensor device which gathers information on the real-time performance of their products or equipment. For example, a business which operates a large fleet of trucks might use IoT sensors to monitor fuel efficiency, and improve the performance of onboard safety equipment, plus reports on how that data is changing over time, allowing the business to act upon potential equipment fault before it occurs, or to implement more fuel efficient routes or driving practices, which can save the business significant amounts in operating costs. In another example, a business which operates public car parts, might use IoT devices to monitor which parking bays are available, display accurate figures in real time to potential customers online, and direct customers arriving in the car park to the nearest available bay, or to the most desirable parking bays based upon the parking patterns of other drivers, or even the historical behavior of that particular customer. Something like, your favorite car park isn't free, but the next one to it is. In the same way that we use log collection and analytics in our own cloud environments, Internet of Things allows companies to bring data collection, insights, and intelligence to every area of their business. Most of these cloud services are designed to support quite specific use cases, so you may find that you don't have an immediate use for them. However, looking back on what we have covered throughout the course, the principles behind the consumption of fundamental services like VMs and storage, apply just as firmly to these advanced services. Once you've taken your first steps into architecting solutions in the cloud, no services are off limits. And so, we've reached the end of this module on Advanced Cloud Services. Let's do a recap on what we've covered. We have looked at serverless computing, even though it isn't really serverless, and how it acts as glue in an overwrought automation framework, linking disparate platforms together as a coherent whole. We examined machine learning and big data and how businesses can make use of this service to unlock hidden patterns and relationships in their own data sets. We delved into cognitive services seeing how the evolution of cloud computing has made it possible to integrate human interactions into any application. And finally, we looked at the Internet of Things, and how businesses can use IoT to create rich data to enhance their own operations, as well as significantly improve the customer experience. And this brings us to the end of the course. Congratulations on making it all the way to the end. I hope that this course has provided a basis for understanding the wide variety of services on offer in both the private cloud and the public cloud, and how we should approach them when architecting solutions, what some of the stumbling blocks can be, but also just how many opportunities there are for us to create interesting and powerful solutions. Thank you again for watching this Pluralsight course on Architecting Cloud Foundations. I've been James Bannan, and I look forward to seeing you in the next Pluralsight course.

Course Overview
Course Overview
Hello everyone, my name is James Bannan, and welcome to my course on Architecting Cloud Foundations on Pluralsight. I am a principal consultant with Vibrato based in Melbourne Australia. I'm a Microsoft Azure MVP, and I've been architecting and deploying solutions in both private and public cloud environments for many years. In this course, we're going to take a vendor agnostic look at cloud computing, both on-premises private cloud computing, as well as public cloud solutions. There is no one size fits all approach to cloud computing, so we will examine loads of different business scenarios to explore what some of the options are and how you go about making the best decision for you and your business. Some of the major topics that we will cover include the differences between public, private, and hybrid cloud computing, what business requirements they each best fulfill, and what some of the architectural caveats are for all of them, the practical differences between the various public cloud service offerings IaaS, PaaS, and SaaS, and what you need know about each of them before making a decision to invest in anything, what are some of the security and management considerations, which are specific to public cloud environments, and how best to manage them or mitigate them completely. And finally, advanced cloud services, such as containerized applications, serverless computing, and machine learning, what they are and why you and your business might wish to take advantage of them. By the end of this course, you'll be familiar with all the major cloud computing concepts and services, and will be confident enough to make architectural choices around what your business technology might look like in the very near future. This course doesn't assume that you have any prior experience with cloud computing, so don't hesitate to jump straight in. I hope you'll join me on this journey to learn loads about the fundamentals of cloud computing with the Architecting Cloud Foundations course at Pluralsight.