Algorithms and Data Structures - Part 1
by Robert Horvick

A look at the core data structures and algorithms used in day-to-day applications.

In this course we will look at the core data structures and algorithms used in everyday applications. We will discuss the trade-offs involved with choosing each data structure, along with traversal, retrieval, and update algorithms. This is part 1 of a two-part series of courses covering algorithms and data structures. In this part we cover linked lists, stacks, queues, binary trees, and hash tables.

Course author
Author: Robert Horvick	
Robert Horvick
Robert spent nearly 10 years at Microsoft creating software that made it easier for everyone else to write software. Most recently Robert worked on Team Foundation Server on the...

Course info
Level
Beginner
Rating
4.5 stars with 1131 raters(1131)
My rating
null stars

Duration
3h 13m
Released
15 Aug 2011
Share course

Algorithms and Data Structures 1: Linked List
Introduction
Welcome to the Pluralsight Algorithms and Data Structures course. My name is Robert Horvick, and throughout this course I'll be introducing you to some of the most common data structures used in computer programming. We'll also look at many of the algorithms that are associated with these structures. This first module will focus on the Linked List Data Structure, as well as introduce some concepts that will be referred to throughout the course. I'll start by introducing the Node. This is the basic building block of many of the Data Structures we'll see in this course. Next, we'll see how nodes can be linked together to form chains of nodes, and then we'll see how these chains form the basis of the Linked List Data Structure. We'll also look at the Linked List and see how there are several types of operations that can be performed. I'll introduce an extension of the Linked List, known as the Doubly Linked List, and finally, we'll look at how the Linked List Data Structure is implemented in. NET and C++.

Node Chains
The Node is the most basic building block for many common Data Structures. The Node fulfills two functions. The first is that it provides a mechanism to contain a piece of data. For example, the integer value 3. The second, is that it provides a means of connecting itself to other nodes via an object reference pointer. This object reference is known as the Next pointer. So, as you can see here, we have a single node. It has a value of 3 and a Next pointer, but it has nothing to point to. So, let's create another node, this time with the value 5. Now, let's connect the first node to the second node. We've now created a chain with two nodes and one link joining them. This process can be repeated for many more nodes. So, let's look at some code and see how a node object might be defined and how the linking process might take place. So, as we saw in the previous slide, the node class has two responsibilities. It holds a value, or in this case, an integer, and provides a pointer or a link to the next node in the chain. Let's start by using the node class by creating the first node in our chain. So, we'll allocate a node with the value 3, and it's going to have the Next pointer default to Null. So, let's go ahead and create the Next node in the chain. Now, this node has a value of 5, and again the Next pointer has a value of Null. The key thing to understand at this point is that all we've done is allocated two independent nodes; there's no link between either node. So, what we need to do to create the link is set the Next pointer of the first node to be the second node, and in this way we've linked the first node and the second node into a chain. And as before, this whole process can be repeated again for a third node, and a fourth, and on, and on, and on. So, let's go and take a look at this in Visual Studio.

Code: Node Chains
Let's take a look at some code that creates three nodes, links them together into a node chain, and then prints the value of each node to the console. First, we have the Node class. As we saw in the slides, the Node class contains two properties: The current node value and the Next pointer. Now, we're going to use the Node class to build up the chain, so we'll start by first creating a node with the value 3. Now, in the comment blocks I'll try and represent what the object chain looks like after each expression. So, first we have a node with the value of 3, and a null Next pointer. Next, we'll create the middle node in the three-node chain. So now we have a node with the value 3 and a null Next pointer, and a node with the value 5 and a null Next pointer. But, this isn't the chain; this is just two nodes off in space independent of each other. To create the chain, we need to link the nodes together by setting the Next pointer on the first node to point to the middle node. Now we've created a chain that is two nodes long with one link joining them. We'll create one more node with the value 7. Now, at this point, we have the two linked nodes, 3 and 5, and we have the 7 node off in space not part of the chain. So finally, we'll link the 7 node into the chain, and we have the 3 node with the Next pointer pointing to the 5 node, whose Next pointer points to the 7 node. Now we've built up a chain with three nodes and two links joining them together. And after that, we'll print the chain. When we print the chain, we're going to use an enumeration pattern that we'll see throughout this course. The idea is simple. As long as the provided node is not null, we'll operate on its value, and then we'll set the node to be equal to the Next node. So, the first time through, we'll receive the first node; the first node is the 3 node. It's not null, so we'll print out its value, 3. And then we'll set the value of node to be node. Next, which is the 5 node. So, this next time through it won't be null, we'll print a 5, and we'll set it to be the 7 node, it won't be null, we'll print a 7, but this time the Next pointer will be null. So, the node value being null will cause the loop to terminate and the method will return. Let's take a look at this in action. Alright, so the first thing we'll do is we'll allocate the first node. Now, if we look at the first node, we can see that its value is 3, and its Next is null. Now, we'll allocate the second node, with a value of 5 and a null Next. So, it's just like we have in the pictures here. When we link them together, first. Next points to the 5 node. Now, we'll create the node for value 7, and then we'll add it to the chain. Now middle, which is 5, has a Next value that points to 7. So, let's look at the first node. We can see that first. Next points to 5, whose. Next points to 7. Now, when we go into the print, let's look at the nodes here. So, at the very beginning, the node is the 3 node, so we'll print out 3, and we'll set node to node. Next, and now we can see the value's been updated to 5 because we're pointing to the 5 node. We'll come through, print out the 5, and we're going to update one more time, and now we're pointed at the 7 node, we'll print that out. This time when we update, node will be null, so we exit the loop, and we're out, and now let's take a look at the console output, 3, 5, 7. So, then we've built up a node chain, we've put three nodes into the chain, and we've enumerated over each of them, printing out each value. This has been a high-level overview of chaining nodes, and it is conceptually similar to the Linked List class we'll be implementing. Now that we have a stronger understanding of how node chaining works, let's start diving more deeply into that Linked List class.

Linked List
Now that we know how to chain together nodes of data, let's see what it would take to make the leap to a Linked List. The first thing to understand is that the Linked List is a single chain of nodes. Linked Lists have a well defined starting point known as the List Head. The List will provide a Pointer to the first node in the list. Linked Lists also expose a Pointer to the last node in the list, the Tail, and Linked Lists provide operations that allow the list items to be managed, searched, and enumerated. It's these operations that make the List truly useful as a collection. We'll look at several of them now.

Add Items
The most basic operation that can be performed is adding an item to the list, so let's start by seeing how to add a new item to the start of the list. It all starts with an empty list. Recall from the previous slide that Linked List is a single node chain with Head and Tail node pointers, both of which have initial values of Null. The first step to adding the node is allocating the node. In this case, we've allocated the node with the value 3. Adding this node to the Linked List means first pointing the Head and Tail pointers at this node. Since the list has only one item, the Head and Tail pointers will both refer to that same item. Now the Linked List has a node to start the rest of the chain. Adding the second node is basically the same operation. We allocate a new node and point the Head node to it, pointing its Next pointer to the node that was previously the Head node. Since we're adding the node to the start of the list, we don't need to update the Tail pointer, it already points to the last node. Let's look at the code for this method. The very first thing we do is store off the Head pointer into a temporary variable. It's the start of the existing node chain, and we'll want to keep our reference to that. The list's Head pointer is now updated to point at the node being added to the front of the list, and the existing node chain, the temporary variable, is unlinked to the new Head node. We then do a little bookkeeping to track the number of nodes in the list, and if there's only one node in the list, we set the Tail node to be equal to the Head node, just as we did during the demonstration. As you can see, this is a very efficient operation. Adding a new node to the front of the list only involved allocating the data and updating a few pointers. The complexity or performance of this operation remains constant regardless of how many nodes are in the list. Now, if we were using an Array instead of Linked List, adding to the front would involve shifting all of the existing data to the right, and if the Array were full, it would involving allocating an entirely new Array, a larger Array, and copying all of the data from the smaller Array to the larger one. This characteristic of efficient insertion is one of the key benefits of the Linked List Data Structure. We just saw how to add a node to the start of the list. But, it can be just as useful to add a node to the end of the list. Like before, we have an empty list with a Null Head and Tail pointers, and it's that Tail pointer that's going to make this operation much easier. When we create our first node, like before, the Head pointer is updated to point to the new node, and so is the Tail pointer. Now, when a second node is created, the Head pointer remains unchanged, and the Tail pointer points to the new node. Having the Tail pointer allows us to add the node to the tail very easily. It could certainly have been done without a Tail pointer, but nearly as easily or with such predictable performance characteristics. Let's see the code for this operation. There are only two cases we need to worry about, whether we're adding a node to an empty list or to an existing node chain. If the list is empty, we point the list Head pointer to the node being added. If the list is not empty, we chain the node being added to the end of the existing chain. In either case, the Tail pointer should now point to our added node, and we can increment the counter, which is keeping track of how many nodes are in the list.

Remove Items
Like Add, Remove can be performed from the front or the end of the list. Let's take a look at an example using the list of values 3, 5, and 7. Removing the Tail node 7 requires updating the Tail pointer and setting the next value for 5 to null; doing that will eliminate all references to the 7 node, which effectively removes it from the list. Now, this is conceptually very simple. Let's take a look at how the list is going to look after this is done. Now, we can see that the 7 node is gone, the Tail pointer points to the 5 node, and the 5 node's Next = null. This is what we wanted. But, you can see from the code that this requires enumerating over all of the nodes in the list. This is because we're only storing references to the Head and Tail nodes. But, to perform this operation, we actually need to update the second to last node. Now, this is not incredibly complex. It's a simpler while loop to iterate until the second to last node. But having to walk the list every time a node is removed from the end can end up having significant performance implications on your application. Imagine a list that contained millions of nodes; removing each node from the end could require millions of millions of operations. So, removing from the front of the list is not nearly as complex; it simply involves setting the Head pointer to the node that follows Head, and that's it. When the list contains only one node, removing it involves just setting Head and Tail to Null.

Enumerate
The last major operation we're going to look at is Node Enumeration. Enumerating over the nodes is not difficult, but is done so frequently that it's worth looking at it in more detail. The key to enumerating over a Linked List is keeping a pointer to the next node to enumerate. Let's see this in action. We're starting with a list that has three nodes in it, the values 3, 5, and 7. We want the caller to be able to write a for-each loop that enumerates over each of these values so they can operate on them. So, we implement the GetEnumerator pattern using the C# yield syntax. If you're not familiar with this syntax, please consult the MSDN documentation for Yield Return, or checkout the C# courses on Pluralsight. The enumerator is a simple loop. It keeps track of the next node we're going to return to the caller in a local variable named current. When the method begins, current is assigned to the first node in the chain. Since the node is not null, the value is yielded to the caller, and the current variable is assigned to the next node in the chain. That node is also not null, so the value is yielded and the cycle continues. Eventually, the Tail node is reached, and after yielding the value, current is assigned to null, which causes the loop to terminate and the method to return.

Code: Singly Linked List
What I'd like to do now is rather quickly go through the Linked List code for an actual implementation of the Linked List Data Structure. Now, this is a somewhat simplistic implementation; it doesn't take into account a lot of the error handling cases that a production class would need, and it only implements a subset of the truly useful behaviors that a Linked List offers, but it gives enough insight into how the Linked List works that I think it's a good foundation to work from. So, we begin with the Linked List node. Now, this is very similar to the node class that we've already looked at several times in the module so far, only this time, you can see that the Linked List node takes advantage of the C# generic syntax. This allows us to hold not just integers but any type of object within our Linked List. One more deviation from what we've seen is that we've added a constructor, and this constructor allows us to create a node with a pre-specified value. This is just a little bit of syntax sugar to help with creating nodes. Otherwise, the node is nearly identical to what we saw before; it has a value, which is the generic type, and it contains a pointer to the next node in the chain, very simple. So, let's go and take a look at the Linked List class. Well, the class is about as simple as it can be. It also is a generic type, meaning that the Linked List can contain nodes that hold any type of data, and it implements the ICollection<T> generic interface, and this interface provides a certain set of operations that all collections that implement this interface are guaranteed to expose. Implementing this interface helps us conform with some. NET conventions, and ensures that we'll be able to drop in where other collection types are used without having to change any existing code. So, as you expect, we have the pointer to the Head node and the pointer to the Tail node. These are auto-implemented properties, so they default to the value null, they have public getters but private setters, because the list should be managing these values and nothing else. And we have Add methods and Remove methods. These are the methods we saw in the earlier slides, so I won't spend a lot of time going into them, but what you'll see is that we have AddFirst, which allows us to add a value into the list, and we'll create the node for them, or AddFirst where a node is provided, and we'll just add that node into the list. Additionally, we have AddLast, and AddLast also has a direct value and a node with value. AddLast also allows adding by value or adding a node directly. Now, these are the same implementations we saw on the slides; likewise, RemoveFirst and RemoveLast, and again, these are exactly what we saw in the earlier slides, so I won't spend any more time going through them. And that leaves us with the ICollection interface. There are a few things in here we haven't looked at, so it's worth going through. Now we have the Count property, another auto-implemented property with a public getter and a private setter. At any moment, the Count property will indicate the number of nodes that are currently in the list. The ICollection interface guarantees a void Add an item. This implementation simply defers to AddFirst; adding the item to the front of the list. ICollection adds a Contains method. The Contains method performs a simple enumeration over the list and returns true if a node with the specified value is found. This enumeration pattern matches what the GetEnumerator does, only it puts the behavior in line with the enumeration. ICollection requires the CopyTo method, which allows us to copy all of the nodes from the Linked List into an Array. And again, it's the basic enumeration pattern, only the behavior internally, instead of yielding the result, is performing a copy of the value into the pre-allocated Array provided by the caller. The ICollection interface also has IsReadOnly. Now, this collection is not Read Only, and it never will be, so we just return false. And here we get to the method that's a little bit more complex, and this is a version of Remove that we haven't looked at yet. And the way this one differs from the Removes we've looked at is that it doesn't require that the item be removed from the start or the end of the list, it allows an item by value to be removed anywhere within the list. That's a little more complex. There are four cases we need to care about here. We have an Empty list where nothing's done; we have a list with a Single node where we just want to remove that node if the value matches; and we have a list with Many nodes. And within the Many nodes, we distinguish between removing the first node in the list and removing any subsequent node in the list. The algorithm is pretty straightforward. While we haven't iterated to the end of the list, so our basic enumeration pattern, we're going to check if the current. Value. Equals the value we're looking for, and if it does, we're going to remove it from the list; if it doesn't, we're going to move on to the next value. Now one way this enumeration differs is that we're not just storing the current node, we're also storing the previous node, and we're storing the previous node because it's possible that we would remove a node in the middle, and we need to be able to update that pointer without having a helper. Now recall, when removed from the end of the list that the Tail pointer allowed us to do that very easily. Well, in this case we don't have a Tail pointer, so the previous variable serves as a proxy for that. So, let's look at that example. So, if we're coming through and we find that we've matched the value, and previous is not null, which means we're not the first node in the list, because previous was null up until one time through this while loop, in that case, we're going to take the previous node we found, which is not the node that matches the remove criteria, and we're going to set its Next pointer to be the current node, which we are removing, Next pointer. So basically we're skipping over the current node. So, let me draw this out real quick. If the Before were the nodes 3, 5, and 7, when we're removing node 5, the After would be 3, which would be previous, its Next pointer would now point to 7, which is current's or 5's Next pointer. So, that's what we're doing here; it's just jumping over the node we're removing. Now, if current. Next is null, which would be a case where we were removing 5, but there was no 7, so for example, it was 3 -> 5 -> null, and now we want it to be 3 ------> null, well in this case, we also need to update the Tail pointer, because the Tail pointer previously would have pointed to 5, the last node in the list, and now needs to point to 3, the last node in the list. And regardless of what position we were at, we decrement the Count. Now, the other case here is the case where previous! = null, and in that case, we've encountered the node we're looking for by a value the very first time through the loop, which means it's the very first node in the list, in which case we can just call RemoveFirst. Now, if we removed an item which we did if we're in the Equals condition, we return true. Otherwise, we'll eventually have current be null, break out of the enumeration loop and return false. So, that's a little bit more complex of a Remove operation. So take some time and look at it, and think about how it works, and look at some of the tests. Now, the enumerator is what we looked at in the slides, it's unchanged. ICollection also requires another enumerator, which is a non-generic enumerator, so we just defer to the specific enumerator we've written; and finally, ICollection requires Clear. Clear in a Linked List is very simple in a garbage collected environment; you just set Head to null, Tail to null, and Count to 0. We can do this in C#, because what that will do is remove all of our references to every node in the node chain, and they'll be garbage collected. If this were a language like C or C++, where garbage collection is not performed by the system, but a responsibility of the programmer, you would have to at this point dynamically delete each node in the list. So, that's our Linked List class. As you can see, it's pretty straightforward. There's a lot going on, but it's not that complex. I would encourage you to take some time, look at the code, and get a feel for how it works, and then see how you can extend it, and how you might make it more robust. A challenge I would throw out is the case of AddLast that takes a node. What if the node provided was part of a different list already and it wasn't the last node? Well the node. Next wouldn't be null, so we'd come through here and we'd have a bug, because when you enumerate, node. Next would still be set to the other list; in fact, we would enumerate from one list and jump all the way into another list. So, there are a lot of things to think about that we're not going to go through and add the robustness for, but that I think would be good exercises for you to think about and to try and handle.

Doubly Linked List
What we've looked at so far is known as a Singly Linked List. Now, it's called that because each node in the chain has a single link to another node. This type of Linked List works great when you only need forward access to the nodes. But, there are times when being able to enumerate the nodes both forwards and backwards might be beneficial. There's a specialization of the Linked List, where each node contains two pointers; one to the Next node just like the Singly Linked List, and one to the Previous node, and it should be no surprise that this is known as a Doubly Linked List. A Doubly Linked List starts with the single node. In this case, we have a node with a value 3, and Null Previous and Next pointers. Just like with the Singly Linked List, we need to create a second node to start the node chain. So, we'll create a node with the value 5, and it also has Null Previous and Next pointers. Now, what we're going to need to do is create the chain by linking these two nodes together. Now, like the Singly Linked List, the Next pointer of the first node will now point to the second node. But because this is a Doubly Linked List, we'll also create a link back from the second node to the first. This means we can navigate from the second node back to the first node as easily as we can the first to the second. And just like a Singly Linked List, this pattern can continue for as many nodes as needed. Now, a Singly Linked List and a Doubly Linked List are very similar in implementation. So, let's go straight to Visual Studio and look at some of the differences in code.

Code: Doubly Linked List
We're looking here at the Doubly Linked List node class. Now, I've kept the same name of LinkedListNode, but I've created a new DoublyLinkedList namespace, and this is so that we can keep a majority of the code the same, which will make creating a difference between the Doubly Linked and Singly List code easier in just a moment. Now, the Linked List node is pretty much the same as what we saw with the Singly Linked List, with the addition of the Previous pointer; that is the only change we've made. And the Doubly Linked List class is also nearly identical to the Singly Linked List class, and to actually show this more easily, what I'm going to do is show a difference of the Single and Doubly Linked Lists class, and just talk about the differences, because we've already gone through the Singly Linked List class in good detail. The Doubly Linked List class is on the left, with the Singly Linked List class on the right. So, as we can see just right off the bat, these are very similar classes. The diff structure is visible here on the left, and you can see that throughout the file it's a very small number of diffs. The first diff is the namespace, so we'll skip that, and we'll go on to the second. Alright, so our first major diff is in the AddFirst method, and recall what we're doing here is we have a list that Before was Head pointing to 5, with a Doubly Linked reference of 7, whose Next pointer was null. And what we're going to end up with is Head pointing to the new 3 node, which has the Doubly Linked reference to 5, whose Doubly Linked to 7, who points to null. So, we're adding 3 to the front. Now the previous AddFirst method is all the same. Temp is set to the Head node, so temp, at the moment we're down here, is set to 5. Now because 5 was the Head node, its previous value was set to null. So 5. Previous should now be set to 3. So, we have our Doubly Linked reference. So, that's all we're doing here is we added basically 5. Previous = 3, and because we're adding to the first, it's a very simple operation, and we haven't added any additional complexity onto the performance characteristics of the Add method; we've added a new point or assignment. So, let's go to the next difference. And again, this difference is all about managing the previous pointer. Here we're in AddLast. Now, if you imagine, we have Head pointing to 3, Doubly Linked to 5, whose Next is null, well now we need 5 to point to 7, but we need 7 to point back to 5, and that's what we're doing here. We're saying node, which is the 7 node,. Previous, should be set to Tail, which at the moment Tail is 5, because we haven't yet updated Tail to point to 7. So basically we're saying, alright, you're the new Tail node, so you're previous node has to be the Previous Tail node. So, once again it's a very simple change that only has to do with managing the previous pointer, and it doesn't change the performance characteristics of the Add operation. Let's go to the next change, and here we're in RemoveFirst. So hopefully that pattern's becoming clear; the Add and Remove methods are where we need to manage the pointers, so that's where we're making these changes. And once again, it's all about managing a previous pointer. So, in this case, we're removing the first node, so we had Head pointing to 3 pointing to 5, although these should be Doubly Linked references in a Doubly Linked List, and now what we want to have is Head simply pointing to 5, and then 5's Previous should point to null, and that's what we're going to do, is we're going to say the new Head node's Previous must be null. Earlier, 5's Previous pointed to 3, that's what we're changing. So again we're seeing the Remove operation has been updated for Doubly Linked Lists, but its performance characteristics haven't changed. And now we'll go here to the RemoveLast, and this is a case where we actually see performance improve, and what's happening here is before we had to enumerate over the entire list in order to find the second to last node, but now we don't need to do that, the second to last node is Tail. Previous. If we want to remove the node 7, what we need to do is change 5 to no longer point to 7 but to point to null, so Tail. Previous is 7. Previous is 5, 5. Next used to point to 7 but now points to null, so 5. Next points to null, and the Tail = Tail. Previous. Originally Tail was 7, so now Tail will be set to Tail. Previous, which is 5, so the Tail is now 5. So, we've taken an enumeration of a list that has really unbounded size and we've turned it into two pointer assignments. So, in the pathologically bad cases, where there are millions of nodes in this list, we've improved the performance characteristics dramatically. And let's move on to the last set of changes. In this case, we're in the Remove method, and again we're just managing a pointer. And what we're saying here is that before we had 3 double referenced to 5, double referenced to 7, which was the end of the list, and we want to remove the node in the middle, we want to remove the 5 node. So, on this line, if that's what we're removing, previous is set to 3, current is set to 5, which means that current. Next is set to 7, so what we're saying is, 7. Previous = 3. And that's what we want. We want 7 Previous pointer to point back to 3. So, we're removing the 5 from the middle. And this is the only case we needed to change. So, RemoveFirst will handle all the pointer management for the first case, and if we're removing the very end, we don't need to manage the Previous pointer because they're all going to be correct, we're just removing the Next pointer for one object and updating Tail. It's only this middle case where we have a change. And that's the last change we had to make in order to convert a Singly Linked List to a Doubly Linked List. So, for the cost of one additional pointer in the node, and a little more pointer management during Add and Remove operations, we were able to remove the most expensive operation that existed within the Linked List class, and that's a really good performance benefit to take for a little bit of overhead, and it also makes the list much more functional. Now as a consumer, for-each will only iterate forwards in the list, that hasn't changed, but once I'm midway through the list, I can manually walk backwards, and that's a very powerful pattern to be able to have introduced. So, as we'll see in a few moments, the Doubly Linked List is so important, that. NET's own Linked List class is a Doubly Linked List.

Modern Implementations
So far in this module we've spent our time discussing the Linked List Data Structure and creating a simple implementation. In the real world, however, you would be unlikely to come across many situations where you would create your own Linked List structure. Typically, you'll be using a language or framework that provides a Linked List class that is already tested, performant, and compatible with your application. Unless you have a proven need to create your own Linked List, I would highly recommend that you use the one provided for you. We'll quickly look at two common Linked List implementations now. The first will be the. NET Framework's Linked List class, and the second will be the C++ language's standard list class. The. NET Framework Linked List class is very similar to the class that we created. It goes by the name Linked List; it's a generic class, so in this example I provided an integer parameter. We can see that we're adding the values 3, 5, and 7 using the AddLast method, and then we're able to enumerate each of those values and print them out. Now, this is very similar to what our code looked like using our Linked List class. So, it's in the System. Collections. Generic namespace; it's a Doubly Linked list. Now, as we saw before, this provides some benefits for performance, and it gives some additional features for enumerating the list backwards, and it has the common operations that we've already seen, AddFirst, Last, the RemoveFirst and Last, and Find and FindLast, and it's also compatible with LinkToObject. Now, there are existing courses on Pluralsight for Link, and I would encourage you to check them out if you're curious what that means. C++ has the standard list class. Now, I've performed the same operation of creating a list, adding the values 3, 5, and 7, and then printing them. So, as you can see, even though the syntax is dramatically different, the concept is very similar, and once you understand the name of the class, replacing AddLast with push_back or AddFirst with push_front, really isn't that much of a stretch. So, to use this class you include list. It is a Doubly Linked List just like the. NET Framework, and it has Common Operations to push and pop from the front and back of the list, and to iterate over all of the values. Now, it has some operations that are quite a bit different than the. NET Framework, so if you're interested in seeing different types of operations you might want, checkout the documentation for the C++ Standard List class, and I'll show you a link to that in just a moment.

Summary and References
So in this module, we've looked at nodes and node chaining, and we've seen how this is the basis to the Linked List Data Structure. We've looked at Singly and Doubly Linked Lists, and seen the differences between them, but also how similar they are, and we've looked at the Common Operations such as handling nodes, such as Adding, Removing, Enumerating, and Finding nodes. We also looked at some Modern Implementations of Linked List and Standard List. In this module, several new concepts were introduced, and I want to give some references to those so you can learn more about them. The first is the LinkedList class on MSDN; this is the. NET Framework's Linked List class. The second is a link on MSDN to the C++ Standard List class. We have the NUnit Framework. In this course there'll be a different video clip that explains more about NUnit including installing, and some basic usage instructions, but if you'd like more information, the NUnit website is available. And finally, on Wikipedia, there's an article on Linked Lists.

Algorithms and Data Structures: Stack
Introduction
Hello, and welcome to the Algorithms and Data Structures course. In this module, I'll be introducing the Stack Data Structure. We'll begin by building on concepts introduced in the Link List module. So, if you aren't already comfortable with Linked Lists, I would encourage you to watch that module first. Through this module, we're going to be learning about several concepts and seeing some examples. We're going to start by learning what Last In First Out means, and how it really defines what a Stack is. Next, we'll implement a Stack using a Linked List as the storage medium for the data, and then we'll implement a Stack using an Array for data storage. After that, we'll see an example of a Postfix Calculator implemented using a Stack, and we'll look at a simple example of how a Stack can make it easier to implement "Undo" operations in your application. Finally, we'll take a brief look at the implementations of Stacks in both the. NET framework and within C++.

Push & Pop
A Stack is a collection in which data is added and removed in a Last In First Out order. This means that the item most recently added to the Stack will be the one that is next handed back from the collection. I think that the easiest way to explain how a Stack works is by seeing a visual example, and I'd like to use one of the most common examples, the restaurant plate stacker. If you aren't familiar with a restaurant plate stacker, this is a device where clean plates are put into a spring-loaded cylinder. As customers need a clean plate they walk up to the stacker and remove the top plate. So here we have an empty plate stacker. Since it's empty, the dishwasher is notified that more plates are needed, and as plates are cleaned, they're added to the stack. In computer science terms, this is known as "pushing" an item onto the stack, and notice that the plate that was just added is at the bottom of the cylinder or the bottom of the stack. But, this is also the top of the stack because it's the only plate in the stack. In a few moments, another plate is cleaned and added to the stack, and each time a plate is added, it becomes the new top plate, the one that the customer would pick up next. As a customer, I can approach the stacker and peak into it. This allows me to see the top plate. I can do this whether or not I want to take the plate off the stack. Now eventually, more plates are added, each one becoming the new top of the stack. Now, at this point, we have a plate stacker with four plates in it. A customer could walk up and could see that there is at least one plate in the stacker. She doesn't necessarily know or even care how many plates are beneath that plate; all she really cares about is that there's one available for her. So, she can take the top plate from the stack. In computer science terms, this action is known as "popping" the plate from the stack. And it should be now clear what Last In First Out means; the last plate added to the stack was the first plate removed by the customer. As more and more customers come, the plates are popped from the stack, each time reducing the depth or the number of items in the stack. Eventually, the stack is empty, and at this point, no more items can be popped from the stack.

Stack (Linked List)
In the Linked List module, we saw how a Linked List is a series of nodes linked together into a chain. We can use this type of structure to help build our stack. The stack would contain a Linked List whose Head node pointed to the most recently added or top item. When an item was pushed onto the stack, in this case the #1, the Head pointer now points to this new node. This is the Top node. When another value is pushed onto the stack, it is added to the front of the list and becomes the new Top. This can repeat over and over. Eventually, the stack owner will need to remove data, and will do so by calling the pop method. When this happens, the front node of the list is removed, and the next node of the node chain becomes the top of the stack. So, why would I choose to use a Linked List as the data storage medium for a stack as opposed to another structure such as an Array? Well first, because it's a Linked List, it has no hard size limit; the only real constraint is the number of nodes that can be allocated, and for modern personal computers this would be a huge number, millions or billions of nodes. It's also a very straightforward way to implement a stack. An Array, for example, would require bounds checking to make sure that a push didn't cause the item to exceed the Array's bounds. But there are downsides to this approach, and many of them have to do with performance and scalability. The first is that adding an item to the stack causes a memory allocation to occur every time. This can end up causing undesirable performance characteristics in high performance systems. On a related issue, the memory cost for each node can be significantly more than the cost of the data being stored. For example, a 32-bit value such as an integer might have memory overhead several times larger than the integer itself. And then there's the bucket of general performance issues. These issues really only affect high performance systems, but they can be very significant. They include things like data locality problems and memory fragmentation; things that Linked Lists run into because they're storing the nodes throughout the heap, but an Array can avoid by keeping all the data really near each other. These issues are really beyond the scope of this module, but there's a lot of information about these topics online and in books, so I'd encourage you to take a look if it's something you have an interest in.

Code: Stack (Linked List)
Here we have a Stack class implemented with a Linked List as the backing store. Now, I'm using the. NET framework's Linked List class, in order to remove any of the overhead associated with list maintenance, and to not create a dependency on a previous module. Now, the class begins by allocating the list. This is the list that will contain all of the items in the Stack. Push, as we've discussed before, simply adds the item being pushed to the front of the list. Pop, checks to make sure the list is not empty, and if it is, it throws an exception, but if it isn't, it retrieves the value from the first node in the list, it removes that node from the list, and then it returns the retrieved value. Peek is similar to Pop in that it throws an exception if the list is empty, but it differs in that it doesn't remove the item from the list, it just returns the first value. The rest of the implementation is very straightforward. Count returns the number of items in the list; Clear simply clears the list; and the Enumerator functions simply return the list's Enumerator. So, as you can see, using a Linked List as the backing store for a Stack is incredibly simple; it's just a few lines of code beyond what the Linked List took to write.

Stack (Array)
We've seen how a Linked List makes it very easy to implement a Stack, but does so at the cost of performance. So, let's take a look at another approach using an Array as the storage mechanism. When this example starts, there's no Array because the Stack is empty, but eventually, a value is pushed onto the Stack. When this happens, the Stack allocates an Array. Now notice that even though a single item was pushed, the Array contains six open slots. This over-allocation of the array will reduce the number of times the array needs to grow as more items are pushed to the Stack. The pushed item is added to the Array in the first slot, and the Stack records that this Array index contains the top of the Stack. When another item is pushed onto the Stack, the item already in the Array remains in the same place; the new item is added after it, and the Top pointer is updated; this can continue over and over. And now because there is data at the top of the Stack, the item at the top can be peeked at, and as items are popped, the pointer to the top item is updated, and this can continue for as long as the Stack is not empty. So, as you can see, pushing and popping items from the Stack involves potentially allocating an Array, writing the item to the Array index, and updating a value that stores an index into the Array.

Code: Stack (Array)
What we'll look at now is a Stack implemented using an Array as the backing store. Now, what you should notice if you've watched the Linked Last Backing Store clip, is that this is a much more complex class. So we begin by allocating the Array that represents the item in the Stack. Now notice that we've allocated with a length of 0, and the reason I've done that is to avoid having to check for null later on. We can just check for length and find we have a length of 0. This makes the code a little more streamlined and avoids a special case. Now size is the number of items currently in the stack. This is not equal to the number of items that can be held by the Array or the Array's length; this is the number of items currently in the Array. So, we'll start with Push, and within Push, there are a few cases we have to be worried about. The first is that size = 0, and this is the case where we're doing our first Push. The second is where size = the current array length. This is a case where we've already added all the items into an Array that we can, and we need to grow that Array. In either case, the size variable will be equal to the length of the Array, and then we'll go inside of the conditional expression. The first thing we'll do is figure out what the new length of the Array should be. If the current size is 0, we'll default to 4. If the current size is greater than 0, we'll simply double the Array. So, if we had an Array length of 4, it would now become 8, 8 would become 16, and so on. The next thing we'll do is we'll allocate the Array based on the new length we have defined. Now this Array will sit in parallel to our current Array for a moment. We're going to copy all of the items from the existing Array into the newArray, and then we're going to point the item's pointer to the newArray. So now our Array of items is the Array we just allocated, which contains every item in our Stack plus an additional buffer space. So now we can add the new item into the item's Array at the index, and then we bump the index. Pop is actually pretty trivial. If size is 0 we throw, just like with the Linked List. Otherwise, we decrement the size index and we return the appropriate item from the Array. Peek is similar in that it throws if size is 0, but instead of decrementing size, it simply subtracts 1 while looking up the item. Count just returns the size. Clear sets size to 0. Now one thing to keep in mind, which I didn't deal with in this class, is that setting size to 0 doesn't clear out the Array. Now, for integers that's fine, but if this Array contained disposable objects or objects that had finalizers, leaving them in that Array would keep a reference to them alive. So if you're implementing a production quality Stack, you're going to need to deal with issues like that, and this is one of the reasons that it's important to always consider using the Stack provided to you by the platform you're on, because it should be taking care of these issues for you. The last things we'll look at are the Enumerators. Now, we're using the yield syntax as we enumerate over the items; the only tricky thing here is we enumerate the items backwards, and that's because the last item in the Array is the first item we want to return. We want to return these items as if you had called Pop, Pop, Pop, Pop, Pop. If we return them from index 0 up to size, they'll be returned in First In First Out order not Last In First Out order. And then the non-generic Enumerator simply defers to the generic Enumerator. So, the class really isn't that complex, but compared to the Linked List implementation it's substantially more complex, but it should be clear that that complexity brings with it some performance improvements that are well worth it.

Postfix Calculator
What I'd like to look at now is an example of how a Postfix Calculator might be implemented using a Stack. The Postfix notation is also known as Reverse Polish Notation, and what distinguishes Postfix notation from the more traditional Infix notation, is that the operators follow the operands. For example, instead of saying 5 + 2, we would say 5 2 +. The reason for this is that Infix notation can be ambiguous when it comes to operation order. In the example show, how should the operators be evaluated? Should they go from left to right, 5 + 6, which is 11 x 7 becoming 77 - 1, for a result of 76, or should some operator precedence rules be applied. For example, 6 + 7 = 42 + 5 is 47 - 1 is 46, or should something else be done? Postfix notation removes this ambiguity, though I personally believe it does so at the cost of a less visually friendly notation, it does lend itself well to computation by a computer. So, let's take a look at this in action. On the left we have an empty Stack. I will use a visual style similar to the plate stacker example from earlier in the module. On the right is the basic algorithm to calculated a Postfix expression, and at the top is the Postfix expression we're going to be calculating. The algorithm starts by enumerating over each token. If the token is an integer, the value is pushed to the Stack. Well, 5 is an integer, so we'll push it to the Stack. And now the next token is evaluated. Because this value is an integer, it's also pushed onto the Stack. The next token is a 7, an integer, so it's pushed to the Stack. Now, we advanced to the next token, and it's not an integer, it's an operator. So what we're going to do is start popping things off the stack. We're going to pop a 7 off, and that will become the right-hand side of the multiplication expression, and then we'll pop the 6 off, which will be the left-hand side; 6 x 7 will multiply together to give the value 42, which we'll then push onto the Stack. Then we continue our process, we go onto the next token, which is an operator, so we'll pop the right value, the 42 from the Stack, then we'll pop the 5 from the stack. We'll add them together, give it a value of 47, which we push back onto the Stack. The next token is an integer, we push that onto the Stack, and then the next token is an operator subtraction. So, we'll pop the 1, this will be the right-hand side of the subtraction expression, and then we'll pop the 47. Forty-seven - 1 is 46, which we push onto the Stack, and that's our answer. Now that's our answer because we've gotten to the end of the token list, and the last item on the Stack is the result. So let's go take a look at the Postfix algorithm actually implemented in C#.

Demo: Postfix Calculator
Let's take a look at a Postfix Calculator written in C# using a Stack. Now, it's a pretty straightforward algorithm, and I've kept it all in one method, not to demonstrate good design techniques, but just to keep the examples simple. So, we start by allocating the Stack. Now this is the Stack where we're going to store the intermediate integer values that haven't yet been operated on. Now, recall from my algorithm, it starts with enumerating over all the tokens. Now, in this case the tokens are the input arguments. For example, if the input arguments were 5, 6, 7 * + 1 -, what we're going to do is evaluate that expression just like we did in the slides earlier. Each of these values 5, 6, 7, is one of the tokens. The first token is the integer, so when we do a TryParse, that succeeds, so we push that value onto the stack. And we'll continue that for the 6 and for the 7, but eventually we'll get to the * or the multiplication operator, and the TryParse will fail; that's going to drop us into the operation evaluation portion of the loop. So what we're going to do once we're in here is stop popping values off; the right-hand side is popped off first, the left-hand side is popped off second, then we look at the operator and we switch over it, we find we had the multiplication operator, so we're going to evaluate left-hand side * right-hand side and push that result back to the Stack. Now, this is the same behavior for +, -, multiplication, division, and modulo. We perform the operation with the proper left and right-hand side ordering, and we push the value back. Once the multiplication is done, we come back again, we do the addition, and then we push one out of the Stack and we perform the subtraction. Once the whole loop is done, we've gone over all of the tokens, we have one value pushed onto the Stack, which we'll pop off and print as the answer. So, let's take a look at this in action on the command line. I'm going to run the Calculator, and I'm going to pass it in like we've looked at; 5, 6, 7 *, that will multiply 6 * 7, the result of which will be added to 5; 1 we push to the Stack and subtract it from the result, giving us 46. Now we can see some simpler calculations, for example, 5 2 +, and that's 7. So, that's a Postfix Calculator implemented using a Stack. Now, one thing to note here is that whether the Stack was backed by an Array or a Linked List or some other structure it didn't matter. It was the Last In First Out behavior of the Stack that made this possible.

Demo: Undo
The last demonstration I want to look at is how Undo can be implemented using a Stack. Now, what I'm going to show here is not an example of proper WPF design, and it's not an example of how Undo should be implemented in every application. The point is to show how a Stack can be used to store these operations in a Last In First Out order, and how that can be used to undo operations in this particular UI application. But these concepts are similar to how Undo is implemented in many applications. So what we start with is a window. This window has three buttons, numbered 1, 2, and 3. These buttons all have the background color red. Now, when I click on any of these buttons, what's going to happen is it's going to change the background color to some random color, and when it does that, it's going to store the state that the button was at prior to the click in the Undo list, and that's going to allow us to make many changes, see what they are, and then undo them one at a time to get back to where we were. Let's just look at this a little bit and see that it makes sense. So, I'll click on button 1. That changed it from red to kind of a chartreuse color. I'll click on 2, red to a blue; and 3 was red to a dark purple. Now, you can see the top of the stack is 3, indicating button 3, and the F0 indicates that it used to be red; the 2 is indicating that button 2 is next, and the F0s are that it used to be red, and then after that button 1 used to be red. So, if I hit Undo, what I would expect to happen, the top of the Stack is item 3, would change back to red, and that's what happened. Now, the Stack only has two items in it. The next top is button 2 should be red, and now button 1 should become red. Now, if I just go a little bit crazy clicking on these things somewhat randomly, you can see that it's storing a lot of values here with a lot of colors and the buttons in any order, but now if I start clicking Undo, it's going to undo everything I just did. I'm not keeping track of the state in any other way; red isn't hardcoded somewhere, it's just using the Stack to store the changes that were made and to reverse the changes when Undo is clicked. So let's go look at the code very briefly and see how this is done. So here we can see the XAML Editor where I've laid out the three buttons, the list box for the Undo display, and an Undo button. Behind each of these are some actions. So, let's look at what happens when we click on button 1. Clicking on button 1 brings us to the button1_Click method, and what we do here is we Push an UndoAction onto the Undo Operation Stack. Let's go take a look at that Stack real quick. So, it's a Stack of UndoActions, and UndoActions take a button. They store which button I changed, and they store the current button's brush for the background. Now the ToString just displays the button number, the Content, and the brush as a hex String; that's where we're getting the 1:FFFF0000, indicating that button 1 used to be red. So, this is a very simple state that we're storing, just the control to update and the background color to apply when Undo is applied. So, the button 1 click pushes the Undo Operation for button 1. It then updates the Background for the button to be some RandomBrush, it doesn't matter, I just generate a few random bytes and then create a SolidColorBrush from those three byte values. And then we Update the list. UpdateList is where the listBox items are added. I just clear out the listBox each time and then enumerate the Stack adding the Undo action String to the listBox. Again, this is not an example of efficient or the best way to create a WPF application or to manage listBox items, it's simply a quick and dirty way to demonstrate how a Stack can be used for Undo. Buttons 1, 2, and 3 all have basically the same behavior. Push the button state, update the button Background, and then Update the List. When the Undo button is clicked, the Stack of operations is looked at. If there's anything to do, the Undo operation is Popped and Executed. Now Execute is a function on the UndoAction, and all it does is it sets the Background of the button to the stored brush. So, for example, the Background of button 1 will be set back to red. And then it updates the list of items in the listBox. So really it's a very straightforward application. Implementing Undo really meant that we needed to have a stack of things that we're undoing. When actions that we wanted to undo occurred, the Stack was updated, and when Undo was hit, the most recent action was applied. So, we can look at it one more time, and we can see that the buttons 1, 2, 3, 3, 2, 1, we're going to reapply them to button 1, 2, 3, 3, 2, 1.

Other Implementations
In this module we used the C# language to create our own Stack, but the. NET framework already has its own Stack class, in fact it has two. It has a generic class and a non-generic class, but pretty much every language includes some type of Stack structure, be it in the language itself or in one of the support libraries. Now in a. NET framework, like I mentioned, there's the Stack class. It implements Push, Peek, and Pop, just like our class, and it stores items in an Array. You can see an example on the right where a stack of integers is allocated; two values are pushed, a 10 and a 20, and then those values are popped off. C++ also includes a Stack in the standard library. You include the Stack header and it's named Standard Stack. It is a template class that takes a template-type argument. Now, this class differs slightly from the. NET framework, not just in the case of the method names or in the renaming of Peak to Top, but it also has a fundamentally different behavior with how Top and Pop are related. In the. NET framework, Peek returns you the value at the top of the Stack, and Pop returns you the value at the top of the Stack and removes that item from the Stack, but in the C++ Stack, Pop does not return the item, it simply removes the item, so you can see in the code, when the Stack is allocated and then items are pushed, we don't pop those items off, we look at the top item to retrieve the value, and then we pop the value off. This is a subtle difference in how the classes work, but it's one that's pretty important.

Summary and Reference
In this module, we've learned quite a bit about the Stack Data Structure. We've learned about how it's a Last In First Out container, and how this really defines the Stack's behaviors. We've learned how items are Pushed onto the Stack, Popped off the Stack, and we can Peek at the top. We've looked at a variety of backing stores, including a Linked List and an Array, and we've seen that the Stack is a common feature in many modern languages and platforms. Before you go, I would encourage you to take a look at MSDN, where there are references both on the Stack class in C# and the Stack class in C++.

Algorithms and Data Structures 1: Queue
Introduction
Hello. My name is Robert Horvick, and welcome to the Algorithms and Data Structures course. In this module, we're going to be learning about the Queue Data Structure. A Queue is a First In, First Out collection. This is in contrast to the Stack collection seen in the previous module that was a First In, Last Out collection. We'll learn more about what this means in just a few moments. We'll also see how items are added and removed from the Queue using the Enqueue and Dequeue operations. We'll look at two implementations of the Queue structure, one using a Linked List and one using an Array, and we'll look at a very useful specialization of the Queue, the Priority Queue. Finally, we'll look at some of the implementations that are available in the. NET framework and C++, and along the way we'll look at quite a bit of code, and we'll see some live examples of Queues in action.

Enqueue and Dequeue
So, let's start by getting the common understanding of just what a Queue is. In its simplest form, a Queue is a collection that returns items in the same order that they were added. You can think of this as a checkout line at a grocery store. The people in the line are checked out in the order that they get into the line; first in the line, first checked out, last in the line, last checked out; it's a pretty simple concept. And now that we have a common understanding of what a Queue is, let's take a look at what it means to add and remove items from the Queue. It all starts by creating the Queue. In this example, I'm using the C# generic syntax to create a Queue of integers. Next I would like to add or Enqueue an item. This is like a shopper getting into the grocery line. What we're going to do is add the value 1 into the Queue. Now that this item has been added, it is the head of the Queue. We can continue to add items to the Queue, 3, 4, and 5, and now we have a Queue with five items in it. Just like the Stack Data Structure we saw in a previous module, it can be useful to know what item is at the head of the Queue, and we can use the Peek method for that, and it returns the value 1, because that is the front or the head of the Queue. And as you can see, just like in the shopping cart analogy, in Enqueuing an item is the simple matter of adding it behind the last item in the Queue. Now that we have items in the Queue, let's look at what it takes to remove or Dequeue them. When we begin, the head of the Queue is at the number 1; that's the first item we added to the Queue. When the Dequeue operation starts, it removes the 1 from the Queue, returns it to the caller, and adjusts the head of the Queue to be the next oldest item or the number 2; now let's see that in action. Okay, so we've removed 1 from the Queue and the new head is 2, so we can Dequeue the 2, which sets the head to 3, we can remove it to 4, and remove it to 5. Now, if we Dequeue one more time, the Queue is now empty, and at this point, no more items can be removed from the Queue, but we could call Enqueue to add more items back and start the process over again. So, as you can see, a Queue is a pretty simple Data Structure conceptually. It's simply an ordered line of items that return to the caller in a First In, First Out method.

Linked List Implementation
We'll be looking at two implementations of the Queue Data Structure in this module, the first of which we'll use a Linked List for the item storage. Now, you may recall from the Stack module that implementing a Stack using a Linked List was quite simple, but that the simplicity came at a performance tradeoff. The Queue is no exception in this regard. To learn more about the specific issues I'm referring to, I would encourage you to go back to the Stack module and look at the Linked List implementation clips. Now we'll start by allocating a Linked List, and items will be added into the Linked List using the AddLast method. Adding to the end of the list allows the items to be added in a consistent order, with each item falling into line behind the one that preceded it. And once we have the items in the list we can see that the Head of the list is also the Head of the Queue. Since we added items to the back of the list, it makes sense that it would remove or Dequeue them from the front, and this would use the RemoveFirst method on the Linked List. And as you can see, each time we remove an item, the Head pointer moves to the next item in the list, but the Head of the list is always the Head of the Queue. Now you might be wondering why AddLast and RemoveFirst? Why not AddFirst and RemoveLast? Well, it turns out there is a good reason. Like the Stack, all of the items in the Queue can be enumerated over. The expectation of the caller would be that the items would be enumerated in the same order that they would be Dequeued. So, in the example we just saw, we would expect the enumeration to have worked 1, 2, 3, 4, 5. So, by storing them in the list in the Head to Tail order, we can use the list to perform the enumeration for us; it just works. So now, let's go to Visual Studio and look at an implementation of a Queue using a Linked List.

Code: Linked List Implementation
Here we have the code for the Queue class implemented using a Linked List. Now, what we'll see here is that this is actually a pretty simple implementation of a Queue. The two methods that are the most interesting are Enqueue and Dequeue. Enqueue simply adds the item provided to the end of the list. It doesn't have to worry about whether the list is empty or what other items might be in the list. The item being added should simply go to the end of the list. Dequeue will simply take the first item in the list, store it off into a temporary variable, remove the first item, and then return the stored value. So, it's pulling off the first item in the list and returning it. If the list is empty an exception is thrown. Now, because it's a collection type, I've added some of the general things we expect, for example, Peek. Peek is going to return the first item or the Head item in the Queue. If the Queue is empty, an exception is thrown. Otherwise, the first value is returned. Count returns the number of items in the list, which is equal to the number of items in the Queue, and Queue clears the list, which clears the Queue. Because our type is enumerable we provide the Enumerator methods, and all we need to do is defer to the list. Now, being able to defer to the list is only possible because we're storing these items in first to last order; we're adding the items to the end of the list, which means the value that we want to enumerate first is the front of the list, and that's exactly how a list enumerator works, and that's why we store items in that order. So, as you can see, implementing a Queue using a Linked List is quite simple.

Demo: Queue
What I'd like to show you now is a very simple visual demonstration of a Queue. So what we have is a Queue which has six visible items. The Queue in fact goes on infinitely, but we'll show the first six items. And when I click the Create button, it's going to start adding items to the Queue, and it's going to add them from the Head of the Queue, which is the first or leftmost item, and it's going to add them to the right. So, we're going to click Create, and as we can see, we're adding items to the Queue. Now, I'm going to have you imagine that these items are some unit of work; these are messages or commands coming in from some process, and the first command is 104, and then eventually we pick up Process and we execute 104, and we can see what we just did here, and now we can execute the next value of 94 and 24, but while we're processing 24, maybe a few more items come in, and then we'll start to process those, and a few more items come in and we'll process those. And what's nice about storing these items in a Queue is that even though we're not processing them quite as a fast as they come in, we're also not losing the values. And this is one of the reasons Queues are so popular in computer science; they're a structure that allows us to take incoming data and store it in a way that allows us to process it later, but in the order it showed up, which is a sort of fairness that you often look for in computer science. So eventually we process all the items and the Queue is empty. Now, I want to show you very briefly how this was implemented, because it's something we'll look at again in a future clip. So, this is implemented as a very simple WPF app, and this is not meant to be a great example of how WPF apps should be written; it's meant to very verbosely demonstrate how this might be done. So, as you can see, we have all the controls we were just looking at, and there are two buttons that do work, Create and Process, and what Create does is it Enqueues into our Queue of integers a random number between 0 and 200. And then it calls this UpdateGrid, and what UpdateGrid does is it draws the integer values within our grid. Now, I went with a very, very linear way of doing this to make it very clear what's happening; we clear everything out, we enumerate over the Queue using the Queue's enumerator. And if we're within the first six items we'll just display that in the appropriate label, otherwise we'll breakout, and that's why we're only showing six items. So, it's a very straightforward demo. But, what I think it does nicely is it demonstrates visually how items exist in a Queue; where they're put into the Queue, and then taken out of the Queue, and then can be put in and taken out.

Array Implementation
Now I'd like to look at how to implement a Queue using an Array for item storage rather than a Linked List. Now, this is quite a bit more complex, but it's worth understanding. When we allocate the Queue, the backing store is an empty Array with five slots. Now, as items are Enqueued, they're added to the Array from the front to the end. Now, at this point, the Queue has five items in it, and the Array is full. So, if Dequeue were called, the first item would be removed. This has opened up a slot in the Array. A second call to Dequeue would free another slot. And what happens if we call Enqueue again? Well, the value is added to the first open slot. Now we have a Queue whose Array has four of five slots filled. The Head item is the third item; that is, it's the next item that will be returned when Dequeue is called. The Tail item is the first Array item; that is the last item in the Queue. There's also an open slot in the Array. That slot will be filled the next time Enqueue is called. So, the question now, with the Array being full, is what should happen if Enqueue is called yet again? Should an exception be thrown, or should the Array grow to accommodate more items? Well let's take a look at the case where we choose to grow the Array. The first thing we'll need to do is allocate a longer Array. Now, this Array is twice as long as the original Array. Next, we'll need to copy the items from the Head pointer of the Queue to the End of the existing Array. Now this copies items 3, 4, and 5. Notice I said the Head pointer of the Queue to the End of the Array. Now unless the Head pointer was at index 0, we haven't yet copied everything, so now we need to copy everything from index 0 of the Array up to the Head pointer-1. It's Head-1 because we already copied the Head value. So, we copied 3, 4, 5, and then wrapped around and copied 6, 7, and you can see our new Array contains the values 3, 4, 5, 6, 7. Now this Array now becomes the backing store for the Queue, and the Head and Tail pointers are updated to their new respective indexes. So as you can see, Array growth is quite a bit more complex. It requires memory allocations, and it requires copying data around in a nontrivial manner. But, the performance benefits of using an Array, as they were with the Stack, are somewhat significant; all the items are kept together closer in memory, the allocations are reduced to periods where the Array is growing, and Enqueuing or Dequeuing items from the Queue requires only setting an Array value and modifying some indexes. So, let's go to Visual Studio and take a look at a Queue implemented using an Array as the backing store.

Code: Array Implementation
Here we have the code for a Queue implemented using an Array as the item store. Now, you recall from the slides that this is actually a significantly more complex implementation than a Queue using a Linked List. Now, we start by allocating the Array of items that we're going to use to store. Now, I allocated an Array of 0 items, and this means I don't need to do null checks later on, and I can just do length checks each time I'm adding. Size, is the number of items currently in the Queue. Size will always be less than or equal to the capacity of the Array. Head is the index of the first item in the Queue; this is the oldest item; it's the one we intend to return next time DQ is called. Tail is the index of the last or the newest item in the Queue; this is the last item that was Enqueued. So, let's take a look at Enqueue, and as you can see it's substantially longer than the previous implementation using a list. So, the first thing we do is we check; is the size of the Queue equal to the number of items that we can store in the Array, and if it is, that means we need to grow our Array. Now this could happen because it's the first item we've Enqueued and our Array has a size of 0, or it might be because we've Enqueued so many items that the Array became full, and now we need to grow it to put one more item in. We make that determination right here. If size is 0, we'll allocate an Array of 4, otherwise, we'll double the size of the Array. We then actually perform the Array allocation. Now if size is > 0, we need to copy the items from the current items Array to the new Array. Now recall from the slides that we can't just copy the items from Array index 0 to the End of the Array. Rather, we need to copy the items from the first or Head item of the Queue to the Tail item of the Queue, and we might need to wrap from the end of the Array back to the beginning of the Array as we do this. And it's that wrapping part that we check for here. If the Tail of our Array, which is the last item we inserted into the Array, is < head of the Queue, which is the oldest item in the Array, well that means we must have wrapped around, because that means that Head, for example, might be Array index 7, but Tail is Array index 3. So, the Queue runs from Array index 7 to the End of the Array, and then from Array index 0 to Array index 3. So, what we're going to do is first copy the items from the Head to the End of the Array, and then copy the items from 0 to the Tail. Once we've done that, all of the Array items have been copied from the old Array to the new Array, with the wrapping all accounted for. If we didn't need to wrap around, all we need to do then is just copy the items from Head to Tail. now regardless of the state, we've come to a point where all of the Queue items are in the new Array, and a nice side effect of this is that the head is at index 0 in the new Array, and tail is at the new targetIndex that we wrote to. And then, as we saw in the slides, the last thing we do is we copy the new Array over the old Array, and we update our Tail pointer. And now, we can add the new item into the Array, increment the size, and Enqueue is complete. That's quite a departure from our previous version of Enqueue using a Linked List; that had a single line of code which just added the item to the end of the list. But Linked Lists need to worry about wrapping or efficient storage. We don't want to waste space that we've allocated, so we need to do the extra work to make sure we make the best use of it. Now, Dequeue is a little bit simpler, and this is because Dequeue never has to worry about growth; it only has to worry about shrinking. So, what Dequeue is going to do is store off the value. This is what we're going to return to the caller. Then we're at one of two cases. If head is at the last index in the Array, we need to wrap all the way so head is at 0. Otherwise, we need to move head forward one. So, think about what I mean here. If we had an Array that was four items long, and head was at index of the fourth item, which would be index 3 in C#. When we pull that item out, index 3, or that fourth item, is no longer a valid item in the Queue, so we need to move the Head pointer forwards, but we can't move it forward beyond the bounds of the Array; we need to wrap it back around to the 0th index, or the first item in the Array. We then decrement size because we removed an item and we returned the value. Everything else is pretty straightforward. Peek simply returns the item at head, Count returns size, and Clear simply resets the values to their defaults. Size is set to 0, head is set to 0, and tail is set to 0. And finally, we have the Enumerator. And here again the enumeration becomes a little more complex than what we saw when we had the Linked list. But, this is pretty much the exact same logic we saw during the Enqueue growth period. Notice we have these two loops and then a third loop, and they looked quite similar to these loops here. What we're doing is we're returning the values in Queue order. Queue order is from head -> end and then 0-> tail, or head -> tail. And that depends on whether or not we're wrapping around, as we are in this case, Or whether we're just enumerating linearly without wrapping around the Array, which we are in this case. So, it's not incredibly complex, but it's a heck of a lot more complex than just returning an eliminator from a Linked List structure. And then the other GetEnumerator will just defer to the one we wrote. So as you can see, an Array implementation is quite a bit more complex than a Linked List implementation, but it comes with the benefits that Arrays have over Linked Lists, which include data locality and performance gains, as well as reducing the overall number of allocations and incredibly fast Enqueue and Dequeue times when there isn't an allocation being performed. So, I hope it's clear why a Array back stores are quite popular, and in fact are the default for many Queue classes implemented in the real world.

Priority Queue
Queues show up in software design very frequently, and quite often they're implemented as a specialization of the Queue, known as a Priority Queue. Priority Queues differ from normal Queues in that they are not First In, First Out, but rather, they return the highest priority items first, regardless of the order in which they were added to the Queue. A good analogy might be a police station call center. People call the police for all sorts of reasons, and each reason has a specific priority. Life and death issues are a higher priority than complaints about noisy neighbors. So when the officer starts his shift, he has nothing in his Queue. Eventually, a call comes in with a noise complaint. Since it's the first call of the day, it's immediately the highest priority item in the officer's Queue. So, the officer begins the trip to the area of the complaint, but before he arrives, a higher priority incident occurs, an auto accident, and this now becomes his focus and the noise complaint falls further in the Stack. While still dealing with the auto accident, a call comes in from a store that has experienced a theft. This is more important than a noise complaint, but not as important as the auto accident. Finally, another noise complaint comes in, and being of equal priority to the existing noise complaint, it's added to the Queue at the same location. So, you can see how a Priority Queue ensures that eventually every issue will be addressed, but the issues will be addressed in priority order. So let's go to Visual Studio and take a look at a Priority Queue implementation, and then see it in action.

Code: Priority Queue
So here we're back in Visual Studio and we're looking at the code for a Priority Queue. Now, to keep our focus on what makes a Priority Queue different than a normal Queue, I've decided to implement this using a Linked List. This will allow us to not have to worry about the Array growth items that make the Array-backed Queue so much more complex than a List-backed Queue. So let's start with the Enqueue method. Now recall, with a normal Queue, Enqueue is a single line long; it's simply an AddLast. But now it's a little bit longer, so let's figure out why. Well, we start with an empty list, and if the list is empty, we just add the item to the end of it. But if the list isn't empty, we need to find the proper point at which to insert the data. So what we do is we start at the front of the list. Now, the front of the list is the highest priority item. So we say, while we're not null, which we won't be the first time through because we do have an item in the list, check is the current value > the item we're adding, and if it is, then we need to go to the next item in the list because the highest priority item remains in the front, so we'll continue to loop around until we find an item in the list where the item we're adding is a higher priority, and when that happens, we'll breakout. Once we've broken out of the while, we check, are we null, and if we're null, that means we made it all the way to the end of the list without finding an appropriate point to add, so we just add ourselves to the end. But if we're not null, that means that we want to add ourselves into the middle of the list. Now our Linked List class in. NET provides an AddBefore method, and that allows us to add our item before the current item, and in this case, the current item is the item we found whose value was less than or equal to our item. So, Enqueue just walks the list of items, finds the appropriate place to insert our item, and performs the insert. Dequeue is exactly the same as it was in the Linked List example. In fact, the rest of the class is exactly the same as the Linked List example; it's only Enqueue that needed a change for the Priority Queue to work.

Demo: Priority Queue
So now we're back to a demonstration that might look very similar to what you've seen before. This is that visual representation of a Queue that we saw earlier in the module, but now I've changed it slightly. I've changed it so that we're using a Priority Queue. In fact, the only difference between the previous implementation and this implementation is that I changed the type of Queue from a regular Queue to a Priority Queue. So as before, we're going to click Create to add an item. Now, if a number has a higher value than another, it's perceived to have a higher priority. So, our first value is 52. Now we'll find out if the number after it has a higher priority. Well, it's 173, so it became the Head. Now if, this were a regular Queue, 52 would still be the Head. So, as we create more items, we can see they're added in priority order, and now when we're ready to process them, we process them in that priority order, from highest priority to lowest priority, and we can continue this process over and over again, adding and removing items at will, but we can be assured that whenever we begin to process items, at that moment, we're processing the highest priority item. So, let's take one look at the code just to see that what I'm saying is true. We'll go into our Enqueuing, and this is the exact same method we had before; we Enqueue a random number between 0 and 200, but this time, instead of going into a regular Queue, it's going into a Priority Queue. So, with that one little change, we fundamentally changed the behavior of the application, from processing items in a First In, First Out order, to processing them in a highest priority order.

.NET and C++
I hope it's clear by now that Queues are a commonly used Data Structure and one that has a place in every developer's tool belt. In fact, implementations of Queues exist in most modern languages and frameworks. In C#, there is a Queue class that looks very similar to the one created during this module. It uses Enqueue and Dequeue, and is implemented as an array. In C++, the standard template library includes a Queue class as well. The C++ Queue class differs from the. NET class in that it uses the Stack conventions of push and pop to add and remove items from the Queue. Also, like the Stack class, the pop method does not return the value being removed from the Queue, rather, the front method is used to retrieve the value from the Queue. Also, unlike. NET, C++ includes a standard Priority Queue.

Summary and Reference
In this module, we've seen how Queues are a First In, First Out collection type. We've seen how to create a Queue using a Linked List, and how this led to a simple implementation. And we looked at a Queue using an Array. We learned about Priority Queues and how they're a specialization that returns the highest priority item from the Queue first, and we looked at implementations of the Queue class provided by. NET and C++. Finally, I want to leave you with some references to the Queue class on MSDN. The first link is for the. NET Framework Queue class, and the second is for the C++ std Queue class.

Algorithms and Data Structures: Binary Trees
Introduction
Welcome to the Pluralsight Algorithms and Data Structures course. My name is Robert Horvick, and in this module we're going to be learning about Binary Trees. In previous modules, we've looked at structures that link data into chains. In this module we're going to learn about structures that order data into a Tree. We'll learn about how this Tree can be used to create a Binary Tree, or more specifically, a Binary Search Tree. We'll see how to Add and Remove items from the Tree; we'll search for data within the Tree; and we'll learn how to traverse or enumerate the data in the Tree using three different node ordering algorithms.

What is a Tree?
In the Linked List module, we learned how chains of nodes can be created by linking data together into a Linked List. This created a linear structure in which we could navigate forwards, or in the case of a doubly linked list, backwards. Trees are similar to Linked Lists in that they chain together nodes of data, but they do it in a hierarchical rather than in a linear manner. Let's look at an example of a Tree structure we're probably all familiar with. This is a corporate reporting chart. At the top is a Vice President responsible for a business unit. The Vice President has three direct reports, two Team Leads, and a Consultant. Each of these reporting relationships is represented by a line, and each of those lines represents a link in a unique chain between the Vice President and the subordinate. We can see this repeat when we go further down the Tree. Each Team Lead has several direct reports, and the Consultant has none. Now, there are a few properties to this Tree that I think are important to discuss. Let's start by sharing some common terms. In this Tree, the Vice President is the root or the top node; it's sometimes referred to as the Head node. The Consultant, and all of the unnamed team members, are said to be leaf nodes or terminal nodes. The Team Leads and the Consultant are child nodes or children to the Vice President, and the Vice President is said to be their parent. So, each Team Lead also has children and is itself their parent. Now, let's talk about the relationship between the nodes. Each node is capable of linking itself to any arbitrary number of children. You can see the Vice President has three children, each Team Lead has four, and the Consultant has none. There's no inherent limit in this example to the number of children that a node could contain. The limit that does exist is that each node has exactly one parent. Now this causes a very important implication, and that is that there is exactly one path from the Vice President to any other node in the Tree, and likewise exactly one path from any node in the Tree back to the Vice President, and therefore, there is exactly one path that can be taken between any nodes in the Tree. That limitation, a single path between nodes, is a fundamental rule that the Tree structures we'll be looking at will never violate.

Binary Trees
Now that we've seen what a generic Tree structure is, let's look at the specific Tree structure this module is all about, the Binary Tree. A Binary Tree is a Hierarchy of Data with some structure rules. It starts out with a Root Node. This is a node that has no parent, and at the moment it has no children. Now we can create zero, one, or two children. In this example, we now have two children; we have a Left Child, and we have a Right Child. Now, each child is itself a tree with the exact same structure limits as the parent. In this case we're showing Left Children and Right Children. So, unlike the previous Tree structure, the Binary Tree has at most two child nodes, thus the name Binary, and those children are known as the Left and the Right Children. Now that we understand Trees, and specifically Binary Trees, we can discuss Binary Search Trees. A Binary Search Tree doesn't change the structural rules of the Binary Tree, but it imposes an additional data rule, and that is that all the values in the Tree are stored in Sort Order; the smallest values are on the left, and the largest values are on the right. We start with the Root Node. In this case, the node has the value 4. The Root Node has a Child with a value 2. Because the value is less than 4, it becomes the Left Child of the 4 node. The Root Node also has a Child with the value 6. Because this value is greater than 4, it becomes the Right Child of the Root Node. And this simple set of rules is followed recursively throughout the Tree. And now once the structure is created, we can see that it's sorted in a way that the left-most node contains the smallest value, and the right-most node contains the largest. This is an invariant rule in the Binary Search Tree, and one that we'll rely on going forward in the module.

Adding Data
Let's dive deeper into the Binary Search Tree by seeing how to add items to the Tree. Adding items to the Tree is performed with a recursive algorithm. Now, throughout the module, we'll see that Trees lend themselves well to recursive algorithms, and that this can help use ease our understanding of Trees. So, let's start by adding some data. Initially we have an empty tree, so any node we add will automatically become the Root Node. In this case we've added the value 4. Now we want to add another value; in this case it'll be something smaller than 4. Because it is a smaller value, the value is added to the left of the Root Node. But what happens if we add something even smaller to the Tree? It'll be recursively added to the left. Since 1 was smaller than 4, it had to go to the left, and since 1 was smaller than 2, it had to go to that left. Now let's add something larger. Since there is no node to the right of 4, any larger value will become the first Right Child of 4, and just like the recursive add to the left, if we add an even larger value, it will be recursively added to the right. Since 7 is larger than 4, it must go on the right of 4, and since 7 is larger than 6, it must go to the right of 6. But, what will happen if we add an Equal Value? For example, if we add the value 4 when we already have the value 4 in the Tree? Well, we're going to treat it as a Larger Value. Now, that's a little bit odd that we would treat 4 as larger than 4, but ultimately we only have two choices; we can either send the node to the left or to the right, and that will be determined by whether we view it as being larger or smaller. So, we're going to view it as being larger. So, the 4 will go down to the 6, where it's determined that it's smaller than 6, so it will go to the left. So, you can see that when we added the values, using the simple left, right, greater, smaller comparisons, allowed us to build up a Tree that satisfies the Binary Search Tree requirement of having the smallest value in the left-most node, and the largest value in the right-most node.

Finding Data
So far in this module we've looked at the physical structure of the Binary Tree, and we've looked at the data ordering requirements of a Binary Search Tree. And now what we're going to do is see why those data ordering requirements make the Binary Search Tree a really efficient structure for searching for data. On the left, we have the algorithm for finding data within the Tree, and on the right we have the Tree we're going to be searching. And let's start by just walking through this a few times. We're going to look for the value 3 starting at the Root Node, and the Root Node is the node containing the value 4. So, that node is not null, so we skip that; 4 is not equal to 3 so we skip that, but the provided value 3 is less than the current value 4, so what we're going to do is now recursively call Find, this time passing in the Left child of 4, the 2 node. So now we start this process over again. Is the current node null? Well it's not because it's the 2 node. Does 2 equal 3, well is doesn't. Is 3 less than 2, well it's not. So, the last thing we can do is recursively call Find passing in the Right child of the 2 node, which is the 3 node. So, we say is current==null, well it's not, and is the current value of 3 equal to the value of 3, well it is, so we return the 3 node. And there we've now found the value 3 within the Tree, but notice that we did it only having to look at three nodes. Now imagine in a Linked List we could potentially have had to searched all of the items in the list. Now let's find the value 5. Well, we started the Root Node 4, and since 5 is larger, we need to go to the right. That brings us to 6. Well, 5 is smaller than 6, so we fall to the left, and we have found the value 5. But now let's look for the value 8, and this one's a little bit different. We start with the value 4, and it's larger than 4, so we go to the right. We get to the value 6, it's larger than 6 and it's larger than 7, and now from the 7 node, because the value was larger, we end up calling Find recursively with 7's right node. Well, 7 doesn't have a right node, so current. Right from the 7 node is null, so once we recursively call Find, well the current node is null so we return null, and ultimately the caller of Find will get a null node value back, and that will inform the caller than no value was found in the Tree. But even here notice in the case where the value did not exist in the Tree, we did not have to look at every node in the Tree to determine that; we only had to look at three of the nodes, and that's the property that makes a Binary Search Tree desirable compared to a structure like a Linked List. In a Linked List, the only way to determine that the value of 8 was not in the Linked List would have been to have looked at every node in the list. So here we've been able to determine that nodes were or were not in the Tree while looking at a subset of the data in the Tree, and that's a very powerful mechanism, and one of the reasons that Binary Trees and similar Tree structures are very commonly used in computer science.

Removing Data
When we looked at adding nodes to the Tree, it should be somewhat clear that it's really not that difficult to insert data into the Tree. So it might surprise you that Remove is actually quite complex. In fact, it's probably the most complex algorithm we'll look at in this entire course. So, Remove has three distinct phases. First, you find the node to be deleted. If the node does not exist, we exit. Now, remember from Find, that it's easy to tell if a node does or doesn't exist in a Tree, and it's pretty efficient to do that. Now, if the node that we found is a terminal node or it's a leaf node, you simply remove it from the Tree by nulling out the parent's pointer to the node we're deleting. Now, a key point here is when we find that node we need to know its parent. In the examples we'll be looking at, our Trees are not storing bidirectional links, so we're going to need to keep track of not only the node we're deleting but it's parent. And for a non-leaf node, what we need to do is find the correct child to replace the node we're deleting. And this is actually a little bit of a tough concept to grasp without showing some pictures; we're going to move onto that pretty quickly, but I want to let you know upfront there are three different scenarios, so this is quite in depth and I would encourage you to watch this a few times if you feel like you're not getting it, because this is really important to get right, and it's one of the most difficult parts of a Binary Tree. The first case we're going to look at is where we're removing a node that has no right child. So, if you look in the Tree, there are several nodes here that have no children to the right; the 8 node has no right child, and the 1, 3, 5, and 7 nodes have no right child. So, this case would apply to the removal of any of those nodes. We're going to look at the removal of 8, because it's the most interesting case. So, the first thing we do is we find the node to remove. The 4 is not it, 8 is larger so we end up on the 8 node. Now, the 8 node has no right child, so what we're going to do is promote its left child into its place. Now, I want you to think about why that's the case. Since there's no child to the right, there is no value underneath the 4 that is going to be larger than 8. So, when we perform the delete operation, we've now promoted that Tree, the 6, 5, 7 nodes, up in place where the 8 was. And this works because we haven't broken the invariant structure of the tree. Everything to the right of the Root Node is greater than the Root Node, and when you go down to the 6 node, everything to its right is greater than 6, everything that's left is less than 6. So, we've been able to retain the rules that the Tree must follow while only having to move one block of nodes around. So now let's look at the next case. In this case, the node we're removing has a right child that has no left child; that's a mouthful. So, look at what I mean here. Let's say we're removing the node 6. The node 6 has a right child 7 that has no left child. So, what's going to happen in that case, is the right child will replace the Remove node; let's take a look at that. So, we're removing 6, we have to find the node first. It's greater than 4, so we end up on the 6 node. The node to the right has no left child, the 7 has no left, so we're going to promote that right child, the 7 node, up to where the 6 node is. And now this is going to mean not just re-pointing 4 to 7, but 7 also has to become the parent of 5. So what we did there was we moved that entire right Tree up into the Remove node slot. And again, this works because we haven't broken the invariant structure of the tree. We know that everything to the right of 7 is going to be greater than 7, and everything to the left will be less than 7. So, let's look at the third case. The node we're removing has a right child that has a left child. So, on this Tree that would be the node 6 again. The 6 node has a right child 8 that has a left child 7, and what we're going to do, is the right child's left-most child node will replace the removed node. So, let's look at that. We're going to remove 6; first we find the node to remove. It's greater than 4, so we go to the right, and we've found 6. The node on the right has a left. So what we do is we find the right child's left-most child. So, the right child is the 8, and its left-most child is 7. And then what we do is we promote that node into the deleted slot. So, we're going to take 7 and move it up to where the 6 node was. And now notice we've retained the invariant structure. Everything greater than 7 is to the right, everything less than 7 is to the left. And the reason we use the left-most child is because we know the left-most child is going to be the smallest value in the Tree. So what we've looked at here is the three different remove cases. Remove is complex. There's a lot going on, and it's hard to explain it all, even visually, so I really encourage you to watch this clip over again if you're feeling like you don't fully understand it, and I would really encourage you to actually draw some Trees out, and think about what it would mean to delete some data, just like we did here on the screen, and we ran through some deletion scenarios; go ahead and do some more of those, and start to come up with other scenarios, like what if the left-most child in this case had had children on the right? What should that Tree look like? As long as the Tree you come out with follows the invariant rules of a Binary Search Tree, you're coming out okay, and you're not going to be able to do that unless you follow all the rules, so this is a great way to really reinforce some of these things and help you understand it.

Traversals
With every Data Structure we've looked at in this course so far, the ability to enumerate nodes in a well- defined order has been important. For a Linked List, we could enumerate from left to right or right to left. For a Stack, we would enumerate in a Last In, First Out order, and for a Queue in a First In, First Out order. But for a Tree it's not quite as obvious what to do, and that's because Trees have multiple links, potentially at every node. So, when you're at that node, should we go left first, should we go right first? And really you don't need to pick one over the other, you can pick all kinds of varieties, and we'll look at three. So, these Tree Traversals enumerate nodes in a well-defined order. The basic algorithm is you process the node that you're on, and then you visit the Left node, and then you visit the Right node, and at each of those nodes you follow that same algorithm; process the node you're on; this is the left, this is the right. What varies between the different algorithms is the order in which we do those three steps, and the three common orders that we're going to look at are Pre-Order, In-Order, and Post-Order. So, let's start by taking a look at a Pre-Order Traversal, and we're going to visit the Root Node of this Tree, and we want to traverse these items in Pre-Order. So, we start with the value 4; well 4 is not null, so we're going to Process that value. As you can see, we've now processed the value 4, and I've indicated that along the bottom. Now we're going to visit the Left Node, so now we've started over. If the Left Node is not null we process it, which we did, and let's go left again, and now we've processed it. Now if we try to visit the Left Node, we will find it's null, because there is no node to the left, so we would now visit the Right Node, which is also null, and we return. So, now we're back at the 2 node, and we've already visited the left, so now let's visit the right. That causes us to process 3. Again, it has no children, so visiting left and right doesn't do anything, we return from 2, which is now processed left and right, so it returns back to 4. Four is now going to process its right side. It starts with the node 6. The 6 node has a Left child, so it visits that; that child has no left or right children so it returns, and the 6 node now processes its right node, which has no children, so we're done. So, a Pre-Order Traversal processed the nodes in the order 4, 2, 1, 3, 6, 5, 7. And this might look somewhat random to you, but what's important is that the enumeration order was stable. We could enumerate this Tree using Pre-Order Traversal a thousand times, and each time it will enumerate in this order. Now, an In-Order Traversal is something that I think most people, when they think of how they want to enumerate, they go for. So, what happens here is we visit the left nodes first before processing, and that has an interesting characteristic that I think will jump out to you pretty quickly. So, we'll start with the Root Node, and we visit the left node, and now we're going to visit its left node. And since it has no left node we'll process it, so we process node 1. It has no right node, so we return. Once we return, we can now process node 2, and now we'll visit its right node, which has no children, so it gets processed, and then we return back to 4, we can now process it because we visited the left, now we visit it's right. At the 6 node we visit the left, which is the 5 node, which has no children, so we process it. It has no right child either, so we go back to the 6 node, process it, go to the 7 node, which has no left, so we process it, and then we're done. And in this case, we've been able to print out the Tree or process the Tree in Sort Order. The Tree is not stored 1, 2, 3, 4, 5, 6, 7, but we're able to process the nodes in that order. So, an In-Order Traversal is very commonly used when you have a Tree and you want to look at the items in Sort Order. Finally we have Post-Order. So what we're going to do is start at the Head node. We're going to visit left and then visit left again; now we'll process that node. Now, we're going to come back and we're going to visit right, and process that node. Come back and now we can process the error. We come back to 4, and now we have to visit the right, so let's visit the right, which visits the left. It has no children, so we process it, we return back to the 6 node, it has a right child, so we visit it, which has no children, so we process it. We return, we process the node, we return, we process the 4. So now we've seen three different Traversal mechanisms: Pre-Order, In-Order, and Post-Order. In-Order is the one that I think most people look at and say hey I understand that, I can see why I'd want to use that. But both Pre-Order and Post-Order do have well-defined uses. They're often used in mathematical expression evaluation, and they're also quite often used when you're doing evaluation of runtime behaviors in a language. For example, compilers use Trees quite heavily, and Traversals dictate which operations happen in what order, and what operations depend on other operations. So, if you think of this as a dependency graph, what we're saying is, Step 1 and Step 3 have to operate before Step 2, and Step 5 and Step 7 have to operate before Step 6, and then Step 4 is the last one to operate because it's the parent; it's operating on the sum of the operations of all of its children. I would encourage you to spend some time learning more about the Traversal mechanisms and seeing some of the context that they're used in.

Code: Binary Tree
We're in Visual Studio, and what I'd like to do is take a relatively quick look at a Binary Tree implementation that is included with this module. Now, I want to be clear, this is not a production quality Binary Tree implantation, this is a learning implementation. Some things are overly verbose, some things aren't done the way they would be done in production, but they're done in a way that helps make them clearer for you to understand. So, please keep that in mind when we're looking at the code, and if you're looking at using a Binary Tree in production, look to your platform and find out what's available. So, we're starting with the BinaryTreeNode. Now, remember from the slides, that the Tree is built up of nodes that contain data. This should be very similar to what we've seen with Linked Lists, Stacks, and Queues. The node contains a Value, and a Left child and a Right child. We also have the ability to Compare the node to other nodes, and that comparison is simply based off the value comparisons. Now, let's look at our Binary Tree. So, our Binary Tree is Enumerable, and the values we store are Comparable, and that's really important because to actually do a Binary Search Tree, every item you put into it has to be comparable to the values that are around it. So, if we couldn't compare them we'd be in trouble. So, we have the head of the tree, that's the head node, and the count is the number of nodes in the Tree. Count isn't really meaningful for the Tree structure, but it's one of those kind of little helpers that we like to use. Let's start with Add. Now Add is pretty simple; it has two cases. If head is null, we just assign head to a new node, and that's the case where the Tree was empty. And if head's not null, recall that we go into a recursive algorithm, where we call AddTo, providing the head node and the value. This is just like we saw in the slides. And just like in the slides, we decide is it < than the current value or is it > than or = to the current value. And that's all you need to do. And go ahead and look at the code, compare it back to the slides to understand it, but it's all very straightforward. If it's < than the current value we start heading left. If there's no left Tree, we make it the left Tree; if there is a left Tree, we recursively call Add. If we need to go to the right and there's no right Tree, we can make it the right Tree; otherwise we recursively call Add. Now, Contains is going to call a helper function called FindWithParent, and the reason we're calling FindWithParent, is so that when we get the value back we also get the parent node back, and this is really important, because remember that when we were doing Remove, Remove needs to know the parent of the node we're removing so that it can properly readjust the links. So, instead of having Contains operate differently than Remove, and instead of re-duplicating that code, I've just pulled out this helper method called FindWithParent, and Contains just ignores the parent, and if the value is not null when it returns, they contain nothing. So, let's go look at FindWithParent, because that's where the bulk of this is. FindWithParent actually isn't that complex of a method. We start with setting current to be the head, because we're going to start with the head of the Tree; we're always going to do that when we're trying to find a node. And what we're going to do now is say, while current! = null, and that's because this is not a recursive algorithm; we're not going to recursively call FindWithParent, we're going to iterate over the nodes. So, let's do a value comparison to find out if we're greater than or less than. If the value is less than current, we need to go left. So, we set parent to current and current to current. Left, and then we'll loop again. If the value is greater than the current value we're going to go right. So parent will be set to current, and current will be current. Right; otherwise, we have a match, and if it wasn't less than and it wasn't greater than, it must be equal, so we break and we return current. So all we're doing here is walking the Tree and making a decision; if it's equal I return it, if it's less than I go left, if it's greater than I go right, and when we're done the value we return is the found node and the out parameter parent will be set to the parent of the found node, or null if it was the head node that it found. So, that's how Contains ends up finding out if the value we're looking for was found. And now there's Remove, and Remove is where things get complex. Recall that there are three cases we need to concern ourselves with, and we're going to look at those somewhat briefly in order, and I would refer you back to the slides to understand these in more detail, because it's really a lot to cover in code, and the slides presented it in a more visual manner that might be easier to understand. So, the first step in every case was to find the node that we're going to remove and to find its parent. If current is null, that means we didn't find it, we'll return false, because nothing was removed. If current is not null, well then we've found what we're going to remove, so we can go ahead and just decrement count right away. It doesn't matter if we do it now or later, but doing it now is fine. So, the first case: The node we're removing has no right child, so the node we're removing left child replaces the current. And that's what we're going to do in this block of code. If parent is null, that means that we are removing the root node. So all we need to do is promote the Left child up to the Root Node. But, if parent is not null, then what we're going to do here is figure out are we updating the parent's left child link or the parent's right child link. Now, this is a bit of information we lost in the find method. We know what the parent node is, but we don't know if we got to the child to the left or the right, so we're just going to figure that out again. If we got to it from the left, then the values are less than, if we got to it from the right the value must be greater; it just tells us which parent link we're updating, but no matter what one we're updating, we're setting it to the deleted nodes left child. In the second case, the current's right child has no left child, so the current's right child replaces current. And again, we start by figuring out, are we removing the head node, and if we are, just promote the child. If we're not, again we need to figure out which of the links, the parent's left or the parent's right, should we set to the right child. And finally, if the deleted node's right child has a left child, replace the deleted node with the deleted node's right child's leftmost child; it's a little bit complex, so here's what we're going to do. The leftmost child is the current node, which is the node we're deleting, right child's left child as our starting point, and the leftmostParent is the parent of the leftmost child. And we just keep looping, and we're saying hey, while there's still a child to the left, keep going left, keep going left, keep going left, over and over and over again until we run out of nodes to the left. Then we start updating the sub-trees, and this is that same kind of logic again where if we're updating the head, then we just promote the left-most child. Otherwise, we need to figure out if it's the left or the right of the parent that gets the left-most child. And I understand I'm glossing over Remove somewhat quickly, but that's because we spent so much time on it in the slides. If you have questions about it, look at the tests that ship with this course, and spend some time running through them. Finally, we have the Traversals. And recall that the Traversals are all implemented as recursive algorithms. And what changes is in what order the processing versus the left and right walking occurs. So, the way I've implemented the Traversals is to take an action or a delegate that will get performed each time the node is processed. So, in the PreOrder case, if the node is not null, we perform the action, and then we go left and then we go right; remember that's exactly what Pre-Order does; it's process the node, go left, go right, and what we're doing here is a recursive algorithm that just loops around and around until we hit the null nodes, which are our terminators, and then we return. PostOrder is the same only instead of doing action Pre Pre, we do Post Post action; it's the same algorithm, we've just changed the ordering of the operations around. You should be able to guess it in order then, if it's not action Walk Walk and not Walk Walk action, it must be Walk Action Walk, or Left process Right. But, I wanted to show an example that wasn't recursive as well, because as great as recursive algorithms are, they have problems and they have limitations, and if you have a tree that had a million nodes, are you really going to call this recursively in a pathologically bad case potentially a million times? You know, you'd run out of call Stack, you'd crash your application; we don't want to do that, and no production quality Tree should do that. No production quality Tree should use this recursive algorithm. So what I've implemented here is I've just taken the In-Order traversal and I've made is non-recursive. And we make algorithms non-recursive using a Stack. So, what we're doing is we first say is the head null, that's is the list empty? If the list isn't empty we have things to traverse, so we create our Stack. And since it's non-recursive, we're only going to create one Stack during the traversal. Our current node that we're on is the head node, and since we're removing recursion, we need to know whether or not we should be going left or right, because when you're doing a recursive call, you call walk to the left, and that all returns and you go on to the next action, which is process the node, and then you call walk to the right, and that performs all the recursive actions and comes back, but in this case we're in a while loop and we need to know, did I already go to the left, should I be going to the right, what is the state that I'm in? And we use this as an indicator; we're saying should I be going left next? And in order traversals we start by going to the left, so it's true. So, goLeftNext is set to true. We're going to Push the current node onto the stack. Now imagine we made a call. In. NET when you make that call, what happens, well stack frame gets allocated, and the values that you pass to that call go on the stack, so what did we do? We put it on the Stack. While our stack count is greater than 0, this is while we haven't removed everything from the stack of if this were a recursive call, while we haven't returned from every recursive call, if we're going left, well let's start doing that. Let's start walking left and Push everything on the stack to the left because remember, this is a walk left first and then process then walk right. So we don't need to do this onesie, twosie, we can just say hey, go to the left all the way. If there are eight nodes we have to walk through let's walk through all eight of them, because that's what would happen. So, we walk all the way to the left, we keep track of what the current node we're on is, and we push it back onto the tree. Once we've walked all the way to the left, we yield that value, that's that C# syntax to return that value as part of an enumerator. So I'm going to walk all the way to the left and we're returning the value of the left-most node. Now, what we're saying is okay, we've processed the value, we've gone all the way to the left, so what do we need to do now? We need to go to the right. So, we set current to be current. Right, if it's not null, and now that we've done that, we need to start going left again, because remember we just made a call to the right, and what's the first thing that right node should do? It should go left. Now, if current. Right was null, what we're going to do is we're going to Pop the value we just processed off the stack, and since we've already processed that value and gone left on that value it's time to go right. So, goLeftNext is going to be false, so we're going to come back up to the top here, is goLeft, nope we don't go left because it's false, we're going to return the value, which is the one we have processed the children but not itself yet, and now if it has a right node we start processing it, otherwise we Pop another value off the stack. And I know it's hard to visualize this, but debug through this a few times, get comfortable with it, and understand how the stack is being used to emulate recursion. And also think about why this is a good thing. You know your Stack can hold a million items in it. And that's going to have memory constraints, and you know, there certainly are issues that might happen there, but you're not going to have the call stack problems that the runtime would have if you tried to make a million recursive calls. So, you've shifted the problem from a runtime call Stack depth problem to a memory constraint problem, and that's one that people tend to be able to understand and handle a little more gracefully. I provided a GetEnumerator that simply returns the InOrderTraversal. Clear sets head to null and count to 0, and count simply returns count, so that's all at the end things we've seen before and could understand.

Demo: Sorting Words
What I want to show you now is a really simple usage of the Binary Tree class that we created, and how it can be used to print out data in Sort Order. So what we're going to do is create the Binary Tree and it's going to hold strings. We're going to read input from the user, and as long as they haven't type d the word quit, what we're going to do is prompt for the input, Read the input, and split it on the space character. We're going to add all those words to the Tree. We're going to print out the count of words just so we can see the count in action, and then we're going to print out the trees using the enumerator. Now the default enumerator for the tree is InOrder. So that will print out the words in Sort Order. And we're going to print a Line, and we'll clear the tree and loop back. So, you can see nothing in here has to do with Tree logic, this is all just using a Tree, but it gives us a pretty powerful behavior, and I'll show you what that looks like. So, here's our prompt, and what we can do is start with Hello world. Now, Hello world is already in Sort Order. But if I were to type World Hello, it's going to flip it around, because hello has a lower Sort Order than the word world. Likewise, if I were to do a b c d, they'll print out in Sort Order. But, if I do d, c, b, a, they're going to reverse their order. So this is the example of where arbitrary input can be put into a Tree pretty efficiently because we've seen how Add is able to operate pretty quickly, and then we're able to enumerate those items in Sort Order. And this is a great example of where a Binary Search Tree does pretty well. It's able to take our input, and without a whole lot of effort, keep it in assorted order for us.

Summary
In this module, we've covered quite a bit, but ultimately what I want you to get out of it is an understanding of the Binary Search Tree, and how its two rules of smaller values on the left, larger value on the right, help make it one of the most useful structures in computer science. We looked at how to Add and Remove items, how Add was somewhat simplistic, but Remove was actually quite complex. We found out how to search for items in the Tree, and we learned about three different Traversal methods, Pre-Order, In-Order, and Post-Order. I hope in this module you can walk away with a solid understanding of what a Binary Tree is, and have a good idea of what your next steps are to learn how to use them efficiently, and to learn more about the use of them can benefit you in your software development.

Algorithms and Data Structures: Hash Tables
Introduction
Hello, and welcome to the Pluralsight Algorithms and Data Structures course. My name is Robert Horvick, and in this module, we're going to be looking at the Hash Table Data Structure. We'll start by taking a very high-level look at what Hash Tables are, and how they fit into the broad category of structures known as Associative Arrays. Next, we'll spend some time looking at Hashing. What we'll see is that while Hashing may appear simple, it's actually quite difficult to get right. Once we have a solid understanding of those concepts, we'll see how to Add items to a Hash Table, and then how to Remove them. Then we'll see how to Search for items in a Hash Table, and finally, how to Enumerate the items in a Hash Table.

Hash Tables
Hash Tables are a type of Data Structure that implements an Associative Array. Associative Arrays provide the Storage of Key/Value pairs into an Array or an Array-like collection, but unlike an Array, the index can be any comparable type, not just an integer. So, besides being comparable, the only other restriction is that an Associative Array generally only contains unique keys. Multiple keys may refer to the same value, but the keys themselves should be unique. So let's consider the Associate Array shown below. This is conceptually what our Hash Table looks like. If we want to add an item to the Hash Table, the first thing we need to do is find out what index in the Array the item should go into. Now, let's say in this example the Hash Table is storing objects that represent people. So, let's try adding a person, Jane, to the Hash Table. What we'll do, is we'll take the object represented by Jane, and we will find the index that the value Jane should be stored at, using Jane's name as the key. So, we have a method called GetIndex that takes a string, and it returns an integer that is the index into the Array where Jane's object should be stored. Now, we can repeat this process for other names. Kelly would get stored somewhere else, and Steve in yet another location. So, what we've seen here is that we're able to add these objects that are people objects into an Array based on their name as the key. So we don't know that Jane is stored at the third Array index, but we know we have a method, GetIndex, that when given the string Jane will return the #2, which is the third index in the zero-indexed Array, and allow us to store or retrieve Jane from that location by name. So, that's a really high-level overview of what a Hash Table is; it's a way for us to store Key/Value Pairs into an opaque Data Structure that we view as an Associative Array using a key other than the Array index to add and remove the item.

Hashing Overview
In the previous slide, we saw that there was a method, GetIndex, that I kind of glossed over. This method took a string as an input, and returned an integer representing what Array index the value should be stored at within the Hash Table. The big question then is what did this method do? And the answer is that it Hashed the string. Hashing is a process that derives a fixed size result from an arbitrary input. And what I mean by that is that any string of any length when Hashed would return a fixed size, or in this case, a 32-bit integer Hash value. Fixed size simply means that every input returns a Hash code of the same size or type. The Hash codes we'll be looking at are 32-bit integer values, but some Hashing algorithms return smaller values, and some return much larger values. For example, there are Hashing algorithms that can return Hash codes that are thousands of bits long. But, for the sake of time and simplicity, we'll be looking at Hashing algorithms that return integers. Now, Hashing algorithms have several properties. One is an invariant, and some are ideals. The first is stability, and a stable algorithm returns the same output given the same input always. So, if you pass in the same string a million times, you should get that same Hash code value back a million times. It's an invariant; every Hashing algorithm has to be stable; if it's not stable it's not useful. Uniformity is another thing that we really look for in Hashing algorithms. Now, a uniform Hash is a Hash that is distributed evenly throughout the available range. So, if we're talking about 32-bit integers, that's really 4 billion values, give or take, that we're working with. So, if you pass in a million strings, you'd expect to see, you know, roughly a million values. Now, there are some mathematical rules that basically say, hey look, you're going to have Collisions, you're not going to have perfect uniformity, and it's impossible to have perfect uniformity, because there are certainly more than 4 billion potential strings in the universe. So once you've given more strings than there are potential integer values, you're going to have some inputs that produce the same outputs. Efficiency is another important feature of a Hashing algorithm. If the Hashing algorithm isn't efficient either in time or in space, those are things you might not want to use if you're talking about something that needs to be done billions of times in a process. You know, if it takes 3 milliseconds to do something, and you need to do it 3 billion times, that's a long time. If you can cut that down to 3 microseconds, you have saved multiple orders of magnitude off your processing time, and that's something that you need to look at when you're Hashing. Finally, we have security. For some problem domains, security is second only to stability, and what a secure Hash says is that given a Hash value, finding an input that could derive to that same value is infeasible. So, if I Hash a string and I get a result back, I don't want to be able to give that Hash code to someone and have them be able to figure out another string or perhaps the original string that led to that Hash value. We won't be talking a lot about secure Hashing in this module, but I just wanted to raise it as an issue, that the Hash algorithms we're going to look at aren't secure, and there are secure algorithms available. So, if you're looking at a problem space where your Hash value is being used for something that requires security, please make sure you're researching the Hash algorithm you choose carefully.

String Hashing
What I'd like to do now is take a look at a couple of string Hashing algorithms. The first is an incredibly naïve implementation. And what it does is it sums the ASCII value of each character in the string and returns that sum as the Hash value. For example, we have the string foo. The ASCII values of f are 102, and the o and the o are 111, respectively, twice. So those three values added together = 324, and that would be our Hash value for the string foo. Now, when you think about our four measurements for a Hash algorithm, this is stable, and it is efficient, but it's really a terrible algorithm. First of all, it's not very uniform, and it's not very uniform because the order of the characters in the string don't affect the value of the Hash. So, foo Hashes the same as oof. That is not a characteristic we're looking for in a Hash algorithm. And this algorithm is ridiculously not secure. I mean, if you give it the number 324, you can just start pulling ASCII characters until you find the value you want. There is nothing even remotely secure about this. So, it is stable, it is efficient, it's a single pass, it's just integer math, but it's not a good choice; it's about as simple of an algorithm as you can get though. And what it really demonstrates, though, is there is a very, very simple way to derive an integer value from a string. So, let's look at a slightly better algorithm now. It's a little bit better, and what it does is it "Folds" every four characters into an integer. And remember, we can do that with ASCII, because each ASCII character is 8 bits, one byte. An integer is 32 bits, or 4 bytes, so we can take four ASCII characters, cram them together and say hey, it's an integer; and that's what we're going to do here. So, we have a string, it's a lorem ipsum, and we're going to take the first four characters. And when you take the bytes of those characters and then just cram them together into a 32-bit value, we end up with a 1. 7 billion value. Now, at the moment, that's our current Hash value. So now let's look at the next four bytes, and they have their own value. Let's add our current Hash value with the new value; now it wraps back around, because we're doing 32-bit math. So, when you add two really big numbers in a 32-bit space they wrap around to a negative value. Let's look at the next four values. They create their own large integer, which we add, and now we're still with a negative number. Take the next four, cram them together, get our 4 bytes, add it to the current Hash, and then we take our next four, but there's only one, so it's all zeros and then one value; that ASCII value's 114, we add it, and we have our Hash value. So, ultimately here our Hash value is a 1. 7 billion number. Now, this is stable, it's pretty efficient, and it has good uniformity. The ordering of the characters in the string directly affects the Hash value; so foo is different than oof, but it's not secure, it's not even remotely secure. We could treat this as basically additive. But even though this algorithm isn't secure, it's good enough for what we're going to be doing for string Hashing. So, let's go ahead and move on and just look at a comparison of some of the Hashing algorithms we know about, and a few we don't know about, but which come with the. NET framework. So first, don't write your own Hashing algorithm. Okay, the. NET framework ships with several, they're good, these are things thought out by mathematicians, by really smart guys over many years that are vetted out by the community, hackers have attacked them for years, a lot of the problems have been found, and solutions are being derived from those. Don't create your own Hashing algorithm. I can't stress that enough. The next thing is to pick the right Hash for the job at hand. Look at the characteristics of the problem space that you're working in, and pick the Hashing algorithm that applies best to it. Let's look at a couple Hashes here. And now, bear in mind, my judging of the Stability, Uniformity, Efficiency, and Security are really fluffy. I mean, these are debatable points, and I don't want to dig too deeply into them, but they're just some kind of high-level thoughts on it. The first thing we looked at was our Additive Hash. This was the Hash that summed up all the values in the string. It was Stable, it was Efficient, not Uniform, not Secure. Our Folding Hash; this was the Hash that crammed 4 bytes at a time into an integer value. You know, it's Stable, it's pretty uniform, certainly more uniform than the Additive Hash. It's efficient, it's a single pass through, it's integer math, but it's not secure. CRC32 is a really common Hashing algorithm. Like the Folding algorithm, it's Stable, Uniform, and Efficient, but it's not a secure algorithm. In fact, there are instances where the CRC32 algorithm has been the root cause of a compromise. The MD5 Hashing algorithm is very commonly used and it's provided with the. NET framework. Now, it's a good balance of stability and uniformity, and I have a red X for Efficient, and you know I did that because as a comparison to Folding and CRC32 it's not that efficient, but really it's a good Hash. I mean, you can do this a lot; you can do this million of times really quickly no problems. If you're looking for a Hash algorithm and don't need security, take a look at the MD5 algorithm, see how the performance characteristics work for you. Finally, we have SHA-2. And I call out SHA-2 specifically because SHA-1 has known problems. So, SHA-2 is stable, it's uniform, it's secure, it's not that efficient, but remember, security comes at a cost, and that cost is going to be efficiency. So it's not a surprise that it's not that efficient. But, it's a great choice if you need secure Hashing. Take a look at the. NET framework classes for it; if you need a secure Hash it's a great starting place.

Demo: String Hashing
What I want to show you here is three simple Hashing algorithms, just to have more of an understanding of what's going on behind the covers. So what this example does, is it just loops on a Console window, prompting for some input, reading the input, and then printing out three different Hashes; the Additive Hash, which is the Hash we looked at where the sum of all the ASCII values are returned; the Folding Hash, which is the Hash we looked at where each 4 bytes in the string are compressed in an integer, and those integers are summed together; there are slides for both Additive and Folding if you want to go back and look at those to refresh yourself, that would be great. And then the last one is called the Djb2 Hash; this was the Hash that was published on the comp. lang. c newsgroup quite a few years ago. You can find a lot more information about it online; in fact I have a link to it in the source code. So, if you're curious you can take a look at it; learn more about its characteristics. I put it in here just to show you another really small tight Hashing function that does some different things, it's a little bit kind of off the trail that we've looked at so far, and it's not a bad jumping off point to learn more about Hashing. So, the Additive Hash, we store the currentValue, we increment every time and return the currentValue. One convention I want you to notice here is that I'm using unchecked, and the reason is that in C# when you're doing integer math, if you overflow the integer, it's going to throw an exception, unless you have it in an unchecked block, in which case it's going to wrap around from positive to negative. So, it's just something that we need to do in order to have the integer math work the way we expect. Djb2, if you really want to understand more of what's going on, like this magic number of 5381, and why we're doing the shifting as opposed to multiplication, you know, go ahead and look at the link for it. I don't want to dive too deep into it because it's not really the point of this. The point was just to show you some different Hashing algorithms and how some of them are simplistic and small, you know, six lines long, and some are longer. And then we have the Folding Hash. In here, this is where we're compressing every 4 bytes down into an integer, and what it's doing is this GetNextBytes method is doing some bit shifting to build up the currentFourBytes, Getbyte, overturn the byte or 0, if we're out of string characters. You know, go ahead and take a look and understand it a bit more, but it's pretty straightforward code. Again, the point is to give you some examples of different algorithms, not necessarily to show you the best or the right way to do things. So, let's take a look at it in action. We're going to pass in foo. Now, you can see with the Additive Hash, the value is 324. Folding and DJB2 have much larger different values. And now here's where Additive Hash falls apart. If I pass an oof, again the value is 324. So that's that Uniformity thing we were talking about. It's not providing Hash values for unique strings across the Array of available values, where you can see that Folding and DJB2 provided values that were quite a bit different than the previous. So, we can provide any length string we want, and what you'll notice is that Additive values keep getting bigger as the strings get bigger, so this is a long string. And you can see there are things that have wrapped around to negative, and that's an example of where if we hadn't used the unchecked keyword, we would have thrown exceptions there. So, if you're curious and you want to learn more about Hashing, spend a little bit of time going through this. Maybe follow some of the links to learn more about specific algorithms.

Adding Data
Now that we have a somewhat firm understanding of what a Hash Table is, and how Hashing is performed, let's take a look at how we Add items to a Hash Table. So again, we're using the example of a Hash Table that's an Array that's nine items long, and we're going to add the person named Jane to the Hash Table. So, we store off the Array length, it's 9. Now, we're going to get the hashCode for Jane, and that hashCode is some integer. Now remember, that integer is going to be anything in the integer value space; certainly it's unlikely that it's going to be somewhere between 0 and 8, so this is not our index into the Array, this is the hashCode for the string Jane. The index into the Array we're going to do a modulus of the hashCode with the Array length, and what that's going to do is take that kind of random but stable hashCode, and give us a value from 0 to 8, so we can use that hashCode to figure out the index into the Array. Once we have the index we can assign Jane to it, and now Jane is in the Hash Table.

Handling Collisions
When you're implementing a Hash Table, Collisions are inevitable. And what a Collision is, is when two distinct items have the same Hash value. So what ends up happening then is those items will be assigned to the same index in the Hash Table, and that's a problem. Because if you already have an item in the Hash Table and another item needs to go there, what do you do? You can't just reject it, because the consumer of the Hash Table, the end user, it shouldn't know that you had a Collision, it should be a totally transparent thing. We're going to look at two common strategies for dealing with Collisions. The first is called Open Addressing. What Open Addressing does, is it says, alright, you've gone to index 3, there's already something there, so let's go check index 4. Is there anything there; let's put you there if there's not. And Chaining actually has a Linked List, and it says, you need to go to index 3, well there's already something there, so let's make a Linked List and have the existing item and this new item both be in it at index 3. Let's take a more in depth look at those. So, Open Addressing. So, we have an item Jane already in the Hash Table, and we want to add the item Steve to the Hash Table. So, we go through our add algorithm, we get the hashCode for Steve, we find the index, and what we discover is that Steve and Jane collide. So, what we're going to do is say while the array index is not null, move it forward. So, we're at this next array index, it is null, so we're going to break out of the while, and then we're going to assign Steve into that index. But think about what this means. We didn't put Steve where the hashCode said Steve should be. So, next time we come looking for Steve, we need to take that into account. Chaining works somewhat similar in that we start with a user in the table, and we want to add Steve. We perform the start of our add algorithm, but what we do now is instead of saying is there a conflict, rather we just add the user to the existing Linked List. We don't know if there's a conflict. The Linked List could be empty, it could have 10 things in it, it could have 100 things in it, we don't know. Chaining allows us to not think about that. And what's nice about Chaining, as opposed to Open Addressing, is that later on when we come looking for Steve, all we have to do is find the index and now find out is Steve in the Linked List?

Growing the Table
When adding items to a Hash Table, we've seen that collisions can occur. And I think it should be clear that the frequency of collisions is going to be a function of two separate issues. The first is how many available slots there are in the Array. If there are only nine items in the array like we've seen our samples so far, you have a more than 10% chance of having a collision on any given insert. And the other side of it is how populated the Array is. And this is known as the Load Factor; it's the ratio of filled slots to empty slots. So, your Load Factor can determine of your nine available slots how many are free. If there's only one free, you have a less than 10% chance of not colliding. So, once you've hit a certain Load Factor or certain ratio of filled slots, it might be time to grow your Hash Table. You know, to say, alright, we had nine slots, only two were open, let's double the size. Now we'll have 18 slots and we'll have 11 open, and we've cut our chance of having a collision down dramatically. So, the way this works is when we're adding an item. The first thing we have to figure out is, is our current fillFactor or our current Load Factor greater than or equal to whatever maximum we decided. If we have a Hash Table backed by an Array with 100 items, 75 of which are full, our fillFactor is 75%. If our maxFillFactor is 75%, or 70%, or anything lower than 75%, we need to grow the Array. And this is very simple. All we do is we allocate a newArray, and that Array is going to be twice as long as the current Array that we have, and we enumerate over each item in the existing Array, we add the item to the new Array. It's actually pretty straightforward when you think about it, because when you add that item to the new Array it's going to be Hashed into that Array, but this time it's going to take into account that there are twice as many buckets available, so it's very likely that it'll be Hashed to a different location. Once we're done, we take this new larger Array, we make it the newArray; that's our new master Array that's backing the Hash Table, and then we add the existing item. This is what we would've done had we not grown the Array. So, growing an Array is really a simple task, it's just a matter of allocating basically a new Hash Table backing store, and Hashing all the existing items into it.

Removing Data
When we implemented the Add algorithm, we had to decide, were we going to Chain Collisions together, or were we going to use Open Addressing to handle Collisions? And that decision impacts the rest of the Hash Table. So, for example, when we want to remove an item, what we're going to do is remove the item by key. So we stored an object with the key Jane, which represents the value that is the person object for the user Jane. With Open Addressing, this is the case where, in the case of the Collision, we move the object forward, what we say is let's get the index of where we think that object should be. So, we think Jane should be at index 3. If the value at index 3 is non-null, check if it's Jane. If it is Jane, remove it, but if it's not Jane, let's assume a collision might have occurred, and go check the next index. And now we kind of repeat here. If that next index is non-null, if it's Jane, remove it; if it's not Jane, continue. And we keep on doing this until we either find a null entry or we find Jane. But this does get more complex. If you had multiple collisions that all caused the walk forward, when you remove an item you create a null slot, and now what you have to do is look at every non-null item after that item, and figure out if it was a result of a collision, and if it was you have to re-add it back into the table. And this gets really kind of tedious to maintain, and it's why I'm not a big fan of Open Addressing when implementing a Hash Table for teaching. What I prefer is Chaining, and this is because it's a lot easier to conceptually understand, and the point of this is not to understand all the subtleties of Open Addressing, the point of this is to understand Hash Tables, so let's not focus on Open Addressing, let's focus on Chaining. All you do with Chaining is you get the index for the key that you're looking at; so we get the index for Jane, index 2. And then, we look at index 2. Is there a Linked List in that index? If there is, then look and see if Jane's in it, and if she is, take her out, and if there's not a Linked List there, then Jane's not there, so there's nothing to remove. It's much simpler. There's nothing else you need to do. If Jane is in there and three other things are as well, it doesn't matter, the Linked List will manage it for us; we can focus on the Hash Table.

Finding Data
The Find algorithm in a Hash Table works a lot like the Remove algorithm, and what I mean by that is it depends entirely on what Collision detection algorithm you've chosen, and it has to handle them appropriately. So, let's find the user Jane, and let's provide the key, Jane, which gets mapped to an index. So, with Open Addressing, we take that index and we look, and is the value there non-null? And if it is, is it Jane? And if it's not, we need to move onto the next, and if that value's non-null we check again, and we keeping walking forward through the table until either we find Jane or we find a null entry, and that's our clue that hey, I can stop now because Open Addressing says that if I hit a null value I'm done. With Chaining, just like with Remove, we get the index, we find the list, and if the list is null, Jane's not in the table. If the list is non-null, we look in the list; is Jane in there? If she is, we return Jane's object, if she's not, we return null. So again, Chaining provides a simpler algorithm. It allows us to defer some of the behaviors out to the Linked List instead of us having to think about them. This allows us again to focus on the concepts of the Hash Table.

Enumerating
As with all the other collection types we've looked at, the ability to Enumerate the Keys and Values in the Hash Table is important, and whether you're enumerating a Linked List, or the Queue or Stack, or even the Binary Tree, you need to be able to do it, and a Hash Table is no different. The difference is, with a Hash Table you need to generally have two different enumeration styles, one to enumerate all the keys, and one to enumerate all the values. Just like Remove and Find, the type of Collision detection we're doing here is everything. So, with Open Addressing, we enumerate every item in the Array. Now, there might be some empty slots in the Array, so we check, is the item null? If it's not null, we return the item, and we just keep doing that over and over and over again. With Chaining, we look at each index in the Array, but this time it's containing lists instead of the items themselves. If the lists are not null, we return each item in the list. So as you can see, whether you're doing Open Addressing or Chaining, the algorithms really aren't that difficult. In fact, Chaining this time is actually a little more difficult than Open Addressing.

Code: Hash Table
What I'd like to do now is quickly go over the Hash Table code. And unfortunately, when I say quickly, I really do mean it; we just don't have time to go through every aspect of this. But, I want to hit some of the bigger points. So, to give you a high-level overview of what this is, I've implemented a Hash Table that uses Linked List Chaining to handle Collisions. So, the Hash Table has a fillFactor of 75%, which means that if more than 75% of the nodes are filled we're going to grow it. And I just kind of cash the value of the number of things we can store so we don't have to continually do floating point math each time we do an add, and we store the number of things that we have in the table. And this is the actual Array that we're storing the values at. Now, notice this HashTableArray class is a class created for this example, and it stores the Key/Value pair, just like the Hash Table has a Key/Value pair. The HashTableArray is an Array of Hash Table nodes, and the Hash Table nodes are a Key/Value pair. The Array has a capacity. So when the Array is allocated, we allocate the new Array, and we assign each index in it to a Hash Table node. And this is not something you would do probably in a production environment, because here we're allocating a bunch of objects when we haven't put anything in them yet. You know, we would probably do this as lazily as possible, but I want to keep this example straightforward, so please keep that in mind. The HashTableArray basically defers all of its behaviors to the Array node. And we can do this because we're using Chaining, which means the node manages the Linked List. And because we pre-allocated everything, I don't need to do null checks anywhere. So, when we do Add, we get the Key. GetIndex finds which index the key should go to, and we store the value there. And we're adding the Key/Value pair, not just the value, because in the event of a Collision, we don't need to know just the index that the Linked List is at, but we need to know which item in the Linked List is being looked for. So, the key needs to be stored with the value in the chain. Add will always add a new value into the chain. This means that if there's already something in the Hash Table that has the key provided, it's going to throw. Whereas Update will update the value in the Hash Table. So, if there's already a value in the Hash Table with the key, the value would be updated, and if there's not, we'll throw. Remove, we just pass through to the Hash Table node. TryGetValue just passed through, and then Capacity, this is the number of items we could potentially hold, not necessarily the number that are in the Array, and then Clear loops through and clears every node out in the Array. So, let's come back to the Hash Table. By default we allocate a HashTable with an array capacity of 1, 000. So, we allocate the array with the initialCapacity. So Add is really one of the more interesting areas, because here's where we're doing the growth. So, we're saying, if our count is >= the max items we're allowed, let's allocate the larger HashTableArray. For every node currently in the existing HashTable, Add that node to the new HashTable, assign the larger array on top of the old array and then reset our maxItemsAtCurrentSize so we know what our next grow factor will be. Once that's done, we can add the new item, and we bump our count. Remove is just going to defer to the array, and pretty much every other method in here just defers to the array. So, if we look at the array, we saw that it's deferring to the node. So, let's go ahead and look at the node, because apparently, this is where all the magic happens. So, the node, as we expect, because we're doing Chaining for our Collisions, has a Linked List, and it's a Linked List of HashTableNodePairs. HashTableNodePair is a very simple Key/Value pair class; it's got a Key, it's got a Value, and that is it. So, it's basically a very lightweight pair class. In the. NET framework there is a Tuple class that can be used, but when it's something very behavior-specific like this, I prefer just to create my own pair class. Add works just the way we described in the slides. If we're adding something for the first time, we've never done anything in this Array index before, items will be null, so we'll allocate the new LinkedList. Otherwise, what we're going to do is just check, is the item we're adding already in the Linked List? If it's already in the Linked List, we throw, because we don't add the same item with the same key, we don't allow it, it's an invariant in our Hash Table. Now, if we've made it this far, one way or another the list is allocated and we know there's not already a value in there with the same key, so go ahead, allocate the node pair, and Add it to the front of the list. Now Update is very similar. If the item is not null, we go through and loop until we find the item we're looking for. If we found it we update the Value, and we update a flag indicating that we did do an update. If we make it to the end and we never found a value to update, we throw an Exception. And this is because Update is not Add. We're not going to add the item if it's missing. TryGetValue looks a lot like Update. Instead of storing a value saying whether we updated the item, we simply store one saying whether we found the item. Once we find the item, we store its value into the provided out parameter, and then we return found. Remove also looks very similar. We're provided the key to remove; we store a value indicating whether or not we actually removed the item. We start at the front of the linked List. While the current value is not null, we check to see if we found the Value we're looking for. If we have, we remove it, then we set removed to true and we break. If we haven't, we move on to the next item in the list. Clear simply clears out the list. And finally we have some Enumerators, one for Values, one for Keys. As I discussed on the Enumeration slides, you want to provide Enumerators for both, and the reason is that there are going to be times where people want to operate on every value, and if you only provide an Enumerator for the Keys, they're going to have to take each Key and then perform a lookup into the Hash Table and then operate on the Value returned. Well, that's far more expensive than just letting them operate on every Value. If we have items in the list we enumerate every item, yielding each value in the list, or yielding each Key in the list. Items, returns an Enumerator for the actual pair object. I've glossed over so much here. I really hope you'll take some time to go back and look at the slides and think about how the different algorithms are implemented, and then come and look at the code, and follow it through, and really try and understand how all this is done. Because, understanding how Hash Tables work is really important once you start picking up these collections and using them, because you want to be able to talk about the performance characteristics and understand the tradeoffs that you're making.

Demo: Counting Words
What I'd like to look at now is a somewhat real world example of a Hash Table in action, and what we're going to do is use this application to load up a document, parse the document out into the series of words, and then count the number of times each word is used. So, what we're going to be loading is the Constitution. So, there are a lot of words going on here, a lot of different things, but some good repetitive words as well, so we should be able to get some insight into how frequently words are used. So, let's load it up, and we can see that all the words have been listed here. And we can sort the words by Count and see that "the" was used 423 times, "shall" 191, or we can sort by Word and start to see there are quite a bit of words there. What we're going to look at here is that the Hash Table Key is the Word, and the Hash Table Value is the Count. And every time we encounter the word "the" we look up the Value, and we increment it and then store it back. So, that's where a Hash Table really shines. Key/Value pairs where we're updating the values, and we have a stable key. So, let's go ahead and look at how we did this. So, our application starts with a HashTable. It has the word string as its key type, and the integer, or the wordCount as the value type. Now this ObservableCollection is a WPF artifact; I'm not going to dive too deeply into the WPF components, but this is the list that all the Word Value pairs go into and how the column sorting and everything was done. If you want to learn more about that, come and take a look at the code for the sample, and start to dig into some of the concepts being used, but for the most part we're just going to gloss over those. When we click the Load button, we call the GetFileName method. This brings up the Open File dialogue. From that we get a file name. We load our data file, and we display the words. So, LoadDataFile is very straightforward. If we have a file, we Open the file, we Read the file, clearing the HashTable from the previous reads just in case we had one in there to begin with, and while we're not at the end of the string, we load each line. In LoadLine, what we do is we split the line, and I put in some common things here, some punctuation, quotes, Tab, space, pretty arbitrary, just things I noticed were in the Constitution. So, we split it into the series of words, and for each word I made it lowercase, just so a capital "The" and a lowercase "the" would still show up as "the. " We store off the count, so we TryGetValue with the word should we return the count. So, if it's the eighth time we've seen this word, the count will come back as 7, because it's the eighth time, we haven't incremented it yet. If we don't find the value, we store the value 0 into the HashTable, and then we increment count + 1, and store it back in the HashTable. So as you can see, what we did here is we retrieved the value from the HashTable, or we create the value in the HashTable using Add, and then we update. Now, I could have used update here instead of the indexer, but I wanted to show off the indexer. And for the most part, that's it; that's what we're doing. We're taking our HashTable, we're splitting the document up into a series of words, and we're adding each word in, and incrementing a counter each time we see the word, and that allows us to display the words in the document.

Summary
So, in this module we've taken a very high-level look at Hash Tables. We've seen that what they really are is a general type of an Associative array. We've learned about Hashing, and how there are the four properties of Stability, Uniformity, Efficiency, and Security, that you need to look at and balance, and decide what's important to you in your business context. We looked at how to Add items to the Hash Table, and how Collisions are handled. But specifically we looked at two different ways to handle Collisions, Open Addressing and Chaining. And then we looked at how to Remove, Search, and Enumerate items within the Hash Table, and how the implementation of Collision handling directly affects how these behaviors are implemented. So, I hope at the end of this module you have a better understanding of what a Hash Table is, what Hashing is and how that all comes together to give you a really powerful data collection that can be used in a variety of development contexts.

Course author
Author: Robert Horvick	
Robert Horvick
Robert spent nearly 10 years at Microsoft creating software that made it easier for everyone else to write software. Most recently Robert worked on Team Foundation Server on the...

Course info
Level
Beginner
Rating
4.5 stars with 1131 raters(1131)
My rating
null stars

Duration
3h 13m
Released
15 Aug 2011
Share course