Testing Automation: The Big Picture

by Jason Roberts

Automated tests are crucial to reducing costs and releasing features quickly. This course will teach you how automated tests benefit the business, the different types of tests that can be created, and how they fit within wider business processes.

Software can be costly to develop, error-prone, and hard to release to production. In this course, Testing Automation: The Big Picture, you'll learn the different types of automated tests that can be created, the various trade-offs involved, and how it can benefit you. First, you'll explore the potential business value of automated tests. Next, you'll cover the difference between unit integration and other types of tests, along with understanding how many of each test to write. Finally, you'll discover how tests fit in with continuous integration and deployment. When you're finished with this course, you'll have a solid understanding of how automated tests can be an enabler for increased business value, and how to start implementing or refining the use of automated tests within your organization.
Course author
Author: Jason Roberts
Jason Roberts

With over 15 years of experience in both frontend and backend software development, Jason Roberts is a freelance developer, trainer, and author. He holds a Bachelor of Science degree in computing,...
Course info
Level
Beginner
Rating
4.5 stars with 83 raters(83)
My rating
null stars
Duration
1h 6m
Released
8 Aug 2017
Share course

Course Overview
Course Overview

Hi everyone, my name's Jason Roberts, welcome to my course, Testing Automation: The Big Picture. I'm a Microsoft MVP, freelance developer, and author. In this course, we're going to learn what automated testing is, the types of test we can write, and how testing automation fits in with the rest of the business. Some of the major topics that we'll cover in this course include what the potential business value of automated tests are, the difference between units, integration, and other types of tests, how many of each type of test to write, and how they fit in with continuous integration and deployment. By the end of this course you'll understand how automated tests can reduce costs, create happier teams and end users, and improve long term delivery speeds. No prior knowledge of testing automation is required before starting this course, nor is any platform, language, or eco system specific knowledge. I hope you'll join me on this journey with the Testing Automation: The Big Picture course, at Pluralsight.
The Business Value of Automated Tests
Introduction

Hi everyone, my name's Jason Roberts, welcome to my course, Testing Automation: The Big Picture. In this first module, we're going to get an overview of what testing automation is, and also the business value that it may bring. So in this first module we're going to start off by looking at exactly what automated testing is. We'll then move on to discuss what the high-level benefits are of automated testing, before discussing the total cost of ownership of software. In addition to the potential monetary benefit, we'll also discuss the happiness factor that automated tests can help us deliver, and we'll discuss automated testing in the wider business context. While automated tests can bring considerable value to the business, we'll also discuss some of the considerations to bear in mind when using automated tests, and finally, we'll discuss the difference between short term long term delivery speeds, and how automated tests can help us here. So, let's kick off this module with a discussion of what automated testing is.
What Is Automated Testing?

So if we're not currently using automated testing, we may be testing the system by using human testers. In this scenario, a human tester will generally be working from some sort of test script, so this test script could be inside a Word document, or perhaps inside some third party manual testing system. Our human tester here will open various test scripts, and then execute the system, interacting with it as specified in the test script. Once these interactions are complete, the human tester here will obtain some results, and compare those to the expected results outlined in the test script. Our tester here can now report the outcomes to the rest of the project team. If the obtained results don't match up to what was expected in the test script, then we can say that the test has failed, but if the results that were obtained match those that were expected as outlined in the test script, then the test has passed. The two biggest changes for automated testing Is that the human tester here is now replaced with a computer, so it's the computers job to interact with the system that we're testing, and obtain the results. Because the computer has to execute the test now, we can no longer use a Word document for example, instead we use test code. This test code specifies how to create an instance of the system that we want to test, which interactions to perform against the system, and then the expected outcomes. Just as with our manual human testing, our automated tests can fail or they can pass, and the developer or the team can take appropriate actions based on the outcome of the automated tests.
High-level Benefits

Let's take a look now at some of the high-level benefits automated tests can give us, and we'll also compare them against manual human tests. First off, automated tests are essentially free to run as often as required. Other than any costs for electricity or hardware for example, we can run them over and over again, and they're essentially free. Human tests on the other hand, cost us every time you want to execute a test run, and this cost is primarily a staffing cost. We can run automated tests at any time of the day or night. They can be executed after each piece of work a developer finishes, or scheduled for example to run at midnight. Manual human tests on the other hand are limited as to when we can run them, depending on if we have staff in the office. Automated tests can execute more quickly than the equivalent manual human tests. Automated tests are generally less error prone, assuming that they've been written correctly, and all other things being equal, they should produce the same pass or fail results in a reliable way. Manual human tests are potentially more error prone, and this is just simply due to the human factor here. For example, we may get tests passing that should have failed, or incorrect defects being raised. Automated test code can sit alongside the production code that it's designed to test, so for example in some source control system. This means that if we're working on multiple releases for example, we can create branches in the source control system, and this means that the test code will always be in sync with the production code that it's designed to test. Manual human test scripts on the other hand may exist in some kind of external system, for example a Word document in SharePoint, this means that it's possible for the versions of the test scripts to get out of sync with the actual production code that it's designed to test. Once again, this may mean that defects are missed, or defects are raised by the manual testers that aren't actually defects. It's worth bearing in mind here though that automated tests do have a creation and maintenance cost, and so do manual human tests.
The Cost of Fixing Software Defects

While we're on the subject of costs, let's take a look at the cost of defects, as found in different phases of the software development process. This is a great quote that sums this up by Kari Ann Briski and others, Minimizing defects is one of the most effective ways to keep development costs down. In a report by the National Institute of Standards and Technology, we get some idea of the relative cost to repair defects when found at different stages of the software development process. So along the bottom of this chart we have the different software phases, including at the end the Post-product Release. So in these examples the relative cost to repair a defect when found in the Requirements Gathering and Analysis or Architectural Design phase, is 1 times. Once we move to the coding or unit test phase, the relative cost to fix a defect increases from 1 times to 5 times, so it's 5 times more expensive to fix a defect to fix a defect here, then it would have been in the requirements gathering phase. Once we move to the integration and component system testing phase, the relative cost increases to 10 times, and in the early customer feedback, or beta test phase, the relative cost increases to 15 times. But look what happens when we find a defect in production. The relative cost increases to 30 times, so we've gone from 5 times the relative cost at the coding or unit test phase, right up to 30 times the cost to fix it once it's released to production. Let's take these relative costs and plug in some actual monetary values. So here the cost to fix a defect at Requirements Gathering stage is $100, so this is our 1 times multiplier. At the Coding or Unit Test phase, we increase this by 5 times, to a cost to fix of $500. The Integration and Component test phase, the cost increases to 10 times, or $1, 000, and at the beta test phase, 15 times, or $1, 500. And Post-product Release, this is our 30 times cost, and in this example, $3, 000. So for example, if we use the sample costs, and plug them into a scenario here. In this first scenario, we're simulating a poor testing strategy in the organization. This scenario is characterized by defects being found later on in the development process. So in this table we can find that we've got 1 defect found at Requirements Gathering, 5 in the Coding/ Unit Test Phase, 3 in the Integration and Component test phase, 10 in the Beta phase, and a whopping 30 defects found Post-product Release. We've got a total of 49 defects found, and this equates to a total cost to fix all of the defects, of $110, 600. If we compare this to another scenario here, in this scenario we're simulating a better testing strategy, and this is characterized by defects being found earlier in the software development life cycle. So here we can see in Scenario 2, we've only got 10 defects found Post-product Release, and we've got a large increase in the number of defects found at the Coding or Unit Test phase. We've still got a total of 49 defects found and fixed, but this time the total cost to fix these defects is only $60, 600. So this means in Scenario 2, we've got a defect detection cost saving of $50, 000. So why does it cost so much more to fix a defect once it's made it into production, than it does at coding or unit test time? Well let's go through some of the steps here. If we find a defect at the coding or unit test time, typically, this could surface by a unit test failing in development. The developer can review the specific failure message for that test, go ahead and fix the code in development, and now the test passes. However, if a defect is found in production, there can typically be a lot more involved to fix it. So here we have the error occurring in production, and this could be identified by some kind of automated monitoring, or reported by an end user. The production bug is logged or a ticket created in some kind of bug tracking system, next a developer has to get access to the production logs, and once they've got access, they need to trace through the logs to try and determine the cause of the error. Once the probable defect has been located in the logs, the developer can try and reproduce it in the development environment. Once the defect has been reproduced in development, the developer can go ahead and implement a fix. Once this fix is tested in development, a new production release can be planned that will implement the bug fix. We can then release the bug fix to production, and hopefully this won't mean an outage for the end users, and finally we can check that the bug fix actually works in the production environment. So I hope this comparison serves to illustrate how fixing bugs in production can be a lot more costly and time consuming than to find them at coding or unit test time.
The Happiness Factor

Now we've got an idea of the potential cost savings of automated tests, let's also consider how automated tests can help increase the happiness factor. So automated tests can result in happier end-users of the software we're developing. They can accomplish more tasks and feel more productive in the time they have, because they encounter fewer problems with the software getting in the way. Automated tests can also help to create a happier development team. This means the teams can spend less time fixing production defects, and instead focus more time on developing new and potentially exciting features. Because we have automated tests in place to find any errors, we can have increased confidence to make changes in the system. Automated tests can help to document the system we're building, and increase the overall system understanding that developers have. And finally, we can have a happier development team if we have less unnecessary stress caused by a constant barrage of production errors. Automated tests can create happier business owners, this can happen from reduced costs of finding errors in production, or increased revenues from less system outage time. Higher quality software may improve the business reputation. Customers that use a company software that's always experiencing problems, may form a negative view of that company, and also pass on that negative view to family and friends. Because we've got a happier development team, this may reduce development team staff turnover. The development team is happy enough that they don't see a need to move onto another company. This creates long term consistency of both development practices, and also support knowledge. This also reduces the cost to the business of finding and recruiting replacement team members.
Automated Testing in Context

Automated testing is not the be all and end all of the quality story when it comes to software development. Let's take a look at some of the other things that can affect software quality. So we've got additional techniques that we can employ, in addition to automated tests. Techniques, such as formal or informal code reviews, this is where one developer reviews another developers code to check for any obvious errors, there's also the related technique of pair programming, this is where two developers sit down at the same machine and code together. Other factors that can influence overall quality include good management and management practices at the organization, a happy and motivated development team, well understood requirements, even if a feature makes its production with no obvious errors, if that feature doesn't fulfill the end requirements of the end user, then we have a problem with the quality or the understanding of the requirements. Finally, software development doesn't sit on its own, we need somewhere to deploy this software to. So good environment configuration and management, and also a healthy DevOps culture can contribute to the overall quality of the system.
Considerations

While automated tests bring a number of benefits, there's a number of things to consider when implement them. First off if the developers aren't as experienced with automated testing, then there may be some training or upscaling costs for the development team, you may need to purchase new hardware, this might involve upgrading the developer work stations to be able to execute the automated tests in a timely fashion, and it may include buying new servers on which to execute tests. If we do need to purchase new servers, then there may be operating system license costs associated with this. And if we're using any third-party commercial, non-open source testing tools, there may be additional license costs here. When we start to implement automated tests, we may get an initial downturn in productivity, and this is because creating automated tests requires some investment in the form of developer time. We also need to consider what the time to market is for the software we're trying to deliver. Let's delve into these last two items in a little more detail.
Short-term vs. Long-term Delivery Speed

So there can be some tradeoffs when considering automated testing, for example, the short term versus the long-term delivery speed. So here we have a chart, and along the bottom here we've got different releases of our software application. Let's consider two approaches here, one is to maximize the short term speed, so for example if getting a product to market is the most important thing, and the level of quality is not as important, then we may optimize for the short-term speed. We may instead consider that the long-term deliver speed of the project is more important than the time to market for the first release. So both of these approaches start at the same point on the graph, the Project Start, but notice here that if we optimize for short-term speed, the time to the first release might only be one moth of development work. This is because we're not investing heavily, or at all, in automated testing. The version here where we're optimizing for long-term speed, takes us a little longer to get to the first release. In this simulated example, the first release doesn't come until three months into the project. We can continue to optimize for short-term speed, in which case we can get the second release deployed to production in only three months, whereas the long-term speed approach has taken us four months to get to the same place. However, at some point during the software's lifetime, optimizing for this short-term speed is going to start to slow us down. At some point these two lines will cross over, and as we add more features to the application, it becomes harder to manage, we may end up with more defects, which require more time to fix. So here, to get the third release to production, the short-term speed has taken us six months now, whereas the long-term speed approach is starting to pay off here. It's only taken us five months to get this to production. To continue this example further, Release 4 in the short-term optimization approach, has now taken us 12 months to get to Release 4, whereas because we've optimized for long-term speed, with good tests and testing infrastructure, we can continue at a steady pace. In this case, rather than 12 months, it's only taken us 6 months to get to Release 4. And we can continue this onwards. When starting a project and considering the testing approach, this short-term versus long-term delivery speed tradeoff should be taken into account. If the most important thing is to get something out of the door, for example to capitalize on some niche opportunity before another company does, then missing out on this potential opportunity may be more costly than the reduction in long-term delivery speed. However, if more predictable delivery times are required, or the software system is considered critical in some way, so for example internet banking, or perhaps some medical application where lies depend on it, then our testing strategy may align more with the long-term speed approach.
Summary

So that brings us to the end of this first module. In this module, we started off by getting an understanding of what automated testing is, and how it differs from manual human testing. We learned about some of the high-level benefits that automated testing can bring, such as the fact that they are quick to execute, and essentially free to execute as many times as we like. We considered the cost of ownership of software development, and then if we find errors in production, this may cost more to fix than if we had found them in development. We learned that automated tests can help us to deliver happier end users, a happier development team, and a happier business. We also looked at automated testing in the wider context. So for example, how it's just one aspect of software quality, that can be augmented with code reviews, and well understood requirements. We learned about some of the considerations that we might want to bear in mind when using automated tests, such as any training, hardware, or license costs, and also how much of a business driver time to market is. For example, are we going to optimize for short-term delivery speed, or try and optimize for a more predictable long-term delivery speed. Join me in the next module, where we'll begin to get an understanding of the different types of automated tests we can create, and how these different types of tests fit together in an overall testing portfolio.
Understanding the Different Types of Automated Tests
Introduction

Hi, welcome back. In the previous module we got an understanding of the business benefits of automated tests. In this module, we'll look at automated tests in a little more depth, and understand the different types of automated tests we can write. So we'll start off in this module by looking at the different types of automated tests we can write. We'll learn about unit tests, integration tests, subcutaneous tests, and functional user interface tests. We'll see how each of these types of test offer different characteristics when it comes to test breadth or test depth, and we'll learn about the different logical phases of an automated test. We'll get a brief overview of mock objects, and how we can use them to isolate the code that we want to test, and we'll learn how data driven tests can reduce the overall amount of test code we need to write. We'll learn that we can make our tests business-readable, so we can bridge the communication gap that can often occur between the development team and the business or stake holders. So if we have all of these different types of tests available to us, how do we know how many of each type of test to create? We'll learn a couple of different ways of thinking about this. So we'll learn about the classic testing pyramid model, and also how we can go beyond this initial pyramid model and decide which type of tests to create. Finally, we'll round off this module by looking at a few of the characteristics of good automated tests. So let's kick off this module and we'll start to look at unit tests.
Unit Tests

If we think about the system or the application we're writing, consisting of a number of different parts, so in the diagram here each of these circles represents a distinct part of the application. In an object-oriented language, each of these circles would represent a class. When we create a unit test, it targets a small, single part of the application here, so once again this could be a single class. We are testing this class in isolation from the rest of the system. Unit tests have a number of different characteristics. First off, they're low level tests. They're focused on the lower level details of the system. In this way, they're highly focused tests. We're not testing the whole system working together, we're just testing a very small part of it in isolation. Unit tests are generally really quick to execute, so we get really fantastic feedback speeds, which means we can find out very quickly if we've broken something. Because unit tests have a small number of moving parts, it's easy to test all of the parts or all of the paths of the code, and get our test code to execute every nook and cranny of the thing we're testing. A unit test may also test more than one thing. So in this example we've got two classes here, and these are considered a unit for testing purposes. We may have another unit test, which tests three classes working together. Or indeed, a unit test which is only focused on a single class, and no other classes. So how do we decide what a unit is? Well Martin Fowler has this really great quote: it's a situational thing - the team decides what makes sense to be a unit for the purposes of their understanding of the system and its testing. To describe it more succinctly, we can describe a unit test as testing a unit of behavior. The team defines what these units of behavior are, and we test each unit of behavior in isolation from the other units of behavior in the application. Let's take a look at some pseudo code now to illustrate what a unit test might look like. So first off, we have the beginning and the end of the individual test, and we're naming this test, Addition. This unit test is going to be testing a calculator, so the first thing we need to do in the test code is actually create an instance of this calculator that we want to test. Once we've got an instance of the thing we want to test, we can go ahead and execute the Add functionality, passing in the values of 5 and 2. Once we've executed the Add functionality, we can compare the results that we got back from the calculator, and if it's equal to the expected result of 7, we can report this test as passed, but if the result is not 7, we report this test as failed. If this test fails, then the developer can go and look at the calculator code, and determine what's wrong.
Integration Tests

Let's take a look next at integration tests. So if we use our diagram here and draw on an integration test, we can see here that we're testing a larger number of classes working together. In an integration test, these classes may not have the same tightly knit behavioral cohesiveness as with our unit tests. Integration tests may also make use of external dependencies, so for example this could be a database, reading or writing to the file system, or using some kind of network resource or API. Integration tests can be set to operate at a higher level than our unit tests. Essentially, we're testing multiple parts of the system working together, or to put it another way, multiple units of behavior working together. Integration tests may execute more slowly than the unit test, especially if the integration tests involve external dependencies, such as reading or writing to the database. When we're writing integration tests, it may be harder to test all all of the parts or all of the paths of the code, because we have more moving parts working together. If we have a look at some pseudo code here, we'll start off with the beginning and ending of the tests, and we're calling this test, Loan Application Successful. This test is going to involve two different classes. First off, we're going to create a new instance of a frequent flyer validator, and we're just going to call this instance v. We're then going to create a new LoanApplicationScorer instance, we'll call this s, and this LoanApplicationScorer requires a frequent flyer validator to be able to do its work. So we'll pass in the instance, v, of our frequent flyer validator. Next, we can go and call the functionality of the LoanApplicationScorer, and into this score loan functionality, we'll create a good applicant. If the result of the score loan functionality is equal to LOANGRANTED, then this test will pass, otherwise if we get a different load decision, such as declined or referred, then this will fail. If we head back to our diagram here, and we'll just map this dependency, so we've got our LoanApplicationScorer class, depending on a FrequentFlyerValidator class. So the FrequentFlyerValidator is a dependency of the LoanApplicationScorer. In the previous pseudo code, this could be described as an integration test, but the team could also decide that the unit of behavior here for the LoanApplicationScorer, also includes the FrequentFlyerValidator. So this could also be considered a unit test, depending on the team's definition. However, if this FrequentFlyerValidator accesses an external resource such as reading valid frequent flyer numbers from an external database, then we could no longer consider this a unit test, because we're accessing an external resource. This would instead be considered an integration test. So generally speaking if the code being tested uses external it would be considered an integration test, and not a unit test. Let's take a look next at subcutaneous tests.
Subcutaneous Tests

So here's our diagram with our unit and integration tests marked, let's now go and add our subcutaneous tests. So notice here that subcutaneous tests involve a larger portion of the system. The only portion of the system that they don't cover are any user interface related components. Subcutaneous tests exist at a higher level than integration tests, but they sit just below the surface of the user interface. With subcutaneous tests, we can test that all of the non-UI components are working together properly. Subcutaneous tests are also potentially quicker to execute than automated user interface tests. We'll look at UI tests in just a moment. We may rely more on subcutaneous tests over automated UI tests if the user interface is difficult to test for some reason. One thing to note is that subcutaneous tests sit just below the user interface surface, this means that if we have any business or other logic in the user interface, we won't be able to test this with subcutaneous tests.
Functional User Interface Tests

Unlike subcutaneous tests, functional user interface tests can test the UI components of the application. Notice here that automated functional UI tests can encompass the entire system, right from clicking a button in the user interface, all the way down to interacting correctly with the database. Unlike with subcutaneous tests, when we're using functional UI tests, we can test logic that's encoded in the UI components. So with functional UI tests, we're testing as if we were the end user interacting with the application. This means we can be manipulating UI elements, such as typing text into text boxes, and to clicking buttons in the UI. Sometimes this is known as full stack testing, because we're testing right from a button click down to the database. One thing to bear in mind with functional user interface tests is we're testing the functionality of the system, we're not testing how the system looks or feels, so for example colors, or fonts, or design layouts. Functional user interface tests can also be the slowest of all of the types of test we write. That's because we're simulating typing text into text boxes, clicking buttons and waiting for the UI to refresh. All of these things take time, and can slow down the tests. Another thing to bear in mind with functional UI tests, is they can be potentially more brittle than other types of tests. For example, if our automated test code is trying to locate a checkbox with a specific name, and the user interface is now updated, perhaps replacing the checkbox with another type of user interface element with a different name, then our functional UI tests will no longer be able to find the checkbox, and the test will fail. There are however testing design patterns we can use with functional UI tests that help to reduce this brittleness.
Test Breadth vs. Depth

So now we've got an idea of the different types of tests available to us, let's consider the difference between these types of tests, as relates to the breadth or the depth of the testing. So on this chart here the Y axis represents the Depth of the tests. So this is the ability to get into all of the low level nitty gritty details. On the X axis here, we've got the Breadth of the test. So this is the ability of the type of testing to exercise a wide range of the functionality of the application. So we'll start off here with unit tests. Unit tests are generally the most in-depth tests that we can write. We can get right into the nitty gritty detail of the individual lines of code in the things we're testing, however unit tests don't cover great breadth of system functionality, they're limited to small isolated units of behavior. Next if we plot integration tests on this pseudo chart, we can see that integration tests offer slightly less depth than unit tests, but they can also offer us greater breadth. While we may not be testing as much detail as unit testing, integration tests allow us to exercise multiple parts of the system working together. Next up we have our subcutaneous tests. These type of tests can offer us greater breadth of system coverage, but at a cost of the depth of testing. Finally, we have our automated functional UI tests. These can offer us the greatest breadth of system execution, right from the user interface down to the database level. However, these types of tests are quite shallow, and it's extremely hard to get down into all of the nitty gritty details of every class in the application.
The Logical Phases of an Automated Test

An individual automated test can be generalized into three distinct logical phases. The first phase of a test is the arrange phase. In this phase, we're getting things ready to test, so we're creating new instance of the things we want to test, or setting up any external infrastructure or test data. The second logical phase of an automated test is the act phase. In this phase we're executing functionality of the thign we're testing, to produce some response. In the third and final phase, the assert phase, we're taking the response from the act phase, and comparing it to an expected value. If the actual result matches the expected result, then the test passes. If we take a look at this code from earlier, our Loan Application Successful code, we can generalize this into these three phases. In the first phase here, the arrange phase, we're getting things ready to test. So we're creating the instance of our FrequentFlyerValidator, and the instance of our LoanApplicationScorer. In the act phase here, we're executing the functionality of the LoanApplicationScorer. So we're calling the score loan functionality here, and getting a result back. In the third phase here, the assert phase, we're taking the result from the act phase, and comparing it to an expected value, in this case LOANGRANTED, and reporting the test as passed or failed.
Isolating Code with Mock Objects

Sometimes when we're writing tests, we may want to isolate dependencies or other parts of the system, so we can focus on testing a specific class or subset of classes. So for example, if we wanted to create a unit test, but the LoanApplicationScorer here has a dependency on a FrequentFlyerValidator, and this FrequentFlyerValidator needs a database to work, we can't create a unit test here. If we didn't want to create an integration test here, we need some way of isolating the LoanApplicationScorer from its dependencies. To do this we can use mock objects. There's different terminology around this concept, including mocks, stubs, test doubles, and fakes. We're not going to get into the details of these in this course, we're just going to stick with the term, mock object. Mocking is essentially the act of replacing the actual dependency that would be used at production time, with a test-time-only version that enables easier isolation of dependencies. If we head back to our diagram, we want to remove the database from the equation, to do this we also need to remove the real FrequentFlyerValidator from the equation, and we can replace this with a mock version. The LoanApplicationScorer here can now use the mock version as its dependency, rather than the real version. If we head back to our Loan Application Successful test, notice that the act phase and the assert phase remain the same. We need to make some changes to the arrange phase however. First off, rather than creating a real FrequentFlyerValidator instance, we're instead going to create a mock version of a FrequentFlyerValidator, and we're just going to call this mv. We can now configure the mock object to behave in such a way as it supports the rest of the test. In this example here, we can configure the mock FrequentFlyerValidator to always validate frequent flyer numbers passed to it. We can now go and create the LoanApplicationScorer instance as before, but this time rather than pass a real FrequentFlyerValidator, we're instead going to pass the mock version as its dependency. When we call the score loan functionality of the LoanApplicationScorer, the LoanApplicationScorer will call into the mock FrequentFlyerValidator, this mock version doesn't require a database, so we can consider this a unit test.
Data-driven Tests

One technique we can apply to any type of test is the technique of creating data driven tests. This essentially allows us to execute a single test multiple times, with different test data. So if we have a look at some tests here, here we've got this test at positive numbers, and we're testing our calculator instance, we're adding the values 5 and 2 in this test, and checking the result is equal to 7. We can add a second test at negative numbers, this time passing the values -5 and -2, and checking the result is equal to -7. So now we've got one test for positive numbers, and one test for negative numbers. If we wanted to add another test to check mixed numbers, so this time adding the values -5 and positive 2, we'd have to write a third test. So now in our test code we've got three individual tests, but there's a lot of code duplication. Essentially the only difference is the values that we're passing to the Add functionality, and the expected result. We could remove this code duplication by instead having one single test, and this time replacing the hard-coded values that are passed to the Add functionality with some kind of placeholders. Here I'm using the placeholders a and b, and in the assert phase, we're checking the result is equal to this placeholder c. Now we can construct a series of test data, each row in this table will cause this test to be executed with the different values for the placeholders a, b, and c. So the first time this test is executed, a will be 5, and b will be 2, and the placeholder c will be 7. So now if we want to add new test cases, we can simply add new rows to this test data. If we had five rows for example, then this test would be executed five times. This means we don't have to go and duplicate all of this code, simply to add additional test cases with different test data.
Business Readable Tests

Another technique we can use on top of any type of test, be they unit integration, subcutaneous, or UI, is the technique of creating business-readable tests. Here we've got the business or the client or stakeholders that determine what products are being built. On the other side here we've got the developers, or the development team. There needs to be good communication between these two sets of people to ensure that the correct features are built. Sometimes however, there can be a communications gap between these two sets of people, resulting in incorrect or missed requirements, or even the wrong features being delivered to production. Business readable tests help overcome this communication gap by allowing the creation of tests that exist in the language of the business or client. So, why might we want to go to the additional effort of creating business readable tests? First, business readable tests allow us to document the application. The tests describe what the features of the application should be, and how they should work. These tests are written in a non-technical fashion in a language the business can relate to and understand. Because tests are easily readable, it helps onboarding new people to the development team, whether they be developers, or even QA or test specialists. The specifications as to how the system should behave are documented in a non-technical way, and underneath the covers this non-technical documentation maps to executable test code, so we can actually execute this documentation in the form of tests. Business readable tests sit alongside the source code, this means they can be version controlled, so our documentation doesn't get out of sync with the actual production code. Because of this the documentation has to stay accurate and up to date, because if it gets out of sync with the production code, the tests will fail. In this way we can think of business readable tests as living documentation for the system, unlike for example a Word document in SharePoint. Ultimately business readable tests can result in better communication between the stakeholders and the development team. Because the tests are writing in a common language at a high level, anybody can understand and verify the requirements of the system. This means that the correct features are being worked on, and we can reduce the wasted effort of developing the wrong things, or some work making it all the way to production, but with some missing features. Ultimately business readable tests help ensure that the right thing is being built, and it's being built right. As an example, let's take our calculator add positive numbers test, we could create a business readable version of this test, at the top we're describing the feature, in this case addition, and then for each test that we want to execute, we describe a scenario. So the first scenario here, Positive number addition. Inside this scenario, there's a number of steps. Given I have a new zeroed calculator, when I add 5 and 2 together, then the result should be 7. And we can go and create additional scenarios, for example Negative number addition. This format of business readable tests expresses the requirements in the Gherkin language. Gherkin is a Business Readable, Domain Specific Language that lets you describe software's behavior without detailing how that behavior is implemented. Gherkin serves two purposes- documentation and automated tests. You can read more about Gherkin and business readable tests at this link here.
How Many Tests of Each Type?

So how many of each type of test should we have in our overall automated testing portfolio? One model to describe the relationship between these types of tests is the testing pyramid model. In this model, we have the fewest number of tests at the top of the pyramid here, and at the bottom of the pyramid we have the greatest amount of tests. So at the top of this pyramid, we have our functional user interface tests, and we have the smallest number of these type of tests in this model. Underneath UI tests we have our subcutaneous tests. So we have a greater number of subcutaneous tests than we do UI tests. Further down the pyramid we have our integration tests, so we have more integration tests than subcutaneous tests, and at the base of this pyramid, forming the bedrock, we have our unit tests. So the greatest number of tests we have using this pyramid model are our unit tests. The further up the pyramid we go, the slower the tests become, so user interface tests execute more slowly than unit tests. As we rise up the pyramid, tests can become more complex because we have to set up more moving parts to be able to execute the test. Tests higher up the pyramid can also be more brittle and prone to breaking. The most potentially brittle tests we can write are the user interface tests. And as we move up the pyramid, tests become more broad, and we saw this earlier in this module when we compared breadth and depth. On the other hand, tests toward the bottom of the pyramid become faster to execute, and so we can get the feedback quicker. They become more simple to set up and execute, they can be more stable and resilient to changes in other places of the code base, but they have a more narrow scope. The testing pyramid can be a great guide if you're completely new to automated tests, and you don't know where to get started with the number of tests to write at each level. However, once a lot of experience is gained with automated testing, the test pyramid might be too constricting, so we can use a different set of heuristics to decide which type of test to write.
Beyond the Testing Pyramid

So what aspects of automated tests can we think about if we want to move beyond the testing pyramid? The first thing we can think about is how quickly the tests execute. So essentially we want to choose tests that allow us to find problems sooner. As we saw earlier in the course, the longer a problem exists for, the more expensive it can be to fix. So here given the choice between two types of tests, if we just had this aspect to consider, we'd always choose unit tests because they execute the fastest. However, we can add some additional aspects here. We want to write automated tests that mitigate risks in the project. So we can choose tests that give us the minimum required level of confidence that the system is going to work as expected, once released to production. We could write 20 unit tests that exceed the minimum level of required confidence, or a single subcutaneous test that allows us to meet the minimum required confidence that we need to release to production. If we had a third aspect now, and that's a focus on behavior, or to put it another way, we should choose tests that have a minimum amount of knowledge of the implementation of the thing we're testing. So we want to test the behavior, rather than how that behavior is implemented. So using this diagram, the sweet spot for automated testing is the intersection of all of these three aspects. Ultimately we can sum up this way of thinking about automated tests by saying that we should write the smallest number of tests possible to reach the required level of quality or confidence in the system being developed.
Characteristics of Good Automated Tests

Let's wrap up this module by a brief discussion on the characteristics of good automated tests. Good automated tests should be isolated. Basically this means that one test shouldn't affect other tests being executed. Good tests should be independent of each other, so they should be able to be executed in any order, and produce the same results. Good tests should be repeatable and reliable, this means that all other things being equal, they should always pass or fail. Also, an important characteristic of good tests is their maintainability, so they should be understandable and easy to change as required. Overall, good automated tests should add value to the project. If they don't add value, then why are we paying the costs to create and maintain them when we could be using that effort to actually add features to the product that will benefit our users.
Summary

So that brings us to the end of this module. In this module, we learned about the different types of automated tests. So we learned about low level unit tests, and also what the definition of a unit is. We learned about integration tests that may talk to external resources, such as databases, and subcutaneous tests, which exist just below the surface of the user interface. We also learned about functional UI tests that simulate an end user using the application. We learned that some tests, such as unit tests, offer great depth, whereas other kinds of tests, such as UI tests, give a broader coverage of the system being developed. We learned about the different logical phases an automated test can have, the arrange, act, and assert phases, and we saw how we can isolate sections of the code base using mock objects. We learned that we can reduce the amount of test code that exists by creating data driven tests, and we saw how we can bridge the communication gap between the development team and stakeholders by creating business readable tests that help to ensure that the correct features are being implemented. We learned some different ways of thinking about how many tests to have at each level, so we learned about the testing pyramid, and also how we can go beyond the testing pyramid, and to use a set of guidelines so we don't waste too much effort writing unnecessary tests. Finally, we learned about some of the characteristics of good automated tests, such as the importance of isolation, and also maintainability. Join me in the final module, where we'll learn how automated tests fit within the wider software development process.
Automated Testing Within the Software Development Process
Introduction

Hi, welcome back. In this module, we're going to be looking at automated testing within the software development process. So we're going to start off by seeing how we can complement our automated tests with manual testing, and we're going to learn about a specific form of manual testing called exploratory testing. We'll also learn a few different ways that we can get our QA specialists involved in the wider software development process. Next we'll learn how our automated tests fit in with the continuous integration process, and also the role of automated tests in enabling continuous delivery. We'll get an overview of how automated testing can be accomplished with test driven development, and we'll also briefly cover some advanced testing tools and techniques. And we'll wrap up the course with some further learning resources. So let's kick off this module by seeing how we can compliment automated tests with manual testing.
Complimenting Automated Testing with Manual Testers

So the first thing to realize here is that automated tests generally don't replace all manual testing. There's a whole class of defects that manual testing might uncover that our automated tests didn't. So in this course we've learned that we can create unit tests, integration tests, subcutaneous tests, and also functional user interface tests. To compliment these automated tests, we can have our QA specialists perform manual testing. Oftentimes this is done through the user interface of the application, but our QA specialists here might also have access to the underlying system database to check that the correct data is being written, or to investigate potential bugs. They may also have access to other files, such as data files or system log files. When we're complementing our automated tests with manual testing, our QA specialists here are probably not going to follow predetermined test scripts. So if our QA specialists here aren't following some predetermined test script, how are they going to test the application? One method they can use is exploratory testing.
Exploratory Testing

The Agile Alliance defines exploratory testing as a practice, a style, or approach to testing software, which is often contrasted to scripted testing. So earlier in the course we saw how we can replace manual scripted tests with automated tests. With scripted testing these predefined test scripts sit at the heart of the testing process. Our human testers here are simply execution engines for these predetermined test scripts. With exploratory testing on the other hand, our QA specialist sits at the center of the manual testing process. With exploratory testing, our QA specialist explores and investigates the system. Their job is to understand and learn more about the product. This can involve using the product in completely different ways, or trying to use the product in weird ways. As an outcome of exploratory testing, the QA specialist can report missing requirements, as well as probable bugs. The can also look for both predictable and unpredictable outcomes. Essentially in exploratory testing, the QA specialist can use their past experience with the product to guide future testing, rather than having any rigid test scripts. With exploratory testing, there's less up-front work required, for example, to go and create all of the predetermined test scripts in advance. Exploratory testing can be a lot more intellectually stimulating for the individual QA specialist, because they're not simply following a predetermined test script that was written by someone else. Exploratory testing can help generate ideas for future features of the product. It's worth mentioning that exploratory testing done right is a disciplined approach. It's not simply a person randomly clicking on buttons and playing with the software. Exploratory testing emphases the freedom, and also the accountability of the individual tester. During exploratory testing, when a bug a found a failing automated test can be written to reproduce it. When the bug is fixed, the test will now pass. In some future change if the bug reoccurs, it will be caught by the automated test. In this way, exploratory testing can help us identify any missing automated tests.
Getting QA Specialists Involved

Our QA specialists can add value throughout the project lifecycle, not just in the testing phase. For example, they can assist with requirements analysis, and take part in discussions. They may be able to help identify missing requirements, or potential testing difficulties with the features that are being proposed. QA specialists can also assist the business analysts. For example, helping the BAs to define the acceptance criteria for a feature that's going to be developed. QA specialists can also pair a program with developers when they're writing automated tests. This can help developers identify missing tests, or missing test case data.
Running Tests as Part of Continuous Integration

Let's take a look now at how automated tests fits in with the practice of continuous integration, and then we'll see how continuous integration, including our automated tests, fit in with the concept of continuous delivery. Continuous Integration is a software development practice where members of a team integrate their work frequently, usually each person integrates at least daily - leading to multiple integrations per day. Each integration is verified by an automated build (including test) to detect integration errors as quickly as possible. Part of Continuous Integration is the execution of our automated tests. Let's start off with our version control system here, so for example this could be get, and in this diagram we've got a dedicated continuous integration build server. We have a number of developers all writing code, so our first developer here commits some code to the version control system, the continuous integration build server can notice this, and kick off a new CI build. Our second developer here commits a change to the version control system, and this can kick off another build, and we have our third developer here, also committing to version control, and once again this might trigger a build on the CI Build Server. So typically when a build is triggered on the CI Build Server, the build server will get the latest changes from version control, it will try and compile or build the code, and if the compilation is successful, we can start off by running our unit tests. Normally we'd want to run the unit test first, because they're going to give us the quickest feedback to tell us if the latest code committed to version control is causing problems. We can then proceed to run the slower forms of tests, such as our integration tests, our subcutaneous tests, we could deploy the application or website to a test server, and then execute the user interface tests. These automated UI tests are going to execute more slowly than unit tests for example, so we wouldn't want to rely on these for quick feedback. Finally the Continuous Integration Build Server can report the results back to the project team.
The Role of Automated Tests in Continuous Delivery

Automation in general, and automated tests pacifically, are key components to enable continuous delivery. Continuous delivery is a practice that allows smaller changes to be deployed to production more frequently in an automated and predictable fashion, to deliver value more often, and decrease time to market for new features. So for example, we have our developer her committing code to the version control system. As we just saw, the Continuous Integration Build Server can notice this commit, and build a new version of the software. One of the outputs of the CI build can be the actual executable or deployable binary that may eventually make it to production. This deployable artifact can be stored in an artifact repository. This means that the same artifacts that were created by the CI Build Server, can be installed in the performance testing environment, in the user acceptance testing environment, and also in the production environment. One of the key things that enables continuous delivery is our automated test suite. Without the predictability and quick feedback of automated tests, it's going to be hard to get the required level of confidence in the time frames necessary to be able to release potentially multiple times per day to the production environment. One thing to note with continuous delivery is that we don't have to automatically deploy every build to production.
An Overview of Test-driven Development

One technique that can be used when creating automated tests is the practice of test-driven development. Test-driven development, or TDD, is a technique for building software that guides software development by writing tests. There's essentially three phases to TDD, the first phase is the creation of a failing test. So before we go and write the production code, we write a test that's going to fail. So for example, if we were adding division functionality to a calculator, we could write a failing test that divides 10 by 2, and asserts that the result is 5. Because we haven't yet written the division behavior, this test will initially fail. In the second phase, we actually go and write enough code until our test passes. So in our case this would be to implement the division functionality. The third phase of test-driven development is the refactor phase. In this phase, we clean up the code we've written to keep the code base maintainable and easy to understand. We then repeat this cycle until the required level of confidence is reached, and the new features are complete. Some of the benefits of TDD include helping the developer thing about the problem or requirement, before they go off and write the actual production code. This can help solidify what's required before wasting time and writing code that doesn't fulfill the requirements. Test-driven development can help produce better code designs by evolving the design over time, and driving that evolution from the tests. TDD can provide a good work rhythm or flow for developers. The automated test suite that results from TDD can be considered a byproduct, however these automated tests have a huge benefit in enabling us to find bugs in the future, and also the resultant tests help document what the system is supposed to do, though this benefit is not specific to TDD. Some of the potential downsides of test-driven development is that it's a skill that the developers will need to learn through practice. It's not something that you can just pick up in half an hour. Another potential downside is that the refactor stage can sometimes be neglected or omitted, this results in harder to maintain, messy code. Religious application of test-drive development may result in over-testing. In the previous module we learned that automated tests ultimately should give us a required level of confidence. This over-testing, the creation of too many tests, can also occur when we're not using TDD. Finally, one of the common mistakes of test-driven development is the over reliance on mock objects. Though this can be avoided with sufficient practice, and also making sure that the unit of behavior in our unit tests is not too small.
Advanced Testing Tools and Techniques

In addition to any unit testing frameworks that we may be using, there's a number of advanced testing tools and techniques that can be employed to improve the overall test suite. For example, we can use auto data tools that essentially help us in the arrange phase of tests, for example the creation of anonymous test data, this is test data that we don't really care what it's set to, only that we need to provide a value for the test to execute. One example of this is the AutoFixture library for. NET. Another testing tool that we can add into the mix is approval tests. This is available for a number of different platforms including Java and. NET, and essentially this lets the developer use their human intelligence to mark a result as complete. This approved result can then be compared to future test runs, and if the approved result differs in a future test run from the actual result, the test will fail. We can employ the concept of live testing during development, so as the developer's writing code, in the background the unit tests are executing, giving us live feedback, showing us if anything is currently broken from the changes we're making. The aim of this is to further reduce the feedback time of any errors that are being introduced. We can also employ code coverage tools, this lets us see the amount of production code that our tests are actually executing. So for example if we had 80% code coverage on our unit tests, it would mean that we have 20% of the production code that's not being executed during automated tests.
Summary and Further Learning

So that brings us to the end of this module. In this module, we learned how we can complement our automated tests with manual tests. We learned that our QA specialists, rather than working from test scripts, can perform exploratory testing. We also learned how we can get our QA specialists involved, including during requirements analysis, and also helping the business analysts to define acceptance criteria. They can pair program with developers, to help write automated tests. We learned that we can execute our automated tests as part of a Continuous Integration build, and how the automated tests in our Continuous Integration build are one of the things that enable continuous delivery. We got an overview of test-driven development, and how it consists of three phases, and finally, a brief overview of some advanced testing tools and techniques. So that also brings us to the end of this course. If you want to expand further on some of the topics covered in this course, there's a number of Pluralsight courses that may be of interest, including the Continuous Integration and Continuous Delivery: The Big Picture course, the Test-driven Development: The Big Picture course, and language or technology specific courses, such as Introduction to Testing in Java, Testing. NET Code with the xUnit. net 2 framework, ASP. NET Core MVC Testing Fundamentals, and also more advanced tool such as Better. NET Unit Test with AutoFixture, and the Approval Tests for. NET course. I'm Jason Roberts from Pluralsight, and I hope you enjoyed this course.
Course author
Author: Jason Roberts
Jason Roberts

With over 15 years of experience in both frontend and backend software development, Jason Roberts is a freelance developer, trainer, and author. He holds a Bachelor of Science degree in computing,...
Course info
Level
Beginner
Rating
4.5 stars with 83 raters(83)
My rating
null stars
Duration
1h 6m
Released
8 Aug 2017
Share course


