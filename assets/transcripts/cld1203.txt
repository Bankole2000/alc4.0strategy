Elastic Cloud Infrastructure: Scaling and Automation
by Google Cloud

In this course, Elastic Cloud Infrastructure: Scaling and Automation, you will learn about deploying practical solutions including securely interconnecting networks, customer-supplied encryption keys, security and access management, and more.

In this course, Elastic Cloud Infrastructure: Scaling and Automation, you will be introduced to the comprehensive and flexible infrastructure and platform services provided by Google Cloud Platform. Through a combination of video lectures, demos, and hands-on labs, you will explore and deploy solution elements, including infrastructure components such as networks, systems and applications services. This course also covers deploying practical solutions including securely interconnecting networks, customer-supplied encryption keys, security and access management, quotas and billing, and resource monitoring.

Course author
Author: Google Cloud	
Google Cloud
Google Cloud can help solve your toughest problems and grow your business. With Google Cloud, their infrastructure is your infrastructure. Their tools are your tools. And their innovations are your...

Course info
Level
Intermediate
Rating
0 stars with 4 raters
My rating
null stars

Duration
1h 48m
Updated
17 Jan 2019
Share course

Welcome to Elastic Infrastructure: Scaling and Automation
Elastic Cloud Infrastructure: Scaling and Automation Course Intro
Hello, and welcome. My name is Jasen Baker. I'm a Google Cloud trainer, and in this course, we're going to talk about Elastic Cloud infrastructure, scaling, and automation. So now that you've learned the foundations, you know where your applications are going to live, whether you're going to be deploying it through App Engine or virtual machines, maybe even containers, you need to start to think about scale. Hopefully you're very successful, and you're going to be growing, so we're going to learn about scaling and automation. In this case here, first of all, we may need to scale by interconnecting our on-premises networks. What about load balancing incoming/outgoing traffic across multiple machines? We've decided not to create a monolithic machine, but now we're distributing our workload because now we have the cloud. We can buy as many resources as we want. So how do we do this automatically? How do we automate the deployment? If we're deploying software applications over and over, we want to exclude and potentially eliminate any of that human error, so that's when we'll talk about automation, or maybe we'll just go into managed services. Why even mess with setting up an infrastructure that we came from when Google has created an entire environment where all we have to do is bring our data? That might be another consideration worth considering.

Interconnecting Networks
Module Overview (Intro)
Hello, and welcome. I'm Mylene Biddle, and I'm a technical curriculum developer for Google Cloud Platform. This is module 1, Interconnecting Networks. There are three general ways to interconnect networks. You can directly connect your data center network to Google's network; you can connect through an intermediary, which is a network service provider that also offers interconnect services; or you can use a technology like virtual private networking, also referred to as VPN, to create one or more secure tunnels through the public internet and connect through them. In this module, we will discuss Cloud VPN, Cloud Router, Cloud Interconnect, direct peering, Cloud DNS, and then you will complete a hands-on lab applying what you've learned about virtual private networks.

GCP Networking Overview
Google Cloud's well-provisioned global network is composed of hundreds of miles of fiber optic cable and seven submarine cable investments. This is a map of Google's global private fiber network. It spans a large majority of the most popular destinations across the world with multiple redundant links. This gives you connectivity to almost anywhere in world. You can see a spattering of where the different points of presence are, which is continuously growing. Listed on the slide are the current and upcoming regions for Google Cloud Platform. Notice a different number in each region. These are availability zones for the region. Please look at the Google Cloud Platform website for more information on regions and zones as they are constantly growing. Let's take a look at Google's Cloud networking as a whole. There are different regions and different levels of availability. From a global perspective, we have a global fiber backbone and networking services that can take advantage of that, including HTTPS, TCP, UDP load balancing, Cloud CDN, and Cloud DNS. If you want to create a virtual private network, you can create a private namespace using our virtual networking. Networking services that support global private spaces with regional segmentation include software network virtualization, global networks, and granular subnetting. For the hybrid cloud solution where you're connecting your cloud infrastructure to on-premises, we can take advantage of several services, including Cloud VPN, Cloud Router, and Cloud Interconnect. Services that enable permissions and control include network IAM roles through Identity and Access Management and firewall rules. You have the ability to connect to Google itself, not just Google Cloud Platform, but with Google as a whole. That includes YouTube, Gmail, Maps, and Android development. This can be done by using a VPN in Cloud Router through Cloud Interconnect or through direct peering. Next, we will discuss concepts around Cloud VPN and then do a lab on VPN. We will then discuss Cloud Router, Cloud Interconnect, external peering, and Cloud DNS.

Cloud VPN (Virtual Private Networks)
As we discussed earlier, Google Cloud VPN securely connects your on-premises network to your GCP VPC network through an IPSec VPN connection. Traffic traveling between the two networks is encrypted by one VPN gateway and then decrypted by the other VPN gateway. This protects your data as it travels over the internet. VPN devices in Google Cloud are software devices. There is no actual VPN on the other side. We do all we can to ensure compatibility between the on-premises hardware version with specific versions of IKE, which can also be run over Cloud Interconnect. Other details about Google Cloud VPN can be found on this slide, as well as in the Google Cloud documentation. With static routing, updating the tunnel requires the addition of static routes to GCP and restarting the VPN tunnel to include the new subnet. This diagram shows the example topology using a VPN tunnel to connect a Google Cloud network and 29 subnets, 1 per rack, in the on-premises network. Here we are adding the 30th subnet.

Lab: Virtual Private Networks (Overview and Objectives)
Lab: Virtual Private Networks (Review)
In this lab, you learned how to set up VPN between two subnets in different regions. You learned how to perform most of the configuration from the command line and how to configure VPN using the GPC console. An important objective of the lab is to show you how to configure VPN manually so that you will better understand what the GCP console does automatically. This can help in troubleshooting a configuration. You used two tunnels and configured a path in one direction in each tunnel. In practice, a single VPN tunnel could be used. More commonly, two or more tunnels are used for reliability and availability. In some cases, redundant tunnels are configured between separate VPN gateways to protect against the loss of a single gateway.

Cloud Router
Let's explore concepts around the Cloud Router service. Cloud Router is basically a router that runs in the cloud. The whole concept is to provide BGP routing. BGP allows you to dynamically discover and advertise new route changes that might be in your network. If you're peering with Google and you have a number of internal routes and you want us to know about new routes that might become available, especially if you're peering with us in multiple locations, you can utilize Cloud Router for that purpose. Cloud Router supports graceful restart and ECMP and supports standard routing features including AS Path and AS Prepend. Here's an example for using Cloud Router with subnetworks. Let's just say we've spun up a new environment in the cloud called Analytics. We have a new subnet range, right here, that we need to make available and make known to an on-premises peering network. Utilizing Cloud Router, it would announce that the new subnet is now available. Then the existing BGP router on-premises will be able to update its routing tables and know how to get to this new allocated subnetwork for the analytics process and vice versa. If you add a new subnet range, a new physical rack goes online with a different set of subnets that can be identified and can tell the Cloud Router on the cloud side that a new network exists. And so now these applications, Test and Prod, perhaps they're funneling data back to the on-premises network. We'll now have a method to get those routes back as well.

Cloud Interconnect
Let's learn about Google Cloud Interconnect. Google Cloud Interconnect provides direct physical connections in RFC1918 communication between your on-premises network and Google's network. Cloud Interconnect enables you to transfer large amounts of data between networks, which can be more cost effective than purchasing additional bandwidth over the public internet or using VPN tunnels. Traffic between your on-premises network and your VPC network doesn't traverse the public internet. Traffic traverses a dedicated connection with fewer hops, meaning there are less points of failure where network traffic might get dropped or disrupted. Your VPC network's internal IP addresses are directly accessible from your on-premises network. You don't need to use a NAT device or VPN tunnel to reach internal IP addresses. Currently, you can only reach internal IP addresses over a dedicated connection. To reach Google external IP addresses, you must use a separate connection. You can scale your connection to Google based on your needs. Connection capacity is delivered over one more 10 Gbps Ethernet connections with a maximum of 8 connections. That's 80 Gbps total per interconnect. The cost of egress traffic from your VPC network to your on-premises network is reduced. A dedicated connection is generally the least expensive method if you have a high volume of traffic to and from Google's network. The minimum deployment per location is 10 Gbps. If your traffic doesn't require that level of capacity, consider Cloud VPN. The circuit between your network and Google's network is not encrypted. If you require additional data security, use application-level encryption or your own VPN. Currently, you can't use Google Cloud VPN in combination with a dedicated connection, but you can use your own VPN solution. Before you use dedicated interconnect, consider the following: You must be familiar with basic network interconnections. You'll be ordering and configuring circuits. Your network must physically meet Google's network in a co-location facility. You must provide your own routing equipment. In the co-location facility, your on-premises network devices must support the following technology requirements: They must use single-mode fiber, IPv4 link-local addressing, LACP for bonding multiple links, EBGP-4 with multi-hop, and 802. 1Q VLANs. In the practical application of a data-intensive application, you'd want to take advantage of using Cloud Interconnect, not just to simply save on bandwidth connection cost, but also because you want to ensure that you can take advantage of onsite backup. If you want to back up huge amounts of data to Google Cloud Storage, you would invest the time to set up a pairing arrangement to do so. In the practical application of latency-sensitive applications, you might have local web servers, but they might be pulling data from on-premises database servers because the data might be restricted and not allowed to be in the cloud for whatever security policy reasons. Peering with Google will ensure that you have the lowest latency possible between the two locations. Cloud Interconnect allows you to take advantage of your content delivery network, CDN, or your CDN partner network. While Google does offer its own CDN, it's still rather restrictive in its capabilities, and leveraging our CDN partners will provide you access to a lot more features. It's best to research which one may be most applicable to you. You may benefit by lower rates or lower latency to the CDN partner services. More information can be found in the CDN Interconnect documentation.

External Peering
Let's learn about the different GCP services to achieve external peering. Direct peering is that private connection between you and Google or your company and Google where we exchange autonomous system numbers. An autonomous system number is your identity on the internet that represents your block of IP addresses. It's much like a domain name that reflects your company's name, which translates to a single IP. An autonomous system number represents your block of owned IP addresses on the internet. If you were to announce your autonomous system number, you could describe it as the route in order to get to me as a company. Here's a good visual for direct peering. Google allows you to establish a direct peering connection between your business network and Google's. With this connection, you can exchange internet traffic between your network and Google's at one of their broad-reaching edge network locations. Direct peering with Google is done by exchanging BGP routes between Google and the peering entity. After a direct peering connection is in place, you can use it to reach all of Google's services, including the full suite of Google Cloud Platform products. Here's a visual to demonstrate carrier peering. As you can see, it's a similar setup, but you have a service provider, so-called middleman. Carrier peering allows you to obtain enterprise-grade network services that connect your infrastructure to Google by using a service provider. You can get connections with higher availability and lower latency using one or more links. As discussed before, here are some more details about autonomous systems. If you do want to peer with Google, you want to look up our ASN number, which is 15169. Here are some key concepts of peering. Border Gateway Patrol, BGP, is used to route traffic among internet service providers, ISP, or any entities who are assigned their own ASNs. Private Network Interconnect, PNI, means private peering. PeeringDB is a freely available web-based database of networks that are interested in peering, and it's a great resource for identifying candidates for peering. Here's an example of peering locations displayed by PeeringDB for the ASN 15169. As we said before, that's Google. Listed on this slide are further details about direct peering with Google. Please take a minute to read over the details, and you can access direct peering documentation online for more information. You can share GCP VPC networks across projects in your cloud organization using Shared VPC. In large organizations, you may need to put different departments or different applications into different projects for purposes of separating budgeting, access control, etc. With Shared VPC, cloud organization administrators can give multiple projects permission to use a single Shared VPC network and corresponding networking resources. Shared VPC allows for the creation of a VPC network of RFC1918 IP spaces that associated projects can use. With Shared VPC, you can allow project admins to created VMs in the Shared VPC network spaces and allow network and security admins to create VPNs and firewall rules that are usable by the projects in the VPC network. Shared VPC makes it easy to apply and enforce consistent policies across a cloud organization. The diagram shows a host project sharing its VPC network with two service projects. It is sharing Subnet_1 with one project and Subnet_2 with another project. The Standalone project has not been designated a service project, so it can't share resources with the host project. You can connect two VPC networks regardless of shared projects or organizations using VPC Network Peering. It allows you to build Software as a Service ecosystems in GCP by making services available privately across different VPC networks in and across organizations and allowing workloads to communicate in private RFC1918 space. It's useful for organizations with several network administrative domains that want to peer with other organizations. VPC Network Peering gives you several advantages over using external IP addresses or VPNs to connect networks, including network latency. Public IP networking suffers higher latency than private networking. Network security. Service owners do not need to have their services exposed to the public internet and deal with its associated risks. Network cost. GCP charges egress bandwidth pricing for networks that use external IPs to communicate, even if the traffic is within the same zone. However, if the networks are peered, they can use internal IPs to communicate and save on those egress costs. Regular network pricing still applies to all traffic. The Internal Load Balancing diagram shows how VM instances in network-B, right here, access the load balanced backends in network-A. The internal load balancing configuration from network-A is automatically applied to network-B in this case. Internal load balancing services are available to clients in directly peered networks only. That is, in the case that network-B peers with network-C, the internal load balance backends in network-A will not be reachable from clients in network-C. If you have peering between your VPC network and another VPC network, you want to block traffic to a given set of VM instances or internal load balancing endpoints. You must use firewall rules to do this because there is no way to exclude certain VM instances or internal load balancers from the peering. If you want to disallow communication with certain VM instances or internal load balancers, you can install ingress firewall rules on the network you want to block the communication to. The diagram shows you how you could create a firewall rule to allow all traffic from subnets 1 and 2 in Network-A to subnet-4 in Network-B and deny traffic from subnets 1 and 2 in Network-A to subnet-3 in Network-B. VPC Network Peering allows peering with a Shared VPC. As discussed earlier, a Shared VPC host project allows other projects to use one of its networks. In the Shared VPC diagram right here, Network-SVPC is in a Shared VPC network in Host Project P1. Service Projects P3 and P4 are able to attach VM instances to Network-SVPC peers with Network-A. This results in the following: VM instances in Shared VPC service projects that are using the Network-SVPC have private internal IP connectivity with any endpoints associated to Network-A. VM instances associated to Network-A will have private internal IP connectivity with any endpoints associated to Network-SVPC regardless of whether they live in the host project or a service project. You can set up VPC Network Peering between two Shared VPC networks. An instance can have multiple network interfaces, one each in different VPC networks, which is shown here on the diagram on the right. In the diagram, VM1, right here, has a network interface in both Network-A and Network-B. Network-B is peered with another, Network-C. IP3 can send traffic to IP2 because IP2 is in Network-B, and Network-B routes are automatically propagated to Network-C when the two networks are peered. For IP2, right here, to send traffic to IP3, you'd have to configure policy routing for the IP2 interface. Flows for IP1 are not installed in Network-C, so Network-C cannot access IP1. Private Google access enables virtual machine instances on a subnetwork to reach Google APIs and services using an internal IP address instead of an external IP address. External IP addresses are routable and reachable over the internet. Internal, or private IP, addresses are internal to Google Cloud Platform and are not routable or reachable over the internet. You can use private Google access to allow VMs without internet access to reach Google services. The services that can be reachable include, but are not limited to, the following: Cloud Spanner, Google Cloud BigQuery, Google Cloud Bigtable, Google Cloud Dataproc, Google Cloud Datastore, Google Cloud Pub/Sub, and Google Cloud Storage. Private Google access does not apply to Google Cloud SQL. You do not get private connectivity to Cloud SQL when you use private Google access.

Cloud DNS
The last topic we will cover in this module is Cloud DNS. Google Cloud DNS is the only service inside of Google that offers a 100% SLA. That is a 100% uptime guarantee. In fact, because nothing runs unless your DNS can resolve you to an IP address, there may be small application issues, small availability issues, but if you can't look up a domain name, the internet might as well be down. So that's why you have never gone to google. com, and it says sorry, I can't find this IP address. This is very important to know. You can actually host all of your authoritative DNS records directly through Cloud DNS. You can call those through an API or through the web console. Cloud DNS managed zones are an abstraction that manages all DNS records for a single domain name. One project may have multiple managed zones. You must enable the Cloud DNS API in the GCP console first by running gcloud dns managed-zones. Managed zones provide permission controls at project level and allow you to monitor propagation of changes to DNS name servers.

Module Review (Outro)
You should keep a few factors in mind when choosing a method to interconnect networks. First, how much bandwidth does your application require? Second, what is your application's tolerance for connection downtime? Perhaps this can be described in a number of minutes per year. Finally, does your application require a service level agreement? The content in this module should assist in helping you make the right decision for your infrastructure.

Load Balancing
Module Overview (Intro)
Hello, and welcome. I'm Mylene Biddle, and I'm a technical curriculum developer for Google Cloud Platform. This is module 2, Load Balancing. The purpose of a hardware load balancer is to receive data at one IP address and redirect it to multiple servers. Load balancing as a service in GCP is much more flexible than a hardware load balancer. For example, load balancing in GCP can be configured to redirect sessions based on the geographic location of the requester, sending the traffic to the closest available server. GCP load balancing can be configured to send traffic to pools of servers with some pools serving as overflow capacity to others. This kid of flexibility and behavior comes with additional configuration complexity, which can all be easily configured using the GCP console. In this module, we will cover managed instance groups and different types of load balancing that you can configure using GCP. We can use load balancing to take more advantage of an augmented infrastructure. We've already configured networking between different virtual machines, but how are we going to route our traffic between multiple virtual machines? Load balancing is the first thing that comes into play. There are five types of load balancing covered in this module. We will get into each one in the following lessons, but it's important to note that HTTPS, SSL proxy, and TCP proxy load balancing are global services, whereas network and internal load balancing are regional. You will complete a hands-on lab that includes both network and internal load balancers. Let's get started.

Managed Instance Groups
Let's learn about managed instance groups before we delve into load balancers. Managed instance groups provide the following benefits. They manage instance groups using an instance template to define the properties for every instance in the group. You can easily update all of the instances in the group by specifying a new template in a rolling update. When your applications require additional compute resources, managed instance groups can automatically scale the number of instances in the group. Managed instance groups can work with load balancing services to distribute network traffic to all of the instances in the group. If an instance in the group stops, crashes or is deleted by an action other than the instance group's commands, the managed instance group automatically recreates the instance so it can resume its processing tasks. The recreated instance uses the same name and the same instance template as the previous instance, even if the group references a different instance template. Managed instance groups can automatically identify and recreate unhealthy instances in a group to ensure that all of the instances are running optimally. In addition to managed instance groups that belong in single zones, you can create regional managed instance groups, which distribute instances across multiple zones in the same region. Regional managed instance groups also support autoscaling, network load balancing, and HTTPS load balancing. You can easily create instance templates using the console. The Instance Template dialog looks and works exactly like creating an instance except that the choices are recorded so that they can be repeated. The steps to create managed instance groups are as follows. You're going to create an instance template, then you're going to create a managed instance group of n specified instances, then the instance group manager automatically populates the instance group based on the instance template. When you create an instance group, you define the specific rules for the instance group. So, is it going to be single or multi-zoned? Where will those locations be? What ports are we going to allow and load balance across? Are we going to autoscale, and under what circumstances do we want to maintain a minimum number of instances? And, what will the health check be in order for us to send traffic to those destination locations? Essentially you're still creating virtual machines, but you're applying more rules to that instance group. When creating a health check, you define periodicity and define the health check. A health check is similar to Stackdriver in that you're specifying the page you want to reach, and you can configure complex criteria to determine under what conditions we want to send traffic to that specific page. In Stackdriver, you can use metrics to determine whether you want to react to certain configurations and send an alert or configuration. The same kind of health check capabilities are built in here when it comes to managing instance groups, load balancing, and autoscaling. The health checker pulls instances at specified intervals. Instances that do not respond successfully to a specified number of consecutive probes are marked as unhealthy. No new connections are sent to such instances, although existing connections are allowed to continue. The health checker continues to pull unhealthy instances. If an instance later responds successfully to a specified number of consecutive probes, it is then marked healthy again and can receive new connections. GCP health checks support HTTP, HTTPS, TCP, and SSL or TLS. To ensure high availability, GCP creates redundant copies of each health checker. These redundant health checkers also probe your instances. If any health checker fails, a redundant one can take over with no delay. If you examine the logs on your instance, you might see health check polling happening more frequently than what you have configured. This is because the redundant health checkers are also following your settings. These redundant health checkers are created automatically and are not separately user-configurable. Managed instance groups offer autoscaling capabilities that allow you to automatically add or remove instances from a managed instance group based on increases or decreases in load. Autoscaling helps your applications gracefully handle increases in traffic and reduces costs when the need for resources is lower. You just define the autoscaling policy and the autoscaler performs automatic scaling based on the measured load. Autoscaling works by scaling up or down your instance group. That is, it adds more instances to your instance group when there is more load, upscaling, and removes instances when the need to for instances is lowered, downscaling. This slide shows some features of connection draining. Connection draining delays the termination of an instance until existing connections are closed. New connections to the instance are prevented, and an instance preserved existing sessions until they end, or a designated timeout is reached, from 1 to 3600 seconds. This minimizes interruptions to your users. The following managed instance group features are still in beta. The Instance Group Updater is an interesting feature. It provides zero downtime and staggered releases, and it allows you to apply rolling updates, as well as canary updates with rollback. Autohealing is a useful features that allows for automated server monitoring and restarts. If an HTTP health check sees that a service has failed on an instance, the instance can be recreated where the service failed.

HTTP(S) Load Balancing
Now we're going to delve into HTTPS load balancing. GCP HTTPS load balancing provides global load balancing for HTTPS requests destined for your instances. You can configure URL rules that route some URLs to one set of instances, and route other URLs to other instances. Requests are always routed to the instance group that is closest to the user, provided that the group has enough capacity and is appropriate for the request. If the closest group does not have enough capacity, the request is then sent to the closest group that does have capacity. HTTPS load balancing supports both IPv4 and IPv6 addresses for client traffic. Client IPv6 requests are terminated at the global load balancing layer, then they're proxied over IPv4 to your back ends. HTTP requests can be load balanced based on port 80 or 8080. HTTPS requests can be load balanced on port 443. The load balancer acts as an HTTP/2 to HTTP/1. 1 translation layer, which means that the web servers always see and respond to HTTP/1. 1 requests, but that request from the browser can be HTTP/1. 0, 1. 1, or 2. HTTP/2 server push is not supported. Here's a walkthrough of HTTPS load balancing. The difference is we're coming in through the global internet. We have the same forwarding rules, which we did before based on IP address, protocol, and port, but now we have a target proxy. That's going to be a URL map. We can get a little more granular here if there are different URLs that are being passed to us. Maybe we want to change which back-end service is going to handle that. A back-end service is going to be similar to a target pool, but we're going to define additional health checks to define when we send traffic to that pool. We don't want to just send traffic to Tokyo because the user is in Japan if the group is down, or perhaps maybe if the instance group is overloaded and we've maxed out the number of servers that we're going to scale. Consider the different health checks that might occur for us to continue that forwarding of traffic. Since we can route traffic to completely different regions, it might even be a failover mechanism. If for some reason latency is too high in Asia, we could route traffic to Southeast Asia. We always apply the firewall rules before we do any type of ingress traffic to be considered before it hits the back end. The back-end service will not send requests to instances reported as unhealthy by the health check. Session affinity can be set so that requests from the same client go to the same instance. Client IP affinity directs requests from the same origin IP address to the same server. NAT and other network routing technologies can cause requests from multiple different users to look like they come from the same address, causing many users to get routed to the same instance. In contrast, one user who moves from network to network may be seen as two different users and not be directed to the same instance. Generated cookie affinity causes the load balancer to issue a cookie named GCLB on the first request from a user, then directs subsequent requests with the same cookie to the same instance. Session affinity can break if an instance group runs out of capacity and traffic is routed to another zone, if autoscaling, change capacity, and load is reallocated, or if the target instance fails health checks. The balancing mode and capacity scaler designate maximum utilization for a back end. The balancing mode uses one of the following metrics to determine whether the back-end instance group is at capacity and the load balancer needs to send requests to another back end. CPU utilization, maximum requests per second per instance, maximum requests per second per group, and CPU utilization and rate. The capacity scaler is an additional setting that directs the load balancer to only direct requests to a given back-end instance group as long as utilization is below a percentage of the balancing mode maximum. For example, if balancing mode is utilization and max CPU utilization is 80%, setting the capacity scaler to 50% would mean the load balancer would see the back end as being at capacity when CPU utilization is at 40% average across the entire instance group. Here's a great visualization of HTTP load balancing in action. Here we have two example users, one coming from North America and one coming from EMEA. Now you have one single IP address representing the globe, but they're entering the Google Cloud Network from different locations. A load balancer is not a physical thing, it is a concept, a list of rules that are applied to any ingress traffic no matter where in the world it comes from. That's why we have global load balancers. They're just a set of rules that come into our software to find networks. If we have two different users who are coming to a guest book application and hit our proxy, what is the URL map that we're going to look for? Once we define our URL map, we're going to send the traffic to our back-end service. Are we going to send that individual user based on their location, their IP address, protocol, etc., to a managed instance group in the US, or to one in EMEA? URL maps are where you can actually route traffic not based on protocol, IP address, or port, but specifically by what was in the URL header. So let's just say you're processing videos and you're accepting those to come in. Now what you're doing is you have a global forwarding rule, which sends it to your HTTP proxy. Let's say you're going to handle HD video separate from standard definition videos, so you might actually map those to a different back-end service. So here you might have a high-definition service, which will then use a URL match to send it to different rules. So we're going to look at that rule and determine we're going to video/hd, which means it's going to our hd-service, or video/sd, and then we'll send that to the sd's back-end service. So when you have 50 different back-end services to choose from, you get a lot of flexibility depending on the incoming ruleset. Now traffic allocation for back-end services is going to be determined based on the zone, regional or multiple regions. These are all things that you can define as well, so again, it's going to be based on health checks, URL rewrites, and whatever other protocols you choose to take advantage of. HTTPS load balancing does use SSL. The HTTPS load balancers handle the termination of the SSL session, and that can be quite important, especially if you're spreading this across multiple locations and spinning up new VMs. Otherwise you'd have to have a single, central repository to retrieve the certificate each time. You can also use SSL load balancing, which then the target proxy and the VMs actually terminate the SSL session. To use HTTPS or SSL load balancing, you'll have to create at least one SSL certificate to be used by the target proxy for the load balancer. Each target proxy can be configured with up to 10 SSL certificates, and each SSL certificate has a created SSL certificate resource. Listed are additional informational points about HTTPS load balancing.

Cross-region and Content-based Load Balancing
Now let's learn a little more about cross- region and content-based load balancing. Cross-region load balancing is useful if you have users all across the world. Of course, this only applies to HTTP and HTTPS load balancers because those are the only globally available load balancers. By default, we will route those requests to the closest regions unless you have URL maps and different rewrites to do so. The great thing is this eliminates the need for a DNS load balancer because utilizing geo-IP lookups for DNS locations can be a little more unreliable, and so with this global load balancer, we can see the destination because we can identify where they actually enter the Google network in order to handle that HTTPS load balancing. In this example, we use an HTTP load balancer to route traffic between multiple regions and multiple zones. Again, because that's the only global availability you can do, you get that single global static IP address representing your entire front-end application. Content-based load balancing applies to HTTP and HTTPS only and involves multiple back-end services to handle different content types. You add path rules to each back-end service such as /video for video services and /static for static content. You can then configure different instance types for different content types. In this example, we're coming in through that HTTP load balancer, splitting traffic content based on user location, and we can do it based on the URL header. So if they're going to /video, we can send that to our back-end video service, or if they're just going to the front-end website, we can send them to a front-end cluster specifically that handles that.

SSL Proxy/TCP Proxy Load Balancing
Let's learn a little more about SSL proxy and TCP proxy load balancing. SSL proxy load balancing performs global load balancing of SSL traffic, routing clients to the closest instance with capacity. Some advantages of SSL proxy include intelligent routing, better utilization of the virtual machine instances, certificate management, and security patching. In this example, traffic from users in Iowa and Boston is terminated at the global load balancing layer and a separate SSL connection is established to the selected back-end instance. TCP proxy load balancing allows you to use a single IP address for all users around the world and automatically routes traffic to the instances that are geographically closest to the user. Advantages of TCP proxy load balancing include intelligent routing and security patching. In this example, traffic from users in Iowa and Boston is terminated at the global load balancing layer and a separate TCP or SSL connection is established to the selected back-end instance.

Network Load Balancing
Let's learn a little more about network load balancing. Network load balancing allows you to balance the load of your systems based on incoming IP protocol data such as address, port, and protocol type. Network load balancing uses forwarding rules that point to target pools, which list the instances available for load balancing and define which type of health check should be performed on these instances. Network load balancing is a regional, non-proxied load balancer. You can use it to load balance UDP traffic and TCP and SSL traffic on ports that are not supported by the SSL proxy and TCP proxy load balancers. A network load balancer is a passthrough load balancer. It does not proxy connections from clients. Here we start with a region because network load balancing is regionally available across multiple zones. Now, as traffic is coming in through the internet, you're going to have a list of different forwarding rule protocols. Is it going to be based on IP address, perhaps it's going to be based on protocols, so maybe you divide traffic between port 443 and port 8080 or different port ranges. You set up the different rules, and then those will be forwarded to a target pool of instances. That pool could actually be different Compute Engine instances. One might be a web-based application server and the other could be a SQL back end. We consider to see if there's a firewall rule that allows this traffic. We process the firewall rule and then we forward it to the individual target pool. If you want a failover, you can also set up a health check and then have a failover target pool, which could exist elsewhere. So let's just say you have a different set of resources that you can failover in case this target pool fails. It isn't restricted to failover, it could also apply to a continuous deployment model. Now how does network load balancing differ when you're running a managed instance group? Here a target pool can have different instances. A managed instance group, however, is going to be all the same exact type of virtual machines. As traffic is coming in, you're going to consider the autoscaler. The autoscaler will have its own rules to determine as to whether we want to spin up additional virtual machines within this target pool. The benefit of running a managed instance group is it does then tell the load balancer, hey, there's these new virtual machines as part of the pool, consider them when you're actually forwarding us traffic. Managed instance groups are created from an instance template. An instance template just defines the virtual machines that were spinning up, so it's a preconfigured environment and you're going to simply spin up additional virtual machines or spin down, depending on the workload requirements. And the great thing is, a managed instance group tells the load balancer when that group has changed so it can adjust the routing for the target pool accordingly. Here are some additional detailed lists of what forwarding rules consist of, and these will be covered in the lab. Of course, you can manage these forwarding rules using the Cloud Platform console, the gcloud utility, or the REST API if you're going to make it programmatically available. A target pool resource defines a group of instances that should receive incoming traffic from forwarding rules. When a forwarding rule directs traffic to a target pool, Google Compute Engine picks an instance from these target pools based on a hash of the source IP and port, and the destination IP and port. Target pools can only be used with forwarding rules that handle TCP and UDP traffic. For all other protocols, you must create a target instance. You must create a target pool before you can use it with a forwarding rule. Each project can have up to 50 target pools. A target pool can have only one health check. Network load balancing only supports HTTP health checks. Network load balancing supports Compute Engine autoscaler, which also allows users to perform autoscaling on the instance groups in a target pool based on CPU utilization or custom Stackdriver monitoring metrics. Session affinity influences load distribution. The hash method selects a back end based on a subset of the source and destination IP, the source and destination port, or the layer 4 protocol, whether it's TCP or UDP. Possible hashes are as follows. You can have a hash of NONE in the five-tuple hashing, which uses the source and destination IP, source and destination ports, and the protocol. Each new connection can end up on any instance, but all traffic for a given connection will stay on the same instance if the instance stays healthy. Then we have CLIENT_IP_PROTO. It is a three-tuple hashing and it uses the source and destination IPs and the protocol. All connections from a client will end up on the same instance, as long as they use the same protocol and the instance stays healthy. Then we have CLIENT_IP. It is a two-tuple hashing, which uses the source and destination IPs. All connections from a client will end up on the same instance regardless of protocol as long as the instance stays healthy. Five-tuple hashing provides a good distribution of traffic across many virtual machines. However, a second session from the same client may arrive on a different instance because the source port may change. If you want all sessions from the same client to reach the same back end. As long as the back end stays healthy, you can specify CLIENT_IP_PROTO or CLIENT_IP options. In general, if you select a three-tuple or two-tuple method, it will provide for better session affinity than the default five-tuple method, but the overall traffic may not be as evenly distributed.

Internal Load Balancing
Let's learn a little more about internal load balancing. Internal load balancing enables you to run and scale your services behind a private load balancing IP address, which is accessible only to instances internal to your VPC. This example shows a three-tier web application with HTTPS, load balancing, and internal load balancing. User traffic goes through the global HTTPS load balancer and is directed between different instances in the web tier. Two internal load balancers then direct traffic regionally to an internal tier for specified zones.

Load Balancing Best Practices
Let's learn about some load balancing best practices. Try if you can to spread and balance across multiple zones in a region. The more the better because these can be just chosen at random, but in this case it's going to prevent any type of zonal outages. Try to overprovision, because the whole purpose of being resilient is in case there's a failure, you can failover to another one. But you don't want your application to suffer latency requirements because now you lost half of it because you only split 50%, so try to provision at 100% for two-thirds of an outage, or 150% for a full-service outage. So if you can go across three zones, obviously it's going to be ideal. You can automatically create failures on VMs. We will talk about metadata tags later, but what you can do is you can actually tag a virtual machine with a special name called failure_zone, and that will automatically remove it from the load balancer. This way you can test for particular failures to see how well it scales up and down or it recreates in case of failure. Because Compute Engine offers a great deal of flexibility in how you configure load balancing, it is possible to create configurations that do not behave well. Please keep the following restrictions and guidance in mind when creating instance groups for use with load balancing. Do not put a virtual machine instance in more than one instance group. Do not delete an instance group if it is being used by a back end. Your configuration will be simpler if you do not add the same instance group to two different back ends. If you do add the same instance group to two back ends, both back ends must use the same balancing mode, either utilization or rate. You can use max rate per instance and max rate per group together. It is acceptable to set one back end to use max rate per instance and the other to use max rate per group. If your instance group serves two or more ports for several back ends respectively, you have to specify different port names in the instance group. All instances in a managed or unmanaged instance group must be in the same VPC network and, if applicable, the same subnet. If you were using a managed instance group with autoscaling, do not use the max rate balancing mode in the back-end service. You may use either the max utilization or max rate per instance mode. Do not make an autoscaled managed instance group the target of two different load balancers. When resizing a managed instance group, the maximum size of the group should be smaller than or equal to the size of the subnet. What about securing our load-balanced servers? For load-balanced servers, you definitely want to create firewall rules. You should create firewall rules that allow traffic only from GCP load balancer networks. You can further secure your instances by disabling their external IP addresses, then the load balancers can use the internal IP addresses. You can then use a bastion host to manage the instances.

Lab: Virtual Machine Automation and Load Balancing (Overview and Objectives)
Lab: Virtual Machine Automation and Load Balancing (Review)
In this lab, you created a pool of web servers, directed traffic to the web servers through an external network load balancer, and then you tested for high availability by shutting down one of the servers. You then launched two additional web servers in a secondary zone and configured an internal load balancer for work distribution and availability. I hope you enjoyed applying the concepts learned in this module in a real world application.

Module Review (Outro)
As you learned in this module, there are several kinds of load balancing available to you through GCP. In some cases, alternative configurations than what we presented would work. Before you choose load balancing configurations, give some consideration not only to what the application requirements are today, but to where your requirements might be as your application and infrastructure evolve in the future.

Autoscaling
Module Overview (Intro)
(Music) Hello, and welcome. I'm Mylene Biddle, and I'm a technical curriculum developer for Google Cloud Platform. This is module 3, Autoscaling. In the last module, we learned about managed instance groups. Managed instance groups offer autoscaling capabilities that allow you to automatically add or remove instances based on increases or decreases in load. Autoscaling helps your applications gracefully handle increases in traffic and reduces costs when the need for resources is lower. In this module, we will discuss autoscaling concepts, policies, and configuration. You will then complete a hands-on lab that includes setting up an autoscaling application and triggering scaling. Let's get started.

Autoscaling
Autoscaling is available as part of the Compute Engine API, so it can be configured via the gcloud command, through a REST API, or through the web-based interface. The purpose of autoscaling is to automatically scale the number of instances in a managed instance group based on a specific type of workload. This can help to reduce cost because it can shrink in the number of instances running. Also, it can autoscale out in order to handle additional amounts of traffic. You can only create one autoscaler per managed instance group. You can't have conflicting rulesets, so once you've grouped up your virtual machines, there will be one autoscaler defined specifically for that particular group. Autoscalers can also be used with zone- based managed instance groups or regional managed instance groups. That means you can build in high level of redundancies, having your virtual machines spread across multiple zones within a region, and then you can use autoscaling to scale those proportionately so you don't have 10 in one region and then 1 in the other. You can actually make it five and five, for example. Autoscaling is also very fast. It will start to look at a maximum of a 1- minute moving window, so it's going to look very often in order to determine if another virtual machine needs to be started. Conceptually, this is how autoscaling works. You have your autoscaler, you have different instructions as to what will cause your instance group here to be updated. These managed instance groups are going to have prebuilt-in templates, which are going to be pulled in from Google Compute Engine. This is where you simply define instance type, specify an image, etc. It might even pull indirectly from an image service, and basically what we're going to do is scale out or scale in certain virtual machines. You can include preconfigured startup and shutdown scripts and specify if there are any pre-configurations needed before the device is added to the cluster. An important point to remember is that you have to define the maximum number of virtual machines running, as well as the minimum number to ensure that we can maintain these at all times.

Policies
Let's learn a little more about autoscaling policies. Policies determine your autoscaling behavior. You have a few policy options. First of all, you can scale based on CPU utilization. We can also scale based on HTTP load balancing. In this case, we can measure for the hardware components, but also maybe the amount of requests per second. The autoscaler will collect information based on the policy, compare it to your desired target utilization, and then determine if it needs to perform scaling. The target utilization level is the level at which you want to maintain your virtual machine instances. For example, if you scale based on CPU utilization, you can set your target utilization level at 75% and the autoscaler will maintain the CPU utilization of the specified group of instances at or close to 75%. The utilization level for each metric is interpreted differently based on the autoscaling policy. The autoscaler does allow multiple policies, but it's only going to be based on a single managed instance group, so you're not going to have multiple instance groups. You can decide under certain circumstances, do I want to scale based on CPU, do I want to scale based on memory, maybe I want to scale based on some custom metrics, or whichever one happens first. What happens is the autoscaler's going to deal with multiple policies by calculating the number of virtual instances for each individual policy that's going to leave the largest number of virtual machines in the group. Here's an example of what an autoscaling policy looks like from the command line in which we have defined a number of replicas exactly what the CPU utilization threshold should be across the entire pool. The command creates an autoscaler that has a target CPU utilization of 75%. Setting a. 75 target utilization tells the autoscaler to maintain an average usage of 75% among all cores in the instance group. The cooldown period is the number of seconds the autoscaler should wait after a virtual machine has started before the autoscaler starts collecting information from it. This accounts for the amount of time it can take for a virtual machine to initialize. During this time, the instance group may exceed the scaling threshold, but it won't launch new instances until the starting instance is available for measurement, at which point it will reevaluate load. For this scaling-out policy example, we're using CPU target utilization, we're checking to see if the aggregate of virtual machines average out to above 75%. We will continue to scale out virtual machines until we fall below that threshold. To explain even further, the percentage utilization that an additional VM contributes depends on the size of the group. The fourth VM added to a group offers 25% increase in capacity to the group. The tenth VM added to a group only offers 10% more capacity, even though the VMs are the same size. In the diagram, the autoscaler is conservative and rounds up. In other words, it would prefer to start an extra VM, that it isn't really needed, and to possibly run out of capacity. In this example for a scaling-in policy, removing one VM doesn't get close enough to the target of 75%. Removing a second VM would exceed the target. The autoscaler behaves conservatively, so it will shut down one VM rather than two VMs. It would prefer underutilization over running out of resource when it is needed.

Configuration
Let's learn a little more about autoscaling configuration. Configuring an autoscaler is easy to do on GCP with the listed four steps. First, you create an instance template. And again, that instance template is basically going to be what is the image, will there be boot up scripts, what type of preconfigured installations will included, etc. You can also define log archiving. This is a good opportunity to set up Stackdriver. Then you're going to create the managed instance group. You're going to define exactly the minimum and maximum thresholds are and what is going to be inside of that instance group. You then create the autoscaler and then you can define multiple or go with a single policy. When you click on an instance group or an individual VM, a graph is presented. The default display is CPU utilization. You can look at the graph over different timeframes. This is very handy for viewing actual traffic, looking for patterns, and determining how best to configure autoscaling policies to meet changing demand. In this example, the scaling of game servers could be based on CPU load, or perhaps network traffic or GPU load. However, each server only has the capacity for 500 game users, so perhaps a better metric would be the number of users instead of CPU load. In this case, you could write a custom metric to pass the number of users on a VM from the application to Stackdriver, then you could set your autoscaling policy based on the number of users.

Lab: Autoscaling (Overview and Objectives)
Lab: Autoscaling (Review)
In this lab, you set up an HTTPS load balancer with autoscaling and verified that it was working. Then you created a VM and customized it by installing software and changing a configuration setting, specifically making Apache start on boot. Then you used the custom image in an instance template, and then used that instance template to make a managed instance group. Then you stress tested the entire system and triggered autoscaling using ApacheBench. I hope you enjoyed applying the concepts learned in this module in a real-world application.

Module Review (Outro)
The most common and easiest metric to use for autoscaling is CPU utilization. While this might work for your application, it might not be optimal. You should give serious consideration to what you're measuring. More importantly, give consideration to how that measurement relates to the user experience and to the performance characteristics that your users care about. We hope you enjoyed this module on autoscaling.

Infrastructure Automation with Cloud API
Module Overview (Intro)
(Music) Hello, and welcome. I'm Mylene Biddle, and I'm a technical curriculum developer for Google Cloud Platform. This is module 4, Infrastructure Automation with Google Cloud Platform APIs. Up to this point, most of the infrastructure you've created has been manually provisioned using the GCP console or Cloud Shell. In one of the earlier labs, you used Cloud Launcher to provision an entire infrastructure solution. In the autoscaling lab, you triggered the autoscaling servers that were provisioned for you dynamically. Now we're going to look at ways you can automate infrastructure provisioning and configuration. These techniques are critical to any production system because they make the generation of the infrastructure documentable, accountable, and repeatable. We will discuss infrastructure automation, images, metadata, scripts, and the Google Cloud Platform API. You will also complete a hands-on lab that explores GCP infrastructure automation. Let's get started.

Infrastructure Automation
Why would you want to automate infrastructure if you could just set it up once, go back, and just maintain it wherever you needed to. You automate because you want to make it repeatable. Automation allows you to redeploy your infrastructure or rebuild it from scratch because you have a repeatable, documented process. Automation allows you to scale, ensuring consistency as hopefully the success of your business grows. You can create a consistent, automated process to ensure ease of growth. If you're going to be setting up large, complex systems, you really need a way to ensure that there's a repeatable process that prevents and corrects human error. When looking at the entire spectrum of automation tools, oftentimes you start with some kind of a base image. Google base images are maintained, updated, and patched. Imported images can be useful for synchronizing with an existing system. You can use the image to create a running VM and you can connect to the VM and change its configuration, install software, and so forth. After the VM is tailored to your liking, you can create a snapshot of its boot disk. Snapshots are global resources. You can use them to reconstitute a boot disk in any region in any network in your project. However, you can't share them between projects. You can also create a custom image. A custom image can be shared between projects, and there are tools for managing those shared images with your image users. When we say custom image, we generally mean that the OS and system settings are customized. A baked image is one with preinstalled and preconfigured software. Baked images are generally faster to become operational than other methods of installing software during or soon after boot. A golden image is one with all the settings just right, ready for sharing. Another reason to bake an application into an image is to lock the application so it is harder to make changes to its configuration. The use of startup scripts and metadata is one method used to implement boot time customization in software installation. The benefit is being able to change configurations, including which software to install, on the fly. It is ideal for passing parameters that can only be known when the VM is being created, and not before. The metadata provides a persistent environment that survives the termination of any individual VM, so you can use metadata to maintain system level infrastructure data and state information. Any kind of image, startup script, and metadata can be used in an instance template. Instance templates give you a system documented repeatable way to make identical VMs. When used with a managed instance group and an autoscaler, they provide horizontal scaling. All of these tools and methods get you consistent, reliable, and automatic VM creation, but what they don't do is provide automation over the rest of the GCP infrastructure. They don't create load balancers, VPN connections or networks. One solution is to install the Cloud SDK on a VM, authorize the VM to use part of the Google Cloud APIs, and write programs that automate infrastructure creation. Remember that everything you've learned to do with the console and the gcloud and gsutil commands was implemented by calling the Cloud API. You could write programs that call the Cloud API to programmatically create and manage infrastructure.

Images
Let's delve into exporting, creating, and managing custom images. You can import boot disk images from your physical datacenters, from virtual machines on your local workstation, or from virtual machines that run on another cloud platform. The image import process can import only one disk at a time, and this example focuses on how to import boot disk images. To import a boot disk image to Compute Engine, use the following process. Plan you import path. You must specify where you are going to prepare your boot disk image before you upload it and how you are going to connect to that image after it boots in the Compute Engine environment. Prepare your boot disk so it can boot within the Compute Engine environment so you can access it after it boots. You're going to create and compress the boot disk image file. You'll then upload the image file to Google Cloud Storage and import the image to Compute Engine as a new custom image. You can then use the imported image to create a virtual machine instance and make sure it boots properly. If the image does not successfully boot, you can troubleshoot the issue by attaching the boot disk image to another instance and reconfiguring it. You can then optimize the image and install the Linux guest environment so that your imported operating system image can communicate with the metadata server and use additional Compute Engine features. To create a VM from a snapshot backup of a persistent boot disk, you need to first create a new disk from the snapshot, then you can use that disk to boot a new VM instance. Persistent disks are zonal resources; however, snapshots are global resources, so you can use a snapshot to create a copy of the boot disk in any region and also in any network in your project. However, you cannot share the snapshot outside of your project. Note that when you create a new disk in a different region or network that egress charges apply to the data. There are many benefits to creating an image from a VM's persistent boot disk. A VM can be generated directly from the custom image without first manually restoring it to a disk. The custom image can be used in instance templates and the custom image can be shared across projects. When you're managing custom images, you can share them utilizing IAM. You can set image families so that, for example, you can always point to the latest version of the image. This way, people don't accidentally go and pick up the older versions. You can also define a deprecation schedule, so when is something going to be deprecated? This gives a warning to the users that they might want to update one that's obsolete. This also lets users know that they shouldn't be using that image because it's eventually going to be deleted.

Demo: Export an Image
In this demo, we're going to create and export an image. The first thing you're going to do is you're going to stop the instance that the disk is attached to before you create the snapshot. That's going to ensure the integrity of the disk contents in the snapshot. So I'm going to go ahead and click Stop. There we go. Now I'm going to do most of this through the command line, so I'm going go ahead and start Cloud Shell. We can see here instance-1 has been stopped. So what we're going to do now is we're going to create a snapshot of the disk and we're going to name it image-snapshot. We'll copy this command, paste it in here, and we are going to change the SNAPSHOT_NAME to the name of the disk from which you create the snapshot, in this case it's called instance-1. We're going to be prompted for a region. I'm going to type in the number associated with us-central1-a. And it looks like we have already created a snapshot with that name before. Let's look at that. So I'm going to go ahead and delete that. (Working) Go ahead and refresh and it's gone, so I'm going to go ahead and run that command again. (Working) It's creating snapshot image-snapshot. Here we can see the snapshot has been created. We are now going to use the image-snapshot snapshot to create a new disk named image-disk. (Working) And there we go, our image-disk has been created. The next thing we're going to do is we are going to create a temporary disk named temporary-disk to hold our TAR file and specify the size to be at least 50% larger than the image disk. We can see here the image disk is 10 GB, so I am going to specify at least 50% bigger, copy this command, and for size I'm going to go ahead and say 15GB. Again with the region. We can ignore this warning because we will not be doing intense I/O operations. Now that we have a temporary disk, we're going to create a new instance and enable storage read/write scope on the instance and attach the image disk and temporary disk to the instance as secondary disks. Go ahead and copy the command, and I am going to modify a couple of things, namely the instance name of what we're creating. And that's not going to work, so let me go ahead and type it in, compute instances create. I'm just going to name this one instance-3 to keep it nice and easy. The scopes are storage-rw, --disk name=image-disk, device-name=image-disk, and then the second --disk is the temporary-disk. Again, it's going to ask me what region to create this instance in. I'm just going to keep it consistent in us-central1-a. I'll take a couple minutes for my instance to run and to be up and running. Here we see instance-3 is up and running. If I wanted to go ahead and connect to that instance, ssh into instance-3. (Working) And here I am logged in to this instance. So I am now going to format and mount the temporary disk. (Working) All right. Then I'm going to mount it. (Working) So you could also mount the image disk and then make additional changes before you create the TAR file. For example, you might want to delete any existing files from the home directory if you don't want the meaty part of your image. You could mount the disk partitions that you need to modify, then modify the files on the disk that you need to change, and then unmount the disk when you're done. So I'm going to create a directory where I can mount my disk or partition. (Working) You can then use the ls command to determine which disk or disk partition you need to mount. You can see these are all my image disks. So, what that does is it prints a list of disk IDs and partitions. So now we can mount the partition if we need to make changes to the disk, then create the disk from the full disk. So, I am not going right here, I'm going to mount dev/disk/by-id/google-image-disk-part1 mnt/image-disk. So if your disk is raw formatted with no partition table, you could mount the full Google image disk. So now I'm going to modify the files in the mnt/image-disk directory and configure the files on the disk. So, if I wanted to I could go in and modify the authorized keys, or something. After you're done modifying the files, then you can unmount the disk. (Working) And there. I always do that. It's not unmount, it's umount. There we go. Now what we're going to do is we're going to create a TAR file of the image. And I'm going to copy this command because it's quite long. Here. Please note that creating a raw disk file on the temporary disk can take a couple minutes to complete, so I'm going to go ahead and pause and just forward to where this is completed. Here we can see that the raw disk file has been created. So now what I'm going to do is tar and zip that file. And I'm going to make a TAR file with that. So what this command is doing is it's creating an image of the instance in the following location. Mnt/tmp myimage. tar. gz. So from here what we're going to do is actually upload this file to a Google Storage bucket. So I've already created a Storage bucket. Let me verify the name where I'm going to upload it to. It's called my-unique-image-bucket. So we'll wait for this TAR to be created, and then we will upload it. Now we can see that our TAR file has been created, so I'm going to use the following command to copy that file into my bucket. So, my-unique-image-bucket. And it is going to take some time, and we can actually go into the bucket using the UI and verify that it has been uploaded. And there is the TAR file of my image. So, you've exported your file into Google Cloud Storage. Now you can share the image with other people, use the TAR file to add a new image to a Google Cloud Platform project, the possibilities are many.

Metadata
Let's now explore metadata for infrastructure automation. Metadata is especially important when starting a VM. Throughout each of the individual phases as a machine is booting up until it's running, and before it's actually put in maintenance mode and shut down, we will query different metadata tags that are available to you. Many data tags are not labels. They are also not tags in the sense that a tag might be used for networking, firewalls or routing purposes. In this case, the metadata are custom key value pairs that we've actually reserved to obtain specific information about the virtual machine. You can create your own custom metadata so you can store shutdown and startup scripts in the metadata tags. There are different types of metadata tags available. Project-wide metadata is queryable by all of the VMs inside of your project. Virtual machine-specific metadata is private to the virtual machine. The VM from within the VM can run a query and found out its instance hostname, the SSH keys, project IDs, and additional details. All metadata are simple key value pairs so you can return individual values for the key and probe an entire directory tree or a list of recursive keys as well. You can query this metadata a number of different ways, obviously, using the web console, Cloud Shell, or using the Google Cloud Platform API. You can use the curl command to repetitively identify if there are any metadata changes. For example, with a preemptable VM, that preempt API notification is actually going to come as a metadata update, so you need to watch and wait for that particular update. If your server is going to be migrated live or if it has to go down for emergency host maintenance, those actions will all be done by updating the metadata tag. In the first curl example, you can use curl to watch for the metadata tag change. Maintenance events involve updating metadata attributes. You're going to get and update about 60 to 90 seconds before the event actually occurs. It's your application's job to monitor for the notifications and react appropriately.

Scripts
Now let's learn about how you can leverage scripts in infrastructure automation. VMs start and stop with startup and shutdown scripts. If you're a Linux or Windows admin, you're probably familiar with this process already. We're going to run these scripts both pre- and post-start. It's important to note that startup scripts always get run. Shutdown scripts are a best effort. If we're shutting down the device, we cannot guarantee that the shutdown script will finish. You can update these startup and shutdown scripts at any time, even after the virtual machine has been created. Here's an example of how you might actually create a Linux startup script. This is a great reference, so feel free to pause the video here and take note of some of these commands. Windows utilizes Sysprep. A major difference is that these startup scripts are going to be hosted in Google Cloud Storage, and they need to be publicly available, although the VM tags will be private. Here are some examples of some of the commands that you can run using Sysprep. Startup scripts can be rerun after boot, and common use cases included software installation, operating system updates, and turning on services. As I mentioned earlier, shutdown scripts are based on best effort. Note that you should give a typical virtual machine 90 seconds for a shutdown event, and a preemptable VM needs about 30 seconds.

Google Cloud API
We've actually already been working with the Google Cloud Platform API already through this course; let's learn a little more about it. You can download the Cloud SDK and create your own automation tools. Client libraries are available for these languages. The SDK is easy to install, just click on this link and run the install command. If you're logged in as a user, you just type gcloud init. That's going to give you a custom URL, which you will paste into your browser, then log in as that individual user so you can actually save a secure token and then authenticate on behalf of that user. You can also authorize as a service account. Here is the command to activate a service account and supply the supported key file that was generated when you generated the service account. On the slide we see a few examples of how to use gcloud to list, add, update, and remove components from the command line. Most of the labs we've completed use gcloud and gsutil. There are two other command line utilities that take advantage of the Cloud SDK, and one is called bq, which is a Python-based tool that accesses BigQuery from the command line. The other is kubectl, which allows you to manage Kubernetes from the command line. Using a VM to create infrastructure can be a powerful tool. You can accomplish this by authorizing a VM with a service account to use the Google Cloud API, and then the software and the VM uses the API to create infrastructure on your behalf. Without having to code the solution, you will learn how to set up and operate such an infrastructure automation tool in the next lab.

Lab: Google Cloud Platform API Infrastructure Automation (Overview and Objectives)
Lab: Google Cloud Platform API Infrastructure Automation (Review)
In this lab, you created an IAM service account, created a VM, authorized a VM to use the GCP API using the service account, installed open source software on the VM, then configured and tested the VM by deploying a Hadoop cluster. You then created a global solution by generating a snapshot of the boot disk with the service account authority baked in. Then you recreated the clustermaker VM in a different region and tested it by deploying another Hadoop cluster in the new region. I hope you enjoyed applying the concepts learned in this module in a real-world application.

Module Review (Outro)
System images allow you to bake configuration into an image, increasing consistency and decreasing the time until the system is fully functional. This method makes it more difficult to implement undocumented changes. Metadata and scripts enable boot time configuration, adding flexibility and allowing a consistent way to install and configure software. They also provide a straightforward method to change that configuration in code. The Cloud API is extremely versatile and gives you complete control over your cloud resources directly from code. The concepts covered in this module should get you well on your way to automating the provisioning and configuration of your infrastructure.

Infrastructure Automation with Deployment Manager
Module Overview (Intro)
Hello, and welcome. I'm Mylene Biddle, and I'm a technical curriculum developer for Google Cloud Platform. This is module 5, Infrastructure Automation with Deployment Manager. Calling the cloud API from code is a powerful way to generate infrastructure, but writing code to create infrastructure also has some downfalls. One issue is that the maintainability of the infrastructure depends directly on the quality of the software. For example, a program could have a dozen locations that call the cloud API to create VMs. Fixing a problem with the definition of one VM would require first identifying which of the dozen calls actually created it. Standard software development best practices will apply, and it's important to note that things could change rapidly, requiring maintenance on your code. Another issue is the difficulty of comprehending the infrastructure that we're building from the code. The more complex the infrastructure, the more complicated the code, and the more likely it is that errors would occur. Clearly, another level of organization is needed. That's the purpose of Deployment Manager. Deployment Manager uses a system of highly structured templates and configuration files to document the infrastructure in an easily readable and understandable format. Deployment Manager conceals the actual cloud API calls, so you don't need to write code, and you can focus on the definition of the infrastructure. In this module, we will discuss concepts around Deployment Manager, configuration, and Cloud Launcher. You will complete a hands-on lab that includes downloading Deployment Manager templates and then using them to deploy a specified infrastructure. Let's get started.

Deployment Manager
What is Deployment Manager? It is an infrastructure automation tool. It allows you to automate the creation of GCP resources. You can create Deployment Manager templates in a cloud API-enabled environment, such as Cloud Shell, and then view the results and manage your deployments in the console. Let's take a look at some orchestration tools on the market. There are more solutions than those listed, but these are some of the more popular ones. Deployment Manager is a declarative deployment orchestration tool specifically for Google Cloud Platform. So if you're all in on Google or just want to automate your processes on our infrastructure, you can certainly do so with Deployment Manager. Deployment Manager also allows you to integrate with other GCP services such as Identity Access Management. Cross-platform alternatives such as Puppet, Chef, and Terraform work across multiple cloud providers. They aren't hosted, and you're ending up setting up your own infrastructure to support those. CloudFormation from AWS is only structured to work with an AWS infrastructure, and it integrates well with AWS services.

Configuration
Let's learn a little more about deployment configurations. Before we can start using Deployment Manager, we have to create a deployment configuration. A YAML file defines the basic configuration, and you can include an import in the YAML file to expand to full-featured templates written in Python or Jinja2. Program configuration is bidirectional and interactive, meaning that the configuration receives data, like machine-type, and returns data, like ip-address. You can use the preview tag when creating a deployment configuration to validate it before using it, as you can see in this example. It would help you to preview the resulting configuration without launching it. All configurations are expanded on the server side within a controlled environment that Deployment Manager maintains. In order to prevent abuse, the environment is closely managed by the Deployment Manager team, and it does have some limitations. Neither your original configuration nor your expanded configuration can exceed 10 MB. Any configurations uploaded to Deployment Manager are limited in the amount of time the configuration can take to run and the amount of processing power the configuration consumes during expansion. If you run into this limitation, contact Deployment Manager for more information. Any Python templates you use cannot make any system or network calls. These templates will automatically be rejected. Listed are some of the basic features of Deployment Manager templates. Templates can be nested so you can create a set of reusable assets. For example, you could use different templates for firewall rules than you would for virtual machines. Segregating the different components that you might want to set up allows you to update them independently without having to use one single master file. Templates have properties and can use environment variables. Templates support startup script and metadata capabilities. Deployments can be updated using the GCP API, and you can add resources or remove resources.

Cloud Launcher
Let's learn a little more about Cloud Launcher. Cloud Launcher is a marketplace of third-party vendors who have already created their own deployment scripts based on Deployment Manager. It is essentially a marketplace to quickly deploy common solutions. Cloud Launcher has separate fees for software licensing and image use all available in the pricing details for each solution. The image fee versus licensing fees are up to the vendor of the solution. Google updates images, but not the running instances associated with Cloud Launcher solutions.

Demo: Cloud Launcher
Alright, so this is going to be a really quick demo on Cloud Launcher. So in the Products & services menu, we're going to click Cloud Launcher. So you're going to recall that deployments you've created on your own are going to be available through Deployment Manager, but Cloud Launcher is a marketplace for deployments that have been made available to the public, so it's a great way to get started with common applications that are tried and true. I'm going to go ahead and search for LAMP. And here we have a LAMP Stack by Google. And you can see there's a little bit of information about this tried and true deployment. The estimated cost to run it is $49 a month. It tells you little statistics about how many people have recently deployed it. It's going to give you more information of exactly what it's launching. It's two VMs, what usage fees are associated with it, whether or not you get a discount for using this deployment, and then the total. So I'm going to do my one click and click LAUNCH ON COMPUTE ENGINE. I'm going to call it, leave the default of lamp-1, I'm going to run it in us-west, and I'm going to click Deploy. So with not necessarily one click, it was probably two clicks, my LAMP Stack is now being deployed, and you can see it redirected me to Deployment Manager because it has it all set up. And you can browse through here and see all of the things that it's doing. It's creating the VM instance. These are the firewalls associated with this deployment. You can go in and look at the specific files, the Jinja files if you wanted to look at what exactly is it creating for you. And if I go back here to Compute Engine and look at the VM instances, we should see that it's starting to spin up my lamp-1 VM. Here's just the coordinator VM. That will eventually disappear. Let me go back and see what --- And here you go, lamp-1 has been deployed. In under 30 seconds I found the deployment I wanted to use, and I clicked one or two buttons, and there you go.

Lab: Deployment Manager (Overview and Objectives)
Lab: Deployment Manager (Review)
In this lab, you customized Deployment Manager templates and deployed a network with two subnetworks and a VM. I hope you enjoyed applying the concepts learned in this module in a real-world application.

Module Review (Outro)
Managed Services
Module Overview (Intro)
Hello, and welcome. I'm Mylene Biddle, and I'm a technical curriculum developer for Google Cloud Platform. This is module 6, Managed Services. In the last few modules, we discussed how to automate the creation of infrastructure. An alternative to infrastructure automation is eliminating the need to create infrastructure by integrating a preexisting solution into your application design. Managed services are partial or complete solutions offered as a service. They exist on a continuum between Platform as a Service and Software as a Service, depending on how much of the internal methods and control are exposed. Using a managed service allows you to outsource a lot of the administrative and maintenance overhead to Google, if your application requirements fit within the service offering. Here's the module agenda. Notice that there is no lab in this module. We will have some demos though. Instead of setting up an infrastructure to support specific types of workloads, you may want to take advantage of some of the many services that we have to offer on GCP. We will be covering Dataproc, Dataflow, BigQuery, and some other managed services. Most of these do revolve around big data and data analytics, which is really where Google shines, especially in the cloud infrastructure. Let's get started.

Managed Services
Let's take a look at the spectrum of solution options. You can pretty much build everything you want by building your own infrastructure using the console, Cloud Shell, and virtual machines. We've also learned a little bit about automating our infrastructure using Cloud API in programmatic deployments using Deployment Manager. Then you can move from Platform as a Service to Software as a Service. Platform as a Service is going to include services like Dataproc and Dataflow, whereas BigQuery is a complete Software as a Service. There is a continuum of options available in GCP. You should consider all options in a particular context. The solutions to the right offer greater outsourcing of overhead, but with a loss of control, and the solutions to the left offer greater detail of control with increased responsibility in overhead. Let's take a deeper look at data processing managed services that are made up of three core services. The first one is Dataproc. If you want to quickly spin up a Spark or Hadoop cluster with both Hive and Pig capabilities, you can do that utilizing a preconfigured Deployment Manager process we call Dataproc. Dataflow is our ETL, batch, and streaming processing pipeline. Dataflow is based on the Apache Beam open SDK framework. If you're familiar with that, you can create your own ETL jobs, submit them yourselves on your own platform, or you can push them to Google Cloud's Dataflow to handle that automatic scaling, real-time streaming, liquid sharding, and using some other cool features we have in order to speed up that ETL processing. Our fully-managed data analytics service that can scale to the petabyte scale is BigQuery.

Dataproc
Let's learn a little more about Dataproc. There's a number of interesting features with Dataproc. It's very affordable because we've automated the deployment process utilizing out compute infrastructure, which already has those built-in features that we mentioned earlier. These include per-second billing, sustained use discounts, the ability to actually use preemptible virtual machines, and many others. We can failover quite nicely when it comes to Dataproc in this type of configuration. Setting it up is also going to be extremely fast. It's easily maintained because we offload a good amount of said maintenance for you. We support all of the major ETL and data analytics software packages that you might want to utilize. We integrate very nicely with a number of other Google products, including Google Cloud Storage and BigQuery. You can output or even ingest data directly from Bigtable because it's using an HBase API. Further features for Dataproc are listed on the slide. Most notably, you will have faster data processing workflows because less time is spent waiting for clusters to provision and start executing applications. I encourage you to read further about the benefits of Dataproc and get more information by accessing the online documentation. To connect to your Dataproc cluster, you can use the web-based user interface to submit jobs. You can use the YARN web UI, HDFS web user interface, as well as SSH through a SOCKS proxy. When starting a cluster, it's not just going to roll out a bunch of VMs. It's still controlled utilizing the glcoud tool and the Cloud Dataproc API so that you can interact with that in a programmatic fashion. You can also monitor Dataproc utilizing Stackdriver.

Demo: Dataproc
Cloud Dataproc is a manage Spark and Hadoop service that lets you take advantage of open source data for batch processing, querying, streaming, and machine learning. In this demo, I'm going to show you how to submit a job for Dataproc both using the command line and the console. In this demo, we are going to use Dataproc to run a sample Spark job. So we already have a cluster running that I have created. So now what I'm going to do is select Jobs in the left nav to switch to Dataproc's Jobs view. I'm going to click Submit job. I'm going to select my Cluster. I am going to select Spark. In the Jar files field, I am going to specify this example right here. (file:///usr/lib/spark/examples/jars/spark-examples. jar) And, in the main class or Jar field, I am going to specify this (org. apache. spark. examples. SparkPi). For arguments, I am going to enter 1000. And then I'm going to click Submit. So what this is doing is this Spark job is calculating pi. So it estimates a value of pi using the Monte Carlo method. I urge you to look that up in our documentation if you want more information on that. So now we can click on the Job ID in the Jobs list, which is just going to show you your project's jobs with the cluster type and current status. The job will display as running, and then succeeded after it completes. To see the completed job output I can just wait here, make sure that line wrapping is available to avoid scrolling. And I can see some of the estimations of pi. And, let's scroll down. Job output is complete. And the job has successfully calculated a rough value for pi which is right here. Pi is roughly 3. 1417338314173384. There we go. So, as we have shown you both in the console and using the command line, Cloud Dataproc automation helps you create clusters quickly. You can easily manage them, save money by turning clusters off when you don't need them. With less time and money spent on the administration of your clusters, you can focus on your jobs and the data.

Dataflow
Let's learn a little more about Dataflow. Dataflow is our online batch and stream ETL processing based on the Apache Beam SDK. It allows you to separate data processing requirements from the data source. Dataflow can help process and order data as it's being streamed in. You can output that or input it directly from many different sources such as BigQuery, Bigtable, or Cloud Datastore. Dataflow creates pipelines, PCollections, transforms, and I/O sources and sinks. Pipelines are a series of computations that accept data and transform it, outputting to output or internal sink. The input source and output sink can be the same in a pipeline, allowing data format conversion. PCollections are a specialized container of nearly unlimited size that represents a set of data in the pipeline. Transforms are a data processing operation. I/O sources and sinks are different data storage formats, Cloud Storage, BigQuery, tables, and more, as well as custom data source sinks. These examples give you a sense of the processing capabilities of Dataflow. In the simple model pipeline, data is input from a source into a PCollection, transformed, and then output. The pipeline is a directed acyclic graph. In the multiple transform pipeline, data read from BigQuery is filtered into two collections based on the initial character of the name. In the merge pipeline example, you're collecting all sorts of data that maybe you've ingested from BigQuery or another data source. We can transform those different names as well so we can split everything back and then flatten those out. In the multiple input pipeline, you're doing joins from different data sources. There are different ways to connect the Legos, but Dataflow's job is to ingest the data in parallel as fast as possible, scale out, and then, of course, output an ETL that's friendly for whatever the normal source it's going to.

BigQuery
Let's learn a little more about BigQuery. BigQuery is Google's fully-managed, petabyte scale, low-cost, enterprise data warehouse for analytics. BigQuery is serverless. There is no infrastructure to manage, and you don't need a database administrator, so you can focus on analyzing data to find meaningful insights using familiar SQL. BigQuery is a powerful big data analytics platform used by all types of organization. This slide lists some statistics for a complex query of over 100 billion rows. Here is an example of the type of query that we discussed in the previous slide with 100 billion rows.

Other Services
Let's learn more about a few other managed services available to you through GCP. Cloud Datalab is a great managed service for data analysts and data scientists because it essentially is a development environment to run interactive tools for data exploration. It runs on IPython Notebooks, and it combines both a Python library system in which you can write Python code with real-time visualization. If you're writing Python code to query from BigQuery, we can output those visualizations directly within the Cloud Datalab interface. Because these are Jupyter Python notebooks, we can actually save these notebooks and share them with other individuals. Cloud Datalab doesn't just support Python. It also supports interactive SQL and JavaScript. Data Studio lets you create dynamic, visually compelling reports and dashboards. With Data Studio, you can easily connect to a variety of data sources, visualize your data through attractive, dynamic, and interactive reports and dashboards, and share and collaborate with others, just as you can in Google Drive.

Module Review
A common issue when using managed services is that there are often unique application requirements that don't quite match up with the service offering. We urge you to dig deeper. Don't just accept that the unique requirement invalidates the managed service as an option. Instead, understand why the unique requirement was specified in the first place. In many cases, it was created to handle an issue that's already been addressed or even eliminated in the service offering. With a little analysis, a managed service might be a great option to consider for your application.

Elastic Cloud Infrastructure: Scaling and Automation Course Outro
This is Jasen Baker with the Google Cloud Platform, and I want to thank you for completing the Elastic Cloud Infrastructure: Scaling and Automation course. So what did you learn? Well, you learned about advanced networking. How you connect on-premises with Google Cloud? How do we scale to Google-size scale? Utilizing auto-scaling. Automating the deployment process. You learned about virtual private networks, so encrypting tunnels between multiple networks, maybe between cloud providers or even on-premise. You also learned about automating the deployment of virtual machines and then sending up load balancing and auto-scaling with those. So that was probably very fun, actually taking the individual VM and watching it scale elastically as your workload started to increase. You also got introduced to multiple types of automated deployments, whether you wrote your own code using the API and infrastructure automation, or you took advantage of Google's scripted platform, Google Cloud Deployment Manager.

Course author
Author: Google Cloud	
Google Cloud
Google Cloud can help solve your toughest problems and grow your business. With Google Cloud, their infrastructure is your infrastructure. Their tools are your tools. And their innovations are your...

Course info
Level
Intermediate
Rating
0 stars with 4 raters
My rating
null stars

Duration
1h 48m
Updated
17 Jan 2019
Share course

