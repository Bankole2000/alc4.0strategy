Google Cloud Functions Fundamentals
by James Wilson

In this course, you will traverse the Google Cloud Functions landscape so you can create high-quality microservices that will enhance the experience of your app or website, while avoiding the common traps associated with them.

Developers want to build serverless microservices so they can create new content, reduce maintenance, scale easily, and deliver new features to users faster. In this course, Google Cloud Functions Fundamentals, you will expand your serverless skills to create high-quality Microservices that will enhance the experience of your app or website. First, you will learn to build unique functions that interact with other Google Cloud services such as Cloud Vision and Cloud Datastore. Second, you will discover advanced concepts such as the function's file system, idempotent function design, and working through memory and timeout issues. Finally, you will explore how to properly log errors and set up monitoring for your functions so you can continue to monitor them once they are in production. When youâ€™re finished with this course, you will have a high-level understanding of Google Cloud Functions that will allow you to create unique experiences for your customers.

Course author
Author: James Wilson	
James Wilson
As a mobile developer, James always had a passion for building exciting apps and always striving to make user interfaces that were easy and intuitive to use. Today, he now works at Pendo where he...

Course info
Level
Intermediate
Rating
0 stars with 6 raters
My rating
null stars

Duration
4h 52m
Released
28 Aug 2018
Share course

Course Overview
Course Overview
Hi everyone. My name is James Wilson, and welcome to my course, Google Cloud Functions Fundamentals. I am an engineer that is building exciting software on the Google Cloud Platform with products such as App Engine, BigQuery, and Cloud Pub/Sub. Serverless functions are taking the world by storm. While they are easy to set up and automatically scale for you, if used improperly they can cause a lot of headaches and leave you wondering what all the hype is about. In this course, we are going to traverse the Function as a Service landscape so you can create high-quality microservices that will enhance the experience of your app or website, while avoiding the common traps associated with them. Some of the major topics that we will cover include leveraging ten different Google Cloud Platform services; an in-depth look at a function's environment and its file system; analyzing the distributed execution of functions and their impact; building idempotent functions that can be retried again, and again, and again with the same result every time; and finally, advanced error reporting, logging, and monitoring, so once they hit production you are ready to handle any issues that might arise. By the end of this course, you'll know how to build complex, serverless functions all the way from writing the code to setting up monitoring once your function hits production. Before beginning the course, you should be familiar with Node. js or Python, and also how to handle simple operations on cloud functions such as creating, deleting, and deploying them. Check out my course, Google Cloud Functions: Getting Started, to prepare yourself. I hope you'll join me on this journey to learn how to build complex serverless functions with the Google Cloud Functions Fundamentals course at Pluralsight.

Building Complex Serverless Functions
Introduction
Serverless functions are taking the developer world by storm. They're super easy to build, they're easy to maintain, and you can get prototypes up and running really quickly. Now while there are many advantages to them, there are also some disadvantages as well. We can't just go and throw any technology we want at a serverless function, it's just not always going to work out. And once you have it deployed, there's the aspect of maintaining it, keeping track of logging, getting real-time monitoring alerts. And finally, they are not magical, they are built on top of something, they're running on an environment, and what is that environment exactly? Knowing that helps you understand the advantages that you have, and also the disadvantages that you could run into. And so, in the course, Google Cloud Functions Fundamentals, we're going to be tackling a lot of these issues. We're not going to just be creating various functions and saying, oh, look at that it works. No, we are going to be building real-world examples, and we are going to run into a lot of problems. And now this is going to be great, though, because throughout this entire course you're going to be leveraging a lot of different technologies from the Google Cloud Platform. So not only will we be leveraginGCloud Functions, but we'll also be usinGCloud Pub/Sub, Cloud Vision API, Cloud Storage, Google Cloud Datastore, which will soon be fully merged into Cloud Firestore. In addition, we'll be using BigQuery and looking to integrate into Stackdriver, specifically with the logging and monitoring system. And as we build all these functions, you're going to get the picture of what the cloud can offer to you. So let's take a look at some of the stuff we're going to be building throughout the course. In this first module, we're going to be setting up our environment on the Google Cloud Platform, and then after that we are going to dive in and start coding. With the first function we're going to upload images to the Google Cloud Storage, and that's going to trigger a function. And we're going to pass those images to the Cloud Vision API, which is going to give us information tags back about our images. And then we're going to go ahead and upload those tags into Cloud Data Store to store for future use. And you'll see how easy this is to accomplish when we're working within Google Cloud directly. The next function is we're going to be taking those same images and generating thumbnails from them. And now with this we have a lot to learn about the Cloud Function environment itself. We're going to take a look at the Docker image that builds that Cloud Function environment, and with that we'll see that we have a tool called ImageMagick installed on our environment that we can leverage from our Cloud Function. And then on top of that we're also going to have to learn about the temporary file system that's available to us as well so we can store images as we're processing, and so there's going to be a lot of great material to learn. And, of course, if we're storing images we're bound to run into memory issues, right? I think we will. And we're going to work through them and fix them. Then we're going to build a PubSub-driven transaction system. We're going to have lots of events being uploaded to Pub/Sub, and we're going to try processing them with Cloud Functions. And we're going to see that the first technology we pick to save this information is not going to be able to keep up. This will have us take a step back and look at the execution model of how Cloud Functions work, and then we'll go and pick a different technology that is better able to keep up with the distributed execution that Cloud Functions provide. And then we're going to write another function that's going to take all of those transactions and periodically bundle them up and aggregate them and save them to the original technology that we chose, Cloud Datastore. Then we're going to start diving into the logging and monitoring system, and we're going to build an HTTP function that can send emails through a service called Mailgun, and with that we're going to look at being able to export our Stackdriver logs to BigQuery. And then we're going to call BigQuery from our Cloud Function, which will send us the results back that we want, and we're going to send a daily email. In addition, we'll look at taking those results from BigQuery and creating a custom log entry for ourselves on the Stackdriver logging system, and you'll see how this whole system fits together, and how it provides you flexibility and will allow you to customize your logs for the unique circumstances you are going to run into. And then finally we're going to take a look at the Stackdriver monitoring system. So once those cloud functions are deployed to production, and something serious occurs, you can be notified, and you can take a proactive approach to fixing the issue. Now the most exciting thing about this course is all the problems we're going to run into along the way. Now you might be thinking to yourself, that sounds crazy, but trust me. It's going to force us to cover the most important material and give you the foundation you need to successfully work with Cloud Functions. Now with that, let's take a look at some of the prerequisites for this course.

Prerequisites
So before we get started, there are definitely some things you should know before starting this course. First, you want to know basic CRUD operations with functions, and I don't mean writing code within the functions themselves, but actually from an administrative standpoint, being able to create them, update them, delete them, and you can do this from the Google Cloud dashboard or the Google Cloud CLI tool, GCloud. You should also be familiar with the various function triggers available to Cloud Functions. We're going to be using the majority of them in this course, but we're not going to give them much explanation, at least not at a basic level. In addition, you should have some basic familiarity with Node. js. We're going to be installing it on our local machine so we can use it throughout the course, and that's because it's just easier to use. We're going to be writing a lot of code, so to be able to pick your favorite code editor and write the functions, and then use the GCloud SDK to deploy them to Google Cloud is just a much smoother workflow and a better user experience overall. Now if you're following along and you want to use Python, since Python functions are also supported, feel free to do so. While it's not covered in this course, if you're comfortable with working in Python, this should be an easy translation for you. And then finally, you want to be familiar with the GCloud command line interface tool. Again, we're going to be deploying functions from our local machine, and we will go and set everything up. However, you should have some basic familiarity about the different commands that are at your disposal. Now, if you feel like there are any gaps in your knowledge based on this prerequisite list, don't worry. My prior course to this, Google cloud Functions: Getting Started, covers all of this material, so you could try to follow along, and if you feel like there are gaps in your knowledge you can always go back and reference that course. Or, you can go watch that course in its entirety, and then come back to this course knowing you have all the information you need to go through this entire course without any hiccups. And so with that being said, let's take a look at what we're going to do in this module.

Module Overview
So we're just going to be going over a few things in this module, and first we're just going to set up the Google Cloud Project. And if you watched the Getting Started course you've done this before, so you should already be set up and ready to go. Then we're going to go ahead and review the Cloud Function permissions. This is something we didn't go over in the Getting Started course, but now that we're connecting and using other services within the Google Cloud Platform, it makes a lot of sense to cover it now, and so we'll be quickly going over the various permissions you have at runtime, as well as giving other uses within your project permission to create, edit, delete Cloud Functions. Then we'll install the Google Cloud SDK onto your local machine. And, again, if you've already done Google Cloud work or watched the Getting Started course you already have this installed. And then finally, we'll be installing Node. js, and we're going to be using the 6. 11 runtime. Now, as of this recording, Cloud Functions is available for Node. js version 6 and 8, as well as Python 3. 7. And while we're using Node. js 6 in this course, feel free to use whichever version you are most comfortable with. Even the Python version, you should be able to follow along with the examples if you're very comfortable with that language. So with that being said, let's set up our Google Cloud Project.

Creating the Google Cloud Project
Okay, so what we're going to do here is sign up for Google Cloud and get our first project set up. It's super easy to do. So as long as you have a Gmail account, you can go to console. cloud. google. com. And one of the nicest features about Google Cloud is that they offer you 12 months free, or up to $300. 00 worth of Cloud services during the free trial period. But this is a lot to work with, and it's more than enough for this course. So go ahead and select your Terms of Service, and then Continue. And now we're at the Google Cloud dashboard. So I'm going to go up and select Start your free trial. I don't want any emails, and I'm going to go and agree to the Terms of Service and then hit Agree and continue. Now here you need to fill in some extra information, and you need to get out a credit card. However, this is the most important part. You will not be charged once the free trial ends. So what that means is you could have a bunch of containers running on Container Engine, or you could have a MySQL instance running on Cloud SQL, and once that free trial ends, your card will not be charged. What will happen by default is your services will be shut down and they will stop. Now, if you want to continue paying for these services you can do that and accept being charged before the trial ends, however, for our purposes, once you're done with this course you want to make sure your credit card isn't charged, and that is what will happen by default. So go ahead and fill out this information, I'm going to do the same, and we'll be right back. Okay, we got our free trial started, and you can see here it's saying Creating project, this may take a few moments, so I'm going to use the magic of video editing and fast-forward through this. Brilliant. So here we are now. We have our first project set up here, and we're at the dashboard. You have a little welcome screen, and if you want to take a tour go ahead and do that, but for this moment I'm going to skip this. At the top here you can see that our first project has been set up for us, and this is the project that we're going to be using throughout the course. Now that we've signed into Google Cloud, let's go ahead and take a look at setting up permissions for Google Cloud Functions.

Runtime Permissions
So I want to talk about the permissions that Google Cloud Functions have within the Google Cloud Platform. We didn't go over this during the Getting Started course, because we really weren't connecting to any internal services within Google Cloud itself. However, that is not the case in this course. We're going to be leveraging a lot of different products, and one of the advantages we have because our code is running within the Google Cloud Platform, we already have access to a lot of stuff. Later in the course, we're actually going to be using a custom client app built in Node. js so we can process a lot of transactions at once, and we're going to have to build a custom service account to be able to leverage certain aspects of the Google Cloud Platform, and you'll be able to see the stark difference when you have to have permissions because you're working outside of Google Cloud, and when you don't have to because you're inside Google Cloud, like on a Cloud Function. And so if you look here on the Identity and Access Management page, I'm scrolled all the way to the bottom, and we have your project ID, @appspot. gserviceaccount. com. And this is the App Engine App default service account. And it has the role of Editor. And now Cloud Functions are using the same credential to get access to everything within Google Cloud. Now what is that exactly? Well, if we jump over to Roles, and you go and type Editor to filter the list down further, and we scroll down, we have Editor at the project level. You can see that there are over 1400 assigned permissions on this role, and so that is giving us access to a lot of different Google Cloud products. And now I'm not going to go through all of them here. You can take a look at it, and if you're ever in doubt of whether you can go ahead and create a Pub/Sub topic, or whether you could query BigQuery, or whether you could write to the logging system, all you have to do is look within this Editor role and see if those permissions are there. And to be honest, they most likely will be, because when you're working with App Engine or Cloud Functions you have access to a lot. So I just wanted to show it to you, just so you're aware as to why we don't have to do a lot of security measures when we're working within Cloud Functions. And now let's take a look at the different permissions you can have for Cloud Functions themselves.

Granting Permissions
So I'm here in the Google Cloud dashboard, and in this video we're just going to quickly look at permissions for Cloud Functions. Now, if you created your own Google Cloud account, you already have permissions for everything because you're the owner of the project, but say there were other developers that you wanted to add on your team. Well, you could simply come up to Products and Services and select the Identity and Access Management tab. Here you can go and select Add to add a new user, and maybe in this case we'll do james-wilson2@gmail. com. This is not a real Gmail address, so I'm not going to fully go through this, however, at this point you can select a role. And when you come down, you can see that there is Cloud Functions here. And so you have two different options you could select. You could have Cloud Functions developer, which is read and write access to all of the function-related resources, or you can choose a Viewer, which is essentially just a read-only access. I just wanted you to be aware that these are here. If you're adding team members, you do have this fine-grained control in letting certain developers into Cloud Functions where you might let other developers into Bigtable or Cloud Storage. And now that we've taken a look at that, let's go ahead and install the Google Cloud SDK.

Installing the Google Cloud SDK
So I'm at the Google home page here, and if I just go ahead and search for Google Cloud SDK, you'll see it's the first search result that pops up, and it's at cloud. google. com/sdk. You can go ahead and select for the platform that works for you, and their directions are really detailed and great. And so I'm going to go ahead and install OS X. And what you want to do is for your respective platform just run through the directions, and it's really simple to set up. So here it says to make sure that we have the appropriate Python version installed on our system. So if I go ahead and open terminal, python -V, we can see that we're running 2. 7. 10, so we're good there. And now you can go ahead and select one of the packages here and extract the archive to any location, and then finally run the bash script. And then it says to restart your terminal of the changes to take effect. If you don't want to go through all those steps, one thing you can do is jump to the Installing the SDK guide here, and they have a bit of quicker version that you can use. Right here you could see that they have this interactive installer. This really speeds up everything really fast, so what you can go ahead is grab this URL, and again they have directions for Linux, Mac OS X, and Windows, and once again, open terminal. I'm just going to paste this in and hit Enter. You want to hit Enter, and then you can accept to provide statistics to them. And now this is going to take a few minutes to run, so I'll see you back once it's fully installed. Okay, so great, now it's asking, do you want us to modify the profile to update your path and enable the shell command completion? And yes, you do, at least I do. This way you can just call GCloud directly from the terminal. Go ahead and use the default. Brilliant. So, now that it's installed, I just go ahead and I'm going command q to quit out this terminal session, command space bar, and type in terminal relaunch. Now if I type gcloud, we can see that it's running. We have GCloud installed, and we're going to go ahead and set up our project now.

Setting up the Google Cloud SDK
So first thing we need to do is we need to log in. So I'm going to go ahead and do gcloud auth login, and this is going to open the browser window, and go ahead and sign into your Google account. It's asking you to accept various criteria that the Google Cloud SDK wants to do with your account. Hit Allow. And great, it now says we're authenticated. So if we go ahead and jump back to our terminal, you can see here that it's giving us some extra directions now, saying gcloud config set project PROJECT_ID. Now we could go to the Google Cloud dashboard and get that project ID, but, again, GCloud is awesome, and they already have our backs on this. So if you just go ahead and do gcloud projects list, and you can see here that it provides the PROJECT_ID. So I can go ahead and just copy that, and then do gcloud config set project and paste that in. It says Updated property core/project. And so to check that you can just go ahead and list out all the configuration properties. And so here we can clearly see that we set up the project and that it's the right account. So one other thing that we want to do is because the Google Cloud Functions are in beta, let's go ahead and make sure that we have all of our beta components installed. And so you can just do this by doing gcloud components install beta. And there you go, for the latest full release, let's hit Yes, and these are going to install. Now by the time of this recording, Google Cloud Functions are in beta, but if you're watching this at a later date, it may finally be ready for production, and so you don't need to do this step. However, there might be some cool new beta features that you would like to try out, and if that's the case, then you do need to install these. Awesome, now that we have GCloud set up, we're going to install Node. js.

Installing Node.js
Now one of the challenges when you're first dealing with Cloud Functions is, well, what version of Node. js are they using anyway? So, here we are at the Cloud Functions product page, and if we go to view document, you scroll down to Concepts, All Concepts, and do Overview of Cloud Functions, and it's on this page. If you look at this paragraph right here, Cloud Functions are written in JavaScript and execute in a Node. js v6. 11. 5 environment. Now this is an older version of Node, however, it's only recently that Node 8 became the latest LTS version. Up to a couple months ago of this recording, Node 6 was still the latest LTS, so they're not too far behind. So now that we know the correct version, let's go ahead and go to nodejs. org. At this point, you can see here that the latest LTS version is 8. 9. 1, but that's not the one we want. So we're going to go to the Other Downloads link, and we'll scroll down to the bottom, and we'll select the link, Previous Releases. And at this point you can see we have a whole list of versions to choose from, and right here you can see 6. 11. 5. You can go ahead and grab the correct package whether you're on Windows, Mac of Linux, go ahead and grab this package file here, and so now it's going ahead and downloading it. So what you can do is, if we open it up, you could just go ahead and see this package will install Node. js 6. 11. 5 and npm v3. 10. 10. And just go ahead and run through the steps here. Agree and Install, and it's going to go ahead and install this for you. So now that this is installed, you can see that it went ahead and updated our paths for node and npm, so if I go ahead and close this, Move to trash. Now I have a terminal window open here, but because we just installed this, I'm just going to go ahead and close this out so we have a fresh terminal to work with. So command spacebar to bring up the spotlight search and type in terminal. So now that I have a clean terminal open, I can go ahead and type node -v, and we see that I'm running 6. 11. 5, so good to go here. Now that we have Node installed, let's go ahead and get our project set up.

Summary
We went over a lot of great material in this module. First we took a look at the course overview and everything we're going to be learning. We then went and set up a Google Cloud Project, we went and got the GCloud SDK installed, as well as installing Node. js. And then we also took a look at the permissions that you have access to when your Cloud Functions are running on the Google Cloud Platform, and again, that App Engine service account had over 1400 permissions tied to it, so you have access to a lot. And, again, as we go through this course, you will see how easy it is to just set up other Google Cloud products and use them within the Cloud Functions. So with that being said, let's go to the next module and use the Cloud Vision API to help us tag our images.

Integrating GCP AI Services Using Cloud Storage Triggers
Smart Images
Hi. Welcome to the module, Integrating GCP AI Services Using Cloud Storage Triggers. Google Cloud offers a lot of cool AI services, such as translation APIs to translate text into any language, they have a speech API for converting speech to text, a natural language API for deriving insights from unstructured text, and a Cloud Vision API which allows you to extract information from images that you upload to it, and we're going to be using that in this module and storing some of the results in Cloud Datastore. So we are going to be doing a lot in this module. First, we're going to demo the Cloud Vision API. Directly from the Google Cloud website, they have a great little tool where you can just go ahead and upload a photo and see the type of data that you can get back from using their service, and so we'll go ahead and take a look at that, as well as the JSON we get back when we use it. Then we're going to go into Google Cloud and set up the different services we're going to be using throughout this module. So we'll set up Cloud Datastore, Cloud Storage, and enable the Cloud Vision API. Next we'll create a new Node project. This is quick and easy to do, and we'll fly through that. And then we're going to start writing our code, and the first thing we want to do is set up a Cloud Storage Function, which is going to give us a Cloud Storage object, and we're going to use various properties on that object throughout the whole module. Next, we'll make sure that we only process images coming through the Cloud Storage Function. Then we're going to build several operations so we can query the Cloud Datastore and create or update an entity within it depending on whether one exists or not. And if we're deleting our image, we're just going to go ahead and delete the entity from the Cloud Datastore. Before we can create our update, we need our information from the Cloud Vision API, and so we'll look at calling the Vision API for our labelAnnotations, and we're going to save any that have an accuracy of 65% or higher. Then the next thing we're going to do is look at returning promises from Cloud Functions. You might remember from the Getting Started course where we discussed having to call the callback to end your function. Well, there is an alternative for asynchronous functions like Cloud Storage triggers and Pub/Sub triggers, and that's returning a promise, so we'll look at that in depth and adapt our function to use that instead of the callback. And then we're going to deploy our function, and when you do you'll see that a GCloud ignore file gets generated, and we're going to use that to ensure that our images that we're going to upload to Cloud Storage, we're saving them in our project, do not get uploaded along with our function. And then finally, once our function is deployed, we'll test it out and see how everything goes. So there's a lot of awesome stuff to do, let's get started.

Cloud Vision API Demo
Let's take a quick look at the Cloud Vision API and see what it can do for us. So, I'm at the Google Cloud home page, and you can just get to there by doing cloud. google. com, and I'm going to go to the Products page. And this might change over time, but you want to find the Cloud AI section, and then come down to the Cloud Vision API and click on it. And now we're at the home page, and one of the really cool things here is it has a demo that you can just go ahead and try the API by dragging an image from your computer. So I have a picture of my kids here, my adorable kids, and I'm going to drag that into the browser, and you can see that it went ahead and ran the Vision API on this image. The API is broken down into different sections of things you can do, and once we get to coding, we're going to concentrate just on the labels, but as you use it, basically each of these sections is a different function call from the Node. js SDK, so it's a really good way to think about it. So if you want face information and Safe Search, you would call two different functions. And it's really cool, though, right, it found two faces, and Face 1 is giving us, they look very joyful, my son always does, and then Face 2, where she's just staring at the camera. Labels is the one we're going to be using in this module, and you can see here that it just goes ahead and finds some different descriptions about the photo. You see Child, Pink, Toddler, Sitting, it looks like they're having fun, and it kind of gives you a probability for these numbers. And so anything that's above 65% we're going to save that description and upload that to the Cloud Datastore database. And you can go through the rest of this yourself, but one last thing I did want to take a look at is JSON. And so, this is really cool because it just gives you the actual JSON object you would get back, which is really helpful with working with these APIs. And so here we have the faceAnnotations, and then if we come down we have our labelAnnotations object here, and so we can see that we're going to be getting an array of objects back, and then the object we have two properties that we care about. We have the description, and so here we see child, and the score, which is what we're going to use to determine whether we want to use that description word or not. So this was a gentle introduction into the Cloud Vision API, it's really cool that they do this, and if you look at some of their other AI products, they have similar stuff set up for the speech, and text, and translate, so I urge you to go and check them out so you get a good feeling of what unique features you can have within your Cloud Functions, or any code in general that's going to use the Google Cloud services. So in the next video we are going to set up our different services in Google Cloud, Cloud Datastore, Cloud Storage, and enable the Cloud Vision API, so see you there.

Setting up Storage, Datastore, and the Vision API
So in this video we're going to go ahead and set up Cloud Datastore, Cloud Vision, and Cloud Storage. As you can see here, I already have the Datastore open. We haven't used this in prior courses, but if you simply go to the Storage area, you can select Datastore and it will take you to here. And there's nothing here right now. It says go ahead and create a data object so you can explore Datastore features, and so that's what we're about to do. So let's go ahead and just set up our location. So it's asking us to select a location, we can do US Central, that's where our Cloud Function is uploaded, and now it is setting that up for us, and it's going to take care of everything under the hood, so we can go ahead and run it. And now while we're waiting for that to do its thing, let's go ahead and enable the Cloud Vision API. And now if you look here, you'll notice that we don't even get any Vision option here, and so an easy way to go and enable any API is to come up here to the APIs and Services tab. And so I come here, and now you can go ahead and view and research all of them. You have the Cloud Vision API here, or you can go ahead and type it in up here, and it will appear. Go ahead and click on it, and it gives you a lot of great information, and just go ahead and enable it. Again, this will take a moment as well, so I'll see you back once everything is enabled. So we're back, and we have the Cloud Vision API enabled now. And you're taken to the screen, and you can see we have an Overview and a Quotas tab, and here we also have a Disable button. Now, let's go ahead and take a look at Cloud Storage, and make sure we have all the buckets that we need. Let's come over here to Products and Services and scroll down to Storage. Now, if you saw the Getting Started course, we had two buckets, we had the staging-functions and this functions-production one. We're going to continue to use those in this course. I am going to go ahead and stage my functions in the staging-functions bucket, and then I'm going to use the functions-production to tie our Cloud Function to this particular bucket. Now, something you might notice are these two appspot. com buckets at the top and bottom of our list. Now those were created when we set up Cloud Datastore, and these are associated with Google App Engine. Now I'm going to come up here to Compute, App Engine. One thing that's required for Google Cloud Datastore is an App Engine app running in your project, and so when we went and set up Datastore, it automatically created this App Engine for us under the hood. And now you don't need to set up and use it, however, it is there, and so that's what those buckets are. I just wanted to explain them to you because you're going to see them pop up once you go ahead and configure Cloud Datastore. So now that we have the Cloud Datastore set up and our Cloud Vision API enabled, and all of our necessary buckets created for Cloud Storage, let's now go and start building our function.

Initializing the New Project
So let's get ready to build our first function. Now, like we said before, we're going to first build a function that can communicate with the Cloud Vision API on Google Cloud. So I have an empty project here in VS Code, and I'm going to go ahead to View, and Integrated Terminal, and you can use Ctrl+backtick, at least on Mac, to toggle this open and closed, which I'm going to be doing quite a bit. And here, go ahead and make a new directory data-functions, and then we can go into data-functions, create a new node project using npm init. So I'm just going to quickly run through these, hit Enter, and just get this set up. And now if I come up here, you can see we have data-functions and our package. json. Now we're going to be using a few dependencies here, so I'm going to go ahead and install those right now as well. So do npm install --save, and we're going to be using storage API, be using Datastore, and finally, the Vision API. So go ahead and hit Enter, and this is going to go and install all three of these dependencies. This might take a moment, so I'll see you back once it's fully installed. Everything is installed now, I'm going to go hit Command+K to clear the terminal, and now we also want to create our index. js file. Now that we have everything set up, let's go and start writing our code.

Checking the Cloud Storage Object for Images
So we have our project set up, and now let's go ahead and start bringing in our dependencies. So the first one I'm going to do is Datastore, and then we'll grab Storage, and we'll do the Vision API as well. And now we can go ahead and initialize our objects. I like doing this outside of the function, because if you think about it, as it brings up an instance, we're able to initialize these once, and then keep reusing them, and so it helps, at least with the execution time. I mean with something small like this it might be negligible, but at the same time too, you're slightly reducing cost, right, for our CPU, memory usage, and execution time, so it's a good habit to get into if you really don't have anything changing between functions to just initialize it outside of the function. And I'm going to go ahead here and create a new client for our Vision API. I actually want to call that client. And now we'll go ahead and create our function. So we'll do exports, we'll call this imageTagger, and here we'll pass in the event and the callback. And now I'm going to go ahead and grab our Cloud Storage object from the data property on the event. You learn this in the Getting Started course that that data property will change depending on the type of trigger that you're dealing with, and in fact, we can just go ahead and log that here. And now I'm going to put a quick check in here. I'm going to say if the object's contentType doesn't start with image, then I want to go ahead and say this is not an image, and we'll call our callback and return out of the function. And now the reason I'm doing this is because the storage trigger is going to fire every time a file is uploaded to the bucket, and we only want to send images to the Vision API, and so this is just a quick way for us to say, is it an image, yes, okay, let's go. It's not? Alright, exit out, we're done. And so, again, just saving time and reducing that execution time as much as you can. And, in fact, you may even want to make this stricter, you may want to just allow png's and jpg's through, so your if statement could be. startsWith image/png or image/jpg, keep going. So now that we have the basic scaffolding set up for our function, let's go ahead and look into querying the Datastore.

Querying Datastore for Existing Entities
So now that we have this initial function set up, we're going to go ahead and create a new function called processLabels, and we'll pass in the bucketObject. And now in here, the first thing we're going to do is we're going to read from the Datastore, and we want to do that because we want to make sure if an entity for our object already exists, then we want to go ahead and update that. Otherwise, we're going to go ahead and create a new entity after we call the Vision API. So here I'm going to go ahead and create a storagePath, and this is going to use the Google Cloud Storage path. We're not making these public. If you were, this would be HTTPS, but since we're just using the internal structure we can go ahead and use the gs:// instead. And I'm going to grab the name of the bucket and the path within that bucket to our file using the name property. And now I'm going to go ahead and build a query from our Datastore, so go ahead and say createQuery, and we're looking at the Images entity. And now I'm going to go ahead, and I only need the key. We'll be checking a particular property with a filter, but I only need the key returned, and you can do that with __key__. And for any Python fans, Pythonistas, that would be dunder key. Now do query. filter, and we're going to check the storagePath field. We're going to make sure it's equal to our storagePath. And now we can go ahead and run our query, and if we don't provide a callback we'll get a promise, which is what we want to use. If we get data back from this query, we can go ahead and process it. So the first thing I want to do is, I'm just going to see if an object even exists, so do objectExists, data 0, length is greater than 0, and then we can use that and say does it exist? And if it does, then we can go ahead and reach into here and grab our datastore. KEY. And this is a little different. You're getting an array of an array of objects, which is why we're querying into it, and that's despite the fact that we're even limiting it to 1. And we need our else condition. By using this datastore. key, this will produce a key and basically reserve it for us for this particular entity. It's just a really nice feature. Okay, so let's first deal with if our object exists, and our bucketObject. resource state == not_exists, which means that it was deleted, then we can go ahead and say datastore. delete(key). then, and just say console. log successfully deleted entity. Okay, so it's really nice what we're able to do here. We just went ahead and, again, just using that Cloud Storage bucketObject we're able to say, hey, wait a minute, does this resource exist, because that's the two values you're going to get. You're either going to get exists or not_exists. And once you get not_exists, you know this is an event in which the file has been deleted from the Cloud Storage bucket. And I'm just going to quickly go up here, and instead of using a null operator do a proper ternary expression, change that to a colon. Okay, and the last thing I want to do here is add a catch to the promise we're returning here, so in case the queries were to fail, we can just go ahead, take this error, call console. error, query run received an error, and pass the error in. We are going to go into reporting errors and doing it properly later in the course, but for now we'll just use this pattern whenever we want to deliver a failure to the Google Cloud Function system. So now in the next video we're going to go ahead and look at calling the Cloud Vision API and writing to the Datastore with our updated data, so I'll see you then.

Calling the Vision API and Returning Promises
So, this is really good so far. So right now, we are pulling in the entity, and if it exists we grab the key, and if it doesn't exist we go ahead and generate a new key. And then we do a quick check to see if our object exists. If it does exist in the Datastore, and our bucketObject is showing that this resource was just deleted, then we're going to go ahead and delete that entity from the Datastore. Let's go ahead and call the Cloud Vision API. We know that we have an image that exists, we have a key for it for the datastore, and we are ready to either insert a new entity or update the existing one. So now we're going to use the Cloud Vision client, and we're going to call a function called labelDetection, and provide it the storagePath that we had built up here earlier, and so it knows where to go to fetch the image. Again, this is nice, right. We are using various services of the Google Cloud Platform ecosystem to integrate these different services to create a better experience. Can you pass an image to the Vision API? Yes, you can, you can download an image and pass those bytes over, and it'll go ahead and process it. But because of the integration that we have between Google Cloud Storage and the Cloud Vision API and the Cloud Function, we can just go ahead and provide it the path, and it's going to go ahead and do a lot of the hard work for us. So, we get a promise back, and we can just go ahead here and log out the results so we can take a quick look at it once our function runs. And now we're going to go ahead and get the labels here. And so if you remember when we kind of did that one sample image before and got a good idea of the JSON data that we're looking at, I'm just going to go ahead, query into the results here, and grab the labelAnnotations array. I'm going to go and get the descriptions, but the only descriptions I want are the ones that had a score greater than or equal to 65%, and so now in here I can go ahead and just say label. score is greater than or equal to 0. 65. So filter is going to loop through and grab all of the correct ones, and then I can go ahead and map this and just grab the description. And so just grabbing those words, it was playground or child or train tracks. And now that we have our descriptions, we're going to go ahead and build an entity for datastore, and so we're going to provide the key, and then we're going to provide a data object. And so we have the storagePath, I can provide the storagePath we already built, and then we can do tags and go ahead and pass our array of descriptions into here. And now that we've created this, we can just go ahead and call datastore. save, and pass in our entity. And the last thing I want to do here is just in case we get an error back from the Cloud Vision API, let's go ahead and log that error out so we know that there was some sort of issue. So returned a failure, and we will go ahead and pass that error into there. And just a quick fix here, this should be error and not log. Great, now that we have this we're calling our processLabels, which ultimately is returning promises. And so I can just come up here, and we can go ahead and call processLabels and pass in our bucket object. Now, typically we want to then go into this promise and call the callback so we are properly ending the function. However, that's not necessary. One option that you do have as an alternative to using the callback is returning a promise from the function itself. Another couple of steps that you need to take to be able to accomplish this, one is actually returning a promise, which we're doing here, and then the next is removing this callback parameter, and that's telling the Cloud Functions that you want to use a promise instead of the standard callback. And, then of course we have our if statement right here, and we can go ahead, come here, and return Promise. resolve since we're existing out right away, we don't want to throw an error or anything like that, because we're just saying it's not an image, we don't care. And then there is one small bug that I do want to fix in here, and that involves us being able to delete from the Datastore. And here this property should actually be resourceState == not_exists, instead of. resource, and we'll just come here and add the triple equals. Alright, so now that we built this, let's go ahead deploy it and test it.

The Tale of Two Functions
so we're going to make some unexpected changes to this function, and we're going to split it into two. Now I'm going to tell you why. Cloud Functions is a beta product, and when this function was originally designed at the time, when you attached a Cloud Storage trigger to a function, it would fire on any changes made within that bucket, and that means if a file was created, updated, deleted. And Google went ahead and made the great change to separate that functionality out, and so you can create different functions based on different triggers so it will only fire when the file is being created or updated, or if it's been deleted or if the metadata of the file has changed, but not the file itself, or if it's been archived. And so with that in mind, we're just going to make some slight updates to this function so it works with the new trigger mechanisms, and it also brings up a good point about having multiple functions within the same code base and the challenges that can come with that. We're just going to make some quick updates here. Now, we're going to use the event. context object, and on there we have an eventType property. And with that, we're just going to say, if our eventType is google. storage. object. delete, and you'll see this when we go and deploy our function with that trigger that we use that exact name. And then I'm going to go ahead and do object. resourceState = not_exists; else, I'm going to say object. resourceState = exists. And now I'm basically just adding this property here, because it used to exist on the legacy function style. But now that Cloud Storage is using the Pub/Sub notification Cloud Storage object, this property no longer exists, and so our function will continue to work as it always has. Now, the last change I'm going to make here is I'm now just going to have this be its own function called tagger. And then I'm going to go ahead and export two different functions now, so I'm going to recreate the function we just had, and call return tagger event, and then we'll create a second function called deleteTagger, and we'll pass in the event and call tagger. Now, basically using this eventType the function will just switch knowing whether it's a delete function or the image function that's going to call the Cloud Vision API, and so what is the significance of this anyway? The thing I find most interesting about this scenario is we now have two functions that are sharing the same code base, and now with that, this could bring some issues, right? Let's say you deployed both functions immediately, however, over time you're just making updates to the creation process, and so you deploy that one over and over and over again. But inadvertently you may create a bug in the delete function, and then one day you go and deploy the delete one and suddenly it doesn't work. Or you're updating the code base thinking you're just updating the create path, and you're being good and deploying both functions at the same time, and yet you inadvertently break the delete function. The whole point of people using Functions as a Service is to create these microservices, yet when you're sharing the same code base, you're almost creating the same monolithic problems you were trying to get away from in the first place. And so you need to be mindful of that as you go and create multiple functions within the same project, because over time it could create a snowball effect, if you will, as your code base is mixed amongst all these different services. Now with that being said, let's go and deploy these functions.

Deploying Image Tagger and Using the GCloud Ignore File
We're ready to deploy our first function. So I do have a few tabs open here for Google Cloud because we're using a few different products, and if you haven't gone ahead and enabled the Cloud Functions API, just go ahead and do that from the Cloud Function's dashboard screen. And that will take a few moments to set up. And we also have our bucket here, and you can see I went into the functions-production bucket. This functions-production is just where we're going to be uploading our images. Now I'm going to go in here and actually create a new folder right now called uploads. Great, and I'll just jump into there. And I also have the Datastore open, and there's nothing here right now, it says go ahead and create a data object so you can explore Datastore features, and so that's what we're about to do. So I'm going to jump back into Visual Studio Code. Okay, so we're ready to deploy both of our functions, however, I do want to introduce a new file to you before we deploy. You see, in my project directory here I have this images folder with three different images, and these are assets that I don't want uploaded and deployed with my function. Now if I went ahead and deployed this function without this. gcloudignore file in my project, GCloud will go ahead and generate it automatically. However, you should really get in the habit of creating this before you deploy any of your functions. And so here this works like a. gitignore file, and so here I'm just going to go ahead and put my images directory in there, and now Google Cloud is going to do all the hard work for me and not deploy it. I'm going to use Command+backtick to open the console. So I'm going to go into my data functions directory here, and I'm going to run our first deploy script. Now, if you remember, we've split our function into two separate functions, so now I'm going to call gcloud beta functions deploy, and I'm first going to deploy the imageTagger, and we'll supply the trigger-resource, which is going to be the bucket that we want to trigger the function. So whenever files are uploaded to the zeta-sky-181023-functions- production bucket, our function will trigger. Now, again, this is the name of my bucket. Google Cloud Storage buckets are globally unique, so yours is going to be different here. And then I'm going to add the trigger-event, which is going to be google. storage. object. finalize. Now this trigger will happen when files are created or updated. So I'm going to go ahead and hit Enter here. Now if you've watched the Getting Started course, you know that deployments can take a couple minutes. I'm just going to go ahead and skip to the next deployment for our delete function so we can keep moving. And, again, I'm going to do gcloud beta functions deploy deleteTagger for our delete function, and again, our trigger-resource is the same. It's going to be zeta-sky-181023-functions-production, and then the trigger-event is going to be google. storage. object. delete. And now if you remember, that is the same trigger that we're checking here, google. storage. object. delete. So I'm going to go ahead and hit Enter, and so now I'm deploying both functions. And so once these are deployed, I'll see you back here.

Testing the Image Tagger
So, I'm going to go ahead into GCloud, you can see we have our imageTagger function here. If you don't see it, go ahead and refresh, you should see it after the deployment, and I'm going to jump up to View Logs here, and you can see the deployment for our function. Now I'm going to go up to our Cloud Storage tab, and we have our functions-production bucket, and it created a folder, uploads, in here. So what we're going to do is we're going to go to upload files, and we're going to go to our project, gcf-fundamentals, data-functions, images, and now we have our three images. I'm going to go ahead and grab all three and hit Choose. And now this is uploading them to Cloud Storage, and at this point it will trigger our function, and send it to the Vision API, and then it's going to go ahead and update the Datastore for us. So let's go ahead and jump over to our logs to see if we can see any progress. I'm going to go ahead and refresh, and now you can see the logs populating. Our function execution started, you could see here what we're getting back from the Vision API, and so here are our labelAnnotations, which are JSON objects, and those are the ones we were filtering and then mapping. And you see here that the function execution finished with a status of OK. And so now if we go over to the Datastore and refresh the screen, you can now see that we have our three images uploaded, and we have the Cloud Storage path for them, and then we have the different tags that are describing it. And so, if I go ahead and move this window over, and let's open image 2, if we pull up image 2 here, you see outdoor play equipment, playground, public space, toddler, child, sitting, leisure, recreation, so it looks pretty accurate in its description if you ask me. So congratulations, this is really cool. Now, we can take this a step further though. At our disposal is ImageMagick that's already installed on the server, so let's go ahead and leverage that and use it to generate thumbnails from our images, but first let's do a quick recap on everything that we've learned in this module.

Summary
So, wow, we went over a lot of great stuff in this module, and congratulations. Let's just quickly recap everything that we learned. First, we demoed the Cloud Vision API and saw all the cool information that we can get from it by uploading one image. We then went ahead and set up our various Google Cloud services such as Cloud Datastore, Cloud Storage, and the Cloud Vision API. Remember, that's a nice thing about the Google Cloud Platform, the majority of features are disabled, and that way you can't accidentally use them and be charged for them, but it is up to you to enable them and set them up if you are going to use them. Then we went and looked at building a query using the Cloud Datastore SDK, and we looked for any existing image entities within our database. And if we got those back, we saved the key to that particular entity, and if not, we went and created and generated a new key using that Cloud Datastore SDK. Once we had a key, we then went and fetched the labelAnnotations by using the Cloud Vision API, and we did that by taking that Cloud Storage object that was passed to our function and building the path to that particular image that was stored in Google Cloud Storage, and it made it really easy to work with. We didn't have to send the image itself up to the Cloud Vision API, since we were working with Cloud Storage, and because that function is running within their system, we were able to easily access the image, and so a lot of work was taken out of our hands by leveraging Cloud Storage and the Cloud Vision API together. And then once we had that data back, we went and either created a new entity in the other store or went and updated an existing one. Next, once that was all put together, we then looked at being able to return promises instead of leveraging callbacks within our function. And so it was really easy to do, right? We just got rid of the callback parameter on our function, and then we return a promise instead. Now don't forget, this can only be used with storage and Pub/Sub triggers. With the HTTP trigger, you still need to call that response object. You don't have the ability to return a promise; that won't do anything for you. Then we took a look and leveraged the GCloud ignore file, and this is a really nice addition, because perhaps you have some test data you're keeping in your Git repository, but you don't want to upload that to Google Cloud, you can use the GCloud ignore file to make sure that certain files don't get uploaded. And in this instance, we used our images folder and our node_modules folder, which should speed up your deployment significantly. And then finally we deployed and tested the function, and all looked right in the world. We're not done yet, though, because we're going to take this example a step further in the next module and we're going to take that same image and produce a thumbnail with it. We're going to have to learn a few things, though, and that is better insights into the actual environment that we're working in within the Cloud Function, as well as learning about the file system and knowing where we can read from and write to within it. So there's a lot of cool material In the next module, and I'll see you there.

Cloud Function's Docker Image and File System
Know Your Environment
When you're getting started with Cloud Functions, one thing that seems opaque is the environment you're running in. You go, and you write a function, you use GCloud to deploy it, and suddenly you can upload files to Cloud Storage or call a URL and your function is being invoked. But what does the environment look like that that function is running in? Well, we're going to start peeling back the layers on that in this module, Cloud Functions, Docker Image, and File System. There's actually a lot of information you can gather from just a couple sources about the environment that our node instances are running in, and you'll see that you have a normal file structure as well that you can read and write to, and we're going to get into the details about that because while you can pretty much read everything, there's only one location you can write to. So let's take a look at what we're going to discuss in detail in this module. First, we're going to go ahead and review the Base Docker Image, and you're going to see by looking at this Docker Image that you can glean various insights, and the most important one that we care about is knowing that ImageMagick, a command line utility for working with images, is installed within our environment. And you'll see some other tools as well within that list. Then we're going to discuss about reading and writing files. Like I said just before, you have the ability to read pretty much everything, however, you can only write to one specific location, and we're going to go in depth by providing a conceptual model of how you can think about the file system and the different instances, and how it's important to not rely on the file system between different function invocations, between different calls to your function. And next we're going to start improving the demo that we worked on in the prior module. We are going to produce a thumbnail image using ImageMagick, and we're going to upload that to the Google Cloud Storage and write the path to our cloud Datastore entity. And so there's going to be quite a few steps for us to get to that point. We're going to wrap promises around some of the file system functions in Node. js, we're then going to download our file from Cloud Storage, we're going to create a child process which will help create the thumbnail by using ImageMagick, which we know is installed on our environment so we can go ahead and call it. Then we're going to take that new thumbnail image and upload it to Cloud Storage, we're going to clean up the temp file system, otherwise we're going to cause memory issues, and then we're going to look into a problem where we might have accidentally stumbled into invoking our function an infinite amount of times and how we can avoid that, because man, that will get expensive. And then we're going to go ahead and upload a couple hundred images and just see how our function performs. And you'll see that we're going to glean some good information by doing that, and we're going to have to adjust the setting on our function to ensure that it's going to work properly. So there is a lot of cool stuff to cover in this module, so let's get started.

The Node.js Docker Image for App Engine and Cloud Functions
So we're going to take a quick tangent here, because I mentioned we're going to use ImageMagick since it's already installed on the environment that's running our Cloud Functions. But how do I even know that? And we can find the answer at the Google Cloud Functions documentation website. We've been here before down here in the Concepts section, and have been to Overview of Cloud Functions where we can find the node environment that we run, but now I'm just going to jump down here to Execution Environment. And here, again, we have more the same information about what Node version we're using. But if I go down to the next section, you have Base Image, and here it says Cloud Functions uses a Debian-based execution environment that includes contents from the google-appengine/nodejs Docker image, and it even gives us a link to the GitHub project. And so if we go ahead and click on it, and I scroll down, and I go to the runtime-image and just select Dockerfile, you can get extra information about some of the stuff that is installed. And here you can see that imagemagick is one of those items that are installed. And there is a lot more to this, and I urge you to go ahead and explore and learn a little bit more about the environment that we're working with, with Cloud Functions. And now let's get back and create those thumbnails.

Wrapping the File System in Promises
So we're back in the code, and we are now going to expand this example. We are going to use the ImageMagick service, which we know is installed on the server, because we took a look at the Docker Image that's used to build our Cloud Function environment, and we're going to generate the thumbnail, and then we're also going to take that thumbnail path and we're going to add that as a property onto the datastore entity that we are saving to Cloud Datastore. So there are a few changes we need to make first to get this function set up and kind of get everything in the right place, so let's go ahead and do that first. So the first thing I want to do is create a new function, and we'll call that processImageLabels, and we'll go ahead and pass in the storagePath and the key for our datastore. Now I can go ahead and come up here, and I'm just going to grab this block of code here and go and paste it down here. Now, one thing we're going to do is we're not actually going to save to the datastore here, because we're going to wait until we also have our thumbnail generated, so I'm going to come here and we'll just return our object data that we want to upload to Cloud Datastore. Add a semi-colon down there, great, so now this function is set up. So I can come up here, and essentially here we are going to Generate Thumbnail and Generate our Vision Labels. I can fix that spelling. Now the next function I want to create is actually some wrappers around the file system. So first thing I'm going to do actually is come up here, and I'm going to bring in some new dependencies that are native to Node. js, and these are all the ones we're going to use as we generate everything we need for the thumbnails. So come here, bring in child_process, exec, we want the path API so we can build the appropriate paths for everything, we want the OS, and finally the file system. Perfect. And now we can come down to the bottom here, and again, we're going to create a new function, and this one we're going to call function mkDirAsync. There's some good Node packages out there for doing this as well, but I'm trying to limit those so that we can just kind of stay with the native Node. js as much as we can. And this is going to return a new promise, and now I'm going to get the status for the directory that we passed in, and this is going to return an error and stats. Now if there is an error, and the error code == ENOENT, then we know that the directory doesn't exist, and that we need to create it. So here I'm actually going to go ahead and call mkdir, and this will return an error. And if it does return an error, if this is undefined, we're just going to call reject on our promise and pass in that error. Otherwise, we can go ahead and say that we created the directory and resolved the promise. And if we get any other error, we're going to go ahead and just reject it as well. Now, if we don't get any error whatsoever, we know that this directory should exist, but we'll go ahead and confirm that the path that we sent down is in fact a directory, and we can go ahead and print out another message saying directory already exists and resolve our promise. And if it's not a directory, we're just going to go ahead and throw an error for the moment, and just say that a directory was not passed to this function. Okay, great, so, again, we are going to be chaining promises, and you're going to see we're going to have about five or six promises we're chaining together so it just makes it easier to work with this by, again, wrapping this in a promise. So now that we've wrapped our code up here, let's go ahead and take a look at building our paths and generating a thumbnail. And first we're going to look at what directories can you even write to in a Cloud Function.

A Cloud Function's File System
Alright, so let's talk about the Google Cloud Function's file system. If you watched the Getting Started course, you've seen this slide before, and this is a small scenario that we put together where we said, hey, what if we have hundreds of thousands of invocations on our function? In this case, this is an HTTP trigger, and we had this conceptual model of Google basically generating a load balancer on our behalf, spinning up multiple instances of our function, and distributing those individual invocations to the different instances. This is all done behind the scenes, you don't see it, you don't need to manage it. Now let's extend this model a little bit from the standpoint of our file system. So what's happening here? Well, if you have a background in containers, you know that as a container is spun up, basically it's siloed off, and it has its own file system, permissions, and its own scope, and so that's a great way to think about these different function instances that are being brought up. And now let's say in our project we had a template folder, and in there we had a couple of HTML files, a CSS file, and a JavaScript file. Well, when we deploy our function, those files are going to get deployed, and then when Google cloud goes and spins up an instance of our function, essentially those files are going to be copied into that instance, and so you're able to read from that directory and pull in whatever files you need. And this is really easy to do, right? We just bring in our path module, and then using path. join, and using __dirname, which is already set up for us, we can then go templates/builder. js, and then we have our JavaScript file to use in our function. Now there's a little more work involved in terms of reading HTML and CSS files, but you should get the gist of it. It's very important to note that the majority of the file system within each of these instances is read-only, which begs the question, where can we write to then? Well, Google has set up a temp directory for us, and you can find it at /tmp. This is the only directory that you can write to within a Cloud Function. And it's very useful though. For instance, as our images are coming in, you can see that we can upload them into this temp directory, and we're going to be able to use ImageMagick to shrink down these files as our Node child_process will be able to access this file and manipulate it. Now if you take a look at this slide, you'll notice something very important, and that's the varying distribution of these files across these four instances of our function. You do not know which instance will ever be called. If you are going to take away anything from this course, one of the most important things is that function invocations should not rely on prior function invocations. Your functions should be stateless. If you take a look at this temp directory, you can never rely on the fact that a file will be there the next time a function is invoked. Let's say there's a bug in our code and instance number 2 crashes, boom, wiped out; so are all those files that were being held in the temp directory, and so you must never depend on that temp file. If you're going to write to it, you do it within that one invocation, and you never try to access that file again from another invocation. Accessing this directory is super simple. So we're just going to bring in our os Node module, and then you're just going to call the. tmpdir function, and that's going to return you this temporary path. Because one day they might change it. Right now it's /tmp, but perhaps they add a few new features and they need to move it for some reason. This is the safe way to do it, and this is how you should always call it if you need to access that temp directory, which it's time for us to do in our demo. So now that we know this, let's go ahead and start building our temporary file paths so we can upload our images into our function instance.

Building the tmp Paths
We're back. We've created a promise wrapper around some of the file system functions, and now we're going to go ahead and write the code to generate our thumbnails, and we're going to concentrate on the paths right now. And so as we went over in the prior slide, the only available path that we have to write to within the function is the temp directory, tmp. And so now let's go ahead and build the necessary paths that we need to generate our thumbnail. So I'm going to go ahead and create a new function, and we'll just call it generateThumbnail, and we're going to pass in the bucketObject into this. Now I'm going to go ahead and grab the filePath from the bucket, and we can use bucketObject. name to get that. And now this is going to be our uploads/image1. png. And I'm going to go ahead and get a parsedPath, so I'm going to use path, which we imported earlier, and do bucketObject. name, and this is going to break it down into its multiple pieces. And now to get the fileName, the image1. png, we're going to go ahead and do parsedPath. base. And just really quick so you can see this, if I go ahead and start node up here, and I go ahead and require path, and then I can write path. parse, uploads/image1. png, you can see here our base name. And so it went ahead and parsed the whole path for us, so that's what we're doing. Now I can come here, and if we want to grab our bucket from the Google Cloud Storage SDK, we could do bucketObject. bucket, and we're going to get the file from that bucket, and this is our original image that we're trying to generate a thumbnail for. And now I'm going to go ahead and get the local temp directory from our Cloud Function instance. And this is really easy to do from a Cloud Function, so we can go ahead and do path. join(os. tmpdir), and then we're going to do our parsedPath. dir. And so now what did we do in here? Well, again, let's jump to our little Node REPL here, and we have directory uploads. And so all I'm doing is path. join, os. tmpdir. Now, this is going to create a different directory than the one we have in our Cloud Function instance, because, again, that is /tmp. And then I am simply taking the directory that was created from the path. parse function. And here you go. So here is our temp directory, and then it simply appended the uploads. And now we'll take this further and join this together to create the file path. So, again, tempLocalDir, and now we'll join the filename to that as well. Now we have everything we need at this point, so the first thing we'll do is we can go ahead and say return, and we're going to call our wrapper around mkdir, and we'll pass in tempLocalDir. Now that's the start. We have parse the paths for our bucket object, we are using the Google Cloud Storage SDK to access the bucket that our file is in, and we are pulling the file out of there, and we're going to be using these in a little bit, and then we also took that parsed file information, as well as using the os. tempdir function call to create our local directory path and our local file path for our thumbnail image. So now in the next video we're going to take this and put it all together, and we're going to go ahead and chain some promises, and use ImageMagick to generate our thumbnail and upload it to the Google Cloud Storage bucket.

Executing ImageMagick in Node.js
Okay, so now that we've created all of our paths, and we're going ahead and creating our directory, under our temp folder, let's go ahead and call ImageMagick. And now there are going to be various steps that we take through this chain, and we're going to take them one at a time. So first things first, let's go ahead and jump into our Promise. Now first thing we're going to do is try to download the file from Google Cloud Storage, right, because we need to grab that file so we can process it. So we're going to download it, and the destination we want to download it to is our tempLocalFile directory. Now, in case that doesn't work, we want to be able to have kind of fine-grained error control here so we constantly know where things are failing. We'll say Failed to download file, pass in the error. And then we're going to go ahead and return a Promise from here, reject. Next, if we're successful, we can go ahead and say print out the file name, just say it was successfully downloaded to tempLocalFile. And now we are going to go ahead and create a new Promise here, and it's within this Promise that we are going to create a child process using the exec call that we had imported earlier, and we're going to use that child_process to call ImageMagick. Now one thing I am going to do here is do escapedFile, tempLocalFile. replace, And I'm going to use a regex here, because one issue I've seen with using this with ImageMagick and calling it on the command line like this is, it doesn't like spaces in the filename, and so just to simplify things we can go ahead and escape any spaces that we have so it doesn't fail unnecessarily. And now I'm going to go ahead and call our exec function. And here I'm going to make the command line call for ImageMagick. Now, we're not going to go too in depth into this, but if you ever want to know more about ImageMagick and what it's capable of, you can go here to imagemagick. org, and it has a lot of great information about the different command line tools and how you can call things, and a bunch of resources. It's pretty crazy the amount of stuff you can do with this small tool. So, anyway, for the meantime we're going to go ahead and call convert on ImageMagick, and we're going to give it our escapedFile path. And we're going to tell it we want to generate a thumbnail, and we're going to have that thumbnail be at least 200x200, so it will figure out the aspect ratio and everything like that. And so if an image is 400 x 300, it will give us a 200 x 100 image. We're going to use the same filename, so we'll just provide the filename as the output. And this is going to override our original image file with the thumbnail file, because don't forget, we already have that original file still saved in our Google Cloud Storage, so there's no reason to keep it in our temp directory. I'm going to pass a custom option to this child_process, and we're just going to ignore the standard I/O streams. If you want to learn more about this, I recommend Samer Buna's excellent course, Advanced Node. js. He goes in-depth into streams and child_processes, and you will not be disappointed with that course. And finally, because we're using the exec function we will have a callback and our first parameter will be the error, and the second parameter will be the standard output stream from the child_process. So we can say if err, console. error, failed to resize image, call err. And we'll reject this if it fails. And then, otherwise, we'll resolve and we will pass in the output. So now that we've called ImageMagick, we are basically saying, if it works go ahead and resolve the promise and we want that output from it. We're going to set up the next step in our promise chain, and then in the next video we're going to upload our thumbnail to Cloud Storage and look at cleaning up our temp file system so we avoid memory issues.

Uploading to Cloud Storage and Cleaning up the File System
Next in our chain of promises, we're going to log some more information, so, again, we know where we're at in the whole process, especially if this fails. And we're going to say successfully resized to 200x200, thumbnailFileName = path. join, thumbnails, fileName. So what are we doing here? Well, we're creating a new path for our Google Cloud Storage. We have the thumbnail, and it's saved in our temp directory, and so the next thing we want to do is upload this thumbnail image to our bucket on Cloud Storage. And its destination is going to be in this new thumbnail path. And we'll just catch the error quick just to, again, know where we are in the whole process. (typing) And we can just go ahead and reject it. Okay, so we've uploaded it to our bucket, and this is great. So, we have downloaded the image from our bucket, we then call ImageMagick using a child_process, and that image is turned into a thumbnail, and then we take that thumbnail image and upload it to our bucket at a different path called thumbnails. And it looks like we're done. However, there is one small extra piece that we need to take care of, and you really need to be careful about this, and that is clearing out that temp file directory. Say we just don't unlink the file, and what's going to happen? This is going to be called thousands and thousands of times, and if you're lucky, then millions of times, and what's going to happen is these images are just going to keep building up in the temp directory. They're not going to go away unless the container is spun down. And eventually you're going to run out of CPU and memory resources as it's no longer able to hold all of those images in memory. And so it's crucial, if you write to the temp directory, that you then remove those files once you're done with them, because it's always good to make sure that our functions are stateless, and by keeping it in the temp directory you're making them stateful, which is something we don't want. So, again, do a new Promise, console. log, Unlinking file. And then we'll call fs. unlink, tempLocalFile. If there is an error, we will once again reject because something is really weird here, since we just used this file it should definitely be there, otherwise, we will resolve and pass in our new file object. And so what's happening here is we're able to take this new file object that was returned from the bucket. upload call, and we're simply passing that into our resolved function, and we're going to pass that back up so we can save it to the Cloud Datastore. We're cleaning up our temp file directory, and so it looks like we're really good to go here, we're in a great spot. And now the next thing to do is, is to put it all together.

Putting It All Together
Now that we have everything we need, let's go and put everything together. And so we're going to create two Promises, one that's going to generate the thumbnail, and the other is going to go out to Cloud Vision and fetch our labels, and then once we get those back we'll upload that data to our database. So I'm going to come back up here, and there are a couple of things I'm going to take care of here. So we have our Generate Thumbnail, Generate Vision labels right here, and I'm going to add actually two else statements. So I'm going to say else if. And one thing I just want to do is, say there is no object that exists, but we are deleting an object? Well, we don't want it to take the path of generating a thumbnail and everything else, because that's just an extreme waste of resources. So why don't we go ahead and catch that state, and just say, if it doesn't exist, let's go ahead and just resolve the promise. The file was deleted, there's nothing to clean out of Datastore, and there's nothing else we need to do. Now, we have the key for our object, we have the path, so let's go ahead and call our processImageLabels function, and this is the one we refactored a little bit ago. And we can go ahead and pass in the storagePath to the image, and we can pass in the key for generating our entity or updating it. And then we're also going to go ahead and create a Promise for our thumbnail, and so we'll go ahead and call generateThumbnail, bucketObject, and pass in our bucketObject. Now that we have these two Promises, we can go ahead and call Promise. all and we can do thumbnailPromise and labelPromise, and we can wait for those two to complete, and once they complete, we can go ahead and take the results of them. And now the results are going to be, we can just go ahead and log them out here, but the first one is from our labelPromise that we want to grab, and that was the entity that we were returning, so let's go ahead and grab that. And then we want to grab the name of our thumbnail. So we can go into our first Promise, grab the first object that was returned from our bucket. upload call, and grab that thumbnailName. And now we'll go ahead and build the thumbnailPath. And we're not going to make this public, so we'll just use the internal Google Storage protocol, and we'll append our bucketName and our thumbnailName. And then finally, our entity, we're going to go into the data object, and we'll call this thumbnailPath. The last piece is calling our datastore again, and we're going to save. Showing a small error here, and I am missing a close parenthesis at the Promise. all. Excellent, so this is really good. We have gone ahead, and now we're pretty much doing two different branches, right? We're going out, and we're calling the Computer Vision API, which is going to go ahead and grab labels for our images, and then we're making a separate promise to go ahead and take that same image, use ImageMagick to generate a thumbnail of that image, and then go and save that to a different part of the Cloud Storage bucket. And then once both of those tasks are completed, we then take the entity that we built from the processImageLabels, we update it with our thumbnailPath, and then we go ahead and save that to the datastore. And with our call up here, there's really nothing new that we need to do. We just, once again, call processLabels and pass in our bucket object, and that's it. Now, there is an awful bug sitting here right now, and so I want you to think about the paths that we're writing to with Google Cloud Storage, and what might occur as we are writing to the uploads and thumbnails paths, and think about it, and we're going to discuss it in the next video.

The Recursive Invocation Problem
Okay, so we're back. Did you think about the bug? Do you know what's wrong? Let's take a look at a slide quick. So, we're uploading our file to the uploads folder. We're then copying that file to a temp directory and using ImageMagick to shrink it down to size. And then we are taking that temp image file and uploading that back to our bucket into a thumbnails directory. What's going to happen then? It's going to trigger our function, right? And then it's going to take that thumbnail image and go ahead and run through our code and try to process it the same way, and it's really just going to reduplicate the image, and then it's going to save to the thumbnail path, which is going to trigger the function again, and it's going to save it to the path again, and it's going to trigger it again, and over, and over, and over. We're in this vicious infinite feedback loop at the moment. And so this is a really easy fix for us, and it just starts right here at the top of our code. And so what I'm going to do is I'm going to use our parse function again from path, and let's do parsedPath, and we'll do path. parse, pass in object. name. And I want this to be the second if statement, so we can go ahead, come here. And now what I'm going to do is I'm just going to check our directory that we're uploading to, and if that directory in our bucket is not equal to uploads, then we'll just say only processing images from the upload folder, and we'll go ahead and resolve our function. And that's it. Now, when we write to that thumbnail path, yes, it is going to trigger this function, but we're going to exit out early, before we go ahead and try generating a thumbnail again and writing to the thumbnail path. This is short-circuiting our function, and doesn't save you money from an invocation standpoint, but if you look at it from the execution time and memory usage standpoint, you are saving a lot of money in this way. An alternative is to write to a different bucket completely where this function is not attached to. You have plenty of options, but this is a quick way to handle it, and you have to think about that, right? Maybe one day they might have a more advanced Google Cloud Storage function trigger where you can give it a direct path within a bucket itself, but right now when you attach a trigger to a bucket, it's going to trigger for every file that is uploaded, no matter what path it is in within that bucket. So now that we've captured this crucial bug, let's go ahead and deploy our function and test it out.

Uploading the Images and Creating Thumbnails
So you can see we've gone ahead and deployed our functions just like we did earlier. You might be on a different version. You can see I've uploaded quite a few here. Maybe you're on version 2 or 3. And now let's switch over to Google Cloud. So, we've gone ahead and cleared the uploads folder, and if you look at Cloud Datastore, all the entities were deleted that were of type image, as we deleted all of the photos. And now, one thing I did was I I went and just copied and pasted the three photos we were using, I just copied and pasted them over, and over, and over again, and now I have about 216 of them. So if I go to upload files, and go to our project folder, and images, you can see here that I have all of the images just duplicated over and over again. And one of them does have that space which we built the regular expression for. And I'm just going to go ahead and hit Command+A to select all of them and choose. And now it's going ahead and uploading all 216 images to Google Cloud. And now this is going to take a few minutes to run. It's going to go ahead and upload all the images, it's going to build as many instances as it needs of our function to be able to process these images, and then it's going to go through that flow. It's going to be sent to the Cloud Vision API, and we're going to generate labels. Meanwhile, we're also going to take a separate path and generate a thumbnail image using ImageMagick. And then once both those tasks are done, we're going to combine that data and upload our entity to the Cloud Datastore. So I'm going to let this go for a minute, and let all these images upload, and when they're done I'll see you back. Alright, so we're back, all of our images have uploaded, and you can also see that our dashboard has caught up and we see a sharp increase in the invocations, and then it drops off. If we go over to Cloud Datastore and hit refresh, you can see here that we have a lot of our entities now. As you scroll down, it'll keep loading more. And we also have our new thumbnail path, and so we've got gs, our bucket, thumbnails, which is the folder we created, and the image name. And so if I go up to bucket here, and I go to zeta-sky, we now have a thumbnails folder. And if you compare the size of these images to the ones that we had uploaded earlier in the uploads folder, you'll see these are drastically smaller. If I go ahead and click one, you can see we now have a little small thumbnail, she's adorable, and it came in at 200x150 pixels, so this is really cool. So we know our function is working properly, we went ahead, we cleaned up our temp file directory, and everything seems to be in good order. And now it's important to do this load testing too, maybe even 200 images isn't enough, but let's go to our function and we'll analyze the invocations, execution time, and memory usage.

Dealing with High Memory Usage
We've run our function a couple hundred times by uploading over 200 images. Now let's take a look at some of the information that's provided to us on the function's dashboard. And it looks like the invocations is fine. And we could get a better look at our execution time, and so here we can see that there was some shorter ones, and then we had some that were a bit longer, even some above 8 seconds, and maybe some of those were the cold starts. And then let's take a look at the memory usage, and, oh, this looks a little dangerous, right? So by default our functions are set up for 256 MB, and it looks like we came really close to this. Sometimes we hit 216 MB. And now these were pretty small images to begin with. I know because I scaled them down after taking them from my iPhone. So what if you were dealing with even larger images? Well, this memory could become a big issue, so it might be the case that you want to come to your function, here we have imageTagger, go ahead select it, come up here and hit Edit, and you want to go to the memory allocated and probably up it. So in this instance maybe it's safe to use 1 GB, and we can go ahead come down here and hit Save. And you can go ahead and set that from GCloud as well. If you do gcloud beta functions help, you'll see some of the flags, and one of them is being able to set the type of memory usage that you get when deploying the function. And so you can see here that it is being updated and soon we'll have a checkmark, and we'll have a lot more memory accessible to that. So it's good to test your functions in different ways. Of course, something you might want to do, especially with the way phones are today and the large images that they can produce, you want to take some of those larger images and run that through, and make sure you do not have any issues. You might find that it's completely unnecessary to have such large files. And you have a function in the beginning that's going to go ahead and process that big image and shrink it down to a particular size, and then from there go and send it to the Cloud Vision API and generate a thumbnail based on that new smaller image. So there's a lot that can be done here, but you have to test your functions and figure out what your needs are. So now that we've taken a look at the invocation, execution time, and memory issues, let's quickly review an execution of our function from the Stackdriver log system.

Reviewing the Logs
And now really quick, one last thing, let's go ahead and check the logs out, as we're creating a lot of log statements throughout this whole process. And we'll just go ahead and select one instance here, matching entries for this execution_id. And we went over the execution_id in the Getting Started course, right? Every invocation is given this unique ID so we can see one running through the system. And so here we can see tmp/uploads already exists, so at this point this was using a pre-existing container where we had created this uploads folder. We can see that our image was successfully downloaded to tmp/uploads. And here we see the Vision API returned with the labels that we're going to later add to the image. And then at that point we have our image was resized successfully to 200x200, and then we're unlinking our file from the temp directory after we had successfully uploaded it to Google Cloud Storage thumbnails directory. And one other important thing to note here, if we go back to Cloud Functions, imageTagger, are invocations. If we had not put that check on the directory path of our Cloud Storage bucket, this would still be going. We would still be invoking this function as the function continuously uploads to this thumbnails folder here. So congratulations on finishing your first complex function. Let's quickly review what we went over.

Summary
So we went over a lot of great material in this module. First, we reviewed the Base Docker Image for our environment, and this is the same one that's used in App Engine and Cloud Functions. And we were able to see that ImageMagick was installed on the server, and there's also Python and Git were running on a Debian Linux OS. And while you're not going to use all those tools, it should just give you a good feeling. It just makes the environment the function is running in more familiar. And then we took an in-depth look at the Cloud Function's file system, alright, because there's going to be times that you need to store temporary files because you're doing some sort of processing on them, and so we went in depth about how you could pretty much read from everywhere, and how you can only write to one location. And we also took a look at the conceptual model of if you have different function instances running, and how you can't rely on knowing which instance is going to be called repeatedly. You don't want to store files within your function and hope to process them later when the function is invoked later. It's just going to end badly for you. We then created a thumbnail by using ImageMagick, and there were quite a few steps to get through that process, we took it step by step, and we accomplished the goal, and it was awesome. We then took a step back, though, and looked at what our function was doing, and we noticed a very serious problem, and that was that recursive invocation. Because we were uploading that file to our same bucket that originally initiated the function, it was kind of creating this recursive feedback loop where then the thumbnail would trigger the function, and the function would run through again. And so we added code to short-circuit that and say, whoa, is this image in the uploads folder? No? Alright, I don't care, stop the function. And then finally, we deployed our function, and we uploaded 200 plus images. Then we took a closer look at the data in our function's dashboard, and we saw that we were using a lot a lot of memory, and the photos themselves weren't that large to begin with. And then we changed the memory usage from 256 MB and upped it to 1 GB. And, again, what you would want to do is take some larger images, upload them and see how that does, and maybe 512 MB would have sufficed. So congratulations on building your first complex function, and you used multiple Google Cloud services to accomplish the task, and you learned a lot. Next, we're going to build a new function that's going to leverage the Pub/Sub message system, so I'll see you there.

Capturing Transactions with Cloud Datastore and PubSub Triggers
Introduction
Serverless technologies like Google Cloud Functions are changing the software development landscape. It makes it easy for you not to worry about infrastructure, and you can just code. But that's part of the problem, isn't it? So many examples are presented to you are just saying, hey, you can quickly write this function and deploy it off into the cloud, and you never have to worry about it. Well, if only reality were that easy. And in this module, Capturing Transactions with Cloud Datastore and PubSub Triggers, we're going to find out the hard way that you just can't connect any technology to a Function as a Service like Google Cloud Functions and just have it work. Sometimes you have to take a step back, reevaluate, and change your solution. So we're going to be covering a lot of great stuff in this module. First, we're going to review a small client app that I've written in Node. js, and this is going to help us manage Google Cloud for us. It's going to set up the things we need for this module and the next, it's going to let you publish messages to a PubSub topic, and another script that will let you just clean everything up and reset Datastore and the Cloud Storage bucket. Then we're going to create a service account for that client app. And one of the key things I want you to take away from this is when you're using Cloud Functions and deploying them up into Google Cloud, you don't have to do this, because you're deploying it into Google's environment, Google trusts it, and you get access to all of their services pretty much, whereas with our client app we need to give it permissions, we need to set everything up so it will work from outside the Google Cloud environment, and this is a great opportunity to see how those two differ. Next, we're going to start writing our function, and this is going to be a PubSub trigger. And we're going to use our client app to publish messages to a specific PubSub topic, and this function is going to run and update entities that exist in Datastore. Spoiler alert, it's not going to go well for us at all, and so we're going to have to take a step back, reevaluate, and we're going to look at why Cloud Functions and Datastore really aren't mixing, especially the way we're using it in this case compared to when we had used it earlier in the course. And we'll decide that using Cloud Storage would be a better approach, and so we're going to update our function to do that. And then we're going to take a bit of a tangent, because when we update our function to use Cloud Storage, it's actually a great opportunity to look at how we can retry functions when they fail, and so we'll go over the steps of how you should tailor your functions to make sure they can be retried, and the safest way to do it. So there's a lot of great stuff to learn, so let's get started.

Reviewing the Client App
Alright, so we're back in our project, and we've made a couple changes since we've last seen it, so let's quickly go over the updates. First, I created a new folder, and within that a new Node project for our PubSub function. I just called the folder transaction-functions and did an npm init within it. The next addition is this client-app, which is also written in Node. js., and this holds some scripts that we're going to use throughout the next couple modules to help us set up Google Cloud and trigger our function, as well as cleaning up any data that might be left behind because of failures. And we'll primarily be working with this app. js file and calling these other three files using app. js. And now the first thing we want to do, though, to set this up is we'll go into our client-app and do npm install. And if you're familiar with Node, this is just going to go take the package. json file and install any dependencies that are already listed here, and in this instance you have datastore, pubsub, storage, and one devDependency we're using is chalk, to add some color to the console. log. Okay, now that all of our dependencies are installed, we're going to go ahead and just run node app quick. And you can see here you get some help, please include one parameter, 0, 1, 2, or type help for more information, so we'll just go ahead and type help. And here it says pass one of the numbers listed below, and here you can see what each number will do as we call them. So 0 will initialize Google Cloud for us by updating data in Cloud Datastore, creating a new bucket and a new pubsub topic, and then number 1 will actually trigger our function, and we'll be using that a lot, and then number 2 just does some cleanup. It's going to reset a couple variables within our Cloud Datastore entities, and it's going to delete any files out of the new bucket that's getting created for us as well. All of our data that's going to be uploaded to Cloud Datastore is in this projects. json file, and it really just has two project entities, as well as some packages for each of those projects. And, again, we went over our entrepreneurs are creating a video game, and they're trying to raise money for it, and so this data reflects that. Now there are a couple more things we need to do to configure this, so we can use it. And the point we're trying to drive home with this is that with Cloud Functions you get all of this for free. You don't need to do the setup we're about to do, but since we're running it from outside Google Cloud, there are some security concerns we need to address before this will even work. And so if you go to app. js, we're going ahead and setting some environment variables that we're going to use throughout all of our scripts. And now, first the most important one is our projectId, and so in this instance mine is zeta-sky-181023, and yours is going to be something else. And so, I'm setting that variable here. I'm using this projectId and appending packaging-data to that for a new bucket, and this will create a new bucket for me. And then we're creating a new topic name for a pubsub topic called packageTransaction. And as you run the initialize script it will go and check for it, and if it doesn't already exist, it'll go and create it. And then finally we have this GOOGLE_APPLICATION_CREDENTIALS, and this variable is required by the client SDKs, to work. And you can see here that we need a JSON file to pass to it. And so, in the next video, what we're going to do is we're going to go into Google Cloud and create a new service account, so I'll see you in the next video.

Creating the Custom Service Account
Alright, so I'm in Google Cloud, and we're going to go and create a service account for our Node. js app. So I'm going to come up to the hamburger menu and go to the Identity and Access Management tab, and then if I come down here, you have this service account. So I'll go and click on it. And you can see our App engine account already here, we talked about that in module 1, and this was our runtime account that can go ahead and give us access to so many different services within Google Cloud. It's saying, I know you're uploaded into my project, I know I can trust you, so go ahead, use Cloud Storage, use Cloud Vision, use Cloud Datastore. So now we're going to create our own key to let us do that from your own local machine. So I'll come up here to Create service account, and we'll just call this Node Client App. And now we need to select some roles. So I'll come down here and first thing we want to be able to do is Datastore. And if we come down to this Cloud Datastore User and you read the description, it says, intended for application developers and service accounts. And this gives us everything that we need, because all we need is read and write access to the data, so I'll select that one. Then I'm going to come to Pub/Sub and consume messages from a subscription. And it's not enough, because we're going to even be creating topics. We're definitely going to be publishing messages to a topic, but that's still not enough. Modify topics and subscriptions, so this level looks good enough, so Pub/Sub Editor, so we'll go and add that. And then for Storage we're going to go and add the Storage Object Admin, which gives us full control of working with the Google Cloud Storage objects So I'm going to come here, I have those three set up, and I'm going to select Furnish a new private key. And you have the option of JSON or P12, and we're going to create a JSON file. And now this is creating a key for us on our behalf, and so we now have this new file. And now you can see here I have my new JSON file, so I'll go and close that. But we're not quite done yet, because one thing that our node client is going to do is it's going to go ahead and create a new bucket for us on our behalf. But one of the issues with the Cloud Storage rules is none of them give us access to that, because that is a project-level privilege. And if I come up here, I would have to come here to our node-client-app, and I would have to go ahead and come and give it Owner or Editor. And we've looked at the Editor one earlier in the course in module 1, and we know that is a lot of privileges and we don't want that for this client-app. Security should always be a top concern, and we should only give access to the things that we need. And so I'm going to come down here to Roles, and we're going to have to create a new Role for this. And just to give you a quick idea, let's do Storage here, and this is going to filter on all of the Roles. And we did Storage Object Admin. And so we get a lot of privileges and permissions here, but notice that they're all for storage. objects, nothing for the buckets, and so that's what we want to add. So I'm going to come here to Create Role; and Custom Role, we'll call this The Special; and leave ID CustomRole; launch stage, we'll just make it generally available; and now Add permissions, and now we get this long list of permissions that we can add. And so here we have our buckets. And now I'm going to pick three here. We want to be able to create buckets, I want to be able to get buckets, and also the list of existing buckets, and we'll go ahead and add permissions. And now I can go ahead and create this new role, The special. So we now have our new role here, our new custom role, and we see that it's enabled, so now let's go back up to our Node Client App and come here to the Custom, and here we have The Special, and we can go ahead and select that. And now it went and updated our service account with those new privileges, and so we didn't give it access to everything by just going and using the Editor role. Now, let's go and add our JSON file to our project.

Using the App to Set up Google Cloud
Welcome back. I went and added the JSON file to the project, and now the next thing I'm going to do is I'm just going to grab the name of this file, hitting Enter and copying it, and I'm going to go up to app. js, and here with service-account-file I'm just going to paste over it. And now I am set up and good to go. So if I come back to my node app here, and I do node app 0, it should go ahead and run. See, it went and created the topic packageTransaction, and it went and created a new bucket as well, zeta-sky-181023-packaging-data. And the Cloud Datastore is taking a moment to create the project and packages, so we'll just give that a moment. Great, so it's gone ahead and created our two projects, as well as each applicable package within those projects, and it also says Data generated testData. json in our project, and that file is right here. And so what this file is, let me Ctrl+A, Option+Shift+F to format it, and all this file is, is the projectId for the Datastore ID, and then the packages applicable ID, and so here we have that, as well as this value property. And that is the value property for each of these, and that's the minimum cost of each of these packages, so $50, $550, $10000, because we're going to be sending this data when we trigger our Pub/Sub or publish to our Pub/Sub topic. Now that was a lot of work to get everything set up. We created a new service account, and we went ahead and configured our app to use it, and let's quickly jump into the initializeProject JavaScript file, and you can see here we're going and grabbing our environment variables, but when we go and we set up Datastore, PubSub, and Storage, we then have to go and pass in the projectId as a parameter, and also if it doesn't see this GOOGLE_APPLICATION_CREDENTIALS it's going to throw an error. And if you compare that to jumping into our code from the last module, what we have to do for a Cloud Function, here we just imported them, created new instances, and just ran with it. It was so much faster, so much easier. So I just wanted to give you insight into the difference between working within the Google Cloud environment with something like Cloud Functions and App Engine versus working outside of that safe environment, and the steps you need to go through to get there. And now if you have different client apps and they all have different credentials, and this one needs access to machine learning products, while this one needs access to SQL databases and this app needs access to NoSQL databases, this can snowball quickly, so it simplifies your life. Now, we'll just go and jump into Google Cloud quick and make sure everything is set up properly. If the script ran, we should be good to go. And so if you come here, we can see in Storage we have our new bucket called packaging-data. In Pub/Sub we have our new topic packageTransaction, and helloWorld was from the Getting Started course. And then in Datastore let's look at our entities. We have ten package entities listed here, and again you can see the minimum cost, pledges are set to 0, our title is set to the correct thing, and totalReceived is set to 0. And then finally, let's look at Project, and you can see here that we have our project as well. Now that everything here is set up in Google Cloud, let's go and start writing our function.

From PubSub to Datastore
Alright, so let's start writing our function. I'm in our transaction-functions folder, and the first thing I want to do is get the datastore package from npm, and with the magic of video editing I will see you back once this is fully installed. Okay, great, it's installed, and we'll go ahead and create our index. js file. And we will go ahead and import Cloud Datastore, and we'll just make that the instance of it. And now we'll create a new function called purchasePackage. We're only going to use the event parameter so we can return promises instead of using a callback. And then here I am actually going to go ahead and log our event Id for later use, eventId. Alright, so first we're going to go ahead and get our pubsubMessage, and that's with event. data. And now, something I want to show you is, in this triggerPubSub file we have our pubsub, we're using our topic and our topic name, which was packagePurchased, to create a new topic here. And I'm actually going ahead and batching them, which is what this maxMessages is and maxMilliseconds, because later we're going to be invoking this a lot, and so it just makes it a little easier to work that way. And I'm going from that topic and creating a new publisher. And now I have this array here of invocations, and as you can see I have 5, 4, 3, 2, 1, and then I map that just so we have a multiplier, so when we're ready to start scaling up we could say turn that to 100, and then we'll send 500, 400, 300, 200, 100 messages. I'm going to leave that at 1 for the time being though. And then what I'm doing here is I'm looping through our test data from the testData. json file, and then I have an inner loop here for invocations array, that's a 5, 4, 3, 2, 1, and it's just going through and grabbing each package for every project, and it's generating some messages for Pub/Sub. So in this instance we're sending a projectId, the packageId, and the value from that testData. json file, and again, that value was the minimum cost property for each of those packages, $3, $15. And then here I'm going and taking that JSON, turning it into a string, and using the buffer. from to create a dataBuffer. And then I have another loop here which is basically just saying loop how many times we want to invoke and send a message, again, 5 times or 4 times, and I go ahead and publish the message, and I have a log statement just saying the message was sent, and I'm holding an array of Promises here so I can wait and make sure all the messages are published before our little script exits out. But the most important piece here is this data that we're sending and turning into a dataBuffer, and so we need to reverse that process in our function. So here I have the const pubsubMessage, let's get the stringData from that, so again we'll use Buffer, pubsubMessage. data, base64, and say toString. And then finally, we'll get that data by calling JSON. parse and passing our string in. We have the data, and now we can use that data to update datastore. And so we did this in the last module, right, we just kind of went and updated an entity. The only difference now in this one is we have a child entity for our project called package. And so if we generate keys in a similar way, here we're going two levels down, we're saying get me this project from this projectId, and then get me its package with this packageId, and we'll just go ahead and log out that keypath, packageKey = datastore. key(keypath). Alright, and now that we have the key, we can go ahead and call datastore packageKey, and then we'll grab the results that are returned, and we should only get one back, but we'll just grab the first one, results 0. And then we're going to update our data, so in this case we're going to look at contribution, we'll do data. value, and if no value was sent in the message, for now we'll just take that minimum cost property from our entity and apply that instead. And we will update the totalReceived property with our contribution. And then finally, we want to update the number of pledges by 1. And then we will call datastore. update(entity), passing in our entity, and we'll just log out Updated entity. Alright, so that was pretty easy. Let's open our terminal and we will go ahead and deploy our function, gcloud beta functions deploy, the name of our function, in this case purchasePackage, and then we have the trigger-resource flag with packageTransaction, and finally the trigger-event, which is going to be google. pubsub. topic. publish because we're going to be publishing to our pubsub topic. Hit enter, and I'll see you back once it's uploaded and deployed.

Publishing Data to the PubSub Topic
Alright, our function is now deployed, and you can see this up here with the purchasePackage function and our topic packageTransaction. I'm going to jump up here and go to View logs, now what we'll do is we're going to jump back to VS Code, and here I'm just going to open a new terminal and go to our client-app. Now one thing I want to do in our triggerPubSub file, which you can find right here, we're just going to up the invocations a bit. So we'll just multiply these invocations by 3, so we'll get 15, 12, 9, 6, and 3. Now I'll just come here and say node app 1, and it's going ahead and publishing messages on our behalf. This might take a second. Alright, so message was sent. Look at all those. So now if we jump back to Google Cloud, load newer logs, we can see our function running here. It's going and updating the entities, and it looks like we're not getting any errors, so everything is looking good. We have our key here that you can see, so here project, and then we have the projectId, as well as package and the package key for that. So if I go over then to Cloud Datastore, and I pull up our package entities, we should see our data updated. And, oh boy, that's definitely wrong. So the cheapest pledges should have been 15, and both of them are actually showing 12, and we have 8 pledges here. We were multiplying by 3, so there definitely shouldn't be an 8, there should be a 9, so something is definitely not right here. So let's just take a closer look at kind of how Cloud Functions execute, and how we're using Cloud Datastore with it.

Distributed Invocation Problem
Let's take a closer look at why our data looked incorrect in Datastore, and we need to look at how Cloud Function executes to be able to understand what's going on here. So on the slide you've seen this mental model before of Cloud Functions executing. We have this load balancer, and it's calling several instances of our function, as its receiving calls to be invoked, and so we're never working with the same function at the same time. And now to the right here we have Datastore with some mock data in there, and now these numbers, 1, 4, and 3, they each represent the pledges field on our package entity. And so let's just run through this. Instance 1 gets invoked, it goes and fetches the package data with pledge equal to 1 and it brings it in, but then at the same time function 3 gets invoked, and it goes and fetches that same package with pledge equal to 1, and then our instance number 1 goes and increments it by 1 and saves it back to datastore, and then our third instance does the same, and so because of this parallel execution, our data is not being updated properly because there are times that we're fetching old data before everything is properly updated. Now, is there anything from the Cloud Datastore side that can help us with this? We could try transactions, but this isn't a Cloud Datastore course, but I can tell you up front transactions aren't meant to be used, especially when you have distributed instances like this calling and trying to update the same entity, and so you're going to run into issues with it. And, in fact, if you look deeper into the Datastore documentation, they recommend that if you're running into trouble like this, that for performance reasons, one of the better things to do is to batch the updates. So, what if we just store them in the temp folder directory? We already know that it exists and we can write to it. No. Just like we discussed earlier in the course, you should never rely on that for cross-functions. Even say you could save 20 entities at a time to that temp storage file and then go and grab them and then batch them together and update the data entity, what happens if one of your instances only gets called eight times before it's spun back down? That's 13 invocations, 18 pieces of data that are just going to get wiped away, so that method isn't going to work. However, we do have one storage system that should be able to scale for us and fit our particular needs. What if we use Cloud Storage? And so in this instance, instead of writing to datastore with this particular function, we can go and as the data is coming in from Pub/Sub, just go and save a JSON file to a storage bucket. And then later we can set up a new function that we can trigger that can then go and collect those JSON files, add up all the numbers, and then update the Datastore on our behalf, and then we can make that triggering mechanism whatever we want. And in this case, why don't we make it a CRON job, every several minutes we say, hey, go and fetch those files, let's aggregate them, and update Datastore. So in the next video, let's go and adapt our function so it writes to Cloud Storage instead of updating Cloud Datastore directly.

From PubSub to Storage
Okay, we're now back in our project, and we want to go and update our function. So we're saving the data to Cloud Storage instead of Cloud Datastore. So the first thing we need to do is you need to go ahead and install Cloud Storage, so do npm install -s google-cloud/storage, and I'm going to hide that for the moment. And then we need to go and bring in some new dependencies here, and we've worked with quite a few of these already. So we have the file system, and we're going to bring os back in, we're going to do the path as well, and then finally we'll do storage. And we'll just go ahead and initialize our instance right away. Alright, and now we can keep the message parsing right here, however, everything below there we're just going to go ahead and delete for the time being. And now what we want to do is we want to almost perform the same process we did in the prior module where we're going to write the JSON data to a temp file directory within our instance, we are then going to upload that to Cloud Storage, and remove the temporary file. So the first thing I'm going to do is get our bucket, and in fact, I'll come up here and we'll do BUCKET_NAME zeta-sky-181023, I'm finally starting to memorize it, packaging-data, cool, because we'll be using that in other functions later as well. So we got the bucket, and then we're going to go ahead and create the fileName. And in this instance, what we want to do is we're going to use the eventId, and this will make sense a little later when we talk about retyring functions. And we'll do tempFilePath, path. join, os, get our temp directory, and you've seen this before, and then append our fileName to that. Awesome, so now we're ready to start our promise chain. So create a new Promise, because we are going to wrap the writeFile call for this. We've got a tempFilePath, we're going to pass in our stringData, and then our callback has an error. So if there is an error, we can go ahead and say Failed to write to the tmp directory, pass in the err, and we'll call reject on our Promise, pass in the err to that as well. And if we're good to go, then we can just say Temp file created for tmpFilePath. And I am being inconsistent with my quotes here, so I try to use the backtick more these days with the ES6 standards. I just find it cleaner to write. Alright, cool, so we have our first promise. And we're going to write the file, and if that doesn't work then we're going to return an error, and if it does we're going to continue on. So we'll just leave a message here so we can kind of keep track of where we are in case we get any failures, so fileName, uploading fileName to BUCKET_NAME. And now we're going to call bucket. upload, and this time around, where we were creating folders within the bucket like in the prior example, we're not going to do that this time around, we're just going to keep a flat file structure, and yes that will cause some issues, but it's good to see them, and so we'll go through them as we run into them. Alright, and we'll just catch this in case something goes awry, so we can say console. error, Failed to upload resized image. And the last thing we're going to do here is call Promise. reject and pass in our error, perfect. So now the final stage that we have in this chain is simply to unlink from that temporary file, right, because, again, even though these are extremely small JSON files, if you've got a lot of data coming through this process, then the instance is going to stay alive and sooner or later you are going to run out of memory, so always clean up after you're done. We'll say Unlinking file, tempFile Path, fs. unlink, tmpFilePath. If there is an error, we will call reject, else resolve. Let's pass in our error into here. Alright, close that, close that. So it looks like we're good to go now. So pretty straightforward, we simply have our pubsubMessage coming through, and then down here we're going to create the file, then we're going to upload it, and finally we're going to upload it to Google Cloud Storage and then we're going to unlink our file. Perfect, so let's go ahead and deploy this function and I will see you back once it's deployed.

Validating the Data in Cloud Storage
We're back, and our function is deployed, so now let's go ahead and test it out. I'm jumping back to our triggerPubSub script here, and where we had a multiplier for 3, I'm just going to switch this back to 1. And we will up this significantly later, however, for now we're just going to test this out quick and we should see 30 files in Cloud Storage once everything finishes running. So I'm going to go node app 1, and this is going to go ahead and generate our messages and publish them to our PubSub topic. Alright, so we got our pubsubMessages published, our function is running right now as I'm talking, and so let's jump to Google Cloud and start checking everything out. Alright, so we're in the Cloud Storage bucket, as you can see, we're in the packaging-data, and I have my files here. And now if I go ahead and select them all and select Delete, you see 30 files will be deleted, so all 30 files are here, I'm going to hit Cancel, and let's take a look at the data in them. You can see we just have a simple JSON object with our projectId, our packageId, and the value that we're supposed to be updating at. So everything looks really good here, and we have all 30 pieces of data uploaded, and so you can see how this is really going to be easy for us to then go and fetch these files in another function and batch them together. Before we do that, let's quickly recap what we've learned throughout this module.

Idempotent Functions
So let's talk about retrying functions, and first, why would we need to retry a function? Well, because it failed, right, and there are various ways that the function will know that it failed, for instance, the runtime could throw an exception because it ran into a nasty bug, either in your code or a third-party package that you're using. Another reason is because you can't reach a specific endpoint for a service. Say you're trying to process payments with Stripe, or validate in an app purchase with Apples iTunes Store, and their service is down, so you're calling it and you're getting a 500 back, you can't process anything, and so you'll want to intentionally fail this function because you can't complete the task that you set out to do. And so the two easiest ways that you can go ahead and fail it if it can't reach an endpoint or for some other reason is you can have your function return a rejected promise, or you can pass an error into the callback. And later in the course when we talk about monitoring functions, we'll talk a lot more about the different ways you can report errors, and how not to report errors, but these are two triggers that you can use to make sure that your function fails and it sets up to retry itself. Now, when you're trying to retry functions, there are some important things to keep in mind, and number one is use the event object. And you were introduced to the event object in the Getting Started course, and the event object is delivered with the asynchronous functions, in this instance the storage functions and the Pub/Sub functions, and with those you have the eventId, a timestamp for that event, and it's that information that's going to be consistent as the function retries itself. You're going to keep getting that eventId every time the function is retrying itself, and the timestamp is going to stay consistent from the initial trigger, so you know how old that event is. And those, to me, are the two most important bits of information on the event object that you can use. And that event object is really important in helping you avoid infinite loops. If you don't take care to have your function stop at a certain point, and you have it set up to retry itself, you're going to be caught in that infinite loop again. And in the code example, you'll see how we really don't address this right away, we're going to address it later in the course, but it's something you need to be aware of. It could be several days that your function will retry itself before it finally is canceled by Google itself, and with it constantly retrying itself you are going to start running into limits and quotas that you might not want to run into, and it can get expensive very fast. Another good thing to do is using catches with promises. And so, again, we talked about how you can return a rejected promise and the function will retry itself, but say you have a really nasty error that you caught with the catch, and you're just saying, wow, you know what, that is such a bad error that I do not want this function to retry itself. And finally, the most important bit of this is you're going to get the best results if your function is idempotent. And so you may be wondering, what is an idempotent function? Well, I'll tell you. Idempotent functions will produce the same result no matter how many times it is invoked. And that's why I'm bringing up retrying functions now, because we built an idempotent function. We're using the eventId as the filename that we're uploading to Cloud Storage. That function can run 100 times over and the end result is going to be a file saved in Cloud Storage with that eventId as the filename. Now, an error could occur, right? For some reason, Cloud Storage could suddenly be down, or I temporarily hit a quota limit on it and I'm blocked from accessing it; however, the end result of the successful function is the same, and so you should always try to keep that in mind when you're going to set the flag for retry. And we'll take a look when we do deploy the function, we will set the flag --retry when we deploy it. You can also hit a checkbox in Google Cloud to have it set to retry as well, there are multiple ways you can do it, and you can turn if off even once it's on. It would just set a new deployment for you. So that's a basic of retrying functions.

Deploying with the Retry Flag
Alright, so we're back, and we're now going to do something you should never do, and that is we are going to intentionally make our function fail. Google Cloud Storage is really resilient and very powerful, so if we sat waiting for it to fail on us, we could be waiting for a while. So instead, I set up this array called randomFailure, and it has four truths and one false, and so that should give us about a 20% failure rate on our function. At the bottom here, I added a new function called randomInt, and you can pass in a low and high one, and this is really just to get an index into that array. So, over here I'll just create a new variable called callIndex, call randomInt, pass in 0 for the low, and randomFailure. length for the high. Then we're going to create an if statement and just grab an index from our array that's returned randomly, cool. And now, if this is false, we're going to go ahead and return an error, and call Promise. reject, pass in a new error with the message Epic Fail, awesome. And so, if this turns out to be true, we can go ahead and bypass this and run through our function normally. Okay, so we're going to deploy our function like we normally do, but we're going to make one slight change to it, and we're simply just going to add the retry flag and hit Enter. Now, if you go to a normal function and you were to create it within Google Cloud, you have a checkbox that gives you the same capability, but in this instance since we're working from the command line, all you need to do is add that flag. So now our function is deploying, and once that's deployed I'll see you back here.

Viewing the Retry Attemps In Logs
Alright, so our function is deployed now, and now let's run up and go back to our command here. I'll clear it, and I'm going to call it node app 2. And if you remember, this is going to go ahead and clean up our Datastore and delete any files within our storage bucket. So when I hit Enter it's going to go to Datastore and set all those values to 0, and then it's also going to find all 30 files in the storage bucket and delete them, so we're good to go. So I'm going to hit Command+K to clear, and now I'm just going to run node app 1 again, and this is going to publish 30 new messages to our topic, and now we can go into Google Cloud and let's check everything out. First I'll refresh so it looks like we have a whole new batch of files here. And now I'm going to go to Cloud Functions, and we'll go up to View Logs right here, and we can see that we got a couple of failures in here. So some of them worked really well, and we still have them running a bit. And now, if I go up to here and click this ID and say Show matching entries, actually I want to do it on the Epic Fail, it's the same eventId, but Show matching entries, you can see here that our function actually failed. This one failed a few times. And what happened, though, is that it kept trying, and then finally it worked, and it no longer runs anymore. And this is really nice, and this is why I wanted to show this example right now, right, because it kept processing it, and the end result was always going to be the same, because the file that would wind up in Cloud Storage is using that eventId as its file name, and so this is never going to rerun and give a different result if it passes. It can run 20 times and pass, and the end result is always going to be the same. Now, say Cloud Storage really went down, in real life, it just, a bad day for Google and suddenly these functions are failing over, and over, and over again? Well, you're going to be stuck in the same problem that we were earlier in the course when we looked at that infinite feedback loop. Now the infinite effect is being caused by something different in this instance. In the earlier example, we were constantly dropping files into our bucket, and it was picking up the new images, and constantly reprocessing, and we had to say, hey, just look in the uploads folder, those are the only images we care about to stop it. And in this instance, because we're really not putting any short circuit in here, if Cloud Storage really did go down, this is just going to keep running too, so you might want to do some extra precaution with this as well. Let's say if the timestamp of the event is older than a minute, stop. If we have five failed attempts, you can log this eventId in Datastore and just increment it five times. Once it hits 5, you say, okay, that's it, stop, don't run this anymore. So you have multiple options, and you really have to think about what you want and what works best for you to make sure that this isn't going to invoke infinitely. And later in the course, once we get to monitoring functions, you'll see how we can kind of monitor some events like this and send and notify us by email to say, hey, we got a big problem in one of our functions on production, go take a look at it. And as one final note too, something you could always do, especially during development, delete it. There's nothing stopping you from going in and just saying, delete the function, get rid of it, and that will stop the infinite invocations as well. So while retrying functions is great, and you want to get the same result every time that function runs, you do need to be careful, because you could put yourself in a trap of that infinite loop again, and that's something you really want to because it will get very expensive. Alright, so we're in a great spot now. Now, let's go ahead and wrap up this module before we jump to the next module and deal with batching all of our JSON files, aggregating them, and uploading them to Datastore, and then hooking up a CRON job to trigger that function.

Summary
Wow, you should be really excited. We accomplished so much throughout this module. First, we leveraged Node. js scripts from our client side to help us simulate a bunch of different users purchasing packages, and we were able to upload all that data to Pub/Sub. And to do this, we had to create a custom service account so we could have all the appropriate privileges on the Google Cloud side, and you could see how much simpler it is to work from the Cloud Functions when you're safely inside Google Cloud versus building a client app like this. Next, we went and wrote a function that was a PubSub trigger, and we attempted to update Datastore, and we found out pretty quick that that was the wrong approach to take. With the parallel execution, the data was just too inconsistent, and so we took a step back and reviewed the execution model and said, you know what, let's try Cloud Storage and see how that works, and we were able to successfully upload our data to Cloud Storage. And one of the most exciting things about the function we wrote was how we leveraged the trigger's eventId to be the filename that we saved up to Cloud Storage, and because of that we found that our functions could reliably fail and try again, and that we would always get the same result every time once a successful execution occurred. And so you were able to see how you should design functions so they could be retried. Now that we have our data saving up to Cloud Storage, it's time to collect those files, aggregate them, and update Datastore, and so in the next module we're going to do that, and then we're going to go ahead and set up that new function to a CRON job trigger, so I'll see you in the next module.

Cron Jobs and Hooking up the Web with HTTP Triggers
Introduction
One of the most exciting things about Cloud Functions is the HTTP trigger and the ability to just quickly have a new URL that you can quickly post or get from. It's nice because you don't have to stand up a whole website or anything like that. You might just have a special use case you need, something small, and you don't need to go through 50 steps to get there. And so in this module, Cron Jobs and Hooking up the Web with HTTP Triggers, we're going to do just that. We're going to continue building on our Pub/Sub trigger from earlier, which is now saving JSON files to Cloud Storage. And we're going to extend that example by creating a new function that is going to collect all those files, aggregate the results, and then update cloud data store for us. And once we deploy that function, we're going to get a URL, and we can use a cron service to set up a 10-minute Cron job to call it every 10 minutes on our behalf. So there's a lot of great stuff in this module. First, we'll create the new HTTP function, and then we are going to code, we're going to fetch the files from the bucket, we're going to aggregate data, and then once we aggregate that data, we're going to update the datastore entities. Then we're going to deploy our new function, but before we even set up the Cron job, we're first going to troubleshoot a lot of issues. Because if you think about it, well, we're going to find here that when we test a few thousand files on this function that we're going to run into issues. And we're going to take them step-by-step and go through them and get through them, but it'll bring up a lot of questions for you to think about, and trying to figure out what your needs are. And so once we get through all of those issues and our function is working, we're then going to set up an account at cron-job. org, and we're going to set up a cron job for our function. So this is an exciting module. Let's get started!

Downloading Files from Cloud Storage
All right, so we are back in our code, and let's get ready to write a ridiculous amount of promises, because we're going to be grabbing a bunch of files, then grabbing entities, updating the entities, and then deleting files, so there's a lot going on here. Now I'm back in the code, and it's up to you whether I leave the randomFailure code in or out. I'm just going to leave it in. Even though we're going to be testing this thousands of time, we won't get anywhere close to the invocation limits set by Google Cloud. All right, so first thing I'm going to do is create a new function called batchProcess, and this function we want it to be an HTTP trigger. So instead of using our event, we're going to be using the request response objects. And now, once again, we're going to use storage, grab our bucket, and now I am going to start sprinkling these throughout our function as a little foreshadowing. And this will just kind of give us memory usage as the function is running. And if I were to just go and type node and do process. memoryUsage, it's going to go and give us this object. And now the heapTotal and heapUsed refer to the V8's memory usage, and V8 is the Chrome engine running under the hood. External refers to memory usage from C++ objects, which we're not getting into. And then rss is resident set size, which we're really not concerned with, either. We're just going to be keeping our eyes on the heap. Okay. So the first thing we need to do is to get our files. And so on our bucket, we'll call getFiles, which will return a promise, and we'll pass in all of our files. And now the next thing we need to do is download all of them. So, let's go downloadPromises, and so this will be files, subzero. We'll grab the first element in the files already because it wraps all of files in an array itself. And then we're just going to map through these and we're going to call download on every file. Because, again, this isn't the actual file, this is metadata information, this is a Cloud Storage object that we've used in the past. And once again, we will take a look at our memoryUsage. So we're going to call Promise. all and pass in downloadPromises, and we'll just wait for all of them to return. And then what we'll do is we'll have our downloaded files and we'll pass that into the resolved promise, and once again we will do a memoryUsage. Now it's time to start working with our data. And so I'm actually going to go and use the reduce function here. And so I'm going to go through all the downloadedFiles, I'm going to call reduce, and I'm going to pass in a function that we have not written yet called reduceMessages, and we're going to pass in an empty object as our initial object that we're going to reduce down to. So I'm going to jump out of the function and come down here and start writing our reduceMessages function. So in the next video, what we're going to do is write this function reduceMessages. So I'll see you back.

Aggregating the Data
So I've come out of our batch process function and we're now going to write our reduceMessages function. So we're going to pass in the parameters that we normally would when writing a reduce, so we have the previous object that we're constantly returning from our reduce function, as well as the current element being passed in from our array, downloadedFiles in this instance. I'm going to take that data, and I'm just going to go ahead and parse it. And so again, Google Cloud just wraps all these files within arrays themselves. So we're going to grab the first element within the array of these downloadedFiles. And we'll do that for every one. And then we'll go and get the projectId out of this data, and we'll get the packageId. So first we'll check and see is our projectId in our object already? And if it is, then we're going to go and check to see if the packageId is already in the object. So, again, what we're doing here is we're slowly building up an object, which is going to contain all the projectId numbers and their packageId numbers, and we're going to accumulate the values of people's contributions and increment the pledges as well. And we already did this in an earlier video, but it's a little more complex now since we're kind of aggregating this data all at once instead of updating each entity one at a time. And so if the projectId and the packageId do exist, then we're going to go and grab that packageData from this object already. And from packageData, we'll say the totalReceived += data. value. Remember, we're saving that value property within our JSON files? And we'll do packageData. pledges equal to 1. Now, earlier in the video we had kind of said oh, if data. value doesn't exist, we will do, you know, entity. minimumCost. But in this instance, for simplicity's sake, we're just going to ignore that and assume that the value property is always going to passed to us. Now if the package is not already in this object, what we then want to do is projectId packageId, and we're going to create a new object with that packageId here, and we'll do totalReceived data. value, and pledges is equal to 1. And then finally if the projectId didn't exist either, we're then going to go into our previous object and go and create that new object. We'll go into it, create a package object as well. And then the last piece, the most important piece in this function, is returning previous. If we don't return our previous object, then this isn't going to reduce down and build up this object for us. All right, so awesome. So a lot of times when you see this, you'll actually see the function kind of written in scope here, but again, just considering how complex this function was, it was easier to break it out and then just pass it in and everything will be parsed properly. And now the last thing we'll do is we'll just go ahead and print out this object so we can see it more clearly in our logs, and this will be the last call we make to memoryUsage. All right, great. So now we've reduced our data. Now the next thing we need to do is go and fetch our entities and update them. So, I'll see you back in the next video.

Fetch the Datastore Entities
Okay, so we've reduced our data and now we need to go and update our data store entities. So I'm actually going to come back down here below reduceMessages, and we're going to create another function. And we'll call it fetchAndUpdateEntities. Seems simple enough. And we're going to be passing that data object in that we just reduced. I'll do queryPromises. So again, we're going to be getting another batch of promises. In this instance, it's going to be data store entities. And now I'm going to loop through our projectKey. So we'll do Object. keys and pass in our data. So this will loop through every project key that we found within those JSON files. And we'll grab the data from there, call it projectData, and get that. Now we're going to do the same for the packages as well, so we're going to go through packageKey, and again, in this instance now we're going to go through projectData instead of passing data in. And now this is going to loop through all the keys it found, so we just get the entities that we need. Now an alternative method is you could just go and say, hey, give me those project IDs, and I'm just going to go ahead and fetch all the packages, and I'll update the ones I need to. Because, again, if you think about it, each project really is only going to have anywhere from 5 to 10 different packages being offered for fundraising, so that is an alternative solution. All right, so let's get our keyPath. So we'll do datastore. key, and we've done this before, too. We did this in an earlier video, in fact. We did Project datastore. int, and we have our projectKey, and then we have Package datastore. int and our packageKey. Perfect. So this will get us the key, so we can go ahead and fetch our entity. It should not be a semicolon there. Okay, so now we're going to have our queryPromise, we'll call datastore. get keyPath. And then our queryPromises array, we will add this new promise to that array. All right, brilliant! So this is the first piece. We are now fetching all of the required entities that we need from datastore. And so in the next video what we'll do is we'll then wait for all these promises and then we're going to go ahead and update our entities. So I'll see you back here.

Updating the Datastore Entities
All right, so we're back and so let's now wait for all of our promises to resolve. And once they do, take the results of that and now we can go and update our entities. I'm just going to put a log message here saying Received all queries, console. log results, const entities, so, results. map. We want to go through all of these and update them accordingly. So again, kind of same pattern. You're seeing this over and over again, right? Everything that is returned by Google Cloud, a lot of the SDKs, are wrapped in arrays, and we're always grabbing that first element of the array to get to that specific object that we intended to get. Get the key from the entity, datastore. key using a symbol. Let's say projectKey is key. parent. id because project is our parent of our package entities. And then we'll get the packageKey as well, which will just be key. id. And now we can go ahead and actually update our entity. And so for the totalReceived property, we're going to go into our data, get the projectId, the packageId, and then the totalReceived. And this is the value we got from our reduced function that we wrote earlier. And pledges, and we're going to go in and do that. And then in our map function we'll return the updated entity. So now that we've mapped through all of the entities and we've updated them, let's go ahead and return them to datastore. And so here we're just making one call to update. And we pass in those array of entities, and then we'll just have a log statement that says Updated entity. And just in case, let's put a catch here in case we do run into an error throughout this process while fetching all those entities, and we'll do console. error. All right, so really quick, just want to fix a couple small mistakes here. So, I used ID and key, so we'll just keep this consistent, and I'm going to update these two variables to projectId and packageId. And then up in our reduceMessages, I put package here, so we want that to actually be packageId as well, and we'll give that a variable. So we're good to go. In the next video, we are now going to call fetchAndUpdateEntities, and we're going to delete all the files from Cloud Storage.

Cleaning up Cloud Storage
All right, so we are now ready to finish up this function here. And I'm going to call this new function fetchAndUpdateEntities, and we'll pass in the data. And then we're just not quite done yet, but we're almost there. So at this point, we grabbed all of the files from Cloud Storage. We aggregated the data, we fetched our datastore entities, we updated them, and then re-uploaded them to cloud datastore. Now the last phase here is to go and delete all the files. So first let's just do a catch on our fetchAndUpdateEntities, just in case something does go wrong, and we'll just respond with 500 send Ka-boom on fetching and updating datastore, just to give us some context as to where we all went wrong. And then up here we're just going to go ahead and look at how many files that we need to delete, and we can actually go ahead and log that out here, so we'll say Preparing to delete fileCount files. Now we're going to create another set of promises. I told you we were going to create a lot of promises in this module. And so we're going to go through every file again, just like when we downloaded, but now we're going to call the delete function instead. And this is going to give us back a bunch of promises, and we're going to call Promise. all and wait for all of these to complete. And then once that's done, we can go ahead and say Processed and deleted files, all is right in the world! And for a response, we'll make it a little less boring and just say Processed fileCount files, just so we can see how many files we did ultimately process. And then just in case we fail here as well, we'll just go and catch this and say console. error, issue deleting files, pass in the error, and then we will once again return server error, something went wrong on our end, say Ka-boom while deleting files from Cloud Storage. I wouldn't say they're very secure messages, but they'll work for us through this lesson. All right, fantastic! So now if you think about it, if we go and trigger this function one time and we had a few thousand files, this is going to generate thousands of processes as it's going through and downloading files, deleting files, fetching entities, and updating entities. So, great work! So now let's go and test this function and see how everything works out for us. So go ahead and deploy your function, and I'll see you back once it's uploaded and deployed.

30 File Test Run
All right, we're back and we're ready to deploy our function. So I'm going to go back into the terminal, and here we're going to do gcloud beta functions deploy batchProcess, and then we're going to use our normal staging bucket that we always use, and again, this one is tied specifically to your project; it won't match mine specifically. And then we're going to use an HTTP trigger this time, Enter, and deploy. So I'll see you back here once it's deployed. All right, the function is deployed. And we have our URL here that we can call to start the process. Now, we're not going to set up the cron job right yet, because to be honest, that's the easy part of all of this. So, before you even do that, it's a good idea to start testing the function. And if you didn't clean up Cloud Storage from earlier, you should still have 30 files loaded in there. If you don't have any, come back to the client-app and do node-app1, and it'll resend messages to upload those JSON files. I'm going to switch back to Google Cloud, and we have our new function here. I'm going to go ahead and open a new tab and put our URL for our HTTP trigger in and just hit Enter. And there you go, we have Processed 30 files. And if I go to cloud datastore and get our package entities, we now see the proper pledges and total received here, and so I have one pledge for the director level and five pledges for the cheapest level. If we go to Cloud Storage, we should see nothing here in our packaging data bucket. So all of our files were deleted and removed. And finally, if we go to Cloud Functions, hit our function, go to the logs. And we can take a look at this function that we had reduced earlier. Those are the entities. And here is the object that we've reduced earlier. And so you could see this is the project ID, and then we went through and added package IDs, and then we have the total received, and the pledges. TotalReceived, then the pledges, so it makes it much easier to see it visually in this context. All right, so that's awesome! Well, that worked out nice, right? Thirty files seem to be easy, so let's up the ante a bit and go for 3000 files in the next video. See you there!

3000 File Test Run
Okay, so we're back and we are now going to push our function to the limit, our batch function. So the first thing I want to do is I'm going to call node app 2. And there are no files to delete our cloud storage, but this is going to reset pledges and total received back to 0. So, our database starts with a clean slate. And I'm going to come to our multiplier here and just tack on two 0s, and so we're going to be sending 1500 messages per fundraising project. And now I'll just go and hit node app 1, and depending on your internet connection, this might take a little bit, mine went pretty fast. Okay, let your function run for a few minutes, and we'll wait until it tapers off and you're not longer getting any invocations, so we know our 3000 files have been fully processed.

Memory and Timeout Issues
All right, so we're back and you can see here we're looking at our purchase package function and had a pretty significant spike when it initiated the 3000 messages. And you could see here that I also am getting some errors as well because I left that turned on. But if our function is working right, and it's retrying itself properly, we should still get those 3000 files once it retries and finally passes those functions. And we'll take a quick look at the execution times, so on this one we were sitting around 300 ms. The higher end was about a second. And our memory usage, not too bad. We peaked around 60 MB, we have plenty of room left memory-wise. So now that this function has run, let's go ahead and start our batch process again. So I'm going to come back up to this tab and hit Refresh. Now this will take a little time to run because now we're doing 3000 files instead of 30. So I'll see you back once the screen updates. All right, we're back and we got Error: could not handle the request. Well, that's weird. A lot of our error messages were kind of nonsense, like ka-boom, something went wrong! But this looks a bit more serious, so why don't we go take a look. So I'm going to back out, go to our batchProcess, and look here. I'm going to refresh so this graph is the proper width. Out of memory. Well, that's weird. So, let's take a look. We're working with 3000 objects now, and so we're now using a lot more memory than we intended. And look, we didn't even get through our form memory logs before this thing ran out of memory, so this thing died pretty quick. So this happened during the phase of the download promises. Let's take a look at our function quick. And so here at this point, we go and generate all of our promises for downloading the files, and we do log out of memoryUsage here. And it's before all the downloaded files complete where it finally cuts out. And now there are multiple ways we can handle this, right? We can go and say, all right, well, let's lower this until we find out how many files we can handle at 256 MB, and we may have to increase the frequency in which we fire our cron jobs, so maybe originally we're going to say, hey, we could do it every 5 minutes, but now we need to do it every 1 minute. Or an alternative is we just increase the memory. So, why don't we go and try that? It's a little brute force, but it should be able to work, right? So, go here to Cloud Functions, let's go to batchProcess, we'll edit, and let's set it to, let's hope 1 GB enough. All right, we'll save, and now it's updating our function. And so once this circle stops spinning, we'll go ahead and retest our function. So, see you then. All right, our function is ready to go, and let's go ahead and redo this. Now again, it is processing 3000 files, so I'll see you back here in a minute. All right, we're back, and ah, we see have the same error message! So, no way could that have used 1 GB. Let's check out our memory usage again. So, I'm back at the function dashboard, and I'm going to just go ahead and refresh. And oh, we got a new color added now. Time out! Oh, I should have known. So, this function was just taking too long. We hit the 60 second mark that is the default. And even when we ran out of memory, we almost hit the 60 second call. Let's check our memory usage. Well, before we timed out, we peaked around 600 MB a call, so I think we're safe to stay at 1 GB. But our timeout is still an issue. So, let's go ahead and update the timeout time. And we'll update it to 3 minutes in this instance, 180 seconds, and we'll go ahead and save our function. Now, again, while this is updating, and let's talk about this, there are multiple ways we can handle this again, you know, and this is a decision that you and your team need to make, saying do we want to just grab 1000 files and process them at a time? Can our cron job run frequently enough to keep up with all these incoming files? So it's really up to you how to handle it. Upping it to 3 minutes is more just a convenience for us at the moment. It's not a panacea for fixing all of these issues. Okay, for the gold this time, let's refresh and see what happens.

Reviewing Metrics and Analyzing the Design
All right, our function passed, and we processed 3000 files. So let's jump back to Google Cloud and check some stuff out here. First, let's go to datastore and check our package entity. So if I scroll over to the right here, you can see that we have 500 for the base level package for our projects, and 100 for the most expensive. And our total received looks like they added up properly as well. Now if you aren't seeing these numbers, something you might want to do is if you are kind of going through the testing with me, you might want to clear the data from datastore and re-run everything. You should see these numbers reflected. So this is awesome! We had a big issue when we first started writing this type of function, and datastore just was not updating properly. We took a step back, kind of looked at, thought about datastore's weaknesses, and decided to take a different approach by uploading to Cloud Storage and creating a separate function that would fetch all that data from Cloud Storage and do a bulk update to our datastore entities. And something also worth noting is while we were going through this whole exercise with 3000 messages being triggered, I had the function set up to fail about 20% of the time. I'd never removed that code. And so even with the retries in place, we had that specific result from our function running properly, no matter how many times it was run that we eventually got to where we needed to be. And one last thing to check as well, let's go to storage, and check a packaging data, we have 0 files in here. And finally, we're going to go to Cloud Functions, batchProcess, and let's look at the execution time for this last run. And we didn't even come close to the minute mark that time. We were actually about 35 seconds. But again, if you're going to be processing 9000 files or 10, 000 files, you're going to have to try to find that right balance between, well, should I just take 1000 at a time and process them, or should I just up the memory and try to do it all at once? It's for you to figure out because there are multiple ways that this can be tackled. One last thing to check out, besides the execution time, is our memory usage. And this time around, we got to 800 MB, so that, again, this is using a lot of memory. And if you go and view the logs, you can look at the last instance, and it really is, if you look at this heapUsed and look at the massive jump between when we actually fetch the files from storage and download all of them, that's where the bulk of our memory is being used up. And so it seems like the easiest choice is to say let's take the 1000 oldest files, process those first, and then keep grabbing 1000 files and process and process them. That should keep the memory usage lower per invocation. And you could keep calling it, right? It'll keep creating new instances. It's just when you're bringing so many files at once into a single instance, that's where you're running into these memory issues. So, it's easy to resolve. So, again, like how we stepped back with saying, well, cloud data storage just really didn't work; what's another approach we can take? Well, we just need to do the same here. Just take another step back and say, well, what happens if we do have 3000 files or 10, 000 files or 20, 000 files? Let's update the function and maybe just do 1000 files at a time and increase the cron job frequency. And with that being said, let's go to cron-job. org, sign up, and hook up this function to a cron job. See you there!

Set up the Cron Job
All right, welcome back. So, I'm at cron-job. org, and you can see the URL up here. And just go ahead and sign up. It's free, it's quick and easy to do, you just verify the email, and then you're good to go. And you'll sign in and you'll see this screen. So, come here to cronjobs, and we can just go and create a new cron job. And we'll call this, we'll name it after our function batch process, and we'll grab the URL, which I still have right here for our HTTP trigger. And you can set up HTTP authentication. We're not going to do that with this example. And then here you could set up a schedule. And so I'm going to say every 10 minutes I want this to execute. And you can be notified by email if the cron job fails, or if it succeeds, and more importantly, if the cron job will be disabled because of too many failures. And then you're going to come down and select create cronjob. And that's it. So in 10 minutes, this is going to go and execute. So this was super simple and easy to set up with the HTTP trigger.

Verifying the Cron Job Is Running
So a cron job is executed, and before it executed, I actually went into the node app 1 and changed our multiplier to 2, so this added 60 new messages to our existing storage bucket. Let's go to datastore, package, and you can see here that we now have the updated values, and this is correct, right? I multiplied it by 2, and so instead of 5, 4, 3, 2, 1, we had 10, 8, 6, 4, and 2. So, congratulations! This was a complex demo with a lot of moving parts, and you had to adapt as we found the limitations of certain technologies we were using. Now, let's review everything we learned in this module.

Summary
Wow! It took us awhile to get here, but we finally have Pub/Sub messages, sending data, and we are using that data to update cloud datastore. And we ran into a lot of issues along the way. We found out you just can't attach any technology to Cloud Functions, and we've had to take multiple steps back along the way to figure out what adjustments we need to make so we could get this to work. And you learned a lot of great stuff by doing that. You learned about thinking how different instances of functions could be called at any time. And you have to think about that parallel landscape when you're designing your functions and how they interact with other systems. So in this module we've wrapped up that demo, we created a new HTTP function, and from there we were able to fetch files from our bucket. And when we downloaded them, we noticed we're using a lot of memory. And again, it's up to you to decide how you want to handle that. There are so many ways to resolve the issue. And once the files were downloaded, we then aggregated all the data and tallied up all of our pledges and the total amount of money we received, and we then updated the datastore entities. And then once the function was tested and ready to go, we then set up an account at cron-job. org and hooked our function up to it. And so now you have a cron job running constantly. I will add that we could have used Google App Engine to set up the cron job. However, for simplicity, I chose cron-job. org just because it would take less time. If you already have App Engine set up in your Google Cloud project, then you can just go ahead and use the cron. yaml to set that up and call the endpoint. So, congratulations, but we're not done yet! One of the best parts about Cloud Functions is how it integrates into the Google Cloud platform. And that includes the logging system. And so in the next module, we are going to take a look at Stackdriver Logging and Cloud Functions and how they interact with each other. And with that, we're going to build a special function that can aggregate some of those logs and send an email using a third party service called MailGun. So, I'll see you there!

Taking Control of Logs with BigQuery and Stackdriver
Introduction
Whether you're developing a function or you already have it deployed to production, one of the most important tools you'll have at your disposal is the logging system. Now at first, it seems like the logging system for Cloud Functions is super simple, where you can put console. log or console. error, and you'll get logs at the info level or error level. However, there's so much more to it! You see, Cloud Functions integrate with Google Stackdriver logging system on Google Cloud. And so, under the hood there are a lot more powerful features that are waiting to be discovered. And so in this module, taking control of logs with BigQuery and Stackdriver, that's exactly what we're going to do. We're going to build a new function that is going to aggregate a lot of our logs and report data about them. And once that data is aggregated, it's going to send us a daily email to update us on how many times particular functions have been invoked. Let's take a look at everything we're going to go over in this module. First, we're going to use MailGun to send emails. We'll walk through setting MailGun up and getting it set up in your function so we can send a single test email. Then we're going to do a deep-dive into Stackdriver Logging. We've really glossed over this in the Getting Started course, and so I'm really excited to cover this material. And you're going to see how the logs are structured, and looking at the log objects and how you will be able to customize that data if you need to. Then we're going to export our logs to BigQuery because the Stackdriver Logging SDK to pull logs in is extremely limited. And so it is common to export your logs that you want to keep for longer term. And once we export the logs, we're going to go ahead and build a SQL statement in BigQuery to aggregate those logs. And we're going to update our function to fetch those logs using the BigQuery SDK. And then once we have all those results, we can go ahead and send an email. And so this is really cool, and you're going to see it is so simple to do. And then finally, we're going to go ahead and take that same data that we are producing for the email and go ahead and log our own custom log entry. And so we'll upload some JSON data. And from there, we'll be able to see how we can build a custom filter with that data so you can quickly look through your Stackdriver logs for unique data that was built and delivered by you. And while we're doing this, another important piece of Cloud Functions that we'll be going through are the process environment variables because they contain a lot of important information that will get our custom logs properly into the Stackdriver Logging system. So this is a really cool module, and I'm super excited. So let's start by getting MailGun set up.

Installing the Dependencies
So we're back in our project, and we have this new folder, logging-functions, which we'll build our new functions in. And I'm just going to go ahead and come here and go into that folder, and then do npm init as we have before, and just run through this. And now we're going to go ahead and install our dependencies as well, so I'm going to use -S to save our dependencies, and in this module we're going to be using moment. js. We used this in the Getting Started course, and this really just simplifies working with dates and times in JavaScript. And then we're going to be using request promise. And you need to pull in request as well manually. It used to pull it in automatically, but it doesn't anymore, so make sure you include both of these. And then finally, we're going to bring in our Google SDKs, and so we're going to be working with the logging SDK and BigQuery. Okay, so now that I have all these, I'll hit Enter. And now while these are installing, let's go ahead and set up MailGun. So MailGun is a free service that developers can use for sending emails. If you're going to be sending a lot of them, you can upgrade to a paid plan, but one thing I do like about this is when you sign up, you don't need to provide a credit card, and they give you a nice Sandbox environment to work with. So go ahead, select the sign up button, and go through the whole process, and I'll see you back here when it's done.

Creating an Email Cloud Function
Once you've signed up for your account, you'll be taken to this home page. You might not be seeing this graph yet; however, you should see the sending domains here. And what you really care about is this Sandbox domain that they set up for you. And this allows you to send to five recipients of your choosing. You have the ability to go and hook up a normal domain if you are going to use this in production, but for our instances, the Sandbox is going to be perfect. So I'm going to come here to Authorized Recipients. And you can see here that I've went and added the email address for the Google Cloud account that I'm using. Now if you're using the same email as your MailGun account email, then you don't need to add this recipient. However, I'm using a different email address between the two, so I needed to add this. And if for some reason your emails are getting rejected, go ahead and add your email here anyway just to make sure. Once you've set up the authorized recipients, we're going to go here to Security, and you're given a private API key and the public validation key. So I'll come here and go and grab this key. Don't worry, it will be gone by the time this course is published. And now we're going to go back to our function. We have our dependencies installed already, and I can go ahead and create a new index. js file. And now I'm going to copy that secret into here. And we want to grab the MailGun domain. So if I go back up to domains, I can come up here and select the domain name and just copy that. And I'll come back into Visual Studio Code and create a new variable called mailGunDomain and paste that in. All right, so let's first bring in our dependencies now that we have everything we need from MailGun. And we'll do require request-promise, we'll bring in moment, and we'll also bring in the Logging SDK, even though we're not going to use it just yet. Awesome! And now with the MailGun domain and the secret, another thing we can do is we can go ahead and set up our messageURI for MailGun, and so this is going to be the URL that we're going to call to send our email. And so it's api. mailgun. net/v3/. And we're going to put our mailGunDomain into here, /messages. Awesome. Let me Command+Backtick just to make some more room. And now the next thing we want to do is create our authorization header. And I'm just going to go ahead and do this in the global scope of the function because it doesn't matter what instance of the function is running, or whenever it's invoked, we're always going to be sending an email, so I might as well just go ahead and set this now instead of recreating it every time my function is invoked. This is called secret, not MailGun secret, toString base64. Okay, so we're going to base64 our secret, and then create the basic header. So we'll do Basic plus our auth, and that should work for the service. Now, we're going to go and create our logSummary function, and again, this is an HTTP trigger so we're going to be using the request response parameters. And now what we're just going to do here, just to test the whole flow out and make sure we have everything set up, we're just going to send a quick email. And so we'll use moment to get yesterday's date. And we'll format that. And I'm going to go ahead and create an options object. And we're going to do a POST, and our URI is going to be our messageURI. We want to add our authorization header, so we'll do Authorization basic. And then we'll have our formData for our email. So this is going to be from no-reply@collaborateapp. com, so a fake domain, to ps. jameswilson@gmail. com. The subject of the email will be Serverless Report, and we'll throw in yesterday's date in the title, and then finally, the text Hello from Cloud Functions. Awesome! And now we'll call request, then pass in all of our options, and then we'll just log out the result. And we'll just log Successful email, and we'll send the response Successfully completed email and logging. All right, brilliant. So we set up our MailGun information, we're using moment to get yesterday's date, we created our options for that we're going to pass into requests, so we're making a POST, we have an authorization header, and all the metadata for our email, and then we're going and sending that email. So now I'm going to do Ctrl+Backtick and clear this out, and we're going to do gcloud beta functions deploy logSummary. We'll do stage-bucket zeta-sky-181023-staging-functions-trigger-http. All right, so that's going to deploy, and once that's sent back, we will call it quick, and hopefully we have an email. So I'll see you in a minute.

Sending a Test Email
All right, our new function is deployed, and so now let's trigger it and see what happens. So, I'm going to jump to Safari. I'll put this in our URL. And we got back the response successfully completed email and logging. So I'm going to go to gmail. com, and here you go, we have it right here, Service Report Monday February 25, 2018, Hello from Cloud Functions! We can see that our from address is the collaborateapp. com that we used to ps. jameswilson, and it has our subject. So, perfect! We have MailGun set up and working. But we're not done yet, though, right? So, we successfully completed the email portion, but not the logging. And before we dive in and start writing code using the logging SDK, I want to take a closer look at the Stackdriver logs in general. And what you'll find is that they're very flexible and powerful and we're going to start to leverage their special capabilities. So I'll see you in the next video.

Log Filters and Exporting to BigQuery
In this video, we're going to look at some of the more advanced features involving Stackdriver Logging. In the Getting Started course, we pointed out the logs and we showed some of these buttons here at the top, you know, letting you select specific functions, you can look at all the logs or break it down to a particular category, you can look at the log levels, whether they were an error, info, debug, whichever. And remember, console. log will deliver an info log and console. error will do the error level. And later in the module, we'll be uploading our own custom log, and there you can actually set one of these alternative levels, such as critical, warning, or debug. And you have a time filter as well. However, there is something that I would like to point out here, and that's when you expand one of these logs. And looking at this data, it just looks like a JSON object. You have key and value pairs down here, you have an object called labels with a property execution ID, and that has a value, you break down resource, and you get even more information, resource, labels, function_name is purchasePackage. And while up front this might not seem that great, it's really cool once you take the fact and say, well, let me go here to function execution started, the textPayload, and click on it, and I can select this button that says Show matching entries. And when I do that, I am taken to a more advanced filter scheme. And up here you can see that it has built a filter with specific conditions based on the data within each of those logs, and it'll deliver these log entries back to you. And so I could hit Enter, and let's look at Severity, and I hit equal, and suddenly I am given all of these different options. I'll go ahead, hit severity equal to ERROR and submit the filter. And if I get rid of text payload and then hit Submit Filter, boom, we have all of the error logs coming back to us with epic fail written in it. And so this is very powerful. And you can hit Command+Z to undo, come back, and set this back up. Now we can use this! Now your first inclination might say, well, can I take this, build a filter in code, and then use the logging SDK to go and fetch those logs? Well, you can. And you would use that using the logging SDK's get entries function and passing this filter into it, which would be a string concatenated version of this with ends in between each line. If I go and submit the filter, you can see that this still works. However, we're not going to do that here because while we could set up this demo, and we can go ahead, pull in all these function execution started, and we could aggregate them, and then we could find out how many functions have been triggered over a certain period of time because we could go ahead and say use the timestamp and say if it's greater than or equal to a specific date, click on here, do show entries, and you see it brought it back up here. Let's do greater than or equal. And suddenly it's sending me functions that are older. You see I removed the function name property, so we're even getting batchProcess in here now. So you could go ahead and do that. However, again, we're going to run into an issue at a particular point in time because if you're bringing back 1000 log entries, this will work, 2000 it will work, but once you start getting up to 20, 000 or 25, 000 entries within a day, you're going to start hitting quota limits, and you're going to be frustrated because suddenly you're going to have to work around it. You're going to have to say, well, okay, well, I can bring so many back in a second, so let me tailor this filter to only bring back so many, and I'll keep adjusting the time, and keep pulling them in. So instead of doing that, why don't we just take another step back and say, well, is there a better way? If we are limited by how many logs we can read at once using the logging SDK, is there a different approach we can take? And so there is. So I'm going to go ahead and get rid of this timestamp filter, and this is just going to be our basic filter here. We're saying give me a resource type of cloud function and we want this function execution started statement. And what this will do is this will go ahead and return all the functions that have actually been invoked and the time they've been invoked at. And then I'm going to come up here to Create Export. And you can go ahead and give this a particular name, we'll call it function_invocation_logs, and then you can select a service. And so we have different options here, we have BigQuery, we have Cloud Storage, Cloud Pub/Sub, or you can even create some custom destination. So Cloud Pub/Sub really isn't going to get us what we need, right? Because, again, we need to aggregate this data, and we've seen from a prior example already when we were writing to datastore, as we've had the pub/sub messages coming in, we had to go and store them somewhere else so we could batch them and aggregate them later. And so we saved them to Cloud Storage. So, Cloud Storage is a good option. But we've already done that, so let's switch things up a bit and let's just select BigQuery. And then you can go ahead and have a BigQuery dataset name and you could see here that I've actually already created one called function_invocation_logs, and you can just select Create new BigQuery dataset if you don't have one already, and it will give you the option to put a name in here. And then you would just hit the Create Sink button. Now, I'm not going to do it because I've already created one, and so if I come over here to Exports, you can see I have my Sink Name Function Execution, and it's going to the BigQuery in my project to the datasets/function/invocation_logs, and it even created a service account on my behalf for this export service. And so in the next video, let's jump into BigQuery and take a look at what this looks like.

Viewing and Aggregating Logs in BigQuery
Okay, so we're back and we're going to go into BigQuery. You could always come up to the hamburger menu here and select it here, but I actually already have a tab open. And you could see that for my project that it went and automatically created this function_invocation_log dataset. And now what this export feature is going to do is it went and created a table for me, and so here's the schema of the table, and you can go and look at all the different properties. So we have logName, severity, insertid, we have resource. labels, resource. labels. project_id, you have seen all of this already. And so this is awesome. This is really useful, but how does the data get in here then? Well, Google Cloud is working behind the scenes, and while it's not publishing the result in real time, it is batching them and uploading them. And what happens is that for each new day, a new table is created for you. And so you can see here that I have two listed here. And if I come up to the table details, you can see here that I have one set up from yesterday, and we have one that was generated today. I left that cron job up from our prior example and it's just been running. And I've gone and executed a couple other functions at the same time. And so we could come up here and actually go ahead and write a SQL statement. And so I got SELECT * FROM, and you can see we have the project ID, we have the dataset name, and then the particular name that I'm looking at. In this instance, this is yesterday's table. And when I go and run query, you're going to see this message. You can just say don't show it again, or show it every time, it's just going to charge you. But you have $300 and 3 credits, so trust me, you have plenty to work with. And go ahead and run the query. And now you could see here that we got 832 results and it's just showing the first 5 here. If we come through, you can see resource type, the project ID, our region, the name of the function, the name of the function name all coming through. And so this is really cool. What we can go ahead and do then is we can build a SQL statement that can go and query and aggregate these results for us. And then we can use that query from our BigQuery SDK in our Cloud Function to return us the results, and then we can go ahead and email those results to whichever recipient wants to receive it through MailGun. And you can go ahead and write the skeleton of that query right here. And so let's go ahead and say we want the count of the resource. labels. function_name, we'll call that invocation_total, we have resource. labels. function_name again, resource. labels. region. Okay, so those are the three fields we want. I'm just going to move this over to make some more room. Let's eliminate the limit, although we're not even going to come close to it. And then we'll just throw in a GROUP BY statement right here, too. And now you have the option of using what they call Legacy SQL, or just normal Vanilla SQL, and so we're just using the normal SQL here, and we'll set that in the SDK as well so it knows it. And so first we'll group by regions, and then we'll group by function name. Now, unfortunately, as of the recording of this coures, with Cloud Function still being in beta, we only have one region to work with. Oh, got a small typo right here. Replace that with a c, and let's rerun that query. And so what would happen is for each region, we would get the function name and how many times it's been invoked within that specific region. And so we're definitely not going to run into any limits here, minus what you're charged for running BigQuery. And so this is really cool, really nice, really powerful. And while your Stackdriver Logs are only retained for a certain period of time, I believe it's 7 days on the free tier and 30 days on the premium tier, they might change that 1 day, but with exporting it to BigQuery, you get to keep them for longer. And where you had these pretty strict limits on how many logs you can fetch using the logging SDK, we're not hitting these limits with BigQuery, so it's a win-win. And had we pulled the logs into our function using the filter, then we would have had to aggregate them in code, whereas we're using this SQL statement, the hard work is already done for us. SQL is taking care of it, BigQuery is taking care of it, and sending us the results back. So this is awesome. So let's go back to the code and update our function to use BigQuery.

Querying BigQuery from the Function
All right, so we're back in the code, and I never imported BigQuery here at the top, so I'll go ahead and do that. And another thing we never did was instantiating a new Logging object, and we also want a BigQuery one as well. Again, look, we don't need to pass in a project ID or set up any credentials, it's awesome! And so we'll make a couple quick changes here as well. We want this to be yesterdayTitle, and we'll make a new yesterday variable right here, which will just be our moment. subtract. We'll make this yesterday. format. And a small bug right there. Let's make this yesterdayTitle. Okay, good deal. All right, so the first thing we want to do is let's get the baseTable set up. And so if you remember the format, again, it's your project ID, 023. Now, one thing that is different here compared to the SQL statement we had written in the prior video was that one had a colon here, and here we're actually going to use a dot, and then we're going to put the name of our dataset, so function_invocation_logs. And then finally, the first part of the table name. And we're going to append the date to this in a moment, which is awesome, right? Because we know it's going to generate a new table for us every day to put those logs in, and we can just go ahead and append the new date to it whenever our function runs. So it's a lot more flexible. And so I'll come here and we'll have a yesterdaySQLFormat, and this one will be the year, the month, and the day. Sweet! So, let's go ahead now and actually build our SQL query. And so we had our SELECT statement. I'll do COUNT. And we're going to do resource. labels. function_name as invocation_title. Oops, total. I'll tell you, the worst part about working with these strings is typos are just so easy to make. So resource. labels. function_name. And then resource. labels. region. And our FROM statement. And so we're going to put this within quotes here as well, so we're going to escape our backtick. We're going to have our baseTable _ yesterdaySQLFormat. And then finally, our GROUP BY statement. So we'll do GROUP BY resource. labels. region, and resource. labels. function_name. All right, awesome. So, now that we have our query built, let's go ahead and create our options object that we're going to pass into our query, and so we have our query, and then we're going to say useLegacySql is false. So we can go ahead here and designate that we're using standard SQL. And then we'll call bigQuery. query options. And we'll get those results back. So there you go, so we got our query, we're sending the options to it, and we should get some rows back here. So in the next video, we'll go ahead and start retrieving the data from our rows.

Emailing Data from BigQuery
All right, so we're back. So let's go ahead and get our rows. Again, we've seen this time and again with all these different Google APIs, they're so consistent, I love it. You just say results subzero to get the rows, grab that first element within the returned array, and now let's build the text for our email. So we'll start off with Here is the Cloud Functions log report for yesterdayTitle. And then we'll see if any rows were returned. So, if it equals 0, we'll append the sentence onto this message, No functions were invoked yesterday, start building and deploying. All right. I'm sure they'll appreciate that message. And then, otherwise, we're going to go ahead and loop through our rows here. So we'll grab the total, and that was invocation_total, const functionName = row function_name. And so you'll notice that it cut off the resource. labels from function_name, even though that's not what we put in the SELECT statement, so it's just something to keep in mind. And you can assume the same thing with region as well. And now for each of these rows, we'll just append a new sentence here saying the functionName package purchased ran five times in region us-central1, and get a new line for the next one. All right, great! And now once we've looped through all of these, we can go ahead and log out the text here, just so we can check it out in case anything goes wrong. And we want to replace our function text with the appropriate text. So I just have two small typos to fix quick in our query, so I'm going to come up here. This should actually be baseTable. And then I have a trailing comma right there. So now that that's taken care of, I can do Command+Backtick, and we'll deploy our function.

Viewing the BigQuery Email
Okay, our function is deployed and I still have the tab open with our URL to the function, and I'm just going to go ahead and hit Refresh. So this is going to run, and it looks like it did. And now if I come over to Gmail, we now have a new email here, so serverless report Thursday February 8. And we see the information for our two functions. So yesterday batchProcess ran 63 times in region us-central1, and purchasePackage ran 769 times in us-central1. This was very easy to set up. BigQuery did a lot of the work for us. We didn't even need to do it in our function because we were able to use SQL in BigQuery to aggregate our results. And there is one final addition I'd like to make to this, and we might find this metadata useful within our log system as well. Perhaps we noticed that we're getting a couple strange errors on our functions. And it doesn't happen often, but it looks like it's happening on days of high volume. So we can add a log with a JSON payload data that we can then use the advanced filters on to find those days that there are high volumes, and we might be able to correlate the days we're seeing those crashes along with it. So we'll go and update our function in the next video.

Writing a Custom Log Entry
All right, we're back. So once we get the query here and we have our results returned, I'm going to go ahead and create a new object, just called data. And then as we're looping through our rows, we'll do it right after we add the text for our email. We'll go ahead and say if region in data, then we'll go into there and we'll create the functionName object for that particular region, and that will have a count property. And if region is not already on our object, we can go ahead and create that, and then do the same here, functionName = with a new object count and total. All right, for data purposes, that should get us what we need. Now the next step is we'll just come down here and create a new function. We'll just call it customLog. Do const log = logging. log. And here we're just going to create a brand-new category. We're going to call it analytics-log. And once this fires, you see it in the system. It'll make a bit more sense. And now we need to provide some information about our function. And now we haven't looked at this object yet, but it's very useful and it holds a lot of important information about the environment that you're running in. And that's just simply the process. env variable that node holds for you. And we use this with the client app to set up certain settings for that particular app, but we haven't used it or looked at it yet within the Cloud Function environment. So, we'll just go ahead and log it out here. And once our function runs, we can take a look at it in the logs. And so I'm going to create a resource object, and if you remember from the logs, we have cloud_function, labels, then function_name, and here we can go ahead and grab our function name from the process. env variable. That way if we ever change this, the name of this function, or perhaps we deploy a new derivative of it, the logs will match that new function name. We don't have to go and update it here. And we could be deploying it to different regions as well, right? And so we can just go ahead and use FUNCTION_REGION. That's already provided in our process. env variable. That's awesome, right? A lot of useful information there. And then finally, we'll just call log. entry, and this will take an object, and so the first thing we'll pass is that resource object, labels. Technically we would set this up to a 24-hour cron job, right? So it just runs every day, so why don't we just give it that category? And then severity, and here, again, we had console. log, we had console. error, and so just to show that we can put it in a different category, let's go ahead and put it under the debug category. And then the next parameter for log. entry, we're passing in the data. And because this is a JSON object, this will automatically make it a JSON payload. If you were just to provide text, it would be a text payload. All right, and now finally we'll do log. write entry, and we'll return that since this will be a promise, and we've done this throughout the whole course, we can go ahead and wait for both our MailGun promise and this log write promise to complete. Request equals customLog, and we'll pass in our data. And then we'll just say console. log Successful log. And we'll add some error handling here as well, so we'll say catch err console. error. Just go and pass that in. And we'll do the same up here as well. And now I don't want this to complete within the MailGun promise, so I'm going to remove that response. Sorry, just move these up here, it's bugging me. Okay, so the next thing we're going to do is do a Promise. all mailGunPromise, and loggingPromise. And then once that completes, we can go ahead and respond. And if we get an error, we'll just send the 500 response, An error occurred while trying to send the report. All right, so that looks good! Let's go ahead and redeploy our function, and we should get a customLog now. I'll see you back in the next video.

Filtering with the Custom Log Entry
Okay, so we're at our logs, and you can go ahead and see that our customLog write is here, one that's at the debug level, which we typically don't get if we're using console. log or console. error, and you could see here that we have a jsonPayload instead of a text payload, and we have the region, as well as the two functions. And so that's pretty cool! And I can go ahead, select this, say Show matching entries, and we could even put a greater than or equal to sign and say 500, right? Submit Filter. And if I were to go, let's rerun this a few times, it might take a second to run. And you see we have them starting to pop in now, so this is working properly. So congratulations! It's really, really cool! And you could see here, you can go and query another function, or you can go and look up a different region. And perhaps it might be better to structure this object a little bit differently, but you get the point of what you can accomplish by writing to these custom logs. And now let's clear the filters here and return back to the normal mode. And you can see up here with all logs that we now have this analytics-log, and we didn't have that before. All we had was activity, which is generated by Google Cloud, and this is when we're updating functions, creating functions, deleting functions, and then we have the cloud-functions, which is the normal kind of using the console. log, console. error, and we now have our new category as well. And so in the next video, let's just quickly now take a look at the process environment.

Cloud Function's Process Environment Variables
Right, so let's expand this log and look at all the information that was provided to us here. And you can see that there's a lot! We have variables such as gcloud project, giving us a project. We have the function identity, that service account identity. We have the function name, which we used. We have the function region, we have the HTTP trigger for our trigger type. So there's a lot of great information in here that you can use if you need to make your functions a bit more dynamic in nature, and you're writing code that you want to be able to be used cross function. Now there's really no documentation on these variables, so do tread carefully, they could be changed suddenly, especially since this product is in beta, but especially by the time it reaches prod, it's probably a good chance that it's unlikely they're going to change anything very drastically. Awesome job in this module! Let's review everything we've gone over.

Summary
We went over a lot of great material in this module. First, we set up MailGun to send emails. And once we were able to send a simple test email, we then dove into the Stackdriver Logging system. We looked at the log object itself and how that is structured. We saw how we can build filters to do custom queries on the logs themselves. And then we saw how we could take that filter and export that to BigQuery. And now you have a new table being set up in BigQuery every day with your custom logs broken out into columns. And we were able to leverage that functionality by building a SQL statement to aggregate those logs that we exported. We then went and updated our function, which was sending emails, to use that SQL statement called BigQuery and return the data back to our logs. And then we extended it even further by taking that same data within the email and creating our own log entry. And you were able to see how you can upload JSON data instead of the standard text payloads that you can do with the console. log and console. error within the standard node. js system. And then finally we also took a quick look at the Cloud Function's process environment variables, and we were able to see how we could get the function name and the region and connect those to labels within the logging system itself. And without providing that information, those logs would never have even shown up in the Stackdriver Logs because they would not have known how to connect to our Cloud Function instance. And so it's really powerful. And to be honest, we barely scratched the surface. The Stackdriver Logging system is a whole technology unto itself, apart from Cloud Functions, and I really encourage you to dig further into the documentation and learn about all the other features it has to offer. Now we're not completely done with Stackdriver yet because logs can help you, especially when you're trying to debug something, whether it's during testing, or when your function is in production. But what about when those real critical errors occur? You don't want to have to wake up every morning and check your logs to make sure everything's okay. You want to be notified when something catastrophic occurs. And so now we're going to look at Stackdriver monitoring and see how when a critical failure occurs, it can help notify us that something is wrong with our function, so you can take action. So I'll see you in the next module.

Reporting and Monitoring Errors
Introduction
We've almost gone through the whole lifecycle of a function. You've built it, you've deployed it, you've tested it, and now you're getting ready to put it into production. And when something goes wrong, you want to know what it was, and more importantly, you want to be notified as soon as possible. So in this module of Reporting and Monitoring Errors, we're going to explore the Stackdriver Error Reporting and Monitoring system and how we can leverage that with Cloud Functions. So we're going over a lot of great material in this module. First, we're going to review the monitored metrics that are provided to you by default. Google has gone ahead and set up several that you may want to use, and we'll quickly take a look at them and set up a small one just so you can see how the whole workflow works. Then we're going to dive into a subject that we haven't touched yet, which is how to properly report errors in a cloud function. There's a lot going on between console. error, returning promises, returning callbacks, and we're just going to straighten this out, make it very clear how you tell Cloud Functions that an error occurred and how you can ensure that that error is delivered to the Stackdriver Error Reporting system. Then once we do that, we're going to take a look at the Stackdriver Error Reporting dashboard, look at an error, see some of the material that we have and some of the information that's provided to us. Then we're going to go ahead and update a function from earlier in the course, the package transaction function, and we're going to put a time limit on how many times it can retry. If you remember, we kind of let it fail randomly, and we would retry it, and we never really put a cap on it. And now we're going to set it to fail all the time and make sure that it doesn't keep retrying after 2 minutes. And with that set up, we're going to use information that we log at that point in time to create a custom log metric. And with that custom log metric, we'll be able to set up our own custom alert in Stackdriver Monitoring, and we'll get an email notifying us that we have reached that 2-minute limit and that we need to investigate and see what's going on. So there is a lot of great information in this module, so let's dive in and start looking at Stackdriver Monitoring.

Monitoring Basics
So let's take a look at the basic monitoring metrics that we have available to us. And now it gets a little funky when you're going through the Google Cloud documentation because you might go to Cloud Functions and be looking for this material, but it's not listed there, not in depth, and you want to go to the Stackdriver Monitoring section of the documentation. And so if we come up, we're on the home page here, and go to Documentation, if we go to Infrastructure and Operations and down to Operations, you see we have the various Stackdriver products available to us, and I'm going to come down to Stackdriver Monitoring. And if you come down to the APIs & Reference section, there's this Metrics List. And here, it can go ahead and click into this, and here you'll be able to find a list of the various metrics that you're looking for. And so if I go to the GCP Metrics List, suddenly we have a whole list here. We have appengine, bigquery, bigtable, and then cloudfunctions. And if I click on that, it's going to jump us down. And you can see here the various metrics that are available to us, and so here we have execution_count, execution_times, network_egress, and user_memory_bytes. And now you can go ahead and within Stackdriver you can set these up and monitor specific functions based on certain thresholds here. And now to do that, you would just go to Monitoring, select your project, and then you could come to Alerting here and say Create a policy. I'm going to go ahead and add a condition, and we'll do a Metric Threshold, Select, RESOURCE TYPE will be a Google Cloud Function, and then IF METRIC is Executions, and the CONDITION is, let's say, above 1000 for 1 minute, for the RESOURCE type cloud_function, and then we can go and select a specific function if we choose to. And here it has a list breaking down each step, batchProcess, TRIGGER_TYPE, which is an HTTP_TRIGGER. And now if we get more than 1000 executions within a minute, we can get notified by this. Now why is this helpful in this particular function? Well if you remember, we set up a cron job to run this only every 5 minutes, and so if this is being invoked 1000 times within a minute, somebody else might be triggering this function prematurely. Now maybe it's you. Maybe you set up appengine and you have some process going, and you're going and triggering this; however, if you haven't done that, then this would be a good way to find out. And, in fact, you could argue that 1000 within a minute is way too high for this particular scenario. I mean, realistically, if you had two invocations within the minute, that would be a better threshold considering the scenario. And that's something you always need to think about when dealing with monitoring. There's no magic bullet. There's no magic number. You have to think about what you're monitoring, what your expectations are, and when you want to be alerted. Answering these simple questions will help you get to that point. So I can go ahead and save this condition, Add Notification, and then here you can go and select various notifications. You could get an SMS message, you could get it published to a Slack channel, or set up a separate webhook that would kick off some other process, or you could simply do email. And then you could do ps. jameswilson1@gmail. com, and come down, give the policy name Batch Processing too High, and go ahead and save the policy. And now we have our test policy, and you can see Batch Processing too High. If the execution_count is above a threshold of 1. 86 for greater than 1 minute. You can see here we actually have the wrong threshold. My finger must have adjusted the text box before I saved it, and we have 1. 86 instead of 2. So be very careful as you're editing these fields. And you can see who it was last modified by. You can see I put the wrong email address. It's ps. jameswilson, not ps. jameswilson1. So you can quickly go ahead edit it, update the condition, and save the policy, and there you go. It is super easy to create and edit these really quickly. And if you want to go ahead and delete it, you can do that too. So at the moment, I am actually not going to go through and trigger this policy because in a little bit we're going to be setting up a custom policy, and we'll go ahead and trigger that one so you can see the whole workflow of that. But I wanted to show you some of the default monitoring that is available to you, and there is very little work for you to do except to go in and set up those thresholds that you want and some sort of action you want taken if those thresholds are exceeded or thresholds are not met. Before we go ahead and create our own custom monitoring condition, let's quickly take a look at the Stackdriver Error Reporting system on Google Cloud.

Delivering Errors in Functions
Reporting errors is pretty basic, however, there are a couple weird caveats and so I would just like to point out quickly the correct way to do error reporting and an incorrect way. And now the ultimate goal here is to get this into the Stackdriver Error Reporting system. And so a few methods that we have here that we can do is console. error, and including a new Error within that log statement, you can throw a new Error or you can use the callback or Promise. reject while also providing a new error. But really the key here is that you're using that Error object. You'll see a little later in the module that we use console. error without providing an Error object, and while it logs with the severity of Error, it does not report it to the Stackdriver Error Reporting system. And so what are some of the incorrect ways that you could accidentally write one day? Well, using console. info or console. log and trying to create an Error in that, it won't trigger the Stackdriver Error Reporting, using console. error without an Error, throwing an arbitrary object that's not an Error like an integer or a string, using callback or Promise. reject without using an Error object, or returning a 500 response on an HTTP call. And so you see the common pattern here. You just can't call throw, or. error, or Promise. reject without including that Error object, so just always keep that in mind. And if you are wondering why you're not seeing some error in Stackdriver Error Reporting, I would look first at how you are trying to write that error into the system because that is probably going to be the number one cause of why it's not showing up. And with that, let's go and check out the Stackdriver Error Reporting dashboard in Google Cloud.

Stackdriver Error Reporting
So earlier in the course, we had updated our function, purchasePackage, to fail 20% of the time, and in the event of it failing, we went ahead and said return Promise. reject new Error Epic Fail. And now because we are using this appropriate syntax and returning the Error in this way and not using, let's say, console. error, we get the added benefit of the Stackdriver Error Reporting on the Google Cloud side. And so we had already ran the function, and if I jump to Stackdriver Error Reporting, and if you go to the hamburger menu under STACKDRIVER, you can see it here. You see our Error: Epic Fail!, and you can see how many times that it's occurred recently, 46. And here you have Resolution Status, so right now it's Open. You can acknowledge it and tie it to a ticket on some issue tracker that you might be using whether you're getting GitHub, or Jira, or any other tool, and you can mark it Resolved as well, which will just make it go away completely. And so this is a really handy tool to use. And if you look up here, you can turn on notifications to alert you when a new error occurs in this project. And so if you come over to Turn on notifications, it is now emailing you directly when this error occurs. Now I don't want to do that, so I'm going to come up here and say Turn off new error notifications for project because this really isn't fitting our needs, right? This error occurred because we are randomly failing the function, and we're randomly failing it because we're trying to show off the retry capability on the asynchronous functions on Cloud Storage and Pub/Sub triggers. What we're going to do is we're going to look at the events context object, specifically at the timestamp property. And if our function has been retrying for more than 2 minutes, we're going to not fail the function, we're actually going to resolve it and make it just stop retrying. But in addition to that, we're going to log a new custom message, which we can create a custom notification for using the Stackdriver Monitoring system. And so I just wanted to show you this error reporting just so you knew it was there and you can use it to manage all the different errors that could be popping up across your various functions. And so let's go ahead and resolve this, and I'll see you in the next video to start updating our function to handle this new functionality.

Cancelling the Cloud Function Retry
Okay, so we're back in our transaction function, purchasePackage, from earlier in the course. You might remember that we had intentionally failed it 20% of the time. And we're going to go ahead and update this, and we're just going to have it fail all the time at this point for this example. But after it reaches a certain time threshold, we're going to then report a special log, and we are going to have it stop from retrying because we know there is a serious issue going on that we need to investigate. And then after that, we will set this up with Stackdriver Monitoring so we can be notified that this horrible situation has occurred. So the first thing I'm going to do with updating it is we want to install moment. js, and we've used this before in the course, just not in this particular function. Okay, great. I'm going to go ahead and update our array here and make them all false, so that way we fail all the time, and then go ahead and grab moment. Now right under where we're logging out the eventId, we're just going to go ahead and grab the originalTime, and we have this on our context object, and that's using the timestamp property. Now no matter how many times the function is retried, this is always going to stay the same, and so we know we can rely on it as the functions are retrying because it's that original timestamp, not the timestamp of the current invocation, and that is a very important difference. And so I'm just going to grab the current time as well. And from now and the originalTime, we're going to generate a duration object using those, and now I can do duration. asMinutes, greater than or equal to 2. So here we're just going to say if this has been retrying for more than 2 minutes, do something else instead. And so here, we're going to do console. error. Now remember, this itself will not report an error, but it will give us a different log message in the logging system; however, this will not go to Stackdriver Error Reporting because we don't want that right now. And then we're going to do Promise. resolve Stopping retry. If we were to do a Promise. reject, that would keep this retrying and just giving us the different error message. But at this point, remember our use cases, something very, very bad happened, and we need to investigate before this continues. So go ahead, redeploy your function, and while we're waiting for that to deploy, I have another console open here, and it's in the client-app from earlier in the course, and go ahead and open the triggerPubSub. js file. And instead of publishing to PubSub so many times, we're just going to go ahead and invoke it once, and this is really going to trigger it twice because remember we have two projects and all that; however, because this has got to be failing every time, we might as well just limit it for our purposes. Okay, our function is deployed, so now I can come back to the client-app and do node app 1 and hit Enter. Great. So it's gone ahead, and now it has fired off two Pub/Sub messages, so I'm going to come up to Cloud Functions. In fact, if you hit forward-slash you can just go ahead up the search bar up top and type in Cloud Functions and hit Enter. Nice little shortcut. I'm going to go ahead select purchasePackage and VIEW LOGS, and you can see here that we're getting this Epic Fail! from before. Now I'll go ahead and I'll see you in 2 minutes because at that point our logs should be updated with our custom log message.

Creating a Custom Log Metric
Okay, so we're back. Our function has been executing for a couple minutes, and you can see that we got our error, Exceeded threshold time, a serious error occurred, and more importantly, you can see that the two invocations we had created from our two Pub/Sub messages that we delivered have ended. We have these Function execution took ms, finished with status: ok, so they are no longer trying to retry themselves after 2 minutes. Now this is great and all, but again, the important thing is we don't want to come in here every day and have to check the logs and make sure we never see this error message, right? We want to actively be notified so we can deal with it right away. So what you can do is come to this textPayload and say Show matching entries, and this is going to go ahead and build a filter, and we had dealt with this in the earlier module when we looked at Stackdriver logging and how we can leverage these filters. And so we are going to make a couple tweaks here. One, we don't want this to be tailored to a specific region, like us-central1. We want to be notified of this function failing like this regardless of the region, so I'm going to go ahead and delete that. And just for a little extra, we're going to go ahead and limit this to the ERROR severity level. So again, you remember we used the console. error and so our severity equals error. And if I go ahead and submit the filter, you can see that it's still working properly. Now I'm going to come up here to the CREATE METRIC button, and this is going to let us create a custom metric for this. And so here we have a Type Counter or Distribution, and so we want to use the Counter, right? And so Counter is just going to count the number of log entries matching that filter, and the Distribution metric is actually going to accumulate this numeric data and give you a bunch of useful information such as the count of the number of values in the distribution, the mean of the values, the square deviation. And if you want more information, you can go over this question mark and hit the Learn more link to read up more about the difference between these two. So we're going to give this a custom name, and we'll call it package-transaction-exceeded-time-threshold, and we'll put the description as Package Transaction took too long to complete. Labels are a more advanced feature, which we're not going to leverage here, but you can specify specific properties on the log message, like textPayload, and see whether it's equal to a particular string. Or you could use a regular expression to try to pull data out of it, and this goes for the JSON payload, the severity level, and Stackdriver Monitoring will pull out those individual values so you can use them. It's an advanced use case, and considering the various specificity of our log filter, we don't need to use them here. In addition, you can only have 10 labels per entry, so keep that in mind as well. You don't want to create too many of them. The Units field is optional, and we definitely don't need it here. But in the event that you are using an N64, double, or a distribution type, you can go and further give this extra information about what type of unit it is specifically. And this is based off the unified code for units of measure, and so you could have things like bit, byte, second, minute, hour, day. You can have more different thresholds such as micro, nano, pico, fempto. I think you get the example, and you can read the documentation for Stackdriver Monitoring to learn more about it. And again, we really don't need it in this use case. Okay, so we have our Name and Description and the Type Counter, so I'm going to go ahead and hit Create Metric. So I'm on the Logs-based metrics screen, and you can easily get to that on this left-side menu if you happen to be in Logs, right here, Logs-based metrics, and if you scroll past, we have the System Metrics, which is predefined metrics that are already set up by Stackdriver. And down here, we have our User-defined Metrics, and you could see that we have our package-transaction-exceeded-time-threshold, Package Transaction took too long to complete, and it has our filter based on the resource. type being cloud_function, purchasePackage is the function_name, and we're looking for a severity of ERROR, and the textPayload Exceeded threshold time, a serious error occurred. So we have our custom metric, and now the next step is we need to go ahead and set this up in Stackdriver Monitoring.

Configuring the Custom Metric
So let's go into Stackdriver Monitoring and set up our alert. So again, we did this earlier. We're going to go ahead and go to the Alerting screen, and we can go ahead and select Add Policy, and now we'll go ahead and create our new condition. Okay, so we have Metric Threshold, so let's go ahead and select that one again, and we're going to leave the RESOURCE TYPE as Log Metrics. You might remember earlier we had picked Google Cloud Functions, but, again, we're using a custom metric, so we want to leave that. And we have our custom metric right here, user/package-transaction-exceeded-time-threshold. Now for RESOURCE, we want cloud_function. But you see it's not showing up here, and this is kind of a weird thing to do with Stackdriver Monitoring. Right now it's not picking it up, and we had just created the metric, and unfortunately that metric needs to be hit and invoked for it to kind of fully process into the system. But right now, we won't see any data if we can't tie it to our resource, cloud_function.

Viewing the Alert
Alright, so we're back and our custom metric has fired. So if I jump up to Gmail here, you could see that we got two alerts, which, again, makes sense, right? We've been sending two Pub/Sub messages, and they're failing all the time. And if you click into the email, you could see it's from Google Stackdriver saying Metric Threshold on Log Metrics, and an incident has been started, and it's got more detailed information that our metric labels is above the threshold of 0. 01 with a value of 0. 017, the time that the violation started, as well as the Project, the Policy it's associated with, the Condition, and the Metric, and the Threshold and Observed. So, this is really, really cool. Now you might notice something that if I go back to the Alerting policies that it actually isn't OPEN or ACKNOWLEDGED, but it's marked RESOLVED. And why is that though? That kind of seems weird, right? I haven't done anything. Well, if you look more into the actual events that occurred, you can see here that, again, it's the same information as before. And if I come up back to Alerting and go to Events, you could see where the event actually started and then it auto resolved itself because it's saying return to normal with a value of 0. 0. And now you might not always want this to happen, but in this case, it's okay, right? Because we at least just wanted to be notified, and we can start digging in and finding out what may have occurred. And now that we know that something went wrong, we can go and start investigating. And what makes sense is to jump back to Logs and see this Exceeded threshold time, click on it, and show the matching entries and see where it came up, and again, eliminate that region. So maybe it's happening cross-region. So this is very useful. This can be customized to so many different scenarios. We just happened to pick one because we were already retrying a function and intentionally failing it, so it was a good use case, but this is incredibly flexible. All you have to do is customize your log filters. You can have custom labels on the metric, and you can set it up, again, to go to a Slack channel, email, SMS. It's so cool. It just kind of blows my mind the way this can be configured. So congratulations! Let's go ahead and wrap up this module.

Module Summary
We learned a lot of great stuff in this module. We first reviewed the basic monitoring metrics that Google provides you and how you can set up policies to monitor those metrics. We then took a look at how to properly write errors in our code so they can be picked up by the Stackdriver Error Reporting system, and then we went and looked at the Stackdriver Error Reporting dashboard and checked out one of our errors that had been reported throughout the course, and you were able to see how you can acknowledge it and tie it to a support ticket or just going and resolving it and setting up email notifications for it. And so we barely scratched the surface of how that can help you in your everyday life when managing Cloud Functions. We then went and updated our function, PackageTransaction, placing a time limit on how many times it can retry, and we kept it at 2 minutes. And with that, we used a console. error statement to state that we've exceeded that time threshold. And with that log statement, we were then able to build a custom log filter using Stackdriver logging and create a custom log metric with that filter. And then once we had that custom log metric, we were able to go into the Stackdriver Monitoring system and set up an alert for it and a notification, which emailed us whenever it occurred. Now this course is about Cloud Functions. We barely scratched the surface of what Stackdriver Monitoring can do. And I urge you to go and check out the documentation and read up more on it because it is embedded into the entire Google Cloud platform, and so not only can it help you with Cloud Functions, but it can help you with other technologies that you're using as well within the Stack. So congratulations! You have taken your functions all the way from the beginning of just writing the code to deploying it with actual metrics attached to it. You should be super excited. Now let's quickly take a look at everything we've learned in this course.

Course Summary
Congratulations! You've learned so much throughout this course. Now instead of going over the functions and everything like that one by one, let's just take a higher-level look at everything we've done. We've connected to multiple services such as Cloud Vision, Cloud Storage, even services outside of Google Cloud such as Mailgun. You learned about Cloud Functions' docker image, and we are able to use ImageMagick, which is on that image, and leverage that in our function to be able to create thumbnails. And as Cloud Functions evolves, it's probably good to check it out every once in a while, see what new things might be added or even removed from the system. You also learned about the temporary file system, and this is crucial if you're going to be doing some heavy-duty processing within the function. Now it might not always be feasible, remember there is a 5-minute time limit, and you are going to be using more memory. But remember, it is available to you, but it is also up to you to make sure you keep it clean or else you will be running out of memory. We ran into multiple memory and timeout issues throughout the course, but as we did, we took a step back and we looked at how we could, A, fix our function and fix our code, or B, set different parameters on the cloud function itself to keep the function working. And depending on your scenario, depending on your needs, it could go either way which direction you choose. We took a look at race conditions as we were writing to datastore too many times too quickly and it wasn't able to keep up, and so we were able to take step back, look at Cloud Functions execution model, and say is there another way? Is there another technology we can use? And we were able to use Cloud Storage to store that information, and then later we batched it up and then saved it to datastore, and we were able to have data consistency with that approach. You learned about idempotent functions, and this is crucial. The best functions are the ones that can be retried over, and over, and over again and produce the same result. This will add resiliency to your system. If things fail, they can be run again and the same output can be expected upon success. And so always think about that, especially if you're setting the retry flag on your functions. If you don't, imagine a function that retried 1000 times and gave a different result every time it ran. Maybe that is what you want, but I would really think about it before you do it. And then we took a look at some more advanced features of the Stackdriver Logging system. We saw how to export logs to bigquery, we looked at creating a custom log metric based on the log filters, and again, we barely scratched the surface of what Stackdriver Logging is capable of. And then we took that custom log metric and set up monitoring for critical errors in our system so we could be notified immediately and we can start looking at fixing them. It would be wonderful if we could all write code that worked 100% of the time all of the time, but that's just not the case. And so take what you've learned here, think about the code you're writing, and think about the different situations you could run into and how you want to be notified in those instances. So once again, congratulations! We looked at a lot of stuff, a lot of different products, and wrote a lot of code, and you should be super excited. My biggest recommendation to you right now is look at the rest of Google Cloud, see what other products we didn't cover in this course that you could use with Cloud Functions, or take a look at some of the other compute products like Kubernetes Engine and App Engine and ask yourself, do I really need Cloud Functions? Maybe half of my functions might be better off living in App Engine and the other half can stay as cloud functions because those are more appropriate. This isn't a one-size-fits-all technology. Use the right technologies at the right time and you can build successful systems. Thank you for watching this course.

Course author
Author: James Wilson	
James Wilson
As a mobile developer, James always had a passion for building exciting apps and always striving to make user interfaces that were easy and intuitive to use. Today, he now works at Pendo where he...

Course info
Level
Intermediate
Rating
0 stars with 6 raters
My rating
null stars

Duration
4h 52m
Released
28 Aug 2018
Share course

