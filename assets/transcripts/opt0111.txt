Creating and Administering Google Cloud Spanner Instances
by Vitthal Srinivasan

This course covers practical aspects of working with Cloud Spanner, Google’s proprietary relational database management system (RDBMS) service. It is the only globally distributed, strongly consistent RDBMS with horizontal scaling.

Relational Databases have traditionally relied on vertical scaling, but Google’s Cloud Spanner is carefully architected to provide horizontal scaling and global replication with all the rigors of strong consistency. Because Spanner is such a unique product, getting the best out of it does require you to understand its subtleties. In this course, Creating and Administering Google Cloud Spanner Instances, you will gain the ability to identify when Spanner is the right tool for you, and then correctly design your data and configure your instance to get the best out of Spanner’s formidable capabilities. First, you will learn where Cloud Spanner fits in the suite of Google Cloud Platform (GCP) storage technologies and how it compares to BigQuery, Cloud SQL, and others. Next, you will discover Spanner’s data model and how it enables horizontal scaling. Finally, you will explore how to use Spanner in conjunction with other GCP services, notably Dataflow templates, for migrating data into Spanner. When you are finished with this course, you will have the skills and knowledge of Cloud Spanner needed to architect solutions to problems that require global replication, strong consistency, and horizontal scaling in a relational database management system (RDBMS).

Course authors
Course info
Level
Beginner
Rating
0 stars with 3 raters
My rating
null stars

Duration
1h 59m
Released
16 Jan 2019
Share course

Course Overview
Course Overview
Hi, my name is Vitthal Srinivasan, and I'd like to welcome you to this course on Creating and Administering Google Cloud Spanner Instances. A little bit about myself first. I have Masters degrees in financial, math, and electrical engineering from Stanford University and have previously worked in companies, such as Google in Singapore and Credit Suisse in New York. I am now co-founder at Loonycorn, a studio for high-quality video content based in Bangalore, India. Relational databases have traditionally relied on vertical scaling, but Google's Cloud Spanner is carefully architected to provide horizontal scaling and global replication with all of the rigors of strong consistency. In this course, you will gain the ability to identify situations when Spanner is indeed the right tool for you to choose and then correctly design your data and configure your instance to get the best out of Spanner's formidable capabilities. First, you will learn exactly where Cloud Spanner fits in the suite of GCP storage technologies and how it compares to BigQuery, Cloud SQL, and others. Spanner, like Cloud SQL, is an RDBMS service on the GCP, which means that it guarantees the ACID properties of atomicity, consistency, isolation, and durability. However, Spanner differs from Cloud SQL in important ways. Cloud SQL supports MySQL and Postgres, but Spanner is Google-proprietary. Cloud SQL is regional, where Spanner can be configured to replicate globally. Next, you will discover Spanner's data model and how it enables horizontal scaling. Here the key is the concept of a split, which can be thought of as a group of rules from related tables that can be moved around independently by the platform. The idea of physically storing data from different tables together is called interleaving, and this is a key driver of the design and architecture of Cloud Spanner. Finally, you will explore how to use Spanner in conjunction with other GCP services, notably Dataflow templates for migrating data into Spanner. Spanner was developed for use within Google several years before it was made publically available. Consequently, the migration path into Spanner was not quite clear initially. However, with the addition of Google- supported Dataflow templates, that concern has been substantially addressed. When you're finished with this course, you will have the skills and knowledge of Cloud Spanner needed to architect solutions to problems that require global replication, strong consistency, and horizontal scaling in an RDBMS.

Getting Started with Cloud Spanner
Module Overview
Hello, and welcome to our course, Creating and Administering Google Cloud Spanner Instances. We begin this first module, which is a gentle introduction to Cloud Spanner. Cloud Spanner is a proprietary Google RDBMS. It is a globally distributed product in contrast to Cloud SQL, which is the other RDBMS offering on the Google Cloud Platform. Cloud SQL is regional; Cloud Spanner is global. Spanner is also an important offering because it is horizontally scaled. Again, this makes it different from many other traditional RDBMS products, including Cloud SQL, which focus on vertical scaling. The defining characteristic of an RDBMS is typically support for transactions and support for the ACID properties. In this context, Spanner offers very strong consistency guarantees, which can be part of as ACID++. Cloud Spanner is pretty cutting-edge technology. It is also relatively expensive. It was only made available for use on the Google Cloud Platform in early 2017, so it's very new and really very cutting-edge. Cloud Spanner is a great RDBMS offering for some specific use cases, and we will discuss its advantages and some of its limitations in the course coming up ahead.

Prerequisites and Course Outline
Let's start with a look at the prerequisites and an outline of the course. Here are some of the skills and software, which you'll find important if you are to derive maximum benefit from this course. You probably need to understand cloud computing at a very basic level. This is not to say that you need to be an expert on the Google Cloud Platform or anything like that, but you need to have a rough idea of how cloud computing works and what, for instance, a cloud VM instance is or a cloud storage bucket is. That level of knowledge is pretty important. It also will be important for you to know and understand concepts from relational database management. If you have no idea about what a transaction is, or if you've never heard the term ACID properties, this course might be a little steep for you. It will help for you to be familiar with SQL queries, at least very basic ones. If you've worked with an open source RDBMS technology, like MySQL or Postgres, that's perfect in particular because Cloud SQL, which is the plain vanilla RDBMS offering on the Google Cloud Platform supports these two flavors. In case you're a bit light on this material, here are a couple of prereqs that might help. The first of these is Creating and Administering Google Cloud SQL Instances. The second is Architecting Google Cloud Storage Configurations. Both of these courses are available on Pluralsight. They've been authored by my colleague, Janini Ravi. So you might want to check them out, particularly if you're a little light on the prerequisite knowledge, which we just discussed a moment ago. Let's turn now to a look at the outline of this course. We begin with a look at why you might choose Cloud Spanner, its features, limitations, and pricing, and a comparison to other GCP technologies. Then we'll move on to creating Cloud Spanner instances and the mechanics of working with interleaved tables and accessing Spanner programmatically. And we'll round out the course with a look at how Spanner integrates with other services, notably Dataflow, Cloud Functions, and Storage Buckets. This is a particularly important part of the course. Because of how new and how cutting-edge Spanner is, it can sometimes be a challenge to get Spanner to work with other GCP services, and it can sometimes be a challenge to migrate data into Spanner. We'll try our best to allay and assuage some of these concerns towards the end of the course. Like many other GCP courses here on Pluralsight, we will base all of our demos around scenarios, which are set at our hypothetical ecommerce retailer, SpikeySales.com. Our hypothetical online retailer is so named because it specializes in flash sales of hot and trending products. This makes it an excellent poster child for use of cloud computing because the level of incoming user traffic varies widely. The number of incoming user requests is very high during their sales, and it's very low at many other times. This makes an elastic cloud-based solution a natural fit for SpikeySales. SpikeySales has committed to using the Google Cloud Platform because of Google's emphasis on serverless technologies, such as BigQuery, Google App Engine, and so on. But in the context of this course, Cloud Spanner has an important role to play. As SpikeySales goes global, they now have a requirement for transactional consistency at scale. In the early days of SpikeySales, most of their users were concentrated in a few geographies close to the company's origins. But now the company has gone global. They have users from around the world hitting their website and performing transactions. It's become difficult for SpikeySales to continue relying on regional RDBMS technologies like Cloud SQL, and therefore Cloud Spanner is the solution that they are looking to. This is the perspective which we will be adopting throughout the demos in this course.

Introducing Cloud Spanner
In this video, we will introduce Cloud Spanner and understand precisely the differentiating factors between Cloud Spanner and other RDBMS technologies. Google Cloud Spanner is a global, horizontally scaling, strongly consistent relational database service built on proprietary technology. This can be thought of as a definition for Cloud Spanner. There's a lot going on here. Virtually every part of this definition can do with some explanation. So let's parse this definition. The first and most important bit, well, Spanner is a relational database service. This makes it one of two managed relational database services available on the Google Cloud Platform. The other, of course, is Cloud SQL. Every major cloud provider has competing offerings. The equivalents on AWS and Microsoft Azure are RDS, or the relational database service on AWS and SQL database on Azure. But Spanner is very different from all of these other technologies in some important ways. The most important of these is probably horizontal scaling. This is Spanner's key attraction. Virtually all other traditional RDBMS services offer vertical scaling. We'll have more to see on the difference between these two models. But in a nutshell, horizontal scaling involves adding more nodes to a cluster. This is the preferred modern way of adding capacity. As a result of this, Spanner can support datasets of virtually any size. Cloud SQL has an upper storage limit of around 10TB. That's the current limit for second generation instances, and it's only 500GB for first generation instances. So clearly Cloud Spanner can support vastly more data than Cloud SQL. Another important advantage of Cloud Spanner is the global nature of Cloud Spanner in contrast to Cloud SQL, which is regional. So Spanner is the only globally distributed RDBMS offering on the GCP. Cloud Spanner also offers very strong consistency guarantees. It is able to do so despite being globally replicated. This is a pretty hard technology challenge to solve. It's not easy for a service to both offer strong consistency guarantees and global replication and horizontal scaling. This is why Spanner is considered cutting- edge technology, and this technology is Google-proprietary. Cloud SQL supports Postgres and MySQL, both of which are open source database technologies. Cloud Spanner is proprietary. As with most other proprietary technologies, this can sometimes be a cause for concern. Migration parts can be a little confusing, vendor lock-in is a risk, and these are important factors to keep in mind while considering Cloud Spanner as a potential candidate for your RDBMS needs. The Google Cloud Platform is justly renowned for its vast array of big data and ML technologies, so let's take a moment to see where Spanner fits in. Cloud SQL, which we've already mentioned, is a regional RDBMS with limited scaling, up to only 10TB. BigQuery, which is maybe the most popular GCP technology, is a data warehousing solution. At heart, it's meant for OLAP and not for OLTP use cases. BigTable is a powerful NoSQL technology, which shares some similarities with HBase. BigTable is a perfect fit when you need fast reads and writes. BigTable works best when data has a natural sort order. Datastore, in contrast, is a NoSQL document database, which can be thought of as similar to MongoDB or even Redis in the non-GCP world. All of these technologies are for structure data. Cloud Storage buckets, on the other hand, can be thought of as blob storage. They work best with unstructured data, documents, emails, music files, video files, images, and so on. This is a fairly confusing alphabet soup of technologies, so let's try and organize it. GCP storage options can be divided into those, which work with unstructured data and those meant for structured data. Choices for working with unstructured data include Cloud Storage Buckets, which are elastic and offer infinite scaling, and persistent disks, which are meant for use from within compute engine instances and are based on pay what you allocate rather than pay what you use. The structured data technologies can be split in many ways. Let's divide them up by the method of access. There are those, which are accessed using a SQL interface, and then there are the NoSQL technologies. If SQL is what you're looking for, then you have a choice of OLTP, that is transaction processing, and OLAP, that is analytical processing technologies. Cloud SQL and Cloud Spanner are the OLTP technologies, and BigQuery is the ubiquitous OLAP SQL-based technology on the GCP. And finally, within the NoSQL technologies, at one end we have BigTable, which is a columnar HBase style technology and Datastore, which is meant for document databases. Let's focus our attention on the OLTP offerings and spend a moment clearly understanding the differences between Cloud SQL and Cloud Spanner. Now both Cloud SQL and Cloud Spanner are relational technologies. That means that they both offer ACID support. In this context, Cloud Spanner has some levels of ACID++ support. We'll have more to say on this in a moment. Cloud SQL is regional. Cloud Spanner instances could be regional or global. Cloud SQL supports vertical scaling, which means that as your dataset grows, you've got to make your Cloud SQL instance more powerful. You add more memory and more disk. Cloud Spanner, on the other hand, scales horizontally. So as your data needs grow, you add nodes to your Cloud Spanner instance. Cloud SQL works best for relatively small datasets of less than 10TB, and it also supports only lower IO operations per second. Cloud Spanner, on the other hand, because of its horizontal scaling can be scaled up to any size and any level of IOPS. Cloud Spanner can support any data size and any level of user traffic, of course provided you have the budget to pay for it. Cloud SQL supports MySQL and Postgres, both of which are open source database technologies. Cloud Spanner is distinctly Google-proprietary. And finally, Cloud SQL is relatively cheap. Cloud Spanner is relatively expensive.

Transactional and Analytical Processing
As a cloud architect, you will be called upon to make choices between Cloud Spanner and BigQuery. so let's also understand when we would choose each of these formidable technologies. At heart, Cloud Spanner is an OLTP, or online transaction processing system, and BigQuery is an OLAP, or online analytical processing system. So in a nutshell, use BigQuery for OLAP, and use Cloud Spanner for OLTP. Well, the next obvious question is, how does one clearly distinguish between these two types of use cases? This is not always a simple question to answer, so let's take a moment to understand the differences between transactional and analytical processing. Let's view this question from the perspective of two employees at SpikeySales. John works in order management support, and he is responsible for tracking and delivering orders and ensuring their timely execution. Anna, on the other hand, is in the business intelligence team. She is a revenue analyst and closely monitors the state of the business. The one number which she tracks every day is year-on-year revenue growth. Let's consider their typical use cases. John, who is in order management support, will often have to deal with delivery issues. Maybe 20 deliveries were scheduled for Kent, Washington. They were delayed because the courier company fulfilling the order had a computer outage. John will have to act. He will have to intervene and assign that order to a different company. This requires John to be paying attention to individual transactions. He has to be aware of the fate of individual orders. Let's take another use case. Maybe three customers would like to ship orders to a different address. Address change requests are common enough. John has a procedure for handling them. He needs to update the addresses on those shipments and reroute them. Once again, this requires John to be aware of the characteristics of an individual transaction. Let's contrast this now with some of Anna's typical use cases. Very often, her manager requests updates on revenues for the previous month or the previous quarter. Or maybe the board or the CEO have concerns around last month's revenue. Anna will then pull up data for a long period of time. She will track long-term revenue performance. She will perhaps compare last month to the corresponding month in previous years. Anna is not focused on individual transactions, rather she needs to have a bird's eye view of long-term trends. Hopefully, these examples have helped to drive home the difference between transactional processing and analytical processing. Let's make sure we clearly understand those differences. Transactional processing is all about ensuring the correctness of individual entries. Analytical processing is all about analyzing large batches of data. In classic transaction processing, access to recent data, perhaps from the last few hours or the last few days, is most important. Analytical processing often requires access to relatively old data, going back months or even years. Transaction processing applications update data frequently, so there are a lot of writes and a lot of updates. Analytical processing centers around one of writes followed by long-running reads. Class transaction processing requires fast real-time access. Analytical processing relies on more long- running jobs. OLTP applications closely enforce schemas, and that's why they usually accept data from only controlled single data sources. OLAP applications, on the other hand, are often data warehousing based. They have data coming in from many different sources. That data then needs to be aggregated. Now I would like to emphasize quite strongly here that these differences apply to classic definitions of transaction and analytical processing. So this list of differences is only partially applicable to Spanner and BigQuery. Spanner goes far behind traditional relational database technologies, and BigQuery goes far beyond traditional data warehousing technologies. In fact, these two technologies even share some important differences. For one, both of them have a SQL-based access metaphor. This is an important point in organizations because of how ubiquitous SQL has become. Both of them scale to petabytes. Neither of them really has an upper limit on how much data they can support. Both of them offer horizontal scaling with replication, and this is because Cloud Spanner is pretty different from other RDBMS technologies. But that gets us to maybe the most important difference, BigQuery supports eventual consistency; Cloud Spanner supports strong consistency. This means that BigQuery is not quite ACID-compliant. Cloud Spanner does offer strong ACID support. BigQuery is serverless. It requires no administration. In fact, you cannot even define indices in BigQuery. Cloud Spanner, on the other hand, requires cluster creation and a fair amount of administration. BigQuery intentionally abstracts the user from schema design, so there are no indices to be created, no provisioning to be factored in. Cloud Spanner, on the other hand, does place quite a great responsibility on the user. It's important to have smart choices around primary and secondary keys. This can really impact the performance of your Cloud Spanner installation.

Horizontal Scaling and Strong Consistency
In this video, we'll talk a little bit about the unique selling proposition of Cloud Spanner. We will see how Spanner is virtually the only horizontally scaled database to offer strong consistency guarantees. The terms horizontal scaling and strong consistency will appear time and time again while we're discussing Cloud Spanner. So I would like to make sure that we completely understand exactly what these terms mean and what they do not mean. Let's start with two types of scaling. In vertical scaling, as the dataset grows in size, we'll respond by making our server more powerful. In horizontal scaling, as our data processing needs grow, we just add more servers. It's important to know that those servers are all going to run on generic hardware. No one of those servers individually is particularly powerful or differentiated. Vertical scaling is implemented using monolithic architectures. Horizontal scaling is implemented using distributed architectures. This monolithic architecture in vertically scaled applications eliminates the need for orchestration. Horizontally scaled architectures require those generic pieces of hardware, those generic servers, to remain in sync with each other, and that's why they do require orchestration. The data stored in horizontally scaled application is too large to fit entirely on any one server. So that gives rise to the need to pack up the data, to shard the data, and store it in a distributed manner. So horizontally scaled applications virtually always shard data. Vertically scaled applications will not shard data. Vertically scaled applications have a single server, and so there is obviously no need for replication. In horizontally scaled applications, because of the large number of servers and also because of the inherent unreliability of any individual node in the instance, replication of data is extremely important. Replication also involves a replication delay. This is avoided by vertically scaled applications. They have no replication delay because they have no replication. Horizontally scaled applications will always have to incur the hit of a replication delay. The need to replicate data and keep those different replicas in sync also makes consistency hard. Vertically scaled applications do not use replication, and so it's relatively easy to ensure consistency. There is one master copy, and that's all that there is to it. Horizontally scaled applications, which have different replicas have to work hard to ensure consistency. As a result, vertically scaled applications usually offer strong consistency guarantees. Horizontally scaled applications usually offer weak eventual consistency. The C in ACID stands for consistency, and so it should come as no surprise that vertically scaled apps offer better and stronger ACID support. This is why they are usually used in OLTP, while horizontal applications are usually used in OLAP. Now once again, I would like to emphasize that Cloud Spanner is different. Even though it's horizontally scaled, it suffers from virtually none of these drawbacks, which we have just mentioned. Let's double-click on two important terms here, strong consistency and eventual consistency. In the case of a strongly consistent system, an update request comes in, the data on that system is updated, and then the request returns. So there's no possibility that any client ever reads a stale copy of the data. In contrast, in a system with eventual consistency, an update request comes in, the master gets updated, but other replicas might not, and then the request returns. Now, at this point, there are other replicas which need to be updated. Because of this period or this lag during which there are replicas which still have the old data, there is always the possibility that a read operation on that replica might end up fetching stale data. So in a system which offers strong consistency, there is no possibility of a read fetching stale data. With eventual consistency, this possibility does exist. Now, of course, there are no free lunches, so strong consistency brings down system performance. Write latency depends on the replication latency. Eventually consistent systems tend to be a lot faster because the write latency there is independent of the replication latency. And now we are in a position to understand the USP, or the unique selling proposition of Cloud Spanner. Currently, Cloud Spanner is virtually the only horizontally scaling RDBMS to offer strong consistency. This is a pretty big deal. This is only made possible because of the concept of TrueTime. Cloud Spanner works on the basis of a highly available distributed clock called TrueTime, which is available to all applications on the GCP. And using TrueTime as its secret source, Spanner is able to offer something even stronger than strong consistency. It's able to offer something called external consistency, which means that the system behaves as if all transactions were executed sequentially on one server and executed in order of TrueTime.

Instances and Nodes
Cloud Spanner is a fairly involved technology, and that's why we have a fair amount of theory and conceptual ground to cover before we can get to the demos. So let's keep going with some of that. Let's clearly understand what an instance of Cloud Spanner is and what it contains. For a vertically scaled technology, like Google Cloud SQL, there is really no difference between an instance and a node or a server. However, in the case of horizontally scaled technologies, an instance is not a node. In fact, an instance can consist of many different nodes. So a Cloud Spanner instance can be part of as a server. An instance has two important properties, the instance configuration and the number of nodes within in. The instance configuration itself might be either regional or multi-region. This configuration governs the location of the nodes. The number of the nodes depends on the dataset size. The rule of thumb enforced by Cloud Spanner is just about 2TB of data per node. Technically, the storage unit used here is tebibyte, but tebibyte and terabyte are just about equivalent. In production systems, it's recommended that you have at least three nodes in your instance, and there's no reason why your instance cannot scale up to even dozens of nodes if you have the budget to support it and if your use case calls for it. In contrast to this, by the way, the largest possible Cloud SQL dataset is around 10TB. That would correspond to a five-node instance on Cloud Spanner. Not particularly large. Let's understand the instance configuration in a little more detail. This is immutable. So after you create your instance, you cannot change your instance from a regional to a multi-region or vice versa. Regional instances will have all of their data residing in the same region. Three read-write replicas will be created, and these will be in different zones. In contrast, a multi-region instance will distribute data across different regions for higher availability and global access. A complex replication scheme exists for both types of instances, and we will discuss Spanner replication in more detail. Coming now to consistency guarantees, we have already discussed how external consistency is supported by Cloud Spanner even in multi-regional configurations. This is made possible through the use of TrueTime technology, which is a highly available distributed clock. We discussed this in passing already. TrueTime makes external consistency possible even with a multi-regional configuration, which means that we could have an instance spread across different global regions and still treat that instance that just one machine or one server. Remember that Spanner is an OLTP technology, so transactions submitted to Spanner are guaranteed to be serializable. The order of the transactions within the database is the same as that which is seen by the users. This is a formidable technical achievement. When might you go with a multi-region configuration? Well, one determining factor might be the availability that you want. Regional configurations have an availability of 4 nines. Multi-regional configurations have higher availability because they will not go down even if any one individual region is taken down. Now, of course, the penalty for this higher availability is a write latency. Regional configurations have lower write latency. Multi-region configurations will have lower read latencies. So again, if your application is global, if you have users all around the world who need to read your data, a multi-region configuration probably makes more sense. Another important decision variable here is cost. Regional configurations are less expensive. We will discuss Cloud Spanner pricing in more detail. Instance configuration was the first important property of an instance. The second was the node count. Recall that each node provides up to 2TiB of data. But it's important to remember that nodes do not only provide storage; they also fill requests. So when you add more nodes, not only are you getting more storage; you are also reducing latency in your instance. This is an important point. Nodes are not pure storage. They are combination of storage and compute. In production instances, it's recommended that you have a minimum of three nodes. Remember that it is always possible to add nodes after creation. This needs to be done manually so there is no autoscaling for nodes in a Cloud Spanner instance. But this is something that you can do manually yourself. It's also possible to set up Stackdriver monitoring alerts for Cloud Spanner instances. One more important point to keep in mind, nodes are not the same as replicas. We are about to discuss replication in Cloud Spanner, but for now just know that adding nodes is going to make each replica more powerful. Remember nodes are not just storage. They are both compute and storage. So when you add nodes, each replica, in effect, is getting additional CPU and memory, and this is why as you add nodes, the latency of your instance goes down, and the performance goes up.

Schema and Data Model
We've been going on and on about how Cloud Spanner differs from traditional relational databases. These differences also extend to the schema and the data model. So let's spend some time understanding Spanner's schema and data model and, in particular, how interleaved tables and parent-child relationships operate. Let's start with the traditional RDBMS layout. Data is arranged in tables, also called relations. These tables have rows and columns. The columns are called the schema. Here, for instance, we have two relations, a customer table and an order table. Every customer is uniquely identified by a customer ID. Every order is uniquely identified by an order ID. The customer ID is also included in the orders table, and this is what makes it possible for us to join these two tables. This is also called a foreign key constraint. Let's now quickly understand how Spanner would go about storing the same data. Relational tables are still maintained, and primary keys still identify rules uniquely. So this bit is the same. This does not change in Spanner versus traditional RDBMSs. But where things get different in Spanner is in the concept of splits. Spanner has the idea of a split, which is a set of rows from different tables, which can be moved around independently. The crucial bit here is that disk split involves data from different tables, and a split can move around independently of all other splits. Splits can be thought of as the individual units of replication in Cloud Spanner. Each split needs to be clearly identifiable in some way. And effectively, requests from users are going to be directed to the correct split based on a primary key. And in this way, we can see that data from different tables from different relations is going to be grouped together or interleaved to form a split. This means that careful choice of primary keys is extremely important in Cloud Spanner. In this regard, Cloud Spanner is somewhat similar to BigTable or HBase. If you are not careful in your key design, your instance will suffer from hotspotting. We need to be careful and design our keys so that requests will evenly get distributed across different nodes. Splits are implemented using parent-child table relationships. Given different tables which have customer data and order data, Spanner will require us to specify a parent-child table relationship. And unlike traditional databases, these relationships can be mastered to several levels. Here we need to define orders as a child table and customers as the parent table. Once we do that, the orders table is going to be split based on the customer ID, that's the foreign key, which, of course, is the primary key of the customers table. And what's more, Spanner will actually physically interleave the rows from the child table with rows from the parent table. And in this way, parent-child splits will be created and stored together. This relationship between the semantics between the meaning of the data in our tables and the physical storage is extremely important. This is key to the high performance that Spanner gives us. If we zoom in on the data within any one split, we can see that any row from the parent table can be referenced with the primary key. Any row from the child table can be referenced using the primary key of the parent table, which is Customer ID, along with the primary key of the child table, which is the Order ID. And from this, we can see an important property of keys in Spanner. The primary key of the parent table needs to be a prefix of the primary key of the child table. If you're familiar with either HBase or BigTable, this idea should again strike you as familiar. There are many structural and conceptual similarities between Cloud Spanner and BigTable. So again, in Spanner interleaved parent-child tables are stored together. The semantics of the data and the physical layout are linked to each other. This gives us a fast retrieval for data that is commonly accessed together. Now at this point, you might be thinking that foreign key relationships and parent- child relationships are hardly unique to Spanner. These are an old and well-established feature of relational databases. And you're right. But the point is that Spanner takes this whole concept one level further. For instance, one can define interleaved tables up to seven levels deep. And in this way, Spanner will actually identify hotspots and split based on the key of the parent table. It will also automatically create splits and go ahead and move data around to balance load. Splits are also the atomic unit of replication, which means that a split is replicated independent of any other split.

Replication
We just mentioned that splits are the atomic unit of replication. Let's now turn our attention to how replication in Cloud Spanner works. It turns out that Cloud Spanner is built on top of a proprietary replicated filesystem, Colossus, and Spanner writes database mutations to file. The filesystem then takes care of the replication, as well as the recovery. But that's not all. Spanner further adds another level of replication. This is based on the logical grouping of similar data into splits. So each split is individually replicated by Spanner. This is not something that is down at the Colossus that is at the filesystem level. Split-level replication happens within Spanner. Splits are only an implementation detail, but they are an important one because they actually govern on the performance of our instance. It's important for splits to correctly reflect logical groupings because that is how the data is going to be split. How can Cloud Spanner infer the semantics of the meaning of our data? The answer is via parent-child relationships, which we enter. This is information or metadata which we provide Cloud Spanner. These parent-child relationships between tables are used to interleave rows from different tables and group them together into splits. These interleaved tables form the basis for the physical colocation of rows, exactly as in BigTable. Users can control splits through their choices of primary keys and parent-child relationships. Now remember that Spanner is a distributed, a horizontally scaling system, and that means that it's going to have both replication, as well as sharding. And if you're going to have different replicas and different shards of the data, we need to have some mechanism to keep those replicas in sync. And this is achieved using Paxos, which is a standard consensus algorithm. This leads back to our discussion of strong consistency. Remember that strong consistency implies that no client can ever read stale data. Spanner offers strong consistency. In fact, it offers an even stronger version of consistency called external consistency. And this is possible because Paxos implements a mechanism for the different replicas to be updated right away and kept in sync with minimal replication delays. This is why Spanner is able to deliver both consistency and performance. Let's see how that happens. It turns out that each split has a designated leader. That leader is responsible for handling all of the writes to that split. It's the master in a sense. In addition to the leader, there are different replicas of the same split. It turns out that there are three different types of replicas. Read-write replicas, each of which can become the leader and can also participate in the leadership election. Then there are read-only replicas, which can neither become leaders nor vote. And finally, there are witness replicas, which cannot become leaders, but which can participate in the leadership election. It's worth our while understanding each of these types of replicas in some detail. Read-write replicas will maintain a full copy of the data of the split. Each read-write replica can serve both reads and writes. It can also vote on whether to commit a specific write. This is a Paxos algorithm. The leadership election, which determines what updates filter through to the split. In a single region Cloud Spanner instance, the only type of replica is a read-write replica. Let's move on and discuss read-only replicas. These are only used in multi-region instances. In order to field read requests and reduce read latency, they maintain a full copy of the data of the split. They are replicated from the read-write replicas, but they can neither vote on commits nor participate in elections to become the leader. Read-only replicas only serve to reduce the latency on read requests. Now it turns out that there are two different types of reads in Cloud Spanner, strong reads and stale reads. We'll discuss this in more detail in a bit. But for now, just be aware that read-only replicas can solve both types of reads. Stale reads can be served by a read-only replica without checking with the leader. On the other hand, strong reads will incur a delay from a roundtrip to the leader. Witness replicas are the third type of replicas. These are also only used in multi-region instances. Unlike the other two types, they do not maintain a full copy of the data. The difference is that they can vote to commit on writes, and they can vote in elections to determine the leader. However, they cannot become leaders themselves. So how many replicas of a given split are actually going to exist in an instance? To answer this question, let's go back to the two types of instance configurations. In a multi-regional instance, there are going to be two read-write regions, one witness region, and additionally, there might be some read- only regions for low latency reads. Each read-write region is going to have two read-write replicas. Each witness region is going to have one witness replica. And in addition, there may be some read-only replicas. In regional configurations, on the other hand, there will be three read-write replicas distributed across different availability zones. And so, we can finally count the number of replicas. There is going to be a minimum of five replicas in a multi-regional configuration and a minimum of three replicas in a regional configuration. This helps explain two facts. It helps explain why both regional and multi-regional configurations have such strong availability. Regional configurations are 4 nines, and multi-regional configurations are 5 nines. And we can also see from this why Cloud Spanner is so expensive. There is a high cost of storage, and there is also a whole bunch of compute going on in the implementation of Paxos.

Best Practices
In this section, we are going to cover some of the best practices associated with using Cloud Spanner. Let's start from the basics. A primary key, as usual, uniquely identifies each row in a table. In addition, a primary key has another role to play in Spanner. Rows which share common prefixes within their primary keys are going to be stored together. So the primary key also determines the order of physical storage. In this respect, Spanner is similar to HBase and BigTable. A secondary index is an index on a non-key column. This is going to require additional space because this index is going to exist outside of the primary storage of the split. In this context, Spanner support something known as a STORING clause. This is an instruction to Spanner to store an additional copy of a column effectively indexing that column for fast lookup. Choose your primary key carefully in order to avoid hotspots. In particular, there are some choices of primary key which are particularly bad, monotonically increasing or decreasing values, auto-increment keys, for instance. These are a common choice on vertically scaling systems. These are a particularly bad choice for Cloud Spanner. This, of course, poses some challenges for migrating data from traditional RDBMSs into Spanner, and so you might want to consider various workarounds. You might want to reverse the order of the key values. You might want to hash your unique key values so that they are no longer naturally increasing or decreasing. You might want to consider using universally unique identifiers, or bit-reversing sequential values in order to eliminate the monotonic nature of the keys. However, none of these is a particularly simple solution. And so it is true that migrating data, which has auto-increment keys into Spanner, poses a challenge. Your choice of keys should also inform how you load your data. Avoid writing those in order of primary key. Consider partitioning your key range and then writing in batches, which are drawn from different parts of those key ranges. The idea is to allow writes to be as well distributed across nodes as possible. Some best practices on row size. In general, do not allow your rows to balloon out of control. Keep your rows to 4GB or less. This includes not only the top-level parent table row, but also all of the interleaved child rows and all of the associated index rows. You should try to be as proactive as possible in using interleaved tables and in specifying parent-child relationships. Interleaved tables allow Spanner to physically collocate parent and child table rows. This creates a hierarchy of interleaved tables. The more information you can give Spanner, the better. It helps Spanner to reduce hotspotting. One especially attractive use case for interleaved tables is for indices on monotonically changing data to not create any non-interleaved indices on such data. It is fine, however, to use interleaved indexes even with monotonically trending data.

Creating and Managing Instances Using the Web Console - I
We've covered a lot of theoretical ground, and we are finally ready to get started with some demos. Let's see how we can create and manage Cloud Spanner instances from the web console. Navigate to console.cloud .google .com. That's the usual starting point. Enter your email ID and then your password. All of these steps are exactly as usual. The project that you are working in will be displayed up top, and the navigation menu is at the top-left. In the navigation menu, the first order of business is to enable APIs for many services, which we plan to use. The Google Cloud Platform intentionally keeps these off by default to prevent any unexpected or unanticipated use and disputes over billing. That's fine. We will just go through and enable the APIs for a bunch of GCP services, which we will be accessing in this class. So in the navigation menu, select APIs and Services and Library. That brings us to a search page. Here we enter Cloud Spanner because that's the first API that we wish to enable. This is simple enough. If we click on it, it takes us to a detail page where we have a big blue Enable button. Let's click that, and that's all there is to it. We are now taken to an overview page where they are some additional details. Let's also go ahead and enable the APIs in this project for various other services. So the Google Cloud APIs, this is a metaservice, which is used by many other services under the hood. Later on in the course, we will be using Cloud Dataflow, along with Spanner, so let's enable that as well. We search for Dataflow and then enable the Dataflow API. Similarly, let's go ahead and enable the Cloud Functions APIs and the Cloud Source Repositories APIs. This is a one-time exercise project. So if you're using an existing project, you probably will not need to do all of this.

Creating and Managing Instances Using the Web Console - II
After enabling all of the necessary APIs for this course, we are ready to get started. Search for Spanner in the navigation menu and click on the Create Instance button. We are prompted for an instance name and are then prompted for the instance configuration. Note that the default instance configuration is regional. Let's choose a region. Let's say us-central1. Note how the cost for the nodes and the storage depend on the region that we select. We'll discuss Spanner pricing in detail, but check out how nodes cost $ .90 an hour, and storage cost $ .30 /GB/month. Nodes are effectively pay what you allocate. Storage is pay what you use. These costs increase significantly if you change the configuration to multi-region. Here selecting the configuration in itself is a little more complex. Currently, the configuration supported are called nam3. That stands for not America 3. The other instance configuration is called nam-eur-asia1. Both nodes and storage are much more expensive with multi-region than with single-region configuration. Nodes cost $3/hour in nam3, and storage costs $ .50 /GB/month. And in case you're wondering what this nam3 is all about, the details are over on the right. Nam3 will store your data in the United States in Northern Virginia and South Carolina. Next is the node count, and there is some nice guidance there are on how you select the number of nodes. The rough rule of thumb is that each node can provide 10, 000 QPS of reads, 2, 000 QPS of writes, and hold 2TB of data. Because this is our first instance, we have a node count of 1. We are ready to hit Create. That really is all there is to it. The instance will be provisioned, and it will show up in our instance dashboard. We can monitor the instance using that monitoring tab over there, metrics, such as node count, CPU utilization, and so on. Now that our instance is coming to existence, let's explore some of the other things that we could do with it. If we select the instance, a menu appears on the right for permissions and labels. Permissions allow us to add new members and to assign roles to them. These could be either individuals or service accounts. Currently, there is one owner. This is spikeysales@ loonycorn.com. In addition, there are four editors. These are all service accounts, which have been inherited from project-wide settings. Let's hide that info panel and see some of the different ways in which we can edit our instance. We select it and click on the Edit Instance button, and this allows us to change the instance name and the node count. Remember that the instance configuration, i.e. whether this is a regional or a multi- region instance, that is immutable. We cannot change that after the instance has been created. Let's change the number of instances from 1 to 2. Notice that the cost of the nodes doubles. The storage cost is unchanged. Let's save these changes and move on to creating our first database. This is easy enough to do. We click on that blue Create database button. Let's call this the spikeysales-database. That's the name of our database. The next item is to--- And there it is. It's ready, and we can now create tables in this database. Let's go ahead and create a simple table, which we'll call spikey_test_table. We can also add columns using this nice UI. For each column, we can specify the name, the type, and whether null values are allowed or not. We'll create a couple of columns. These are the Name column and the Age column. Name takes string values of up to 1024 characters in length. Nulls are not allowed. The Age column is of type INT64. We hit Continue, and then this is worth commenting on. We are asked to set a primary key. In Spanner, every table must have the primary key. That key could either be a single-column key or a composite. Here we are going to go with a single-column key, and that's going to be the Name column. At this point, we can hit Create, and our table has come into existence. We can also view the equivalent SQL by clicking on that little link there. I will call out the heading of this dialog box. This says Equivalent DDL. DDL is an acronym for data definition language. This means that the SQL here is going to actually define a table or a schema or change the structure of the database in some way. On the other hand, SQL statements which query data or which update or delete existing data are called DML, or data manipulation language statements. DDL and DML are the two types of SQL statements. Let's keep going with our exploration of the UI. Note how the Name column has a little key icon next to it. Notice that we can also click on the Indexes button there to create or work with other indices. Lastly, there is a Data column, which we can use in order to insert data into our table. Let's go ahead and insert our first row. We are prompted for each of the column values. We type those out and hit Save, and our first row is successfully inserted into this little test table. Let's go ahead and insert a few more rows. One little bit worth commenting on, when you hit Insert, there is a little checkbox there for an empty string. This is a safe, verifiable way of importing empty strings into your data. In any case, we enter a couple more rows. And by the end of this exercise, we have three rows in our database.

Creating and Managing Instances Using the Web Console - III
The primary key column has a special role to play in Spanner. Because data is arranged by the primary key, it's very easy to filter on primary key values. Here check out how in the UI we can filter on a specific value by just typing it out. Let's continue with our exploration of Spanner's functionality. Let's edit the schema. We are able to now add a column. This time, the column we add is for gender. It's a string column, and we are going to have to permit nulls because we already have preexisting data in our table. So this Gender column is added successfully, but all of the old, preexisting rows are going to have null values when it comes to this column. Let's change that. Let's go ahead and edit our rows so that we can change and add a gender. This is one way of doing it. We can type in the gender painstakingly row by row, but that's going to be difficult. As we shall see, a far more efficient way of doing this is using an update query. Spanner and most other relational databases will have special efficient ways of performing such bulk operations. These are implemented using a special kind of transaction called partition DML transactions. Such transactions avoid database-wide locks. This is why bulk operations are so much faster than individual user DML operations. In any case, let's keep moving and run our first query. We click on the Query button up top. That takes a to a nice, web-based UI where we can type out our first very simple query. We are simply doing the SELECT * from our spikey_test_table, but we are doing a LIMIT 100. We don't want more than 100 rows in the output. Pretty standard SQL functionality. Let's go ahead and run that query and see how things go. The query results are displayed down below. But what's more interesting is the explanation. We can see that the query took 3.45 milliseconds to run. There are also statistics about the CPU time that was spent executing this query and other interesting implementation details, such as the relational operator tree, which was used by the underlying SQL engine.

Working with an Instance Using gcloud
In the previous demo, we used the web console to interact with our Spanner instance. This time, let's go ahead and use the gcloud command line utility instead. Hopefully you have encountered the Google Cloud Shell before. In case you haven't, this is a simple and easy way to interact with the GCP from a terminal window, which happens to run from inside your browser. You simply click on that little icon over in the top-right, which says Activate Cloud Shell. This will open a terminal connection on an ephemeral VM. It's important that you not use Cloud Shell for any long-running commands because this ephemeral VM will time out your sessions every half an hour or so. The great thing about this VM is that it will have all of the latest versions of different services and Google Cloud SDKs installed. The first thing that I'd like to do once Cloud Shell does open up is to just change the prompt. The current prompt is a bit too verbose, and it's not going to allow me to display all of my commands clearly. So I'm going to run this little bit of Shell script to change to something much more succinct. A little more housekeeping and start by running gcloud auth application-default login. This is not really needed from the Cloud Shell. But if you're using the GCP from your local machine, this is important. This process will require you to test a link into a browser. Sign in with whatever email ID you are using. This, in turn, will give you a verification code. You copy-paste that verification code back into Cloud Shell, and then you are verified. Your credentials will be saved on that Cloud Shell machine in a temporary file, and you are ready to start using the GCP in earnest. Another command that might be useful if you're using your local laptop, you need to get the latest version of all of the components. So you run sudo gcloud component update. This is a good way of making sure that you have the latest up-to-date Google SDK components. Now, finally, we can start interacting with Spanner. Let's run our first simple command using the gcloud non-interactive shell. So this starts with gcloud spanner followed by the individual command. Here we would like to list all of the different instance configurations. And here we can see, nam-eur-asia1, nam3, and the other multi-regional and regional instance configurations. Let's run a simple command to create a new instance called spikey-test-spanner. It will make use of the same nam3 configuration as before. We also specify a description and the number of nodes, which we set here to be just 1. Once the instance creation is done, we can switch back to the web console, refresh our Spanner dashboard, and we can see that our Spikey Test Spanner instance appears there. To do the same from the command line, simply run gcloud spanner instances list. And once again, we will get information about both of our instances. It's also easy enough to use the command line to update the description or even to increase the number of nodes. Let's increase the number of nodes from 1 to 2, and that is reflected in the web console dashboard as well. And to delete an instance, simply run gcloud spanner instances delete followed by the name of the instance. Here we are deleting spikey-test-spanner. We hit Yes when we are prompted for a confirmation. And then next time we list the instances, spikey-test-spanner is gone.

Pricing
Let's take a look at how pricing works for Cloud Spanner. The cost of your Spanner instance is going to depend on three variables, the number of nodes used, the amount of storage used, and the amount of network bandwidth that you consume. At a very high level, you pay for nodes based on what you allocate; you pay for storage based on what you consume. Let's double-click on each of these components. The number of nodes is driven by the node cost. Every hour Spanner will calculate the maximum number of nodes that was in existence in the instance in that hour and then multiply it by an hourly rate. Effectively, each node is billed for a minimum of 1 hour. How much is the hourly rate? Well, that depends on whether the configuration is regional or multi- regional, and the differences are not small. As of the making of this video, which is in November of 2018, the hourly rate for regional configurations varies, but it's roughly in the ballpark of $ .90 to $ 1.10 per node per hour. The hourly rate for multi-regional configurations is significantly more expensive. Currently, that is in the range of $ 3.00 to $ 9.00 /node/hour. This is a steep cost. Forgetting to delete or turn down your Spanner instances is one of the most expensive mistakes that you could make on the Google Cloud Platform. Now let's talk about the storage cost. This is billed for the space that's used by tables, secondary indices, as well as overhead associated with those indices. This storage cost also varies based on your instance configuration. For regional configurations, it's in the range of $ .30 to $ .40 /GB/month. For multi-regional configurations, it goes up to between $ .50 and $ .90 /GB/month. And you can see that that is orders of magnitude more expensive than a Google Cloud Storage multi-regional bucket, which currently costs only $ 0.026 /GB/month. And the third component of Spanner pricing is the amount of network bandwidth that you consume. As always, there's a lot of fine print. Networking costs are amongst the most difficult to estimate. The simple rule of thumb is that ingress is free, while egress varies widely in its cost. A couple of other points worth keeping in mind. There is no additional charge for import or export of data, so at least this bit is free. If you make use of Dataflow workers or GCS buckets as a sort of adjunct staging area for your data, standard pricing for those will apply.

Summary
We've now come to the end of this, the first module, in our course. This module was all about getting started with Spanner, which, of course, is a powerful proprietary Google RDBMS. We saw how globally distributed Cloud Spanner is and how it manages to offer horizontal scaling, along with strong consistency guarantees. This is a very rare and unusual combination, which makes Spanner virtually unique. Spanner is also fairly expensive. The cost of Spanner includes a node cost, which is pay what you allocate, and storage cost, which are pay what you use. So you should be careful and not leave your Spanner instances lying around. Delete them as soon as you're done with them.

Creating and Managing Tables in Cloud Spanner
Module Overview
Hello, and welcome to this module in our course on Cloud Spanner. This module is all about creating and managing tables in Cloud Spanner. We will begin by understanding what interleaved tables are and the semantics of on delete cascade. We will move on to programmatic access from Python, and we will see how this can be done for different types of transactions, read-only and read-write. We will then move on to secondary indices. These exist outside of the primary index of a table. We will understand how commit timestamps can be used in situations where an application is fine with a certain amount of staleness in the data, but does not want latency at any cost.

Interleaved Tables and Cascading Delete
Let's go ahead and get started with a demo in which we will see how interleaved tables are created and how cascading delete statements work with such tables. Let's start with our Spikeysales Instance. If we click on this, we see that there is a spikeysales-database. Let's go ahead and add a table in there. Let's call this table spikey_customer_details. Let's add a bunch of columns. The first column is a CustomerID. This is going to be the key column. After that, we will go ahead and add in a bunch of other columns, such as FirstName, LastName, Gender, and so on. Once we are done, let's hit Continue, and then we specify our primary key. At this point, we have a single column key, which is the CustomerID. Let's hit Create. That will bring our Customers details table into existence. Next, let's go ahead and insert a bunch of rows so that we can do something meaningful. Now, of course, there are several ways of getting data into a table. One of these is to use this Insert button here and actually type all the values, another is to use an insert query, and a third is to use the command line, to use gcloud spanner, along with the insert command. Okay, now we can get started with the interesting stuff. Let's click on the CREATE TABLE command, and let's select the Edit as text option. We can now paste in a CREATE TABLE command. Note that this is a table called spikey_order_details followed by a bunch of column definitions. This includes the CustomerID, by the way, and then for a PRIMARY KEY, which is a composite key, which includes both the CustomerID and the OrderID. And finally, for the most interesting part of this table definition, we are interleaving this table. This is the spikey_order_details table with a parent table. The parent table is the spikey_customer_details table. And there is, of course, a parent-child relationship inherent in this interleaving, so we need to also specify the delete semantics. Here we go with the ON DELETE CASCADE option. We hit Create, and our first interleaved table is about to come into existence. Notice how the primary key of the parent table is a prefix of the primary key of the child table. Let's go ahead and insert some data. As usual, we will do so by clicking on that Insert button. Now we've got to enter values for both the columns of the primary key of the parent, that's CustomerID, and also the primary key of the child, which is OrderID. There's nothing particularly remarkable about inserting rows into an interleaved table, so we go ahead and insert a few more rows exactly as before. When we are done, we can switch to the command line and run a simple SELECT query and examine the contents of our child table. This is a simple SELECT * from spikey_order_details. By this point, we have inserted four rows, and all of these appear. Let's move down one of these CustomerIDs at random. Let's call it the first one. That's AG-10270. Then let's go to the Customers table and delete that particular row. Now we had specified the delete semantics as ON DELETE CASCADE. So if we delete this row from the parent table, the corresponding rows from the child table should vanish as well. Let's see if that happens. Let's run a DELETE statement in which we are explicitly deleting from the table spikey_customer_details for the key value AG-10270. And when we hit execute, there's no response from Spanner, so hopefully it was successful. Let's switch back to the web console and confirm. And indeed, that particular row has vanished from the customer_details table. Let's now see what happens in the child table. So let's click on spikey_order_details and navigate to the Data tab. And there we now see that the child row is gone as well. There's no row in our child table corresponding to the foreign key that we just deleted. The on delete cascade has clearly worked. Now we mentioned that Spanner supports interleaving up to seven levels of hierarchy. So let's go ahead and see how we could create another further level of nesting. We will add a table called spikey_supplier_details. This will reference both the Orders table and the Customers table. So in this way, we will have three levels of nesting. To see how this is done, let's switch to the command line. Let's clear our terminal and type out the query. This is a fairly involved query. Notice that it uses the DDL update key word. The rest is fairly standard. We have a CREATE TABLE statement with the ddl flag in there, the table name, that's spikey_suppliers_details. The interesting bit is that the primary key is now a composite of three attributes. That's the CustomerID, the OrderID, and the SupplierID. Notice also how the order of these keys is important. The primary key of the parent table is the first. It serves as a prefix, then comes the OrderID column, and finally is the SupplierID. This order of the keys in the composite primary key is very important. And finally is the interleave statement, the INTERLEAVE IN PARENT. Notice how we only specify the immediate parent, spikey_order_details. And last of all, we specify the delete semantics to be ON DELETE CASCADE. We execute the command, and it goes through successfully. As always, the return value is a bit cryptic. But to confirm the results, we switch back to the web console, check in our spikeysales-database, and there, sure enough, we see the spikey_suppliers_details table has appeared. We have successfully demonstrated the creation of interleaved tables with three levels of nesting and also seen how the ON DELETE CASCADE semantics work.

Types of Reads
Let's spend a little bit of time talking about the two types of reads supported in Cloud Spanner and their use cases. Now for context, remember that Spanner is a horizontally scaled and replicated relational database, which means that there are multiple replicas of the splits that we've discussed earlier. These replicas need to be kept in sync. Spanner offers a strong form of consistency called external consistency, which ensures that no client need ever read stale data. But what if you have a client, which is fine with reading some slightly stale data? In order to support this, Spanner has two different types of reads. The first kind of read is called a strong read. Here the reading application will want to see all of the rights that have been written to the database so far. Now a write will only be committed after the Paxos consensus algorithm has played out, so this may be a little slow. If your application is not okay with that latency, but it is okay with slightly stale data, you can opt for a stale read. This is meant for apps, which are tolerant of stale data, but which are very critical to latency. Now it's important to note that, by default, all reads are strong. So by default, your app will only be getting the latest data, which includes all committed writes. Based on what type of read you want to go with, there are actually three different types of read operations. Your use case may or may not require transactional support. If it does require transactional support, there is a further choice. If you are going to perform a write based on the results of one or more reads, then your workload is a read-write transaction. On the other hand, if you are only going to read data, that is if you're going to make multiple reads that require a consistent view of the data, you should go with a read-only transaction. Note that both read-write and read-only are still transactional. This means that they will require a consistent view of the data. Now there are also use cases which are non-transactional. These can be dealt with using a single read method. For instance, a single standalone read call or multiple such independent read calls, which are not going to read data across different row, these do not require transactional support at all. Let's now try and tie together these two different concepts, the three types of operations and the two types of reads. Let's say that we are performing either a read-only transaction or a single read method. We can specify a timestamp bound. This timestamp bound determines whether we are going with a strong read or a stale read. In other words, the timestamp bound tells Spanner how to choose which data to return. We might to allow some staleness in return for lower latency. If this tradeoff is acceptable, we will go with a stale read. By default, the timestamp bound specified is strong. This is what you would use for a strong read. For stale reads, you have two choices. You could either specify bounded staleness, or you could ask for an exact timestamp bound. This is equivalent to requesting a specific state from the past. As a cloud architect, you should use timestamp bounds wisely. You should be willing to accept some staleness when latency is more important. And you should go for exact timestamps whenever it's important that you get the latest data.

Table Schema Definition
Let's move on to a demo in which we'll see how Python can be used to carry out schema definition and CRUD operations. CRUD, of course, is an acronym for create, read, update, and delete. Now by far, the simplest way of using Python on the Google Cloud Platform is from Cloud Datalab. This is a way to run hosted Jupyter Notebooks. These are hosted on a cloud virtual machine instance. All that you need to do in order to get started is to run this simple command, datalab create followed by the name of the VM that you want to host your Datalab instance and the zone in which that instance is to be created. Datalab is a very powerful tool. It allows you to run hosted Jupyter Notebooks on a cloud VM instance. So we go ahead and run this command, and a whole bunch of stuff happens behind the scenes to provision that VM instance and install the Datalab container on it. As an aside, you might be interested to know that Datalab is actually a Docker container, which is running on this particular server. In any case, if we now navigate to the Compute Engine tab, we will see our spikey-spanner-datalab instance there, just like any other VM instance that we might have created. Our Datalab Docker container accepts incoming connections on a specific port. To hit that port, just click on that little eye icon in the top-right. Let's go ahead and click on it. This also allows us to change the code on which the container is listening if we want. 8081 is the standard. Let's go ahead and connect on it. This opens up a browser window. In this browser window, we can effectively navigate the filesystem of this VM instance. Here let's click on the notebooks folder. This is where we will create the Jupyter Notebooks that we are going to work with. So we add a new notebook, and now we are in business. We can type Python code here. This is an untitled notebook, so we can name it something meaningful, like WorkingWithCloudSpanner. Now Datalab supports both Python 2 and Python 3. By default, it's Python 2. Let's change the kernel to Python 3 in that dialog box in the top-right. We can now go ahead and use pip install followed by the client library names, exactly as we would with a Jupyter Notebook. The first library we want to install is google-cloud-spanner, and then we are in business. We can start with the import statements. We can instantiate a spanner_client. We can create variables with our instance_id. And we are now ready to connect to a database and start running some queries. Before we do that, let's switch back to Cloud Spanner and create a new database called spikey-test-database and load it with some data, which we can use for these operations. We are intentionally only creating an empty database from the web console. We will then go ahead and populate this and perform all of our table definitions and CRUD operations from Python. So we are now ready to switch back to our Datalab notebook. We will point our instance to this particular database. So the database_id is spikey-test-database. Once we have a reference to this, we can go ahead and run some create_table commands. The syntax for this create_tables command is fairly straightforward. All we really need to do is to embed our SQL statement inside a method call to the update_ddl method on the database object. Do notice that we are basically repeating the creation of the interleaved tables, which we had previously done from the web console. We have a spikey_customer_table and a spikey_product_details table, and the two are interleaved. This is a blocking call, so the result of this particular DDL operation will only come back after it's been executed. Let's invoke this create_tables function and see what happens. We hit Ctrl+Enter as usual. It takes a little while for the operation to complete, but eventually it goes through successfully. If we switch to the web console, navigate to Spanner, and check in our Spikeysales Instance, we find that this spikey-test-database now, indeed, has a couple of tables. We have successfully demonstrated table creation from Python.

CRUD Operations
Let's pick up right from where we left off. We had created a couple of tables in the spikey-test-database. Now let's see how we can insert data into these tables. Here we have an insert_data method in which we create a batch of rows, which have to be inserted into the database all together. Cloud Spanner is going to apply statements from the same batch in order. And if a statement is in error, the previous statements will not be rolled back. In this case, important to know. Next comes the table, which here is the spikey_customer_table, along with the columns, and finally and array of values. So we are inserting multiple rows at one go. Let's now repeat this operation, but this time we will insert into the spikey_product_details table. We are ready to give this function a spin, so we invoke the insert_data method, and this goes through successfully. If we now switch to the web console and navigate to the Data tab, we find that the rows we put in do indeed appear, both for the spikey_customer_table and the spikey_products_table. Let's switch back to Datalab and run a database.update_ddl statement. We will alter both of our tables and add columns. So we are adding a Price_Paid column to the product_details and a Wallet column to the customer_table. Let's invoke this add_columns method, wait for the operation to complete, switch back to Spanner, and indeed, the Wallet column appears in the customer_table with nulls for all of the rows, and the Price_Paid column appears in the product_details table, once again, with nulls in all rows. Let's remedy those nulls. Let's switch back to Datalab and define an update_data method. Once again, we create a batch, but this time, it's an update statement, which we are going to run. We specify the columns, CustomerId and ProductId, along with the values of the Price_Paid. As long as we specify the key columns, that's enough information for Spanner to know which rows to update. And we can do the same with the spikey_customer_table. We'll put in a wallet balance. We invoke this update_data method. It's gone through successfully. Let's switch back to the web console. And this time, the Price_Paid column has all of the values we specified. Likewise, the Wallet field is populated as well. Let's round this out by deleting some rows from Python. So we switch back and define a delete_data method. The code for this method is pretty succinct. As always, it takes a lot of effort to build and very little to destroy. The interesting bit here is that we create a keyset. This is an array with all of the keys which we wish to delete. We create a batch with these keys, and that really is all that there is to it. We give this function a spin. We delete the data. And now when we go back to the product_details table, the row that we deleted no longer shows up. We have successfully implemented create, update, and delete commands from Python. Now all that's left for us to do is to read.

Reading Data
Let's now jump into a series of demos in which we will demonstrate reading from Cloud Spanner. This time our function is called read_data. As always, we have a with statement, but this time it's not a batch; rather it's a snapshot. Now a batch is something which we use for update, create, or delete statements because we want all of them to happen in order. A snapshot, on the other hand, represents the data which is within the database at any given point in time. We shall see in later demos how it's possible to specify a specific timestamp. If we omit a timestamp while creating the database.snapshot, we are effectively asking for the latest data. Now let's do this. Let's create a keyset, which has all of the keys in the database and then pass this keyset into a snapshot.read statement. In that snapshot.read, we have to specify the name of the table, as well as the columns which we wish to read. And this will return an iterable called results. We can iterate over these rows and format them nicely. Let's go ahead and execute this by invoking the read_data method, and it nicely prints all of the data in the customer table. Remember that there are two types of reads in Spanner, strong reads, which fetch the latest data, and stale reads, which are okay with some staleness. Let's check out an implementation of a read of stale data. This method is called read_stale_data, and here inside the body of the method, we import the datetime library and set the staleness that we are okay with to be equal to 15 seconds. This staleness object is merely a datetime.timedelta object, but we pass this into our snapshot, and we specify this as the value of the exact staleness, which we are fine to live with. The rest of the code in this method is pretty familiar. We create a keyset, pass that into a snapshot.read, and iterate all the results. Let's give this method a spin, and we see that it does, indeed, return all of the CustomerIds. Next, let's break into a method in which we read a range of keys. This is something which Spanner is able to do very efficiently, and that's because all of the data is stored in sorted order of primary key. We create a snapshot object as usual, but the interesting bit is where we create a spanner.keyrange. Notice that we have to specify the start_closed and the end_closed values. These are the bounding points of the interval of the keyrange interval that we are seeking to read. We pass this keyrange into our keyset and then invoke the snapshot.read method. All of the rest of this is exactly as usual. The return values are only for the keys within the range that we specify. Let's round out this demo with a look at how read-only transactions can be implemented. The only significant difference here is that when we instantiate our database snapshot, we have to specify the multi-use parameter to be true. This is telling Spanner that we wish to carry out a read-only transaction, and we have more than one read. All of those reads need to be executed at exactly the same timestamp. Once we do this, we can have multiple reads within the body of this with statement, and these reads might either use the snapshot.execute_sql method or the snapshot.read method, which we have been using so far, and Spanner will ensure that all of these use the same timestamp. We go ahead and execute this method, and we get results from three reads. All of these are part of the same read-only transaction.

Transactions
In this video, we'll take some time to understand the different types of transactions supported by Cloud Spanner. Recall that earlier in the course, we had discussed how there are transactional and non-transactional ways to work with Spanner. Transactions might be read-write or read-only, and non transactional single reads are possible as well. So let's become a little more rigorous. Let's define a Cloud Spanner transaction. In general database theory, a transaction is defined as an all or nothing set of operations. And a Cloud Spanner transaction is not all that different. It is a set of reads and writes that are executed atomically at a single logical point in time. This is equivalent to the serializability of transactions in traditional computer science. It turns out that there are three types of transactions in Cloud Spanner. There are locking read-writes, which have pessimistic locking and which might abort. There are read-only transactions, which do not commit and hence do not acquire locks. And finally, there are partitioned DML transactions, which are used for bulk operations by the system. Let's quickly cycle through their use cases. You would use locking read-write transactions for writes which depend on the values of reads or if you have multiple rights which need to be atomically executed. You would also use locking read-write transactions for conditional writes. Moving on, read-only transactions do not commit, and therefore they cannot abort. You'd use these if you want to perform more than one read at the same timestamp, effectively, if you have multiple reads which require consistency. The third type of transaction is a little different. Partitioned DML transactions are used for large-scale UPDATE or DELETE statements. These are system created. They do not roll back. This is a way of getting efficient execution of large-scale UPDATEs and DELETEs by partitioning the key space. These transactions do not lock the entire database, which is good. But they cannot be rolled back, which may sometimes be a concern. Let's clearly understand the differences between traditional DML and partitioned DML. Traditional DML is used for regular transaction processing of the sort that we, as end users, are accustomed to. Partitioned DML is for databases-wide operations. Regular DML includes user transactions, which affect a small subset of a database. Partitioned DML is ideal for bulk updates, deletes, filling in default values, and such large-scale operations. Traditional DML operations are usually not idempotent, which means that you cannot safely execute them repeatedly. In contrast, partitioned DML operations must be idempotent. The system should have the freedom to run a partitioned DML statement repeatedly without altering the state of the database relative to executing it just once. Regular DML transactions can span multiple statements. Partitioned DML is single statement only. Regular DML supports any form, any type of statement, however complex. Partitioned DML statements must satisfy a specific condition. A partitioned DML transaction can only include one statement, and that statement needs to be fully partitionable, which means that it should be possible to express that one statement as a union of a set of statements that each such statement only accesses a single row of a single table, so no self-joins, no multiple table statements. If you think about simple column-wide update or delete statements, these are classic examples of fully partitionable statements. And finally, regular transactions are explicitly created and submitted by users. Partitioned DML transactions, on the other hand, are system created. Spanner will accept a command from a user, decide that this calls for a partitioned DML transaction, and go ahead and execute it.

Read-write Transactions
We've executed read-only transactions. Let's now go ahead and implement a read- write transaction. In this example, we will perform a write, which is contingent on the value of a previous read. We are going to transfer money from the wallet of one customer to another. The first time around, the transfer is successful. The second time around, there isn't enough money in the wallet, and the transaction has to abort. The source for this transaction is going to be the wallet for AA-10480, which has an opening balance of $ 1700.40, and the destination is going to be the wallet for CG-12520, which has an opening balance of only $50. Let's switch over to Datalab where we have coded this up. We have an outer method, which is called transfer_wallet_amounnt, and then we have a nested inner method called update_wallet. This helper method takes in a transaction object, which we will instantiate below. Let's create a keyset with a specific key for the source wallet and another keyset with the specific key for the destination wallet. Now we had read the contents of the source wallet up above, and we used that for our conditional write. We are only going to transfer the amount if there is enough money in the source wallet. So if Andrew Allen has enough money in his wallet, we will reduce his balance and increase Claire's balance. And if not, we will throw an exception signaling that Andrew Allen does not have enough funds for this transfer to go through. And finally, we are able to actually execute the transaction by invoking the transaction.update method. Notice how we have to specify the table, the columns, and the values. Now all of this code is happening inside the inner helper function called update_wallet, which took in the transaction as an argument. And you might be wondering where the transactions is instantiated. The answer is it's not. This method, the update_wallet method is actually a callback function, which we pass into the database.run_in_transaction method. So it's actually Spanner which instantiates the transaction and passes that transactions into this update_wallet method as an argument. So the code for this read- write transaction was significantly more subtle than that for the read-only transactions we had encountered so far. We are done. We are now ready to give this method a spin. Let's try and transfer $500, and this goes through successfully. We can verify that these changes are reflected in the database by shifting back to Spanner where we see that Andrew's wallet balance has reduced by $500, and Claire's has gone up by the same amount. But if we now try to transfer another $1700, this will not fly. We get a value error because Andrew Allen does not have enough funds to transfer. And the crucial bit is that the integrity of the system is still maintained. Andrew's balance has not reduced. If we try and do a smaller transfer of $250, this time it will go through successfully, and the changes will also be reflected in Cloud Spanner. And in this way, we have successfully implemented a read-write transaction in which the write was contingent on values which were read in.

Secondary Indexes
In Cloud Spanner, the choice of primary key also dictates the physical layout of our data. If we now want to use secondary indexes, which are indexes on another non-key column, this is a special procedure. Let's see how this is done. It turns out that creating a secondary index is fairly simple. Simply select your table and hit the Create Index button. We are prompted for an index name and then for some columns, which are going to be used in the index construction. In this example, we create a secondary index on the FirstName column, and we are fine to go with the default Sort ascending option. There are various additional index options, which we could explore. But those are pretty advanced, so we will leave all of these options unchecked. If you'd like to learn more about them, here are some really helpful tooltips. This is in the Index option guidance section. In any case, we are now done. We can hit Create, and our secondary index will come into existence. Notice the name of this index is spikey_firstname_index. Pay close attention to this because we're going to need to use this in our queries. Note that we can also create secondary indices from the command line using the ddl update command. Here we create another secondary index called spikey_lastname_index. And we can also create a secondary index from within Python by using the update_ddl_method on our database object. Now creating a secondary index is simple enough. We are able to do so in three different ways. But the noteworthy bit is in actually using the secondary index. By default, Cloud Spanner will not automatically choose to use secondary indices. So we have to direct Cloud Spanner. We have to force it to use the index that we want. And that's why within our SQL query, we need to have this highlighted syntax, the FORCE_INDEX clause followed by the name of the secondary index we created. This FORCE_INDEX clause needs to be included explicitly in the SQL if you're using the snapshot.execute_sql method. Alternatively, we could choose to use the snapshot.read method and simply specify the index as one of the parameters. Let's round out this demo with a look at a STORING clause, which you might want to use when your most common query uses a combination of key and non-key attributes. For instance here, we are using a STORING clause along with a CREATE INDEX statement. The index is actually on the Product_bought column of the spikey_product_details table. But in addition to this, we are also asking the index to store the values of the Price_Paid column. Why would we want to do this? Well the Price_Paid column is clearly not unique. It's not the kind of column we can include in an index. But it is going to be included in our most common queries. And that's why we want the physical construction of this index to store the values of the Price_Paid along with the Product_bought. So we go ahead and construct this index, and then we are able to use it for queries like the one you see on screen now. Because this is a query which retrieves the Product_bought, along with the Price_Paid, the efficiency of this query increases significantly because of our storing these two values together.

Commit Timestamps
Let's round off this module with a demo which shows how we can work with commit timestamps. A commit timestamp is a pretty specific construct in Cloud Spanner. This is a way to determine the exact time when a transaction was committed to the database. We can use commit timestamps to record exactly when a change occurred, and this is implemented using TrueTime technology. Using a commit timestamp is a two-step process. We first need to define a timestamp column and set the allow_commit_timestamp flag on it. This will create a column. We then go ahead and write the timestamp as a part of our transaction into this column. Let's turn to Python and see how this is implemented. Here we are altering our spikey_product_details table to explicitly add a column. That column is called the LastUpdateTime, and it's of type TIMESTAMP. And now here is the important bit. We need to add an OPTIONS clause in which we explicitly set the allow_commit_timestamp flag to be true. Let's go ahead and invoke this method. A column will be added to our spikey_product_details table. This is of type TIMESTAMP, and it's initially populated with only null values. Now we actually need to instruct Spanner to write values into this column. For this we have a method called update_data_with_timestamp. The only noteworthy part of this query is that we specify values for the LastUpdateTime column, and while doing so, we simply make use of the spanner.COMMIT_TIMESTAMP field. Once the batch update is successful, we can switch back to Spanner and verify that values have indeed been written into the LastUpdate Time column. This column can now be queried like any other column in a Spanner data table.

Summary
This gets us to the end of a module in which we covered a fair amount of ground. Let's quickly summarize. We learned how to construct interleaved tables to more than two levels of nesting and implement cascading delete on them. We spent a lot of time understanding how to programmatically access Cloud Spanner from Datalab. We implemented read-only and read-write transactions. Read-write transactions, in particular, had a subtle syntax, which involved the use of a callback method. We saw how we can construct a secondary index and force Cloud Spanner to use it. We also demonstrated the use of the STORING clause for non-key attributes, which are commonly queried, along with indexed attributes. And we rounded out the module by understanding what a commit timestamp is and how it can actually be implemented in Spanner in order to record the exact instance when a change was made to the database.

Integrating Cloud Spanner with Other Google Cloud Services
Module Overview
Hello, and welcome to this, the final module of this course. This module is all about interacting with Cloud Spanner from other GCP services. In particular, we will spend a lot of time exploring the relationship between Spanner and Dataflow. We shall see how Dataflow is used as a connector between Spanner and Cloud Store to afford a seamless migration path into Spanner. We will also see how Spanner can be queried from Cloud Functions and how Stackdriver monitoring can be used to keep tabs on your Cloud Spanner resources.

Export Databases to Cloud Storage Using a Dataflow Template
In this video, we will explore the seamless interconnection between Cloud Spanner, Dataflow, and Cloud Storage. Remember that Cloud Spanner was only made available for use outside Google, that is on the Google Cloud Platform, in quarter 1 of 2017. For most of 2017, persistent reports were of problems getting data into Cloud Spanner. Google took note of this migration issues and worked in seamless integration with Cloud Storage using Dataflow. So in this video, we will explore exporting data from Spanner to Cloud Storage. And then in the next one, we will reverse this direction. Let's start by creating a bucket. We are going to make use of gsutil. If you have not used gsutil or you're not familiar with Google Cloud Storage, I'd suggest that you go back and check out some of our other Pluralsight courses on Google Cloud Storage. Here I am using gsutil followed by mb to create a bucket on Google Cloud Storage. If you are running this command from Cloud Shell, this is going to work just fine. If you're running this from your local machine, you might need to run one command before this. That is gsutil config. In any case, let's go ahead and run this command. And we see that a bucket is created for us. We can switch to the web console, navigate to storage, to the storage browser in fact, and there we will find spikeysales_data_backup. That's the bucket we just created. Okay, now let's turn back to Spanner. Let's go back to our familiar Spikeysales Instance. Here we will select our spikey-test- database and click on the Export icon on the top-right. This leads us to a page where were can specify the export location. We click on the Browser button. This will allow us to browser through all of our Google Cloud Storage buckets. We select spikeysales_data_backup and move on. So we've now defined the data sync. We next need to choose the database. This is the spikey-test-database. This is going to have the data which we seek to export. Notice how so far we have not had any indication at all that what's going on under the hood is a Dataflow job. Well that's about to change. Click on the Pricing info tab, and this gives us a whole bunch of fine print, which makes it clear that it's actually a Cloud Dataflow template, which is going to be executed. And this explains why need to select the region. Now the recommended region is us-central1. So let's just go with that. That's where Dataflow resources are concentrated. This is so unusual that Google feels compelled to make us explicitly click that checkbox to confirm that we've understood the charges. While discussing pricing, we had mentioned that import and export are free, but Dataflow is not. And here, we do need to pay the charges incurred by Dataflow. So if you're fine with that, we go ahead and hit Export. This triggers a Dataflow job, and we can view those job details right here by clicking on that button which takes us to Cloud Dataflow. We won't go into the nitty-gritty of Dataflow, but you should be aware that this is a serverless compute technology, which allows both batch and streaming pipelines to be executed in a serverless manner. Autoscaling is taken care of completely by Dataflow. Here we see one minor warning. This warning has to do with the name of the job. There are still some rough edges in the templates in the integration between Dataflow and Cloud Spanner. That's fine. This is not a detail that we need to worry about. We can see that autoscaling has happened. The job has run successfully. So at this point, we can switch back to Spanner, and we get confirmation that the export was successful. Let's go to Google Cloud Storage to the browser, and there we can see in our spikeysales_data_backup bucket that the database contents have indeed been exported. If we explore that, we see that there is one JSON file for the export as a whole, a manifest file for each table, and then the actual data from each table in that database. Let's explore the contents of some of these files. Let's switch back to Cloud Shell and run a gsutil cat command. Let's check all the contents of the spanner- export.json. This merely contains the names of each of the tables and the corresponding manifest files. We can also monitor the status of this and all previous such Dataflow jobs within Spanner. By navigating into the correct part of the Spanner dashboard, we can see that our export job was indeed successful.

Import Databases to Cloud Storage Using a Dataflow Template
Let's now execute yet another demo, which demonstrates integration between Dataflow Cloud Storage and Cloud Spanner. Here we are going to make use of the import functionality to get data into our Cloud Spanner instance. This is very important functionality as far as Spanner is concerned. For the first year or so of Spanner's existence, many users had concerns about migration parts into Spanner. Now because of this great integration feature using Dataflow, Spanner has become a realistic option no matter what database technology you are migrating from. You simply stage your data into Google Cloud Storage and then stage it into Spanner. So let's pick up right from where we left off. We had the Spikeysales Instance. Now let's click on the Import button right up top. This leads us to a dialog where we specify a source folder. Once again, we can browse, and let's navigate to the bucket which we just wrote a moment ago. This is the result of the export from the previous database. That was the source of the data. We need to specify a sync. That's the destination database. Let's call it spikeysales_imported_database. As before, we need to specify a region. This is for the Cloud Storage location. This is us-central1. Let's go ahead and check out the pricing info, which again tells us that this is running Dataflow under the hood. Notice so far that there is nothing in this dialog to tell us that we are actually using Dataflow. Let's confirm those charges, those are related to Dataflow, and then hit Import. Once again, we'll now see the job has been kicked off. We can view the job details by switching to Cloud Dataflow. There is the import Transform pipeline. It's running. We can also examine autoscaling. There is a warning there. This warning has to do with some internal communication within Spanner and Dataflow, and we don't really need to worry about it. The job completes as we can see from the green checkmark and also from the autoscaling graph on the right, which shows that no nodes are currently in use. We can close this and go back to Cloud Spanner. We see that the import job was successful. This whole process took 8 minutes and 47 seconds. We also see that our spikeysales_imported_database has appeared. This is in our Spikeysales Instance. All of the tables from the original spikey- test-database have been successfully recreated. Let's query the data to make sure that everything is in order. We hit the Query button up top and type out a simple query to pull in all of the spikey_product_details. And we can see that we do indeed get all of the data as we were expecting it. We have successfully exported data from Cloud Spanner into Cloud Storage and then reimported that data back into Spanner and queried it. The cycle is complete.

Using Cloud Spanner from within Cloud Functions
Let's change tracks a little bit and see how Cloud Spanner can be queried from within Cloud Functions. We are not going to go into Cloud Functions in a whole lot of detail, but they are a serverless compute solution. They are the equivalent of AWS Lambda. Let's start by switching to Cloud Shell. Most of our work here is going to be done from within the command line. We create a directory called spanner_cloudfunction and cd into it. Then we do something interesting. We open up the code editor by clicking on the little pencil icon on the top-right. In case you have not come across code editor before, this allows a nice UI on top where we can navigate the filesystem and a command line down below. Let's hit Refresh just to make sure that everything is up to date, and then we can go ahead and get started. Let's enter the spanner_cloudfunction's directory and create a new file there. This file is going to be the index.js file. We're going to go ahead and implement our cloudfunction code in this file. We start by defining a bunch of variables. So we reference the Spanner library. We go ahead and instantiate the Spanner object and specify the instanceId and the databaseId. Note that this instanceId is the name of our Spanner instance, and the databaseId is just spikey-test-database. Now let's code up our actual Cloud Function. This takes in a request and returns a response. We start by referencing some of the global variables we had created up above, and then we also have our simple query, which is simply SELECT * FROM spikey_product_details. And our return value is simply the results of running this query on the database object. Well, there is just a little bit of preprocessing that we'll do before that. First, we convert each of the rows to JSON, and then we go ahead and nicely format them with some descriptive information. Because this is an HTTP response, we also set the status to be 200. This is indicative of success. Let's throw in an exception-handling block. Here we return 500 and indicate a server error, and we are good to go. We've coded up our Cloud Function. This is in the index.js file. Let's also go ahead and add another file. This is the package.json. This is metadata, which is required while we deploy our Cloud Function. So note here, we need to specify the name, version, licenses which are used, etc. Notice, for instance, how we are telling Cloud Functions that any version of Node.js is fine provided it's greater than or equal to 4.3 .2. In addition, there's also some information about the scripts and dependencies. The dependency here is on Cloud Spanner. There is some additional boilerplate code, which we are not going to go into great details in. All of this will be explained in a lot more detail in our course on Cloud Functions here, on Pluralsight. Once we are done, we exit out of the code editor by clicking that same pencil icon over in the top-right. And we are now ready to deploy our Cloud Function. We do so by invoking gcloud functions deploy. The name of our Cloud Function is spikey_spanner_function, and it's going to be triggered by HTTP. In other words, this is a webhook. When we run this command, the HTTP picks up the index.js file and the package.json and successfully publishes it. The most interesting part of this response is the httpsTrigger URL. We can see that there. It's the URL that we can hit in our browser in order to invoke our Cloud Function. Let's switch back to the web console. From the navigation menu, we will find the Cloud Functions section, and there we will find that this Cloud Function is indeed available. It has been successfully published. The name is spikey_spanner_function. Let's try out this Cloud Function using curl. For this, we'll switch back to Cloud Shell, type curl, followed by the HTTPS URL. And indeed, we are successful. All of the rows in our spikey_product_details table are returned nicely formatted into JSON. We can also do this from the web console by clicking on the Cloud Function. We can see here by the way that there was an invocation in the monitoring graph. We navigate to the Trigger section and click on the invocation URL. And the results of our Cloud Function now appear nicely formatted in our browser. Again, the focus of this course is on Spanner rather than on Cloud Functions. But you should be aware that you can explore the code, that's the index.js, as well as the package.json from right here within the browser. We have successfully created a Cloud Function which queries Cloud Spanner and invoked it using an HTTP trigger.

Stackdriver Monitoring of Cloud Spanner Resources
In this demo, we will see how Stackdriver can be used to monitor Cloud Spanner resources. Stackdriver is the name of a large suite of ops-based services. Offered Stackdriver monitoring is the one we are going to use here. So in the navigation pane, we find Stackdriver and then click on Stackdriver, Monitoring. Here we need to first create a workspace. Now a workspace is a Stackdriver monitoring concept. This is a way of aggregating different projects into one unified integrated view sometimes called a single pane of glass. So let's create a workspace and add all of the projects that we are interested in. Here the only one we care about is spikey-spanner. Now Stackdriver also has some AWS inter-compatibility. This dates to the time before Google acquired Stackdriver. But here, we are fine to skip the AWS setup. Next we get some helpful tips about installing the Stackdriver agent for virtual machine instances and then prompts about reports. We are not interested in any reports. All we want to do is to get started with our monitoring. Let's begin by creating a dashboard. This dashboard is called the Spanner Database Dashboard. That's quite descriptive. And then let's go ahead and add some charts. The first chart has to do with the CPU utilization of our Spanner instance. So we've got to locate our Spanner resource and then choose CPU utilization as the metric. This combination of a resource and a metric is enough to define a chart. So we can move ahead and pick the type of chart that we are interested in. Let's go with a stacked area chart instead of the default line bar. Notice that we can also do various other cool things. We can filter to remove noise, we can group by in order to smoothen, we can aggregate. There is also a choice of aligner. We are going to go with a mean. So this will allow us to view the mean CPU utilization measured over a brief interval. We hit Save, and now a nice chart appears on screen. Notice that this dashboard can include multiple charts. Let's go ahead and add yet another one. Here we will specify the API request rate. Once again, we have to specify a resource, and once again, that resource is our Cloud Spanner instance. This time, the metric type is different. It's the API request rate. Let's go ahead and select that. We can see some pretty spiky charts. We hit Save, and that's it. We are good to go. We now have two charts in our dashboard. Stackdriver is tightly integrated with every important GCP service, and so we can use this to monitor not just CPU utilization or the API request, but also the network traffic and a host of other metrics. And in this way, we have successfully demonstrated the integration between Stackdriver monitoring and Cloud Spanner.

Summary
We have now come to the end of this module and the end of this course. So here is a quick recap of the territory we just covered. We saw how Dataflow templates have been built into Spanner to provide a seamless migration path for data in. These Dataflow templates make it very easy to interface Cloud Spanner and Cloud Storage. This used to be a concern in the early days of Spanner, but now no longer so. We also explored the use of Cloud Functions. We saw how easy it is to query Cloud Spanner from within a Cloud Function. Lastly, we rounded off the course with a look at Stackdriver monitoring of Cloud Spanner resources. Before we say goodbye, here is a reminder. It's also a call to action. Do remember to delete all of your Cloud Spanner instances. Otherwise, you might be left with a bad case of sticker shock the next time you get your invoice. This is particularly true because of the high node cost associated with Spanner instances that are left on. Other resources which we used in this class were a VM, which was running Cloud Datalab. Remember to delete it. We also used Google Cloud Storage buckets. Get rid of those as well. We also used a Cloud Function. Please be sure to go ahead and delete all of these resources, and don't forget the service accounts that we created. And finally, here are some related courses for further study. You might want to check out this course on Architecting Big Data Solutions Using Cloud BigTable. We've seen how Spanner shares many similarities with BigTable. There are some useful tips there on good key design, and you might also want to check out a course on BigQuery, which, of course, is the very popular data warehousing solution from the Google Cloud Platform. That's it folks. I hope you enjoyed the course. Thank you for watching.