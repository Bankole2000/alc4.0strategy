Implementing DevOps in the Real World

by Richard Seroter

You know the ideas around DevOps, but how do you actually put those ideas into practice? In this course, learn about why DevOps matters, and see a detailed walk-through of the practices and procedures that help you get started with DevOps today.

At the core of DevOps is a thorough understanding of how to optimize software delivery for throughput and quality. In this course, Implementing DevOps in the Real World, you'll learn how to introduce common DevOps practices into your organization. First, you'll learn the fundamental principles behind DevOps, and quickly see how to put those in action. Next, you’ll discover how leading companies approach team dynamics, automating infrastructure, and break down organizational silos. Finally, you'll explore how to fundamentally change how to build and run software. When you’re finished with this course, you'll have a foundational understanding of the core practices of DevOps teams, and the reason behind them.
Course author
Author: Richard Seroter
Richard Seroter

Richard Seroter is the VP of Product Marketing at Pivotal, with a master’s degree in Engineering from the University of Colorado. He’s also an 11-time Microsoft MVP for cloud, an instructor for...
Course info
Level
Beginner
Rating
4.5 stars with 115 raters(115)
My rating
null stars
Duration
2h 53m
Released
27 Jan 2017
Share course

Course Overview
Course Overview

Hey everyone, my name is Richard Seroter, and welcome to my course Implementing DevOps in the Real World. I'm a senior director of product at Pivotal, and I've spent years working in DevOps environments. I've taken what I've learned and studied and put it all into this course. Did you actually know that high performing companies deploy to production 200 times more frequently than low performers or that they recover from failure 24 times faster? That matters. In this course, we're going to dig into DevOps and make this more real to you instead of just talking about the theory. I want us to discuss the specific practices that leading companies use to improve how they solve business problems with software. Some of the major things we're going to cover here is how enterprise DevOps can differ from how small companies may run DevOps, what a typical week might look like for a DevOps team, the reason behind the activities performed by those teams, specific tips for getting started today. By the end of this course, you'll know what changes you need to make to adopt to DevOps culture and exactly how to do it. Before beginning this course, it's helpful for you to be familiar with general concepts around software development and delivery. But don't worry, there's something here for everyone. I really hope you'll join me as we figure out practical DevOps with my Implementing DevOps in the Real World course here at Pluralsight.
Who Cares About DevOps?
Introduction and Overview

Hi there, my name is Richard Seroter, and welcome to this course about implementing DevOps practices in the real world. In this first module, we're going to talk about why DevOps matters and what some of the core principles are and some of the changes you might see after adopting this way of working. I'm a senior director of product at Pivotal, and I've spent the last five years of my career working in DevOps environments, so I've seen firsthand the challenges, the benefits, how you adopt and scale this. I also completed a graduate degree in engineering management with a focus on disciplines like lean manufacturing, which actually inspired the DevOps movement. So in this particular module, we're going to start by talking a little bit about why DevOps matters, what got us to this point. We're going to talk about the challenges of transforming into a software-oriented company without doing a DevOps practice. We'll discuss some of the benefits of a DevOps culture using the Puppet Labs DevOps survey for some statistics. We'll discuss some of the core DevOps values, look at some of the changes that come when you adopt DevOps, what kind of organizational changes might you see and the like. We'll talk about some of the objections because you should feel well-versed and be able to talk about some of those. We'll talk about some of the challenges with small-scale DevOps versus large-scale DevOps and what are those differences when you're doing this at an enterprise. We'll discuss some of the week in the life that's going to be the foundation for this course. And then we'll discuss a handful of learning resources that you can use to study up even more after this course is done. And finally, we'll wrap up.
The Business Imperative

I like this quote from the CEO of General Electric. "Every industry and company that is not bringing software to the core of their business will be disrupted. " And that really reflects some of the modern challenges that most businesses face. Every industry now has this priority rightfully on customer experience, which is often driven through software. Think about how all of us interact with most companies nowadays. It's through some sort of software experience. I just read an analyst report where financial services leaders were asked what's driving their transformation needs, and 62% said provide better customer service or experience. That was twice as much as operational efficiency, something we often associate with IT for the last decade or so. Pretty much what you were doing before with technology isn't good enough given the modern demands. Some of these trends that you see as a result is you see a decline in the consumption of commercial software. As companies become more software oriented, they end up building their own software or using open source software. At the same time though, the challenge is so many companies have spent years making it hard to ship software that all the sudden their biggest challenge now is getting software to production quickly, to get things in the hands of their customers and improve the experience. The other trend you do see is this sort of traditional segmentation of plan, build, run isn't in three different groups in the company anymore. There's not the strategy group, and then a handful of developers, and then a separate ops team. Instead, we're almost going back hundreds or thousands of years through history and showing that this sort of structure where you have more adaptive teams, that you have more encapsulation and teams that can control their destiny, is actually successful, and it can apply to business as well. Basically, we're in this age where the competitive advantage you have is often being able to have a fast time to market and constantly experiment. And if you can't do that, you're going to lose to somebody else who can.
The Challenge of Becoming Software-driven

That might sound a bit dire and a little nerve racking because, of course, change is tricky. But there are different expectations in the market, but there's challenge with becoming a software-driven company if you don't have the culture and you don't have the setup ready to go. So what are those things that make it difficult to pivot exactly as you are into a software-driven company? Well, there can be a lack of trust sometimes where you've got this lack of trust between the business units and their IT partners. IT isn't seen as a business partner, but a cost center, a group that you have to engage with to get things done, a group that charges you for things. It's not a group that you would collaborate with on innovation, but instead one that you almost dump your projects onto if you have to, if you have no other choice. You've got to break that down. This is something you see in a lot of enterprises where you've got this oppressive technical debt, this idea of software and technology that's keeping you from doing new things because you've actually made some choices that have now increased your cost later on. And because you can't pay that down fast enough, it's harder to do some of these new projects. Lack of automation is a real challenge. So many of the companies that have moved forward as software companies have strong automation stories. Instead, if you don't have that, every deployment is some sort of adventure, and every time you have to troubleshoot a problem it's like a murder mystery. That can't be something you can do if you're trying to become a consistent software shop. The one I see a lot as well is competing silo priorities. You have functional silos, and each one has a different definition of success and done. For one team, success may be finding so many bugs in the QA department, and their definition of done is when they've given a bug report back to a dev team. And all these different teams have different definitions of what it means to be good, and when you do that, you've lost sight of the customer and what the actual deployment pipeline is. Similarly, you focus on efficiency and local optimization instead of focusing on the actual service or the customer experience. Instead, it's about getting as much as you can out of each group, which is noble, but at the same time sometimes efficiency is done at the cost of flexibility and adaptability, which is something you're going to need in this customer-oriented world. And then finally, you just may have a culture that doesn't accept risk, doesn't like change. It's something where the skill sets are wrong, and so that can be very difficult to all the sudden expect some sort of top-down directive from your CEO saying we're going DevOps. That won't matter if you don't fix these six things, and we're going to talk about that throughout this course.
Characteristics of High Performing Organizations

Let's talk a little bit about DevOps, some of the business benefits, because it's not about doing these things for the sake of doing them. As I've said up so far, there's a real business imperative to become good at software, and that often required a DevOps transition, but there's different definitions of DevOps. It's often defined by outcomes, as mentioned in the book Start and Scaling DevOps. It's about cultural norms, technology practices that enable the fast flow. Jeff Sussna said DevOps core insight is that reducing the distance between people, teams, and activities combined with reducing the batch size of your work allows you to deliver more value more continuously with greater quality. It's a good definition of DevOps. It's a different combination mainly of cultural norms, of practices, often complimented by technology. But as we think about high-performing IT organizations, every year Puppet Lab sponsors a state of DevOps survey, and over time this has taken input from thousands of professionals, 4, 600 just in the last year alone. And what they learn is things like high-performing organizations do 200 times more frequent deployment. The lead time for change from the time you commit code until it gets to production is under an hour for high performers. It's between a week and a month for the media performers and between one and six months for low performers. That may be you, depending on what your culture is, and that's so frustrating when you want to be able to deliver value. What's also interesting from this result is that those companies adopting DevOps can actually linearly increase the number of deploys per day as they increase the number of their Devs, which is really interesting because most of us think as you add more developers you slow things down. All these other things get in the way and slow us down, but instead, when you're doing DevOps, you can actually have more deployments the more Devs you have. That's pretty amazing insight. What's also great is there's a 24 times faster recovery from failure. This is extremely important because we know the cost of downtime is high. But one of the findings they've had is that those companies doing DevOps could resolve production incidents a lot faster. You have this mean time to recovery measured in minutes, while the medium or low performers measure that in days. That's a big difference, and so that's something where it can be exciting and there's a real tangible value in having a DevOps culture. What's also pretty fascinating is three times lower change failure rate. So when you're making a lot of changes, there can be this concern that you're less reliable, that you have more change failures, that you could have these issues. Instead, what was found is you actually have a much lower change failure rate. What was also interesting, I found, was that there wasn't a statistically significant difference in the change success rate between orgs where dev deployed code or ops deployed code. If you do it right, that becomes a nonissue, and it really isn't any worse if devs are doing it if you've got the right pipeline set up. What's also fascinating is DevOps organization spend 22% less time on unplanned work. This is break/fix work. This is emergency software deployments, patches, things like that. This can sometimes happen if work isn't done correctly the first time. So, high-performing orgs spend about half their time on new work, 20% on unplanned work. In contrast, the low performers spend about 38% of their time on new work and about 27% of their time on unplanned work. And then you have 50% less time remediating security issues. This one is also exciting because sometimes security is bolted onto so many project teams at the end. Instead, when you have this product culture and you have this DevOps-oriented customer experience-oriented team, security is part of everyone's work, not something that's just latched on the end. You can achieve this through better partnership with security, security capabilities in your automated testing, and even having better security libraries used by the team. It's something that's much more integrated, as proven by these survey results.
Core DevOps Values and Characteristics

I want to take a moment and talk about some of the core DevOps values, some of those things that you have in DevOps. And if you took my previous course about DevOps and getting started with that, I also covered some of these, but I wanted to also reflect on that a little bit. So, as we think about core DevOps values, trust is a huge one. This is trust in each other, trust that people and teams will do the right thing. You'll also see trust, but verify as a principle in DevOps that of course you trust people, but you also have automation and spot checks and things to make sure people are following process the right way. Empowerment's a huge part of DevOps, this idea that teams and individuals are empowered to resolve problems and deliver their service. It doesn't necessarily mean ownership, because sometimes ownership creates its worn bottlenecks, but instead it's about making sure people feel like they can actually control their destiny a bit, and they're not stuck behind a lot of teams or processes that prevent them from helping the customer and solving problems. Of course, the mirror piece of that is accountability. I can't empower people, then not make them accountable. Part of this is the bill that you run at mentality. You'll see me talk about over and over again with DevOps here is that when you build the service and run the service you're now accountable for those choices you made. So we empowered you to build a great service and to ship it. Now you're on the hook. When things go wrong you may get paged. May be up to you to fix that problem. That's a good way to introduce even more trust within the team because you're going to make good choices because you're going to pay for them later. Continuous improvement is the foundation of many of these processes, whether it's lean manufacturing, whether it's DevOps. It's about always learning, always getting better. And it's not trying to find this perfect place, but it's making sure that you're always reflecting on what you're doing and improving that, assuming that everything is kind of an experiment. And it's always by getting new data, getting new information, and simply getting better. And that's often done with data-driven decisions. It's not simply anecdotal, or it's not just the whim of some sort of leader. Instead, when you're changing processes or investigating why something failed, it's about using data, not just emotion. It's about actually making good decisions based on information you planned on capturing ahead of time. It's difficult to improve things that you don't measure, so it's about measuring things and having data to back up your choices. And then finally, one of the core aspects of DevOps is empathy, and it's empathy for the customer, whether that's an internal team who acts as a customer, colleagues, external facing folks, partners. Whatever it is, it's about not doing just what's best for you, but it's also about doing the thing that's going to improve the experience of someone else and understanding where they're coming from. Let's talk about some of the characteristics then of a DevOps environment. What do you see? Well, it's optimized for throughput. You're changing your focus from optimizing the system for accuracy to optimizing it for throughput, a value for the customer. It's mentioned in the book Start and Scaling DevOps. It's a good way of framing that. It's not about becoming as efficient as possible at the cost of customer experience, but velocity, which is that mix of speed and direction. It's the focus on customer value. It's focused on getting things through the pipeline, not having a bunch of releases, or features, or requirements bottled up work-in-progress behind each team. It's about getting that through the system. And speaking of that, it's a clear view of the entire pipeline. When everyone recognizes the entire value stream and doesn't consider anything done until it goes all the way through, you've changed how things work. When definition of done isn't my step is finished, but that the product is out the door, that's a fundamental shift in how your organization will think about software delivery. You're often thinking about this as a product or a service, not project-based. Major change you see with DevOps is not thinking about the start and end of a service. Instead, you're thinking about the lifecycle of a service, and so this has a customer-centric definition of done. Have you delivered value to the end user, whether that's internal or external? Then you're finished. So what you really want to have is that your teams owns, it feels responsible for the idea all the way up to working code. That's the definition of done, from the time I thought about in some sort of business unit or even within IT until that code is running in production and collecting metrics. Then I'm finished. And that goes for project teams, QA, whatever it is. All of that applies. In DevOps environments, you see small, frequent software releases. As my friend Kotay likes to say, it's small batches. It's the idea of constantly having small bits that have a smaller debug surface that have a smaller chance of going wrong because I'm not changing the whole system at once. I'm constantly shipping small batches, getting some feedback on them from customers, and then I can make changes or continue to improve on them based on that feedback. And that's what is important, feedback loops. Power is continuous improvement. DevOps organizations have strong feedback loops, not just from customers back to developers, but from operations to development, from development to ops. Every group is able to provide discreet feedback that gets consumed. It's not about feedback for the sake of it. It's about using feedback to improve. While tools aren't some sort of required component of DevOps, it's not you have to buy X to be DevOps, there is a core focus on using automation and using technology to make DevOps happen. It's hard to do DevOps without some sort of extreme automation, so this is a great way to actually establish repeatable processes and build trust with other parts of the organization, like information security, by actually showing that you're doing thing in a repeatable fashion. Finally, you're simply focused on outcomes, not activities. It's not about the motion. It's instead what are the outcomes we're after? And the outcomes are typically customer-centric. That's what you're focused on, not that we did all of these steps and eventually maybe did something. That's not what's valuable. What's valuable is the result. What's important though is that there are a lot of different definitions of DevOps, and people think of DevOps different ways. The core principles are what matter, not the specific practices. We're going to talk about a lot of steps in this course that you may follow and some you may not. The core ideas are what matter, and so these kind of characteristics and values are the most important thing to pay attention to. So, when you are adopting DevOps, what sort of changes may you see? Well, you may see philosophy changes, this idea of again not focused as much on efficiency as you will adaptability. That's really more important as a core competency. Same with moving from a plan implement model to a do adapt model, not assuming you can master plan software and services ahead of time and now it's just an execution problem. Instead, it's about doing something and adapting based on feedback. Also part of the other philosophy change is all the sudden you treat code as something that's release quality at all times. That is not something most of us were used to, right? The idea that you would finally pull software together at the end with integration testing and then you could release is was what we used to do. Much, much different this way. You're going to see some changes to the team makeup. Instead of project teams where developers are then reassigned and shuffled around after each release, never getting feedback from what they're doing, instead, you're actually keeping that team together so that they iterate and learn together and improve together and make better software. You probably won't have multitasking anymore. Ideally, you don't have developers on three different projects and operations teams on nine different things. Again, you could, based on your organization, but ideally you're keeping teams single-focused on a service. Likewise, you may not have the same functional discipline called out in silos. You still need those skills like project management, architecture, QA, and the like, but it might be something that's actually embedded in some of these teams instead of in silos with their own definitions of done. You probably will see some tool changes, tools and services that enable automation, products that you do use that have APIs so that you can integrate with those things and automate those steps like change management systems or ticketing systems, having things with APIs that you can integrate with and add to your deployment pipeline. There's a good chance you're going to use more open source software and less packaged commercial software. The final change you may see is actually satisfaction changes that Puppet Lab surveys showed that people who worked at DevOps shops were actually two times more likely to recommend their place of work. So, if you do DevOps right, you're going to have an easier time hiring people, you're going to have happiness changes, customers, employees who are happier, and you're simply going to have more responsiveness, more reliability, not having a bunch of firefights on the weekend.
Some Core Objections to DevOps

Let's talk about a few objections you may hear to DevOps within your own company, hey, within your own head, as this is something that can feel radical, and you want to think about well, does this work? So let's say this person says we're already doing DevOps. We have a QA and release team. That doesn't count. DevOps doesn't mean having a team that does QA and testing. DevOps means that you have a cultural shift in how you think about delivering software. It's a mind shift. It's a team shift. It's not about separating out disciplines saying well, we have a team that ships. Having a release team does not make you DevOps. Arguably, having a DevOps team doesn't make you DevOps. You want to have feature teams, software teams, service teams that are focused on a customer-facing service that have a bunch of disciplines embedded that let them ship software faster. This person still may say well, this philosophy doesn't scale to companies of our size. That's great if you're a startup. That's great if you're five people in a garage. We are, you know, a Fortune 100 company that has 10, 000 developers. We're not doing DevOps. And that's really not the case. You're seeing some of the largest companies in the world adopting and consuming this. Even my own experience took this from a couple of dozen developers up to hundreds of developers in about a year. So this is something that has scaled for some very large companies. You're seeing a greater focus, frankly which is exciting, on actually doing DevOps in the large, and so this isn't something that is resigned to small departments, small companies. Instead, this is something where there are lots of case studies. Attend conferences here this year and listen to some of these case studies from large, established, hundred-year-old companies who are now shipping software dozens or hundreds of times a day. Another objection might be well, we don't have the tech skills that startups do. That's fair. It's understandable that companies that are born in the cloud or born as software companies have a different skill set. Now, as Adrian Cockcroft, former lead of Netflix used to say to CEOs and CTOs who would make this claim would be well, we hired those people from you, like we found the tech talent from you. They didn't come out of nowhere. And so often it's about investing in your own folks. So as leadership team, this is why DevOps can't just be a top-down directive or even just a grass roots exercise. There has to be both sides where someone invests financially in adopting this model and training folks and sometimes bringing in some folks from the outside. But many times, investing in the people you already have who have some great talent, you just have to continue to nurture it. Another core objection I've heard are executives don't think IT has value and it's actually decreasing our IT spend. This is a fallout from not having a lot of trust between the units. And so instead, IT is just seen again as a body shop, as a group that is the department of no that makes things difficult. And so sometimes this is where you do have to start incubating these ideas proving that IT can actually deliver a ton of value by delivering software with some unique insight while not being irresponsible and just making bad choices. Instead, IT has some good processes and talent and procedures. They can help you deliver software if you streamline it. So many cases like this, you have to start by incubating the idea with a smaller team, smaller service, and then prove yourself. Start to build that trust back, and then all the sudden you probably will see a change in how IT is invested in. Now I've also heard, and it's a fair objection, is our compliance needs are too complicated for this. That's great that you're doing this for your app that adds stickers to a picture. We actually transfer money between banks. Again, that's a decent issue to bring up, but what we've actually seen in a lot of cases as an industry is that DevOps increases your security profile. You have a better audit trail because all of the steps are automated. Operations teams aren't just logging into boxes and making changes. Everything is done in something that can be audited later. You're seeing security built into the deployment pipeline where you're doing code scans, where you're actually doing penetration testing afterwards. You're doing a number of things to make sure that your code is actually more secure end to end, and then in production running in a way that's much more secure by using automation. So in many cases, you're working in the most compliant environment possible. You're able to do DevOps. There are a lot of good case studies here. Now you may have an objection that says look, my company buys commercial software. We don't build anything. That may be the case. Many shops are buy versus build, and so there is a massive shift though right now to becoming build versus buy. And there are certain things you always buy. You should not be building your own database from scratch right now. That's probably a waste of time. You should not be building your own container orchestration engine. There's good technology out there. So in many cases it's about choosing the right things to build, not just simply switching over and building your own CRM system. But in many cases, some of the problems you're trying to solve nowadays don't have commercial software that does it. Or if you do, you will be doing a very weird customized flavor of it. So as you start thinking about customer experience, in many cases you're introducing new software experiences that weren't thought of 10 years before by your company. And so it's about looking at some of those unmet needs, looking at what job to be done your customers are after, and you're probably going to find some software that you need to build to satisfy that. One that we definitely see from time to time is simply this is just another fad. I can wait this out. This is rational unified process. This is maybe even agile. This is maybe something else, that SOA. Alright, these are things that I can just wait this out. My career will continue with no change. I don't think that's going to be the case here. I think we're entering a place where this is the new normal, and this idea of being customer-centric and using software as an engagement mechanism with our customers, that's not going away. As we become more mobile, as we integrate partners differently, as we have all of these different experiences, software is going to power that. And if you don't become software oriented, you're simply going to be overtaken by someone who is.
How Enterprise DevOps Differs From Small DevOps

Now one fair objection is what we talked about, this sort of does this scale to a company of my size? So let's talk about that a little bit because we can get this out of the way right now. DevOps is going to work differently at enterprises than it does at startups. What works for a small team may not scale to a more complex organization that has more functional silos, that has more distributed teams. It might have some out sourcing arrangements that doesn't have a staff that can be split up into a bunch of project teams, may just have some technical debt. Lots of different things that mean an enterprise scenario may be different. The core principles should be the same, some of the practices will be the same, but it's naive to assume that everything simply maps from any sort of environment, that all DevOps looks alike. That wouldn't be the case. One reason is you have more stakeholders. You have more parties to communicate with. You have more folks to get consensus from. Your deployment pipelines are probably more complex as they span different sort of considerations around compliance and security or different functional units. And so you simply have more folks to keep in the loop there, and that's something that you have to factor in to your procedures and even your ceremonies around DevOps. You're going to have some functional silos in place. Now, again, I've proposed that some major change management and reorganizations may be needed. And that's going to be one of the biggest challenges you face when trying to scale DevOps across your company is this sort of organizational change management. But the constraint may be all over the place. You may have challenges where planning and creating requirements happens. You could have a bottleneck in actually getting environments. How many developers struggle to just get a dev test environment? Or the dev process itself may be a bottleneck, or how you do testing, or how your ship code or monitor. Different companies are going to have different places where the constraint to their flow is, and these functional silos, you have to start breaking that down to make that happen. Again, one of the fastest ways to do that is to get folks in a room and actually do a value stream mapping of what it takes to go from an interesting idea to production. What are all those steps? And make it visible so folks can see the actual visceral impact of some of the silos they have. You may have existing technical debt to manage. You're not starting greenfield. You're not starting out with just a brand-new idea and no technology and you get to pick anything. Instead, you have systems that aren't automation friendly. You have existing systems of record where you have data that may not be able to be changed all the time. And so that technical debt may change how you think about DevOps and how you might add proxy layers that sit in front of legacy systems. You're probably not DevOpsifying your mainframe. That's okay. There are ways to get around that, but while still respecting that that's where your core data is. There may specific compliance needs. And so as you think about using public cloud or using open source, there may be considerations that you do have to factor in. Maybe you can't do an Amazon Web Services or a Google Cloud Platform, and so you feel like you can't do DevOps if you don't use cloud. That might not be the case. There's a lot of great automation that sits behind the firewall that can still help you do a DevOps environment even if you're not using native cloud services. One that definitely comes up is your company may have some existing outsourcing arrangements. This makes the colocation requirement of having teams physically together difficult. But we've seen many examples of teams that are still remote and working well together, but outsourcing as a traditional means of saying I'm going to hand over operations or I'm going to hand over development to a third party, it will be very hard to keep that model and actually do DevOps. Not impossible, but unless you have everyone as stakeholders in a visible deployment pipeline, it is hard to actually deliver this sort of process. So what's this course going to look like? Well, we're going to spend the time talking about a week of DevOps. What are the steps? What are the different things that get involved? An example there on the right. So we're going to spend the rest of the course doing this, going through specific processes, specific things, tips to get started so that you can actually finish this course and feel like you have a whole handle on what you want to do next and maybe make some variations. I would expect you would because this won't exactly map your environment, but gives you some ideas for exactly what to do. We're going to talk a lot about core practices. What are those core things that matter? Again, your implementation may be different, but the core practice is the same. We're going to talk about and recognize some of the change management you need. Where is something going to be completely new? Where are going to be thinking about things differently for the first time? As you think about services as teams and you think about not just having project teams, or you think about what kind of meetings or notifications to have, we want to make sure that we try to make it clear so you have a lot easier time implementing this. And then we leave you a lot of getting started tips. How you could you start tomorrow on doing some DevOps in your organization? How do you start incrementally introducing some of these changes with very practical advice?
Related Books to Read

Besides living DevOps, I actually read a lot, and I think it helps me get some different perspectives and makes sure that I get outside of my own bubble and experience. So, there are a few books I thought I would recommend for you to take a look at. Well, there's The Phoenix Project, which apparently everyone who does DevOps needs to read at least once. It's a great book. It's a more story-based scenario that says look, let's walk through an actual fictional example of an enterprise that has challenges and starts implementing these sort of DevOps practices. Really a good story, good read. One of the new books that came out from the same author is now meant to be a companion book is called The DevOps Handbook that definitely influenced some of my thinking around scaling DevOps and is definitely worth a read. Some good practices, some good detailed implementation information. Starting and Scaling DevOps in the Enterprise. Really great book as well. Read this recently. Again, some good information about actually taking DevOps practices and respecting the enterprise constraints, but being able to introduce this successfully in those environments. Lean Enterprise is a great book. Again, it talks about doing lean and delivery-focused things in a large company. Well worth the read. There are also some potentially non-traditional books like The Goal. This is one that really started the movement, and this was more again from a lean perspective and looking at the theory of constraints and applying some of these practices that worked in manufacturing, and thinking about those and surfacing this idea of throughput and delivery in an IT environment as well. You could start to apply that. It's not IT-centric, but it gives you a lot of ideas which actually inspired The Phoenix Project. A little more non-traditionally, but I enjoy Team of Teams. It's a great book that looks at the idea of changing the culture to become more nimble and adaptable and not have necessarily this top-down planning organization that loses to more nimble competitors. In this case, this is dealing with actual armed conflict. But at the same time, a lot of good at insight and advice about those that are trying to change an entrenched culture. Similarly, great book American Icon looked at the transformation of Ford from a company that was dealing with the automotive crisis, and they had to, again, become data-driven and empowered to actually save their company. And so the story is often about how did they break down silos, how did they increase transparency, and how did they become more adaptive to customer needs and not just simply giving into the corporate inertia. Same with Creativity, Inc. It's the story of Pixar, but again, how does a very nimble, quick moving Pixar get acquired by Disney and still survive? And it was a really, again, good story on those changes that you have to make and how do you somehow make sure you keep your core culture and your core principles while becoming more nimble. And then finally, the great book Designing Delivery from Jeff Sussna. It's a different way of thinking about services, and it's important to read as you think about what are the new roles of things like QA. What are the ways to think about actually delivering a service successfully to customers? All of these are great books to read to simply give you a different perspective on what you're trying to solve and not just see DevOps as a set of practices or a set of tools you have to implement, but respecting some of the organizational change and some of the actual psychological components of what you're trying to do here.
Summary

So in this first module, we did a quick overview. We talked about why DevOps matters, looking at this software imperative and that companies need to be thinking about becoming software-driven or else they're simply going to be overtaken because there's so much competition, and most importantly that we all as consumers have different demands on the companies we interact with. We expect to have a core digital experience. We're not going to tolerate things that don't give us a great experience any longer. But there are challenges of becoming software-driven if you don't understand some of those things holding you back, whether that's tech debt, or culture, or lack of trust. You can't simply gloss over those and immediately start shipping software every day. We saw the benefits of a DevOps culture using real statistics from real surveys that show that DevOps teams are shipping software faster, having a lower change failure rate, being able to solve problems faster, having more secure code, less randomization. This isn't just a movement where it's just another fad. Instead, this actually dramatically impacts the health of your company and the bottom line. We talked about some of the core DevOps values, making sure that those are something that you're thinking about as you're doing this, not just adding practices or process, but actually recognizing why you're doing it. We talked about some of the changes philosophically, team-based, that come with DevOps. We addressed a few of the core objections you may experience yourself or have within the team. We respected, I think, some of the enterprise-scale DevOps challenges as you have different types of organizations, different kind of partnership arrangements, different types of technical debt. And while that's not an excuse to say well, we're going to do just a half version of DevOps, it does mean that your practices may be a little different to reflect that. So coming back from a great conference saying this is how Spotify does DevOps, that's how we're going to do it, may not be the same thing. Finally, we talked a little bit about the week in the life viewpoint. We talked about some of the learning resources, some of my favorite books in this space that will hopefully even make you better at DevOps. So, I hope you're excited about this course. We're going to jump right into Monday in the week of the life of DevOps here and talk about some of the core practices that you can get started on.
Week of DevOps – Mondays
Overview

Hi, my name is Richard Seroter, welcome to this module in a course about implementing DevOps in the real world. We kicked things off in the last module and looked at why DevOps matters, and here we're going to start our week in the life of a DevOps team and look at the first set of core activities you might have in a regular week of a DevOps team. Specifically, we'll start by talking about the value of team standups and a standup exercise, we'll talk about using the on-call engineer rotation, and what that means, we'll discuss planning software sprints, we'll talk about how to review new software requests that come into the team, we'll discuss some things to think about when merging and testing code constantly, and then we'll wrap up.
Holding Organization and Team Standups

So, as Monday morning gets started, we have our organization, our team standup that goes on. So what is that? Well, it's a regularly occurring meeting to inform people about milestones or blockers, help needed, things like that. It's not a status meeting. If you have a scrum of scrums, you have a team of teams, this could be a couple of meetings, one for the entire org, like all the DevOps teams, and another for each individual team, each service team or app team to sync up on the day. So who attends? Anyone who wants to in the particular teams, but there should be a representative from each of the individual product teams or application teams. Ideally, the person on call for a given team, and we'll talk about on-call in a few moments, for the given team as the representative there. Why does this belong in DevOps? Well, a few reasons, transparency, one of the core principles there, as you want to make sure that you're able to elevate work and make the work visible, raise any issues, things like that. It's a focus on throughput, because you're trying to remove blockers, and that's done by raising the need for help, and also focusing on delivery, trying to orient the team on what matters the most, specifically, helping the customer. Let's talk about this in detail, so why a daily standup actually matters. Well, first of all, it does orient everyone. Standup is important, not just as a ceremony, but as a relevant way to give situational awareness to the team. You're making sure that, as we all have independent teams, building separate applications or serving different functions, you're able to raise the most important things, and that's the key, you're drawing attention to important information. If you have functional orgs, this can help orient all those parties to the issue of the day, severity 1 issues, or requests for pairs if you need some help coding something, if there's an emergency patch going to production, making sure everyone is aware of that, and might have to plan accordingly, or maybe a specific customer who is in need and had a tough day, and we want to make sure if they call again we give special attention within our application team. The key is, you're using this not, again, as a ceremony, and saying, if we have a standup therefore we're agile, and we do DevOps, instead you're focusing on the principles which is orienting teams, drawing attention to important info, and also coming together to remove blockers. If someone is blocked, it's not about just moving to the next work item, it's about figuring what that block is, and everyone comes together to help remove it. This is an easy way to raise awareness and needs of help for these standups. At Pivotal, where I work, every standup has a brief section for helps, where someone can raise something they struggle with, and ask for someone to pair with them to fix it. Finally, this facilitates continuous improvement. I've been in standups that dragged on too long, and each team out of 10 or 15 would actually go through and simply say, no updates, which was annoying. No one wanted to set through that. So instead of just having the ceremony, we looked at how do we improve that, and instead changed it, and we only talked about the calendar for the day, it was a shared calendar that the team used. What were major deliverables going out, what are things folks should be aware of, and then finally any major customer issues that people should all be thinking about as they're maintaining and building their service for the day. So you can also be using this as a chance to constantly improve the process. Get started by setting up a shard calendar with some milestones. Make sure the team can see what's going on. Again, make it visible if there's major deployments, and things like that. This is going to be helpful, especially if you have a shared support team, to make sure they're aware of any major deployments going out or infrastructure updates, so consider a shared calendar that everyone can look at. Keep these things brief. Avoid status updates. Have someone who is really keeping it on the schedule, and making sure this doesn't become something where everyone is going through or talking about their day, talking about major things. It should be 3 or 5 minutes, something that's not spending a lot of time. If teammates are super long-winded, I've actually seen teams who actually pass a dumbbell around so it's so uncomfortable to hold it for so long you end up speaking quickly, and every person who speaks has to hold the dumbbell, and it keeps things moving pretty quick. Nonetheless, the key is, make sure that this is always on the calendar, never cancelled, and it's a place for the team to really get up to date. It's important not to force everyone to speak. It's okay if there's nothing to ask for here. People are going to tune out if everyone goes around and just says no update or nothing to add. So instead, open it up for helps or just share events or major things going on, and if someone has something to share, they will. One of the most important things you can do as a leader on the team or in the organization is set an example. If you're an executive, hopefully you're attending these and actually asking for help yourself, or being hyper-transparent. Set an example to prove that it's safe to raise issues here, or to ask for help or say you have a blocker. Some teams have been conditioned to not necessarily show that there's bad things going on, and so it's often up to the leader on the team or within the organization or in the company, to show that it's now safe to ask for help, it's safe to make mistakes, and admit something might have not gone well.
Assigning an On-call Engineer

Next up in the day on this Monday you may be assigning and on-call engineer. So what is this all about? Well, the idea of rotating team members through an on-call rotation. The on-call person is responsible for outward-facing support, issues, defects, things within the team to keep from randomizing everyone else. Who is involved? Hopefully it is always just a different person on that application team, it rotates through on a calendar, hopefully very predictably so people can plan accordingly, but the idea is everyone on the team is in this rotation. Why does this belong in DevOps? Lots of reasons. First off, it develops empathy. You make sure that when you're running the service and you're talking to customers, you're dealing with your end users, you're seeing what their pain points are, you're not disconnected from that. Instead, you're thinking about the whole user experience. It helps put a focus on the customer, it helps make you more responsible, it helps improve quality, and makes sure that developers are seeing what the pain points are so they can improve the service. This helps put that focus on the customer, making sure that the on-call engineer, and then through the rotation, everyone, has a good sense for how people use the application, and what goes wrong, and being able to fix that and actually engage with the end user. One thing I've seen is this helps encourage code instrumentation and improvement. You end up building better logging, monitoring, automation, that might have fallen to an Ops team if you were just throwing it over the wall to that group. But when you build and run your service, you actually care about some of those operational components. The key is that everybody who built the service and developed it, gets to experience what it means to live with the architectural and coding decisions they made, something mentioned in the DevOps handbook, and so that makes a huge impact on how you then think about the service. On-call engineer attends the daily standup. They probably submit feature ideas that people consider for the backlog. I've worked on many teams where I can tell who the person on-call is for the week based on the feature requests that come in, because I know they were working it and then they asked for better instrumentation where they realize that there was a really annoying bug that they might not have cared about if they weren't running the service themselves. A really important part here is that it helps other team members stay focused. It keeps other team members from being randomized by inbound requests. Those defects are important, or those deployment requests, or whatever things you might assign to your on-call person, but you don't necessarily want to keep context switching between the folks who are trying to code very day. So this should rotate on a weekly basis, you don't want to burn your people out, but you're keeping people focused by having others focus-dedicated on making sure you're running that system effectively. It's important not to assume that that person gets anything else done that week feature-wise. Their job is to handle tickets for the team, respond to escalations, and the like. In my previous job, we actually had the on-call person responsible for any production deployments. It was very automated, but that person knew that that was their role that week. It's a great way to learn the system, but of course being careful, because on-call may be tough if you have a really large service and it's massive, and no one person knows everything. Finally, it's a great way to record some experiences to help the team. There should be a shared Wiki or some sort of place where you could have some lessons learned for each on-call person, so as you onboard new people, as other people take on the on-call rotation, they can recognize that, hey, this is how I fix this particular issue, and restart these services, or hey, this is something that's tricky about this environment. You might want to make sure you do this particular sequence when you bring the service back online. A few tips for getting started today. First, sign up for a pager service, probably not a traditional pager because this is a modern generation, but you want to have a service, there's plenty of services available, where you can have a clear notification process so that when things go wrong you can notify the on-call person effectively, reliably, you're not hunting around for particular mobile number or something like that. Really, really important to make it easy to find out who is on-call for the teams. If you have a number of different application teams, you don't want to have to hunt around in the case of emergency to find out who is on-all. This could be something you add to your channel and your chat tools, anybody looking at the channel can see who is on-call, or it could be on a Wiki, it could be on an internet page. Whatever it is, make sure it's very easy that frontline support people and others and other teams, know who your on-call person is for the week, so you don't, again, randomize folks with people jumping into your chat room asking who's on-call right now. Make sure you constantly have some dry-run exercises to flex the process, to try and see what happens. See how people respond to this, making sure that your process is good, that you can improve it as necessary. Make sure that you understand these escalation processes, and how you do all your deployments and the like, so that the on-call person doesn't feel intimidated. Finally, something I really enjoyed/hated in my last job was the idea of an executive on-call, so one of the executives was always on-call just like a developer. So when I was vice-president of product, my job was to also be on-call on rotation, and when there were high-priority issues I would also get paged, and I would have to jump onto a bridge and monitor the situation and help improve that. The value there was it helped remove blockers, you could trust that that person would be able to do that, but it also gave me a better understanding into customer issues, helped me prioritize accordingly, because I was a stakeholder in this. So I would encourage you, if you're introducing DevOps to your organization, make sure that your managers, and hopefully even your executives, become part of an on-call rotation so that they can empathize with the customer and with your Dev team, so they understand what you're dealing with.
Planning the Next Software Sprint

Such an important part of DevOps is delivering, and shipping software, and making sure that you're constantly taking into account user feedback and improving what you're delivering. And so sprints, the idea in an agile world of having software sprints that are short, but they're bringing in new work to a current development cycle. Your sprints could be a couple of days, it could be a month, but the idea is you have some sort of defined window of which you ship at the end of it. Who is involved in Sprint planning? Well, the whole application team, including a product owner, is part of this sprint planning process. You can imagine this belongs in DevOps for lots of reasons. You can focus on small batches. It helps with your delivery pipeline, it gives you fast feedback. If it's done right, you understand how your sprint cycle works, your planning software, you deliver software, get feedback, and that's going to influence your next sprint, versus traditional classic ways of waterfall processing in many cases, where you would do a massive planning exercise and do requirements for months and months. Then you would get a development phase and a testing phase, potentially not getting any real feedback even from users, until way late in the process when it was too late to make any major changes. The software sprint model in agile is about trying to get feedback into the process and constantly iterating. A key part of DevOps is how you approach sprint planning. So the product owner maintains some sort of backlog, a product owner is responsible for maintaining a prioritized backlog of activities based on the cost of delay or business priority. There is some mechanism they're using to keep this prioritized based on customer input, and corporate priorities, and team input. All these sort of things will influence the stories on a given backlog. It's important to start your meetings with a retrospective, you rewind a little bit, and try to figure out, how did the last sprint go? Were we doing things successfully? Was there a flaw in the process? Did something take way too long than we thought it did? These meetings are ripe for continuous improvement themselves, so even the sprint planning meeting should be up to continuous improvement. So do retrospectives at the start to figure out what worked, what didn't in the last sprint, and not being afraid to change the process. Something that was new to me when I was getting into DevOps in agile, was the that the team itself decided the amount of work in the sprint. So, while the product owner decides what work is in the sprint, the team decides how much they end up taking in. The team takes responsibility there. So they discuss the items in the backlog. They might decide on some general implementation information, but that product owner should have some requirements in place ahead of time, and what the team is doing is figuring out how much work can they accomplish. There's probably some velocity they have in mind, based on how much work they traditionally do in a sprint, those could be points, those could be however you measure that in your world, but you have some idea of what your traditional velocity is, how much work can you get done. Then the team takes responsibility for scope. Instead of someone coming from on high saying this is what you will ship in the next two weeks, the team can say, look, based on these stories, this is when we're full. And what that means is that team takes a different level of ownership when they've signed up for that amount of work. I've witnessed this over and over again as the team self-governs itself, and they're keeping themselves honest on work, and if they're falling behind, other people jump in, because they collectively committed to an amount of work. There wasn't some manager who just told them what to do, they volunteered for that amount of work, so it's a big change versus a sort of top-down direction. What's really important that I've seen is that the sprint scope doesn't change. So once you commit to a sprint, the idea is you should not be scope creeping. You're not taking in new stories, you're not having an emergency that just comes in and everyone drops everything. Now, in some cases you may have to do that, but by treating the sprint as sacred, it also forces you to keep shrinking the sprint, because if you have a 6-month sprint, of course you're going to have some scope creep, of course you're going to have things change, but by having 1 or 2 week sprints or even month-long sprints, you can really keep that fairly fixed. Worst case, if something does have to come in, it's important to subtract something back out. So one of the best ways product owners can build trust with the developers is showing that they're looking out for them, and if there's new work that has to come in, you subtract the same equal level of effort out and put that back on the backlog. Really, really important that the team always ships at the end of a sprint. Creating small units of deployments, that test surface is smaller, you can ship that. I've seen that if you don't force this, if you say, hey, we might have three sprints before we ship something, inevitably things drift out of the sprint, or things don't get finished, because there's not the same sense of urgency. When I know I am shipping at the end of every week, or at the end of every month, or whatever your window is, it's going to force you to wrap things up, and you're not going to have that buffer where you can just afford to let things slide because you'll try to make it up in the next sprint. Now in some cases, if you're doing continuous deployment, then you may always be shipping. You might be doing continual planning, which I've done before as well, where there isn't even a formal sprint planning session, but the product owner keeps a prioritized backlog, and the team keeps grabbing the items from the queue, maybe with a short meeting to prioritize the work or explain the work, but you don't have as much rigor or ceremony around an actual planning meeting. So if you've gotten to that maturity level, fantastic, but that's something that takes a little while to get to. One of the best things I've seen is constantly putting pressure on the sprint window size. It should keep shrinking, because it's about, not only small batches, but by shrinking the window you keep finding inefficiencies that you tolerated through brute force or heroics before. When you have six months, you can sometimes mask inefficiencies. Hey, maybe testing took two weeks and that's not great, but that's fine in a six-month window. I can't take two weeks of testing in a three-week sprint, that's clearly ridiculous. So, as you look at certain things around infrastructure, around testing, around your design, even your discussion and your planning, as you keep shrinking the sprint window, you're going to find things that might have been hidden bottlenecks that come to the surface, so put your pressure on yourself, if you're a dev team, or if you're a product owner or manager, put it on the team by aspiring to continue to lop off time off a sprint and get down to a nice quick window that forces new efficiencies. If you're getting started with sprint planning, make sure you have a product owner. It's such a key role, and it should be full time, not shared. You don't have a product owner crossing four different teams, have dedicated product owners. Maybe you are switching someone who was a business analyst, there could be some natural transitions, but the key is having someone who is responsible for your product backlog, who is a facilitator with your stakeholders, whoever those may be, and is looking at the right strategic priorities for your team, and you're putting the pressure off of your devs to try to prioritize. Instead, your product owner should be the person who understands the life of the product. I'd recommend beginning with one-month sprints if you're new at this. If it's too short to start, you're going to struggle to spend any time developing, you're going to spend too much time doing other things just getting ramped up. But if it's too long, it's easy to scope creep, it's easy to let stuff slide. So start, a month is a good time. Try to start with that if you're new at this, but keep trimming time. Get down to two weeks, get down to one week. Figure out different ways to surface up those things that are inefficient. I've often learned the hard way that you should not be using sprint planning sessions to design software. Instead, those become 6 and 8 hour meetings, and everyone hates each other by the end. So after a few of those, we learned that you had to do more requirements work, more quick planning sessions before the major sprint planning session to really understand the work better, and only use sprint planning to take in the work and quickly decide on level of effort, not to actually design the solution. I mentioned it earlier, but it's so important, that if you add work to a sprint, you have to subtract work from sprint. Treat that velocity as important, that if you do 50 sprint points of work, however you measure that, then trying to do 60 is unrealistic, unless somehow you have some amazing way to add new people or add new productivity within the team. so, simply, it's an important way to build trust between your product owner, your engineers. Don't just add new work and expect them to finish that. Change the process regularly to improve. How you plan, how you execute, all those things are ripe for change. I would typically watch our team constantly change how we scored work, because if you had a number of points for a particular story, teams kept using that max score for a story, and it became really tough to predict, because am I using the max score because it's about that much work, or because it's ten times that much work. So figure out different ways to decompose work, figure out new ways to score points, figure out ways to measure velocity. All these things should constantly come up in your retrospective, so you're simply getting better at delivering software, and you're getting more out of your team, and they feel more ownership and a better chance they can succeed.
Triaging New Feature Requests

Next up on our Monday, we're triaging new feature requests, things coming into the team. So what is this? Well, it's basically a chance to review new features, bugs, from the customer. The customer could be internal, the customer could be an external person, a partner, and ideally this is on a regular basis. Who's involved? Well, it should be the product owner, it should be team representatives, probably the on-call engineer, potentially some cross-functional stake holders, and others, people who care about what work you're taking into the team, what things are your priorities. Why does this belong in DevOps? Lots of reasons. Continual improvement. You don't have a static set of requirements if you're building a service. You're constantly getting new feedback from frontline support, from your end users, from your metrics. All of these things are showing you new things about usage, about defects, about unused things that you should remove, about things that are being used more than you thought, that you should spend more time on. It's about continual improvement and spending time thinking about what makes that customer's experience better. So it's important to do realistic reviews of new requests, and first of all, doing it regularly. Reviewing new items, figuring out what should be accepted into the backlog. Typically, I've done this on a weekly basis in some sort of a regular cadence, where the team gets used to doing these sort of reviews. It also helps users know that you're being responsive. There's nothing worse for most of us that when we submit a feature idea to our favorite software board online and it feels like that thing goes nowhere. It's a black hole, nothing seems to get updated. That's almost worth not even taking it in, because now I don't even know if you've seen it. So by being responsive to those folks, you're asking for their feedback. Let them know that you've actually reviewed it, that hey, you've rejected or accepted it. Make sure that they see a constant rhythm, and then they're going to contribute more. If they don't feel like their voice is heard, your users aren't going to submit ideas. If they do, you're probably going to get some great insight. But you shouldn't be taking everything. You're going to want to prioritize things you're going to want to reject aggressively and wait for those things to come back. Cross-functional participation is super important here. So, this makes sure that things that might be unimportant to one constituency could be useful to another. Support, operation, security, development, compliance, all these different groups have different sets of priorities, and so the product owner should be able to hear from them. I've sat in plenty of these meetings where I would be feature triaging something that came in, going, I don't think this really matters. And a support person raises their hand and says, actually I've gotten six calls on that in the last two days alone, it would be great if we fix that. I didn't know that. That's really important, and all of a sudden I would take that into the backlog and prioritize it accordingly. Or, something I thought was very important, I would find out from one of our customer-facing support people, that hey, nobody really ever talks about this feature, and maybe if we look at our metrics no one seems to care or even use that much, maybe we shouldn't take it in. So it's important to have a lot of different perspectives here to make sure that you're properly reflecting the voice of the customer. You should be immediately adjusting your backlog priorities after this. If your backlog isn't constantly changing priorities, you're not reviewing things enough. So after each meeting, your product owners should be inserting new stories into the prioritized backlog, and maybe even retiring some other stories that aren't as important. It's a very fluid thing, and that's great, because that means it should be constantly reflecting what matters most to your product or service. So for getting started, a handful of ideas here. First off, triage features AND bugs in this meeting. This isn't just for people coming up with brand-new ideas. This is for people who are also submitting bugs, often from the internal teams. If a developer comes across a major bug, and they file an issue in your source control system or wherever you file issues, then this is a great chance to go through them. Now, in my previous job we were pretty aggressive with bugs. You either did it now, meaning the current sprint, you would do it in the next sprint, or you would do it never, and we would not maintain a bug database of 100 or a thousand things, because you'll never get to it. Instead, if it was important, we would fix it now or the next sprint. Otherwise, we would wait for it to come up again. That's a pretty aggressive approach, but sometimes more effective than burdening the team with a giant bug database that they're never going to see and never going to complete. Now you may have certain sprints where you do nothing but bug bashing, and maybe if you saw that you were doing a lot of feature meetings that you took in a lot of bugs, you might take the next sprint and do that, lots of ways to still address it, but you should be looking at features and bugs, because both are prioritized work that should be visible in your backlog. I've gotten in trouble in the past before where bugs were invisible work, and all of a sudden velocity was going down in the team and we couldn't figure out why, until we realized the team was rightfully trying to bash bugs, but those weren't stories on the backlog, they weren't even stories in the sprint board, and so it was invisible work. All of the work, all the requests, should be visible. As I mentioned, the support team, the on-call engineer should attend. Make sure this isn't something where you just have a couple of engineers and a product owner deciding everything. Instead, you want a good voice, you want a voice of your stakeholders, and that may be people outside of just your application team. I would recommend that you reject at least half of the requests that come in, not because they're not great, but because you're not going to do them all. Now, unless there's a very low volume, then of course you may take them all in, but in many applications and services, the number of requests coming in is more than your velocity out, and that's okay. That means you have an active user base, that's great. I would rather reject the customer's request and give them a reason why, then take it into a never-ending backlog that we'll never look at. That's worse, that's false hope. So instead, be critical. Really make sure you're only taking in the things that seem to matter the most, and hopefully these are data-driven decisions, it's not just based on what you feel like doing. But by rejecting the ones that aren't as important, you're keeping your team focused. And then finally, as part of that, the product owner should be constantly retiring and adding these stories. When you're reprioritizing, things that start to fall off the list should get away. I used to try to make sure I never had more than 100 things in our product backlog. If there were more than that, then those would go into an icebox, or those would go into something else that would just be archived, because at that point I'm not going to get to those. There's no real point in keeping them. Now, again, they may inform future decisions, but your backlog should be tight, it should be things that you could finish over the next six months, and anything more than that, you're probably not going to get to.
Merging Code with the Master Branch

We've had a busy Monday so we're going to finish our day by merging code with the master branch. What is this about? Well, it's about taking some of the offline work we may have been doing in our distributed source control system, and adding it, integrating it, with the rest of our code base. Who is involved? It should be all engineers, and we'll talk about different strategies in a moment, but really everyone should be involved in this and not having a lot of different long-lived features branches that you're stuck integrating with later. Why does this belong in DevOps again? Small batches, keeping your code at production quality at all times, limiting your work in progress, and limiting the amount of integration you have to do later. These are all very important parts of DevOps. It's about fast feedback. If I'm working offline for weeks and months, I'm not getting feedback as I'm building things. I want to make sure that I'm constantly keeping my code in good quality, that I'm constantly keeping a release version of my app, and I'm working in small batches. So continuous integration drives confidence, my title here, and it's really this idea of when I'm constantly integrating my code, I have confidence that I have a good working system. Many of you, myself included, have spent time on multi-month or multi-year projects where the integration testing phase was so crushing, because you've just spent all this time building code, but you kind of knew as you were going that you didn't know if everything was going to work well together, and then you have this multi-month effort to actually integrate all of your work with infrastructure and code modules and backends, and all these other system, and it's exhausting, because those things weren't necessarily tested along the way, you've got all these other defects, timelines slip, that's no fun. Instead you want to be thinking about continuously integrating. So test coverage is key to trusting your automation. You want to make sure that you've got a good number of tests that you can make sure, if I'm going to have automation, it's combining my code, it's not just about building it, it's about running through my test suite. And so to do that, I might have to go back to applications that didn't have good test coverage and add it. That's something Pivotal does with most customers as we help them automate, is go back and add some new tests to their existing services so that they can introduce continuous integration, which is often a game-changer for so many companies the first time they start continuously integrating their code. But to do that, you have to start with your tests. Fast feedback, though, reduces wasted time later on. That continuous integration process runs on check-in, and any breaks are immediately addressable. I'm not trying to fix things I might have coded months ago when I finally integrate, and I run my tests three months later, I don't know what happened there. I don't want to waste that time. The book Starting and Scaling DevOps in the Enterprise mentioned, the purpose of the automated testing is not to reduce the cost of testing, but to enable the tests to be run on a more frequent basis to provide feedback to developers in order to reduce waste in new work. So the idea is that you're constantly integrating code, you're running your tests, so that you don't waste time later on. What's exciting though is frequent tests mean a smaller debugging surface. We've all been there where we build code, we deploy it, and something that we might have done a long time ago broke something, and now you're spelunking through your code trying to figure out what happened. Instead, when you're doing small batches, you might be digging through a couple hours' worth of work, and so you've got the smaller debugging surface, much, much easier to see what might have gone wrong. It's important to aim for green builds, meaning it was successful, and so green builds are a good focus. That's going to be something that's important. You want to make sure that the team is constantly keeping their code at production quality, you're constantly keeping a green state. Now really mature teams, over time actually don't fear red, because they know how to, and where to fix something. So, ideally, you get to the point where you do see red, that doesn't worry the team, it doesn't paralyze you. Instead, everyone knows what they need to go do, you have something where you build an actual skill in going back and figuring out how you recover from that failed integration test and get back to a green state. So for getting started, a few pieces of advice here. Source control repository is a must-have. You're not even starting this conversation if you're keeping source control on a file system somewhere on individual desktops, so you do need to invest in a good source control repository system. I'm a big fan of visual indicators of build status. In my old job, we had lamps set up in the development rooms, and when the build broke, the lamps turned red. When they were successful, the lamp lights were green. These things are easy to set up, and really something where the whole team then knows the state of things, and frankly, anybody walking by the room knows the state of things. At Pivotal, we use a continuous integration tool and put the visual pipelines up on TV monitors throughout the entire building. Whether you're an executive or you're an engineer, you can see the state of the build of a particular subsystem. Again, really quick visceral feedback, and if you constantly see something red that's probably a problem, and so visual indicators are huge as you start elevating what the team is working on. This is somewhat controversial. I see some religious debates online as people discuss, should you be using feature branches, should you be checking into trunk or master, what's the right way here, but a lot of consensus seems to be around that branches are a source of waste, that if you have these long-lived branches, and people write code in a branch of code and eventually merge it back in, that that's a lot of work you have to support in there. So if I'm only merging occasionally, that's a lot more code I have to merge in, I could have a bunch of inconsistencies with the last thing I was dealing with. So the longer I let teams work in their branch in isolation, the harder it's going to be to integrate that later on. And so, while it may make sense in some cases, and some teams that do releases put their release into a branch, of course that makes sense so they can keep working on master without their code, but ideally with developers, don't have developers all working in their own branch and then being forced to smash things together every few weeks. It's going to be a lot of pain. So keeping a large source control repository green is not easy, so it does take a certain discipline, and you may find it easier to just let everyone work in their own branch and not break each other, but you're going to pay for that at some point. And so I've successfully seen teams working together in one repo and checking into master every night, not just working in branches, but it does take a certain discipline. Finally, making working code a forcing function. You know I like this idea, and Start and Scaling DevOps mentioned this as well, is that when teams don't necessarily just rely on processes and documents, but instead, look when my code works, that next thing should be ready, and it's visible, because I might have visual indicators, and so this means it's ready to deploy, or this means it's ready for a team to start doing usability testing or what have you, but working code is the measure of success here. I don't need to have a meeting that says when we're ready to ship. I have working code when I have working code, which should almost always be the state.
Summary

So we finished our first day of DevOps. Hopefully nobody quit and enjoyed themselves, but the idea is that you're going to have team standups. You're going to have this every day. You're going to see this actually every day of the course here, because it's important to have that rhythm, and making sure that teams are aware of what's going on. You're going to have an on-call engineer rotation, ideally. This builds empathy, this makes sure that you don't randomize your team, and it makes sure your customers are getting a very responsive experience, while your engineers are getting a better understanding of how to run the system and keeping it online for your customers. Planning software and doing it efficiently and quickly, is really important. If you're doing agile and delivering software in a constant rhythm, then you're being responsive to customers and you can adapt to changes successfully. But you want to make sure you're planning successfully, and so using continuous improvement in this process, make it better and better. Do planning up front so that these are quick meetings, and make sure that you're measuring how much work you're doing, and not adding more work than the team can take on. Reviewing your software requests is an important part of your lifecycle, making sure that you're taking in feedback from stakeholders, from support folks, from metrics, from other places, and using that to prioritize new work, whether that be bugs, whether that be new features, but you're constantly triaging. Merging and testing that code constantly is extremely important, as you want to be in production quality or release quality at all times, and having good test coverage lets you introduce continuous integration, and by making sure that you're having a master branch that you're checking into regularly, you've got something that's always in a good shippable state and you're not going to pay a large integration cost later on. We're going to jump into Tuesday in the next module, and keep going through some core principles of DevOps and how you can practically implement them.
Week of DevOps – Tuesdays
Overview

Hey there, my name is Richard Seroter. Welcome to this module in a course about implementing DevOps in the real world. Our last module looked at things like on-call engineers and planning software sprints, and using continuous integration. In this module, we'll look at things around infrastructure, outages, and more. Specifically, we'll talk about handling support tickets that come into the team, we'll discuss patching infrastructure and how that's a really important part of a DevOps lifecycle. We'll chat about pairing on cross-functional features and what that means as you're working on things that might be infrastructure and dev oriented, and how you work on that successfully. Errors happen, problems happen, so we'll talk about detecting and responding to outages. And then finally we'll talk about making regular communication with stakeholders, specifically executives, so that they're aware of the progress and you're able to bubble up important issues. And finally, we'll wrap up the module.
Holding Team Standups, and Handling Support Tickets

So let's start our Tuesday. In Monday I discussed the idea of a daily standup, and that's important. Again, organization, the product team, the service team, they're going to have potentially separate standups, but this is an important daily meeting of the organization to raise key issues, request for help, and the like. Now yesterday we talked about continuous integration and checking source control in. If we had a consistently broken build after everyone checked in, that might come up in standup today, and the first thing we do after standup is go address that and get back to a green build. Or we may start splitting into pairs to work on items for the sprint that we planned yesterday. So this meeting is going to constantly be adjusted based on what happened yesterday and what the team should know about. The whole team is involved here, interested parties, that's important, and everyone is involved because this is part of DevOps. It's transparency, it's focusing on delivery, focused on awareness and transparency, because everyone is a stakeholder here. All the engineers, all the developers, should care about this service, and so it's about bubbling up the things that matter. Next on this Tuesday, let's assume that we have a support ticket. What is this? Well, it's handling support tickets, end users, internal or external. We'll raise issues about functionality or performance or defects. These support tickets may route to the team that owns the service. A core part of DevOps is if you build it, you run it. Now that's not always the case, but that's something that's typical in DevOps, so when you do have that, these support tickets may get handled by a frontline support team that you have that spans different application services and the like, but they should route to the team that owns the service, at least for tier 2 or tier 3 help where it's not a simple problem that a shared support desk could take care of. Who is involved here? Ideally, it's the on-call engineer that we assigned yesterday on Monday. That's their job. The on-call engineer is responding to these, working with frontline support, working directly with the customer, and they're not necessarily distracting the rest of the engineers who are building the application. Why does this belong in DevOps? Well, responsiveness, you want to make sure the teams who may be paying for support or who at least have an expectation of how support should work, that they're getting what they need, that they're seeing a response, especially if it's business critical, when a system is down that they're seeing responsiveness, which is important. That's why you arguably do DevOps is to shrink that failure time, and you want to make sure customers are seeing that response, and some accountability. The team that builds the service should feel some pain if they're causing support issues or that there's things that they've influenced that are actually creating instability. So the team closest to the code handles the tickets. So it is the primary responsibility of the on-call engineer that week. The on-call engineer prioritizes any issues that arise during that week. They're going to need good telemetry though, they're going to need a place to reproduce the issue, so it's important for that on-call engineer to have things at their disposal to quickly zero in on where the problem is. That if there is a defect or a bug, they're able to have a dev test environment or some place they can spin up to reproduce that and try to see what happened. And if there is an outage or there's something that actually requires a recovery, that they've got the automation and tools necessary to do things in a safe fashion. It's really important to use the same ticketing system as the support personnel, so instead of having different systems and degrading the information as it transfers from one to the other, important to use the same one. Some tickets may be handed over from the previous on-call engineer, or they may be net new, but you want to have someplace to be tracking these particular issues and closing them out, especially for audit purposes and the like. This could become a group effort to resolve, so the on-call engineer may bring in others to help them solve a hard problem or a part of the system they're unfamiliar with, or even engage other teams if the issue may span products. This is where enterprise DevOps may differ from what you see in a smaller company. The service may be more complex than one on-call engineer can go and debug. So some of these systems may span different ownership, they may just be more tricky, you may have a shim in front of a classic legacy system, where that on-call engineer doesn't know that legacy system, and so this may be something in enterprise DevOps where you do have more people have to swarm on a problem when something goes wrong. What's good, though, is again, part of the on-call engineer experience, part of even getting defects in or having issues, or having performance problems, is it should spark new ideas for features to put back into that triage meeting every Monday. So all of this is feedback. The data is feedback, the customer input is feedback, support information is feedback. All of this should help you think about, how do we build a more stable, performant, functional system, and there's a lot of sources of input for that. So another few tips for getting started. Well, consolidating unique ticketing systems. I mentioned it there earlier, but if a mistake or a performance problem is copied between ticketing systems, you're degrading the quality, you're losing information as you transfer from one ticketing system to the next. Not to mention, if I close it out in one system, now I have to cascade it to another, and the customer shouldn't have to deal with organizational inefficiencies when they have a problem. So one of the first things you hopefully do is consolidate any unique ticketing systems you might have between your various teams. I mentioned it earlier with the on-call rotation, but make sure you're not assigning the on-call engineer additional work. They're going to focus a lot on the things here when issues get raised, and problems happen, they're going to want to be able to focus their full attention on that. It's important to have a test bed read to reproduce issues. This is where clouds and containers and other things can be handy when I need to quick stand up a clone of production and potentially simulate whatever the customer did to try to prove that that worked. Now maybe you could do that in production as well, that might be fine, but you also may have another place where you want to try to fix the problem and see if it does get fixed, and push that through. So having test beds available, whether that's locally, in the cloud, somewhere else, it's going to be important to be able to do that so you're not handcuffing your on-call person when they're trying to fix problems. Part of the DevOps piece is trust, and its collaboration within the team, and so you're going to lose trust if you constantly have on-call engineers hand over their hardest tickets to the next on-call person, and so you're going to have issues that come in that are tricky. It's really the responsibility of that on-call engineer to work through their tricky issues and not hand over their worst to the next person, because you know what, the next time it may be their turn to get the worst from someone else. So good ticket hygiene is important, filling out good data, when there is a problem making sure that you have a lot of information to the next person if they have to get it has good contacts done.
Patching Infrastructure

So systems will experience problems, infrastructure will go bad and go stale, and so teams are going to need to continually update things. So patching server clusters, product teams, application teams need to deploy an OS patch to production, for example, on this Tuesday. Who is involved? Well, the engineers on the application team, potentially the on-call engineer, depending on what the work is. And this belongs in DevOps for lots of reasons. Security, there is still plenty of surveys you'll read where a number of zero-day exploits are sitting there unpatched because teams are so frightened to patch production systems because they don't have automation, it's not reliable, there are so many things they break once they patch one layer that it breaks the next layer. So that's a real problem in many enterprises today, and so be getting good at being able to constantly patch or replace, or whatever you do with systems, very, very, very important. It's important for consistency, to make sure that you've got things in a consistent state between your dev test and prod, and having things in a good infrastructure state, being responsive to security problems, and simply, again, making sure if there's problems that are caused for your customer, that you're able to patch infrastructure or patch applications to fix that. Hopefully the object of this is to make patching routine and boring. But upgrades are required up and down the stack, especially as you have application teams that own the whole stack potentially down to the metal, and so a defect could be at the operating system, the platform level, a database level, messaging, application, it could be all sorts of places. So you may have in some enterprises, platform ops team that run certain platforms for you. Every team may not have their own messaging engine. Every team may not have their own database engine, although they may have their own databases. You may have some shared things, or if you're running a Platform as a Service, you may have enterprises run that Platform as a Service. It makes no sense to run their own container engine potentially, or their own cloud foundry PaaS, whatever it might be. But at a minimum, you do own your app down to whatever that manage layer is, whether that's the metal or whether that's a platform or something else. So upgrades to code, dependencies, app servers, those are all on you, but they should be boring, and hopefully not something that are super exciting for you, because that means you haven't automated it yet. This is part of where Infrastructure as Code comes in. This is one of those core things that came up as part of the DevOps movement, was treating infrastructure as just another configuration that's checked into source control and the like, and infrastructure can be created constantly without human interaction, and through automation. There's multiple approaches to patching here. So what you could do as you're patching infrastructure, you could patch existing services using things like Configuration Management tools tested against non-production environments first, and passing that through environments. Plenty of tools in this space, things like Chef and Puppet, and Ansible, and Salt, and CFEngine, a lot of good tools for configuration management against existing servers. You could also create gold images or containers and push those to production to replace existing ones, the sort of immutable server pattern where sometimes that's easier to deal with. Now, the complexity is always going to be somewhere. You're going to have complexity building these images now, but there's still something about being able to make sure I don't have a bunch of servers in production that may have different configurations. You hear the term configuration drift, because this server you may have logged into the box or remote-desktop'd in or SSH'd in and changed the config, but then you didn't change it over here, and so any time you allow someone to muck with the server directly, you stand a chance of being out of sync. So with immutable servers, you never patch a server, you never touch a server, you would always replace it in the need of something happening so you have not one-off configurations there, everything is identical. So a couple of things, you could patch servers, you could do gold images, there's a few patterns there. Finally, this is consistently applied to each environment, so as the team is doing patching they should be starting at the beginning of the deployment pipeline. They might work through a local environment, they might work through a temporary staging environment. Whatever it is, when you're patching servers, you should not be starting by going to the production servers and running some sort of patch command. This should be something you're trying in other environments, you're checking in your infrastructure configurations into source control, you're doing builds, you're testing the infrastructure with your latest version of your app, running automated testing, and so forth. So you're making this just like deploying code the same as deploying infrastructure. So a few tips for you getting started, doing integration testing with infrastructure, making sure a patch at one layer didn't break functionality at another. Changes to infrastructure should be tested with the latest build of your app in an automated fashion so that if you do patch an OS you didn't break the database. You don't find this out in production. So it's something that you can set up this sort of routine. Define a policy corporate-wide that you should never log into a server. Now maybe never say never, but the idea is you shouldn't be trouncing production servers directly. You should be using configuration management to push updates that come from a configuration tested and built somewhere else, or you should be using immutable servers, or you should be doing other things, but whether it's shell scripts or you've got local scripts, whatever it might be, but the idea is you should not be logging into servers and then eventually having configuration drift. Stay away from that, and instead treat servers as something you only interact with through automation. As a first step, many companies do introduce configuration management everywhere first. They don't try to jump right to immutable server patterns and start building gold images of containers or virtual machines. Instead, they start with a great configuration management tool and start treating infrastructure as code, they're checking in these scripts, they're checking in these definitions. And maybe it becomes less useful over time as you build a gold image that has everything in there, or you use configuration management to actually build the gold image, but not all the individual servers, whatever you might do, but you might aim for immutable infrastructure over time. Again, it's not easy to do immutable infrastructure, you have to build these images well, you have to have a good way of doing rolling deployments to replace the old ones, it's not a simple magical solution, but it does give you a really nice consistency story in production.
Pairing on Infrastructure-centric Feature

One of the changes as you start doing more DevOps is this idea of not just having application features. You may actually have infrastructure-related features because your application team is doing things with infrastructure. It's not just having a shared Ops team that only builds servers for you and sets up databases, and configure queues. Instead, that may be on your application team, and so there's going to be cases where you might pair with another developer or engineer on an infrastructure-oriented feature, Who is involved? Well, engineers on the application team. Now, in an enterprise you may not have enough Ops people to put on every individual application team, that may not be how you're set up, so you may have operations team still a separate thing somewhere, but you might have them work with some of these application teams, they may be using some shared services, and you can pair occasionally with them. The idea is, you're trying to make sure everyone is part of this deployment pipeline, and part of the application lifecycle, they understand what they're building, and so you may be renting folks in or bringing people in. Hopefully you can put them on these teams, even if temporarily, and in this case, let's say we're pairing together because it's important for cross-training. You're getting developers understanding a little more about infrastructure. They may never be remotely the experts that your infrastructure engineers are, but they learn enough to be dangerous to maintain these systems, and it builds a better relationship, especially if you can't afford to have Ops people perpetually on your application team, at least you're creating some relationships there so that they can work better and easier with less friction. So infrastructure is code too. We talked a little bit about this with patching, but the idea is that you hopefully can hire generalists with specialties. Now you'll hear the term T-shaped developers where you have generalists who can work across multiple parts of the stack, and this protects you from having individual experts and just having people who only do databases, and if there's no database work, I guess they're bored, or frontend work. But at the same time it's naïve to believe that you're just going to have a bunch of generalists or full-stack engineers who can do everything well. That's not really realistic. Instead, you're going to have people who cover different things within the team. You're going to have the skills within the team, people who do know testing well, people who do know frontend work and backend work, and databases, and messaging, and the like. But they hopefully also will be able to pair and work with developers in other areas. So they may be super deep in one area, but they're functional in some other ones there. Systems are too complex for any one person to know everything that has to happen in the team, but you do want to make sure, again, you've got a nice mix of skills. So as I'm pairing with these developers and an engineer or an Ops person, I'm going to have people that can cross-train each other. As you're doing this pairing on infrastructure features, all the configuration details, again, should be in source control. If this new capability is infrastructure-aware, you want to make sure that you're checking in all the configs, you're running infrastructure tests in whatever environment you've set up, and you're testing this rigorously in a production-like environment. This is, again, where cloud can be awesome because it's really easy to get high quality machines. You're getting tons of CPU if that's what your workload in production looks like or you're getting GPUs, or you're getting whatever you need. You're not running on a laptop, saying, I guess this looks like production. So a few tips for getting started when you're doing this sort pairing. Making the infrastructure work visible is important. Put stories on the backlog for things like upgrades or features that require an infrastructure component, automation that you're adding to the infrastructure should be there. I used to love walking into a team room and seeing an Ops sort of engineer sitting next to a developer, and they're both working on a script together that might be part of a deployment or was part of an infrastructure build, and each person was contributing their knowledge together, but they were pairing directly on the scripts that they were going to need to check into the application. And part of that was because this work was always visible. These stories were just as important as a pure software story, and teams would take those off the backlog and pair on them if one of them didn't have the expertise for it. So pair Ops engineers with software engineers. This is a great way to cross-train operations knowledge, software knowledge into a service team, into these application teams, and start to build some comradery and break down these barriers between traditional development and Ops. And then finally, test infrastructure just like software, same type of behavior. Plan your tests out, plan your failures, and then see if those things do fail. Automate your test scenarios, and test that infrastructure the same way, so that Ops people, who may not traditionally be doing test-driven development are starting to learn by pairing with a software developer on how they might think about building infrastructure features in a test-driven world.
Detecting and Handling a Service Interruption

I would like to tell you that if you do DevOps your systems will never go down, but then I would be lying to you, and then you wouldn't take this course anymore. So instead, service interruptions will happen, but DevOps teams don't fear them. Now they don't like them, no one likes when there's a problem, but at the same time, if you've done things well this is not something that everyone's hair is on fire and all hands are on deck, you've got processes and procedures to handle this. So detecting a service interruption is a big part of running a service, it's going to happen. You're going to have performance issues or you're going to actually have downtime that you have to recover from. Who is involved? The support staff, the on-call engineer, potentially the whole team if it is something that spans that. It's part of DevOps because you deal with responsiveness, issue resolution, accountability. that team, that, again, build that service should be responsible for keeping it online for whoever your users are whether these are internal users or partners or actual end customers, it doesn't matter. You're responsible for some of that. So the key is a disciplined approach to problem solving. This means upfront instrumentation pays off here. Good telemetry is important, and when you have that in place incidents can be detected, sometimes even before the customer notices. So metrics matter, but you have to have the right instrumentation to collect the metrics that actually matter. Are you watching the key metrics, though, are you detecting deviations or unexpected patterns? so you should be thinking about your instrumentation story upfront, not when there's a crisis. That shouldn't be when you're all of a sudden hunting for statistics, hunting for metrics, you should constantly be making sure you've got that instrumentation in place. It's so important to use facts to figure out causation. What are the pieces of the puzzle here? The DevOps Handbook says that high performers use a disciplined approach to problem solving, using production telemetry to understand possible contributing factors to focus their problem solving, as opposed to low performers who just blindly reboot servers. Now if I asked you who reboots servers when there's a problem, some of you will raise your hand, and that's fine, because sometimes that fixes the problem, but at the same time, high performers know where they can go to fix the issue. Iv'e worked on teams where in the immature days if the customer calls and says something is slow, everyone just starts from the very basics, and they start looking at all the different places where this could be. Is it our network performance, is it the application, maybe the database, and you spend hours and hours looking for something. But then I would watch that team mature over time, and when that same request would come in, they would be able to look at telemetry and say, look, it's our upstream network provider, that's where we're hitting some latency. The rest of the metrics are fine. And they could instantly zero in on that, make some phone calls, and figure out what was going on. Your mean time to recovery is so much faster when you have good instrumentation and good telemetry. Communication is so key here where there is an interruption. Instead of maybe trying to mask it and hope nobody notices, instead, letting the impacted parties know, starting to collect a timeline of events, very very, very important, and we'll talk about this in a moment, some of the things you can do, but you shouldn't be fearing communication. Don't be scared to tell people something is down. It's not fun to put that up on Twitter on a status handle, it's not fun to update your web page with a disruption notice, but it's much worse when your customers gest surprised by it and can't get any information. At the same time, though, be very careful about over-alerting, because it causes fatigue. So if I'm constantly detecting false positives, and I'm just blasting my team with things that are going on, or sometimes when there's not even an action they can take because it's something that has to recover within itself or whatever it might be. You're going to wear the team out, and they either start ignoring it, or you burn them out and they don't want to work there anymore. So you do have to constantly, again, continuously improve your alerting internally to make sure that you're not over-enveloping teams with false positives or noisy alerts, or things that aren't actually productive to them getting the service back online. So a few tips for getting started. Identify the core metrics that measure availability for you. This is not universal. Now you might think it is, but in some cases, what is available can be different based on the type of service you're offering, how you have caching in place, all these sorts of things that might determine whether your system is online or not. So you want to make sure sometimes your health checks, for example, are flexing the system to ensure not just that you get an HTTP OK back, but that the business service is behaving well. And you hear the term inside-out, outside-in testing and monitoring, really are you testing from internal points, testing internal components, but also making sure that you have monitoring in place to test from the outside, test the user experience. You might have everything look great to your perspective, and meanwhile you've got issues with your cloud provider network or something else, so that your users are experiencing terrible latency. You want to make sure you've got both sides being checked so you're detecting this, and that's where it's great to have visible dashboards up around the office if you can. Make sure that people can see, how are you tracking to these metrics, what are the trends, and then even being proactive when there are anomalies, when there are things that are not standard or outside the norm, are you getting notified for those. And if you're not, hopefully you improve and the first time you have an outage for something and didn't get a notice ahead of time, then you add the right instrumentation so that it doesn't happen again. I often learn the hard way that you really need a unified way to communicate status. So whether you set up a chat channel, which we had done, so everyone knew to jump in internally into a particular chat channel to get any status updates. Having a public status site so everyone knows to go, if there's an interruption, here's where you go to check. And potentially worst case, something like email. But what you don't want to have is have different stakeholders, whether those be executives within the company or even customers, just randomly calling engineers or other executives. It simply distracts the team, it's not productive. So I would say our team got much smarter about making sure that we would almost train our stakeholders, and train our customers where to go if there was an issue, and therefore, keeping the team that had to stay focused on fixing the issue, focused on fixing the issue and not replying to random chats, emails or phone calls when their entire attention should be on getting the service back online. So really play through this, maybe run some simulations, and help training everybody to know how to get the best information.
Sending Status Updates to Executive Stakeholders

Now I don't know about you, but I don't particularly like status meetings and status reports, but especially as you deal with something like DevOps, it's really important to make sure that you are constantly sending some information to some of your stakeholders, especially if this is new within your company, because this is new and radical. You want to prove that this is working, not just go off and insulate yourself and just tell people to trust you. Instead, it's about being visible. What's going on, what are some problems, what are some key metrics, what's the service health, and so it is important to do that. Who's involved? Team leaders might be collecting that information and then sending it out to your various executive stakeholders so they can see how well this new model is working. Change management is important. You've got to change some hearts and minds sometimes when you're doing DevOps, and so to do that you have to be transparent about your successes and your failures so they can also, as executives, adjust their mindset and how they think about funding and planning, and talking about your software deliverables. This belongs in DevOps for a couple of reasons, transparency and trust. To build trust you have to have a lot of honest communication and make sure that the team can also celebrate the wins together that come from a continuous delivery pipeline. So communicate and iterate is what I call it here. So DevOps is a cultural change. It requires transparency. These initiatives are often very high profile and can be seen as risky. You're upending the status quo. So it's important for service teams and application teams to share their progress with executive stakeholders and skeptics, because you know what, the best thing you can do is make sure you share this widely so that other teams may want to emulate what you're doing. We often saw that where I did this before, where you would share this information and then get calls from other teams saying how do we get some of that, because our team isn't delivering as quickly, how do we do the same process you are, so you're also evangelizing a lot, and at the same time, you're trying to push back against folks who may not like the DevOps change. It is a big transition for teams, and the best way to keep your momentum is to demonstrate success. What's important here is not to get caught in the technology too much. Obviously, you're a team delivering software or delivering technology, so what are those core business metrics that you want to make sure you demonstrate, and then also show, of course, some technical things like upcoming release markers, what are some milestones coming up, what is your change success rate, so if there are concerns that, hey, this team constantly ships every two weeks, even over the holidays where traditionally we used to have a change freeze, can you prove that that's not going to be a problem, because you have a great change success rate. Likewise, list out some of your priorities that are coming up, and also list out your risks, make these things visible. So if your proximities happen to be out of whack with executive management, they can see that, and not when it's too late. And if your risks are something they can help resolve, again, you're elevating those. Build momentum through demonstrated progress. Send this out regularly. Recognize that this can be politically sensitive. You might be upending a powerful project management office that doesn't like this DevOps culture where they don't get to control big fat releases, that might not be popular. It's important, though, to continue to be visible and demonstrate momentum. It's really hard to slow down success when you have it. Once you have it, it's really who wants to stop it, and so demonstrating success visibly is important. As you're getting started, choose a handful of core metrics to share. Don't share everything, this shouldn't be a 9-page report that no one reads. So focusing on business statistics, release highlights, adoption statistics, revenue, those sort of things are really important, but keep it succinct, because otherwise it's going to be something that gets filed away and no one looks at it. But figure out a regular rhythm. Don't miss an edition. Send this out, in this case, every Tuesday afternoon, and everyone should be able to say, look, I'm well-informed. No one should be able to credibly say, I didn't know what was going on, because you're actively pushing information out, and if they choose to ignore it, that's their choice. What's really hard to get used to is showing good and bad news to build trust. This was an example in the book about Ford that I mentioned in the first module that was the new leader of Ford goes into status meetings, and every one would just show nothing but green dashboards, everything was great, and he knew it wasn't. He knew the business was struggling, but there wasn't a culture of trust where people felt safe to demonstrate what they saw as weakness to the rest of their executive board. Instead, you've got to demonstrate that, even within your DevOps teams, and we used to do that in my old job, I do it in my current one here at Pivotal, where teams are comfortable saying, things aren't going well, or we're going to miss a release date, or we completely screwed up this feature because our requirements were wrong, but we've gotten back on track based on great feedback, and we're going to keep going. You have to be comfortable with that. And it takes a little while, and hopefully your executives also demonstrate that cultural change so that you feel more comfortable there. But keep refining these points based on feedback. The goal is for this to be read, not filed away. Make sure that you're bubbling up the things that matter. Ask your stakeholders what matter, and show that information first. Update your metrics, your format, your audience. Keep refining these sort of reports, because you have to win over people to your DevOps work.
Summary

So you've got two days of DevOps down. We've talked about handling support tickets, and the on-call engineer prioritizing this work to make sure that we're running our service successfully, that we're being responsive. Patching infrastructure is a super important part of a modern DevOps team. Infrastructure shouldn't be sittng unpatched. If you have unpatched infrastructure, it means you're insecure, you're going to have problems later on, but patching can be complicated when you have a whole stack you have to deal with. So whatever stack you have to deal with as your application team, either down to the metal or down to a platform, make sure that you take responsibility there, but script things out. Treat this like infrastructure, just like if you're pairing on a cross-functional feature where maybe a DBA and a software engineer work together or a tester works with the infrastructure person. Whatever it might be, when you're pairing on cross-functional features like infrastructure and the other, make sure you're always using source control for whatever you're working on. Outages are a fact of life, disruptions are a fact of life, but the key is with good telemetry you have good awareness before it hits your customers, and even if you have to respond quickly to something you didn't detect, having good telemetry to know where to zero in on and fix something and be data driven is very important, and demonstrates a maturity in a DevOps world. Communication is really important with your stakeholders, not only when there's outages, but regularly demonstrating progress, demonstrating the business impact of your work, because all the technology in the world might be awesome, but if you're not dealing with business outcomes, you're not going to have a DevOps team for very long. I hope you enjoyed this module. We're going to keep going on the Wednesday of DevOps with additional tips and additional advice to hopefully get you really ready to go with a high-functioning team.
Week of DevOps – Wednesdays
Overview

Hi, my name is Richard Seroter. Welcome to this next module in a course about implementing DevOps practices in the real world. Our last module looked at things like outages and handling that, communication with executives. In this module, we're going to look at things like onboarding new team members, conducting retrospectives, and more. Specifically, we'll talk about onboarding new team members, what that looks like in a DevOps structure, establishing some sort of monthly operations review or a way to do read-outs to your colleagues, conducting retrospectives or postmortems after an incident and learning from that, collaborating across teams, and then wrapping up this Wednesday in the week of a life of DevOps.
Holding Team Standups, and Onboarding New Engineers

So we start our DevOps day just like any other, with an organization or team standup. In this case, again, you can imagine the point of this meeting is to improve situational awareness, keep people aware of the most important things of the day. In this case, we might talk about yesterday's service incident. If you remember, from our Tuesday day there was a service interruption. It could have an outage, it could have been a performance problem, so we may talk about that today and make sure the team knows that there's a retrospective coming up later in the day. Again, it's important to have everyone involved in this. You may even have outside parties jump into today's standup because of the incident yesterday. They might want to hear about plans, or what they learned from that, or what the impact might have been on key customers. So you might have people who do attend occasionally, as much as there's a core team that always attends. And mportant to be transparent there, again, we're not hiding the details of things that go wrong, or not letting people know about upcoming meetings. We're focused on delivery, we're focused on the customer, and we're focused on removing any blockers. Later in the day, of course, we're going to onboard a new engineer. It's time to bring in new engineers from the outside. It could be part of a rotation. Many enterprises, especially as you're starting to train people in this model, you may be rotating people through a high-performing DevOps team that's delivering an application or a service. You may have other developers come in and learn the process so they can take that back to their teams. Same with operations folks or information security people, maybe even QA depending on how you've rearranged everyone, so there could be a constant rotation of new people within the team. While you still have a core team, so you have some continuity, there's going to be constantly some fresh folks joining the team. Really, who's involved? Well, it's the application team and any stakeholders who are rotating people through. And it belongs in DevOps for lots of reasons. There's always cross-training, it's one of the key values of continuously learning in a DevOps structure, and also being able to grow this. One of the challenges with enterprise can be making this go beyond just one department or one application, but actually make this something that's organization-wide, so cross-training and evangelism, to some extent, is something that's important in DevOps. So it's about speed to productivity. I mean that's really the point of anytime you're onboarding new people is, is it taking six months for them to be useful and productive to the organization, is it taking a couple of days? The faster that is, of course, the faster the company gets value, and for the employee it's faster they feel useful. Everyone wants to feel like they're adding value right away. So a lot of places that new employee is going to go, everything can't be tribal knowledge or locked in the heads of indispensable experts on a team. That's the worst way to onboard new people when the only way they can learn something is by asking the person next to them or trying to get some times with that one DBA who knows the entire system. That's not a great way to dispense knowledge. So things like source control is a record for new team members. They should be able to look at the source control system and learn about the application. They should be able to run it all locally. They should be able to check out the deployment pipeline and see how the code is put together, and what are the other scripts that come into play to build out the infrastructure or preload the database. Likewise, if you have a Wiki from the on-call engineer, this is a great place for that developer to be able to come onto the team and pick up some lessons learned, understand what the team has been going through, they understand any common errors, and they're going to get a nice crash course on what matters in the system. This is a really good opportunity to test your deployment pipeline. If engineers are expected to ship quickly, this lets you see how a new participant works with your CI and CD process. In some companies, they actually have developers on their first day, even in the job interview, deploy code, because they want to show that their deployment pipeline can do proper code scanning, it can properly make sure your tests are good, and runs through a series of things so that even if you don't do something right your deployment pipeline catches it. But if you do do something right, how great is it if your new hire can fix a customer bug on the first day and get it into production at the end of the day, that's fantastic. I've seen a lot of success when teams actually pair to accelerate readiness. So I watch this every day in my job now at pivotal. New hires come aboard and instantly pair with someone experienced, and they're contributing by the end of the week. So they can sit with them, they not only are learning the system, they may even be sometimes learning a new programming language, or learning new frameworks that we use on the team, and so everyone pairs with someone more experienced as they get started, and very quickly then they get up to speed, and I've seen this in other jobs as well. This is a great way that you do have to accept that the person they're pairing with probably has slower velocity for that week as they're bringing someone on board, but you also do get some of the benefits of pairing from a testing perspective and from a code quality perspective. Likewise, this is probably a way you're going to want to interview folks, as I've also, again, worked at places where the interview process involves the candidate actually pairing with someone from the team for half a day. It's really hard to mask behaviors or put on a different face for half a day when you're pairing with someone, so you really get to see their compatibility with your tools and processes, and the people. So a few tips for getting started. So, again, new team members pairing with existing ones. As you set up that rotation, don't make that random. There might be some people on your team who are very good coaches, who make good sense to do that with. Let them know ahead of time so they don't feel like they've just committed to all this work, and now they have to double up in another week because this week is not going to go as quick as they thought. So have a process in place for that, but really think about that and have people sign up. Encourage people to use the application they're actually building and supporting. Nothing really helps you more than when you're a consumer of the thing that you're actually building. And maybe it won't be part of your day-to-day life, I wouldn't expect that most developers just use CRM systems for fun, or use claim management systems for fun, but they should be able to use the system, walk through it, understand the user experience, and that's a great way to get started and get some empathy right away for who is consuming what you're building. Add this person to the on-call rotation quickly. Now it can be pretty scary if three weeks into a job you're the engineer on-call, but this is a great way to learn the system. All of a sudden you're going to get paged about something important and need to look through the system and figure out what went wrong, or understand how to get a service back online, and it's an important crash course, an important way to instantly understand the application. So don't wait months to put this person on-call. Probably do it within the first six weeks. And then finally, iterate the onboarding documentation based on feedback. Is it complete enough, did someone have to keep coming back to the manager to ask questions or the onboarding information with the pointers to the Wiki and the source control, and your pipelines, and other onboarding documents, were they good enough? Did that person get to a point where they were adding value quickly, and if not, collect feedback from that person and iterate the documentation, maybe even asking that engineer to iterate the documentation themselves.
Attending a Monthly Operations Review

Later in the day on Wednesday we may be attending a monthly operations review. Now this is, again, one of these practices that you may decide doesn't work for you, or you would change it something about that, but the core is the principle, what are we trying to do there. Well, the idea is for chances to briefly share the health of their service and improve as a collective unit. Who gets involved here? Well, the application team leaders, and often the executives get involved here in a sort of regular rhythm. And it belongs in DevOps for a few reasons. It puts the focus on the customer by constantly elevating that as a core concern by talking about customer-based metrics and health of your service, not just, hey, this thing was really cool and look at this new tech we used, but how is this doing from a usability perspective. Are we growing adoption, are we growing revenue, making sure there's always a focus on the business itself, as well as continuous improvement by talking to your peers and giving a chance to at least elevate any concerns that you have. So accountability breeds focus, my title of this slide. Really, the idea of remaining focused on customer metrics. Teams should be loosely coupled, and they're not going to be hyper-dependent on each other, but they may be collectively delivering a certain experience to customers. So Netflix obviously delivers a core service of making movies on demand whenever I want them, even though they're made of plenty of microservices inside, or Amazon, or plenty of modern companies. Pivotal has 60 some application teams, but you're still delivering a core service to the customers, so there is a collective experience that you want to make sure you reflect. And it's easy to focus on operational metrics, just what was your uptime, and throughput, and things like that, and sometimes you can miss the business or customer ones. So it's important to have a quick pause and say, again, how is this doing? Are user adoption trends right, is the response time good, am I having abandoned carts if this is a shopping site? Do I have a lot of closed accounts, so am I potentially creating a negative user experience? So this can be a short meeting where each team has 5 minutes to share their core metrics, and executive management can ask a few questions about negative trends or positive trends and product priorities, but it's meant to be a chance for each team to get a quick overview of where everyone else is, and also get executives better situational awareness. Because you want to use some collective knowledge here. So are teams getting together to learn from each other? We'll have some other things coming up in future days. But you don't want local optimizations to screw up global concerns. Here's a chance for everyone to have some global context about the overall service they're delivering to customers. They may think of new things that they should be sharing metric-wise themselves. They may even find out new things other teams are building, potentially, and avoid recreating the wheel. So it's about getting together and make sure you're using your peers. DevOps is great to have these independent teams, but it shouldn't be independent at the cost of losing sight of the bigger picture. It's about improving situational awareness for executives. It may not be feasible for team leaders to get together all the time, it would distracting, it wouldn't be helpful, but at the same time, you do want to have your executives know what's going on, so they don't have to keep tapping you on the shoulder saying what's going on here, how am I going to make this particular budget decision, is this a good team or service? Instead, giving them an active view of that data is going to be helpful. The book, Team of Teams, went through this example a lot as well, as you had different leaders in a military organization trying to understand what's going on, and in one case the author mentioned that many of the visits that he had to the teams had a lot of objectives. In one case, it was to increase the leader's understanding of the situation to communicate guidance to the team, and also lead and inspire. So these chances is also for executives to give good context to the actual DevOps teams. It's not just a readout. It can be a chance for those executives to share core business metrics that they have. And then, finally, what's really important is this is not a problem-solving session. This should not be an 8-hour meeting where each team gets completely grilled, and they have to answer all these different questions, you're going to design solutions there, you're going to drill into every problem. That's not going to be helpful to everyone. You are going to have people then tune out. So this is a chance to mention any key concerns, and if need be, have follow-on discussions to drill in further, but the key is, this is not to solve problems, this is to help people see what's going on. So a few tips to get started. First you want to make sure that each team is collecting success metrics, and what success looks like for their service. That won't be uniform. Each team is going to have some different things that matter depending on if they're a frontend service or backend service, or what type of part of the experience they are for that customer. And you're going to iterate on this after the first few of these sessions. You're going to see what other teams do, you're going to see what matters most. You might have other people provide feedback that, hey, I would really like to know this about your service. I don't really care about this. That doesn't help us understand how well it's doing. So, again, a good chance for continuous improvement. I say schedule monthly here. Again, the key is, there's a rhythm. You could do this biweekly, you could do this every quarter. It really doesn't matter too much, the key is frequently enough that it's useful, but schedule it, keep it consistent, make sure people understand that this is a regular occurrence. It's important if you're a leader listening to this, that you lead by example and mention challenges yourself. Set a precedent that showing all green and hiding problems is not what this is about. Executives should set an example by starting with transparency themselves, showing things that might be issues, and also praising those that surface issues. If the first time someone raises a negative problem that they get barked at or shot down, guess what, you'll make sure that you've trained everyone to never bring up negative problems. Instead, if someone brings up a negative issue, and they show, hey, this trend is actually going the wrong direction, instead of worrying you're going to shut down their service team or yell at them personally, you might say, hey, I'm really glad you showed this, we're going to have some meetings afterwards to figure out how we help turn this around. Showing that is going to be very, very important, so lead by example here. And then actively follow up on problem areas. Keep a parking lot of things that you want to follow up on after this meeting, and set those up immediately. So if you do want to drill into a roadmap for a service that you're worried about has flatlined and hasn't really grown for a while, that shouldn't happen in this sort of meeting. It should happen one on one with the leaders from that team or other people in engineering. So make sure you've got some action items there, but don't use the meeting to do this.
Conducting an Incident Retrospective/Postmortem

Later on in the day Wednesday, as we mentioned in standup, we're going to want to conduct an incident retrospective. Now you might call this a postmortem as well. We used to call them retrospectives because there wasn't a body to bury, nobody had died, so instead it was a retrospective, we were going to reflect on what happened. Who is involved? Well, the on-call engineer and any stakeholders that really were part of this. This could be some of the other application teams, this could be people from the support organization or a shared Ops team, whoever is supposed to be involved who has information to share and who needs to improve. Why does this belong? This is a core part of DevOps. If you're doing DevOps successfully, you should be really good at having retrospectives and learning from that, and making a better service. The whole point of this is to improve the service, not to find someone to fire. But the point of it is continuous improvement, transparency, and hopefully for the customer as well. They should see this transparency as well. So I say here in the title, your culture is revealed during retrospectives. I believe that. I've sat in some great ones, I've sat in some not great ones, but the idea is that this is where you really see if a lot of your cultural values are real life or just something that are up on a bulletin board. It's not about assigning blame. Most of the accidents caused by people are the result of process problems. As the DevOps Handbook says, when accidents and failures occur, instead of looking for human error, we would look for how we can redesign the system to prevent the accident from happening again. In this meeting, the point is to review what worked, and then what didn't. So that's the point of a retrospective, that you're trying to learn to get better. Now you have to start by assembling a timeline, a calendar of events. Before the meeting, it's important to do that as well so you go into the meeting with this all ready. The first task in your postmortem is to make sure you're showing the timeline of things that happened, and so that might be the person who was on-call, that might be something else, but hey at this time this is when we detected the first performance issue. At this time the system went down, at this time we contacted so and so, at this time the first service came online, at this time this is when the service was fully available, something that's exact, which means you must have good telemetry, you must have someone thinking about this during the outage or performance issue or whatever it is itself. Otherwise, it might be harder to collect this after the fact. Use facts to review what happened, very, very important. So you don't want to have revisionist history. You don't want to have people coming in, well, if we had just done this then this would have happened, that doesn't help you. Instead, review what happened, what you actually did. Don't try to couch it and make it sound like it was just bad luck. Instead, this is what happened, and this is what we're going to talk about, instead of trying to make it seem better or worse than it was. One thing that a lot of people at my old job, again, when they would witness this were kind of amazed by the process, but explaining what worked and what didn't. The first 10 minutes of the meeting would be people adding things to a shared document or record that would say, this is what went well. And in some cases, even something that seemed like a minor issue, you could still have 20 things in what worked, and 50 things in what didn't, because everyone is starting to brainstorm about things that could have gone better. Instead of just using these meetings to quickly find that you can blame so and so and fire them, when the culture shows that you're really focused on continuous learning, you're going to find out that a lot of things come up on how you can improve the process. Arguably the most important thing you do out of a postmortem is actually act on it. If all you've done is gotten together, listed a bunch of things that didn't work well, and sadly nodded your heads and walked out, that's a waste of an hour, or two hours, or however long it took. Instead, everything that didn't work well on your list should turn into some sort of action plan. Maybe it's improving documentation, maybe it's improving your communication structure, maybe it's going back to the system and adding another high-availability component because you found a single point of failure somewhere. And then that should go onto backlog somewhere. So that's why you have product owners in the room, because this should be something that they prioritize. Now maybe it's not the most important thing, because you hit some sort of perfect storm scenario where the technology failed, and it's really hard to imagine that happening all the time. Maybe that doesn't become something on your next sprint, but maybe it does because it happens enough where the impact is so high that it is worth bumping something else down in priority. The key is, assign owners, figure out an action plan, and make sure that you should not see the same thing pop up over and over in retrospectives. Finally, put the record in a discoverable place. What happened in the retrospective, what have you learned, and put this as public as possible. If you're not going to put this on your public blog as some companies do, which most of us really admire when you see really detailed postmortems and retrospectives on company blogs that say this is what happened. This is where things failed, this is what we're doing to fix this the next time, and I apologize customer, but we hope you trust us. That does build a lot of trust. You'll see the response to that is often congratulatory, not insulting them for having problems that went wrong, they're showing that they're learning in public. Now if you're not going to be that forward thinking, you may decide to say, look, we're going to at least put this on our intranet or somewhere where internal employees can read this and check this out, and other DevOps teams and other application teams can learn from it. A few other tips for getting started. Set up a retrospective the day after your next incident. Here's your first chance to try this out. The next time there is a major performance problem or the system goes down, even if you have maintenance that's not supposed to be disruptive and it accidently takes down the system, plan a retrospective. This is one of the first things you can do. Choose some sort of interface for collecting feedback. This is not email. So figure out some sort of shared editable list that you might want to use where people can list what happened, what worked, and what didn't, and even start listing out action plans. So there should be some sort of Wiki or collaboration tool that you like using that everyone can interact with live, even up on the screen while you're in this meeting. Define very crisp action items. Things like be more careful or be less stupid, that is not an action item, that's not an actual countermeasure, as mentioned in the DevOps Handbook, prevent errors form happening again. That's just hoping that people make better decisions with the same system they're working in. That doesn't fix anything. Instead, it should be about actually having proper countermeasures. We need to fix this step of the process. We have no guardrails on this particular script, which let someone accidentally delete all of our DNS settings. Whatever it is, make sure that you have actual real action items that you know can fix this problem. Assign someone to keep and share those minutes. Measure in the heat of the moment and discussing things going on. You might not have someone actually capturing some of the things you've learned, and all you have is some short snippets and a list somewhere about some of the things that worked and didn't. So have someone there, you might have rotating people go through this, who actually keep track of what the major points were, and what you're going to learn from, because this is going to be very important, even days and weeks and months later, when other things do happen, and you want to see how you handled it in the past.
Collaborating Across Teams

We're going to close out our Wednesday, it's been a busy Wednesday collaborating across teams. So teams are going to exist without some fixed dependencies sometimes between teams, they're fairly independent, they can do things on their own, but cross-collaboration is sometimes needed. You need things from other teams. So who is involved here? Well, application team leads, engineers, other people who need to help solve a problem. And this matters in DevOps because you're removing bottlenecks. It's a core part of what it means to focus on flow and focus on delivery, is finding things that aren't working or finding things that keep you from shipping, and removing them. Sometimes that means working with other parties that you end up having dependencies on in your system or in your deployment pipeline, and trying to improve the flow. This is about collaboration in and across teams. No team is going to be entirely independent. So sometimes I think it's easy in DevOps for us to think about these just hyper-independent teams that have no interaction with anyone else. They can literally just do everything from idea to deployment all the time, completely by themselves with no other contacts. That's not entirely reasonable. Now in some cases, sure, you hope that that's going to be the case for a lot of your work, but you are typically part of a larger organization, sometimes part of a larger department, or even a part of a larger system of which you're a component. And while good contracts and interfaces mean that you're not constantly having breaking changes between each other and you can iterate independently behind your service without breaking others or dependent on others, there are going to be intersection points where you need something from somewhere else. So teams may have to share when they deploy, because other teams are depending on a new feature they're doing, or a new location for a geography or a new type of service. You might want to collaborate on a shared problem and avoid duplicate solutions to undifferentiated work. So you might do some things where you find out, hey, they're setting up X platform, I would really like to use that too, maybe we don't share an instance, but we want to share the technology, let's partner on that. And part of that is thinking locally, but acting globally. You want to make sure your team is successful and you're optimizing your independence, but purely, local optimization is going to have negative impact on your whole organization. Making unique programming language choices, making entirely unique CI/CD tool and and process choices. All of this limits rotation and can increase your costs over time. Again, I've worked in teams where there was a lot of independence the team had with what they chose to build their app with, how they deployed it, and sometimes that's good. You like to have some guardrails there, but you don't want to make it too constrictive, because teams may be able to optimize with different tools. In one case I had a team that wrote all of their apps in Erlang, which was useful, and I guess powerful for their service, but all of a sudden when people rotated in, or when people left, that was not a broad skillset within the rest of the organization. It was hard to get people into that team, because they had a fairly specialized programming language. So sometimes you might have to impose some sort of constraints, or at least a pool of choices instead of giving people just whatever they would like to do and hurting the organization more broadly. A really important piece here is to create systems for quick collaboration. What I don't want to introduce with this sort of idea is that you need a lot more meetings. The fewer meetings the better, and so how are you helping people quickly collaborate, maybe with chat tools, maybe with a good chat room or other tools for rapid feedback and collaboration that don't require 10 people to sit in a room and do something. In my previous job we had white boards all over the building so that people could stop in the hall and quickly plan something. It didn't require a lot of formality, instead the point was let's solve the problem and let's keep going versus let's stop everyone's work, immediately screech it to a halt, and plan a whole series of workshops in order to make a decision. So a few other tips for getting started. Invest in a chat room technology. This has become really important for so many organizations who don't like using email because you end up losing so much contacts for people who join late, not to mention it just becomes noisy and easy to ignore. They don't really love as much the one-on-one chat, because all of a sudden no one else can participate, and some good information gets locked into that. Instead, good chat room technology that you're seeing a lot of companies invest in, make this a great way to quickly collaborate, share information, and even integrate a lot of different bots that put things like your build success in there, or other check-ins or other things that you can have a very interactive lively environment there. So invest in that, it's a good use of money for your team. Set up some loose boundaries and guidance for commonalities. This can be tricky, but I would encourage you as you're getting started with DevOps and application teams that are independent, consider a handful of choices for programming languages, CI tools, configuration management tools. Pick single choices for things like ticketing and chat tools and content repositories. Those shouldn't be choices per team. Everyone shouldn't choose their own ticketing system. You shouldn't have teams using different chat tools, or even choosing different content repositories. That shouldn't be negotiable. You may say, look, this team is allowed to use Java or. NET or Go or NodeJS, and that's it. Or, again, make this based on what your application is going to be, and make sure you're accounting for those things, but it really probably shouldn't be everything. Same with CI/CD tools. Again, you want to give teams some freedom if their app is actually unique, but at the same time, you don't want to hurt the organization as a whole because you're using so many independent things and increasing your costs. One thing I've enjoyed seeing is fostering cross-team collaborations through things like hackathons and excuses to get together and build stuff. Whether it's quarterly or whether it's monthly, taking time and letting teams work together on things, maybe it's a new technology, maybe it's trying to build interesting applications that are customer facing, maybe it's trying to figure out a new mobile experience, but foster some of those cross-team relationships and improving collaboration and communication through things that don't feel like work, that feel like something that are going to help us actually learn new things.
Summary

So this is an exciting Wednesday of DevOps. We brought a new team member on board, and realized that things like Wikis, and things like your source control system, and even your CD pipeline are super great sources of information that are helping someone understand this application in context, as well as using the app itself. Some sort of operational rhythm is helpful, and this isn't just a written status report that people can ignore, but instead potentially a 3-hour meeting once a month where every service team spends a couple of minutes going over their business and operational metrics so that executives and others can get a view of the health of the overall service that you're delivering to your customers. As your teams get larger and larger, potentially this becomes more difficult and you might not do this with 300 DevOps teams, that's unrealistic, but there might be seams where you can do this within collections of teams, other ways to simply, again, improve situational awareness. How you do retrospectives is probably the most indicative of what your culture is from a DevOps perspective. How much are doing blameless things, how much are you focused on learning, and transparency, and communication? All these core DevOps principles come to light in a retrospective when you try to figure out, how do you prevent the system from behaving in the way again? How do you help people not make the same mistakes by improving the quality of the system they work within. Collaboration is always such an important part of DevOps, not just within your team and working well within your team and pairing, but also working across teams, as it's almost impossible to say you will never have dependencies or relationships with other application teams that use services that probably you depend on. Hopefully it's not constant, where you're blocked because of those teams, but there's actually a lot of great chances to collaborate on shared technology or a new interface or feature that they're building that you want to use. You should be considered a customer to them, so how are you behaving that way and working together? That wraps up this module. We're going to go ahead and jump to Thursday in the rest of our week in the life of DevOps.
Week of DevOps – Thursdays
Overview

Hi there, my name is Richard Seroter. Welcome to this next module in a course that looks at implementing DevOps practices in the real world. Our last module looked at things like retrospectives, and cross-team collaboration. In this module, representing Thursday, we're going to look at things like improving processes, updating teams, and more. Specifically, we'll talk about improving a team process we potentially learn something from our retrospective that we now want to implement. What does that look like? We want to create content about releases, and what does that look like in a DevOps world, and why is that so important. We'll update our employment pipeline, we should keep improving that through automation in other ways. Something you may not be as familiar with, but right-sizing the engineering teams, and not necessarily keeping application teams fixed in DevOps, but being able to shrink and grow them based on business demand. And then finally, wrapping up the module.
Holding Team Standups, and Replacing a Team Process

Like every day, we start our day with the organization and team standup. This is a way to update on daily events and issues. We may share some of the key learnings from yesterday's retrospective here and say, hey look, we know that when this happens we're going to have this concern. We haven't fixed it yet, but be aware that this is the way we resolve this, this is the way you might detect this, and help the team as a whole understand what you've learned from yesterday's retrospective. This is important that all those team members, any interested parties who, again, may be jumping in to see what's new, are hearing this information, and keeping that transparency is important. One thing we want to do from that is actually let's say replace a team process following the retrospective, we might have learned that getting our resolution team, our fixed team, our incident team together faster, more consistently is really important, because during the incident we had a couple of days ago it took us a while to get everyone onto a bridge, a conference call, to help resolve the issue. There's a way to improve that so that we have a more consistent and a faster experience, because the goal is to shrink that mean time to recovery, and anything we can do to do that is going to add business value. So who might be involved in replacing that? Probably the team leaders, but also the individual team members who have a say in that and may be impacted. This belongs in DevOps because it's about continuous improvement, and in this case about a customer focus and improving some of those core operational metrics around recovery and failures. So DevOps is all about improving flow. It's one of the core tenants of it, and arguably the reason it exists is increasing flow. So it's hard to improve what you're not measuring. it's not impossible to improve things you're not measuring, but it's also hard, in many cases. I can't say this should be faster if I don't know what normal speed looks like. So it would be great to have data on mean time to recovery in the example we have here. So if I'm measuring mean time to recovery, and say it usually takes us 2 hours to get the system back online, if that's considered good then I guess I don't do anything. If that goal is I want to decrease that, I want to shrink that time, then it's time to change the process. So if I have data, if I have metrics, and I don't want to get overwhelmed with metrics, but if I have some core metrics, then I can actually improve on them if I consider them good or bad. So it's about acting on broken processes, elevating constraints, looking at those bottlenecks and focusing on them. In the book, Start and Scaling DevOps, they mention, so start with the bottleneck and fix it. Then identify and fix the next bottleneck. This is the key to improving flow. Team of Teams, the book mentions the entire bottleneck concept is not geared to decrease operating expense, it's focused on increasing throughput. Since the strength of the chain is determined by its weakest link, then the first step to improve an organization must be to identify the weakest link. So it's about taking action on those things, not just saying that process is terrible, let's go around it, which happens in many organizations, and I'm sure I'm guilty of it myself, where you don't like a process and so you just don't do the process. That's not the right way to do it. Instead you fix the process and take an active approach, and if this is the processes that we believe in, we should want to do all of them, they should all be effective. Automation is often, but not always, the answer. We shouldn't always fall back on the process that says, hey, everything is about creating robots. That may not be the case. Sometimes it is about training, sometimes it's about human communication. When it comes to improving how we bring the team together for an incident, we could choose to automatically look up the on-call engineer, spin up a conference bridge, create a chat channel, all through automation. That is great, but you also may need to update your process personnel and decide to have an owner of a particular incident so that they manage it, or an engagement policy with other teams. So those aren't automation activities, those are often human activities, and so improvement is going to be a lot of different things. Hopefully a lot of it is automation, because that's going to improve the consistency and reliability of some of those actions, but a lot of this is about people assignments or people responsibilities as well. What's important here is experimenting and measuring. And that's why small batches is important, that's why continuous improvement is important, that's why fast feedback is important, because I want to try new things. Hopefully I'm measuring and collecting data about what I tried. If it's not working as planned, I'm able to do something different. This is why I want to have good feedback loops with my operational people, with my customers, with my other stakeholders. So if things aren't working well, I'm hearing about it, and I can do something better. Some other tips for getting started. So solicit feedback from the entire team. Make sure you understand that everyone is a stakeholder. Everyone wants shared ownership of a solution. You don't want to get paralyzed by having 4, 000 suggestions for something, but you do want to make sure that the people who use the process, live in the process, are giving feedback, and it doesn't just come from on high from someone without context. What can be so hard in a continuous improvement model is knowing when to stop at "good enough", that it might not be perfect. You may never get to perfect, but you want to make sure you're not tinkering with it forever, because then you may miss the next bottleneck in the pipeline by overoptimizing another one. So it's about hitting a good point that solves a problem, that increases throughput, and in some cases there's a constraint that you can never move beyond a certain level of throughput, and instead you just have to optimize some other areas around it or try to figure out ways to at least maximize the throughput you do have. But make sure you understand and take a deep breath at points, and know when you've hit a good enough point. This may be mildly controversial, I hope not, but really at this point you should only be buying products and services that come with APIs. Whether this is your chat tool, whether this is your telecommunications platform, your content repositories, if any of them don't come with APIs, they're probably not a good buy for you, because then you're not going to be able to automate them. When these things come with APIs, then all of a sudden I can introduce even operational aspects, not the app I'm running, but the things that support my app. I'm able to also create automation around those things, which is such a powerful solution and helps me create consistent processes that don't expose themselves to human error. Finally, be great about evangelizing and advocating your work to other teams. So as you do start making these improvements and you improve a process, other people might want to improve their processes too. They might want to use the automation you've built, they might like the way you create a certain human-based sequence or responsibility chain, so make sure you're sharing those in different forums so that people can also get better based on what you've learned.
Creating Product Documentation

One of the more underrated aspects of DevOps from what I can see is its impact on communication outbound and documentation. I haven't heard of doc ops, but there is something around how do you deal with documentation when you have continuously changing systems. I don't just have the annual release notes that come out. So there is this process of constantly sharing information through things like a knowledge base, through release notes, through tutorials, other things that have to keep happening because you are changing a system over and over again and people can get out of date. So who gets involved? Well, the application team, potentially if you have a content team, document writers, others in the company hopefully contributing as well, people in your field. And this belongs in DevOps for lots of reasons, knowledge sharing, throughput, people can't use things they don't know about, so you're helping improve throughput, improve efficiency for your own customers, by helping them stay well informed. You don't want the first time that someone learns about a feature in the support system is when a customer calls to complain about it. So faster deliver means more frequent explanations. As velocity increases, it's harder for users to track the changes. It's exciting when you're shipping five times a day or five times a week, or five times a month, but gone are those days of annual releases with big release notes, and formal training, and painstaking tutorials. So, all of a sudden, how do I deal with that when I'm shipping all the time. Those things I might have done before are woefully out of date all the time. So your application training course that you've put together that takes six days to take, it might be stale within weeks, and tutorials need constant refreshing because you're tweaking the user interface, and those steps that you wrote about two months ago are actually a little different because the menu is moved around. So it's something to consider up front is what is the impact on your user on a constantly changing system, how are they staying informed? One solution to this is that content must be easy to create, edit, and find. Not every team that I've seen is fortunate enough to have a fully stocked technical writing team. In some cases, it's great if you do, Pivotal does a great job with this. Other companies I've worked with have people who have a small team, but they need help, so it's about democratizing the content creation, things like tutorials, even things around some of the extra release notes. Make it easy to contribute information to a team knowledge base or even a public knowledge base, because you're going to want the help. You don't want to have a bottleneck around communicating your features, and therefore you're holding back all these cool releases because you haven't explained them yet. That's a terrible bottleneck. I don't want to have all of my work done, and shipped, and in production, but I can't flip the feature flag on because I haven't yet documented it. That should not be your constraint. So think of ways you could elevate that constraint and fix it, in one case by democratizing the content creation process. At the same time, though, just being able to change content isn't good enough. I need a good change log so people can see what's new, and I need a good broadcast function. So I need a strong history function. What's going on? It's not just to set up Microsoft Word docs that you update and overwrite and no one knows what changed from one release to the next, you may use things like wikis. You might use a proper CMS, Content Management System where people can, again, see differences from release to release, or even Git-based repos, as I've done in the past, where the content actually is checked into GitHub and then generated into static pages. In this case I have a really great history of what's gone on with the documentation. For broadcasting, using social media, internal chat tools, intranets, web casts, as ways to let more people know about what's out there. It's important not to just say, hey, if I build it, they will come. When I'm changing systems every week, I can't assume people are always stopping their day job to read my documentation. Instead I need ways to push information, or at least push notifications to people so they know things have changed, that there is something for them to look at. So some tips for getting started. Set up an open content system. Don't have a single gatekeeper/bottleneck to content getting released. Still have "owners", but many contributors. So there still may be a single team that publishes the content when they're ready, because releasing documentation may have a lot of implicatios from support perspective, even licensing and others, but open up the contributions so you can have many people sign up to be contributors, maybe that's even open source if it's a public knowledge base, but you want to make sure you do have people who are curating this, and so you want to define a light review process. Hey, if you're using GitHub people can submit pull requests, and then you submit and look at the documentation. In my old job, we actually had tests run on those pull requests to make sure that your links were good, all these sort of things, before integrating it with the rest of the content code base. What this meant is that we were actually doing continuous integration of our documentation, and we'd actually integrate it with our chat tool, so that you could type in three words into our chat tool and actually push a new version of the documentation to the production site. So by automating this and having some faith in what you have built from a testing perspective, that's great. Now you also may, again, want eyeballs on it, because automated tests can't tell you if someone wrote a terrible sentence or threw a bunch of curse words in there, so you still may want to make sure that you have good quality content. But with some integration testing you can also take out some of those steps and save yourself some work. And then finally, again, opening this up to outside teams and outside parties. Get bold. Think about ways that you could open up tutorials to people in your field who use the product, and maybe sell the product, and actually hear from customers about what their pain points are, what they wish they knew how to do better. They might be able to write up a great tutorial and save you a ton of time.
Upgrading the Deployment Pipeline

Our deployment pipeline is one of the most important assets we have in a DevOps team, so how am I thinking about that and maybe adding new steps over time, improving my deployment pipeline, making it better, faster, more secure. This will probably happen within the application team. Now, again, if you have a shared operations team there may be some collaboration here as they are using their expertise with scripting and some other components, if there is infrastructure deployments that come as part of a deployment pipeline, you may be collaborating there, so there may be some cross-team work. And, of course, this belongs in DevOps because it's about increasing automation and improving the reliability and throughput. I want to have a great deployment pipeline that anybody on my team could launch at any time. I don't have these same concerns when DevOps first came up of, hey, I've got a bunch of developers pushing code to production. That was a very simplistic view point of it. Instead, it's about the developers potentially initiating the process, but going through an extremely rigorous deployment pipeline that runs more tests than were ever run before, and more checks than ever before. So this simple idea of developers just x-copying a deployment file onto a production server and rebooting a web app, that's never really the principle of DevOps. Instead, it's about increasing automation so that you improve throughput reliably. Over time, your deployment pipeline grows into a system of record of sorts. This shows you the history of deploying systems to production. You're building a better audit trail over time. Deployment pipelines do get built incrementally. Instead of building this as a massive pipeline to start with, it's probably incremental. But what's interesting is instead of being less compliant with certain security regulations with DevOps, there's a good chance you'll be more compliant, because now I have a full trail of everything that's happened to production systems. It's easy to audit the whole thing, not just ask for samples. I've worked places where the auditor comes in and just asks for a handful of samples. How much better is it when you can point them at the full history of the deployment pipeline and say check it all out. Everything from patching servers, to deploying code, to doing a security fix, all of this went through our deployment pipeline and you can see that history here. And you can also see what production looks like on every server because we keep a consistent image. Again, your first attempt at continuous integration, continuous delivery or deployment may be lightweight. You may just start with CI processes, improving your test coverage, and starting to at least create production ready packages as the result of a CI process. Eventually, you may add more packaging steps, more improvement to that, to make sure you have a more complete system, and that often happens by partnering with groups like Information Security and Compliance. Instead of necessarily imposing on them and saying, hey, here's what you need to do to work with us in DevOps, bring them in. Let's white board and map out your deployment process and where security steps should come in. Partner early with them to create consensus. Instead of creating blockers to what you're doing with DevOps, you're going to end up creating advocates, and you're going to have them introducing more secure things. I've got some customers at Pivotal who go ahead and have worked with their security teams to expose APIs to things like code scanning and checks, so now the team is actually doing things like calling code scanning tools as part of their CI pipeline because they worked with a security team to expose APIs. At the same time, they also worked with their team doing change management and configuration management and automatically create tickets in those systems when they do deployment because that's part of their process, and instead of stopping their process to manually create tickets, their automation pipeline calls APIs to create tickets when they do deployments, and add the right metadata. That was a case of great collaboration, and instead of being enemies with these teams that might be a little wary of DevOps, you bring them in and partner with them and build a better system. This is where cross-functional skills within your team come in handy. Just having a core set of software engineers who don't know infrastructure can be a hindrance when you start doing things with deployment and infrastructure and networks and the like. You don't want people guessing here. You don't want them deploying a database with no security or doing firewall roles that allow everything. So you do want to have cross-functional skills in here that know how to successfully package an app, harden a server, and do deployments successfully. A few tips for getting started. Do value stream mapping with cross-functional teams. One of the best things you can do as you start your DevOps process is get all of your stakeholders in a room, hopefully with a large wall, use sticky notes, use a whiteboard, and actually do value stream mapping with them. Look at the entire deployment pipeline together, and see where the steps come in, see where you interact, see where there might be bottlenecks because there's a lot of people who have to approve something or the like. Get those stakeholders involved. You're going to have a much easier ride if you're not fighting everyone internally, and instead focused on your customer experience. You should be testing changes you're making to your deployment pipeline repeatedly in non-production environments. The first time I try out a major change to my deployment pipeline should probably not be production. It should be in a place where I can have a production clone that I test that that pipeline is solid and even do some improvements there. Now this is where the cloud is beneficial because I can temporarily spin up environments that look exactly like production without wrangling with an internal infrastructure team. But again, this could be a chance where you improve collaboration by setting up this precedent. I mentioned examples earlier with one of the Pivotal customers, but use or create APIs to integrate with the existing audit-visible systems. If there's ticketing systems, change systems, security systems, the like, help those teams build APIs around those if they don't exist already so that your pipeline can integrate with those, and you don't break their process by simply improving yours. Now in some cases you might have to because their process is terrible, but hopefully the spirit of their process is good, and you want to make sure that you are an integrated part of that so you're not battling with other teams or being called non-compliant just because you've introduced automation. So partner with them, maybe build some new APIs that now will help all the other teams remove a blocker as well.
Right-sizing the Team Roster

This last activity on a Thursday is one, again, you may not think of a lot to worry about, but one, that in hindsight, I should have spent more time on in one of my previous jobs, because people who are new to DevOps think of it as wow, I'm funding a lot of these teams that never seem to change, that seems really expensive. And so the idea of right-sizing a roster, and moving people between teams is important so that stakeholders and people even funding those teams realize, these aren't static units. There might not always be 15 engineers on a load balancer service, there might not always be so much on this data service. There's going to be times where it grows and shrinks. So who gets involved in this? Well, it shouldn't just be executives who are playing a board game with different team rosters. There are personnel decisions, there are things to consider, so it should involve the teams leaders as well, so that you're making smart choices, not just moving pieces around the board. But why does this belong in DevOps? Well, it belongs because of responsiveness. It's about reacting and responding to business need. We can never forget that the only reason an application team exists is to serve the customer and help the company make some money, in most every case, that's the purpose for existing, and so I want to be responsive to business need. If I'm doing a service that's flat-lining, maybe that's okay, and it's consistent revenue, but I might not need a perpetually large team on that anymore. It's time to put them on the growing business. If you're not doing those things, you're losing sight of what matters. So you want fluid team structures to support adaptable strategies. The whole point of doing DevOps is about being adaptive to business need and improving throughput and getting things in the hands of customers, so adaptability is such a big part of that. Application teams aren't a fixed size. I haven't locked them into a safe. One objection to DevOps from management is this idea that they're going to have dozens of people on a particular service. They're funding products, not projects, so it is different. I'm not just saying this project is going to run for six months and I need 3 million dollars, instead I'm saying I am funding this perpetual unit of people to run this service. It's a different way of funding, it's a different way of thinking about your lifecycle there, but you also want to then demonstrate as engineering leaders that you can make business driven adjustments to that, that you're not just perpetually asking for a ton of money. Your engineer count may swell during key periods of releases, and then contract afterwards. So I've done this before. A big milestone coming up, so I put more people onto a team to address it. And you make sure maybe months ahead, so for a few months that team swells to a big size because there's a big important thing that they're working on. Afterwards it settles back down to its operational size, it doesn't need so many people, and they can go back to other teams. Now this is also where it comes in handy, as we talked earlier, about onboarding new teams and your technology choices, because if I have this super specific customized set of technologies, it is hard to swell my team without actually slowing throughput. I want to know that if I add more engineers I can go faster. That does not work if I don't have a structure in place that makes that possible. If adding people slows me down, then this isn't going to help me. And you may be rotating operation security expertise throughout the team if you don't have enough operation to security people or scrum masters or architects or whatever roles that you haven't consolidated into a DevOps team to embed into every team forever. It makes sense to rotate them through. So, again, your right-sizing these teams by potentially adding outside experts for a particular period of time to cross-train, to collaborate on a specific feature or capability, and then they may rotate out. So your teams probably don't have the same rosters over and over again. The point is you want some knobs to turn as business leaders and as engineering leaders to give maximum flexibility to whatever that business need is. IT isn't a sunk cost, it's an engine for delivering relevant services to the end user and to the internal stakeholders, and when you change that mindset from just being the sort of thing I constantly fund that keeps the lights on to something that drives innovation for our company, you have to show that you're also responsive to the things that matter the most to the business. So there are going to be knobs that you're going to be turning up and down and showing that you're a partner. So a few pieces of advice. Create a culture of rotating opportunities within the team. You don't want right-sizing to feel like punishment, so you want to have a culture where people are used to being able to rotate between different teams, sometimes voluntarily, sometimes as in this team is going to shrink. We're going to bounce people to other teams. Let us know where you'd like to work on. But making that feel like it's not a punishment, it's not a bad thing, instead it's just a natural lifecycle of this organism that is IT. Use data to make some of these decisions here. So understanding throughput for a given team is going to be important. So whether you're doing true scrum, and you have burndown charts, and you're looking at velocity and other productivity, you want to make sure you're moving people to respect that, that if a certain team has a certain velocity, I might not add 10 more people to it because it either might slow or they have the right velocity. So use data to figure out which teams need help as you're right-sizing, what the impact may be based on that person's skillset. You want to know the benefit you're going to add to other teams by making them bigger. It can't just be this is an important release, we'll add 5 people. How did you arrive at that number? Try to have some data so you know their velocity per engineer on that team, and so if I add 3 more, in theory I should have that much increase in velocity. Data is going to help you with that. Now avoid over-adjusting, in terms of doing this too frequently. If every week teams are being right-sized, you're going to get everyone seasick, there's too much going on there, it's too much uncertainty, and that's going to create angst and actually keep people from being productive. So make sure that this is centered around something you might do quarterly, or for some reason if there's a core business change that that's when it happens, but this shouldn't be something where people feel like organizational change happens every week. And this goes back to the other point I made earlier. Define technical standards that reduce the transition time for engineers. Think about the scenarios where you want teams to have freedom on their technical choices, and think about the places where you don't. What are the things that have the highest onboarding cost, the highest transition cost, and potentially standardize on those.
Summary

This Thursday of DevOps saw lots of action. We changed some team processes, we improved what it means to get a bridge going and get people on board to shrink our mean time to recovery in case of an incident. We learned that from our retrospective and then implemented the team process, but important to communicate that out and to figure out the right thing to do. We also want to create content about releases product changes, because now that we have increased velocity our content becomes stale much faster, and so you do need a sort of documentation Ops process that thinks about how do I increase the velocity of my documentation to match the increased velocity of my software delivery. We update our deployment pipeline. This often happens incrementally, but as I add things around security and different environments and other sophisticated things around zero-downtime updates and things like that, important to be able to practice that in earlier environments. And then, finally, right-sizing the engineering team, making sure that we're using business priorities to figure out how many people need to be on given application teams and not accidentally making teams too big when there's an actual core business need sitting out somewhere else, making this part of your culture through where teams accept this and actually like this. This gives them a lot of career opportunities, gives them some different flavor of what's working on in different teams. They never get stale, so this is a good thing, and it's something that becomes a responsibility of engineering leaders to welcome into their team. We're going to jump into Friday in our week of DevOps here in the next module, and wrap things up.
Week of DevOps – Fridays
Overview

Hi, my name is Richard Seroter. Welcome to this last module about implementing DevOps practices and principles in the real world. Our last module looked at things like changing deployment pipelines and adjusting teams. In this module, our last day of the week, Friday, we'll look at things like deploying software and cross-training our teams. So we'll start off talking about packaging our code for software releases, we'll deploy that application to production, look at some patterns there, and why that matters, we'll attend a team lunch and simply build some better relationships within our team, we'll do some cross-training to make sure everyone is well-informed about technology, but also the capabilities that we're building in our services and applications, and then finally, we'll wrap up the course with a summary of the whole course.
Holding Team Standups, and Packaging Code for Release

Like every day, we start with an organization or team standup. Again, that rhythm is so important, it just becomes part of everyone's day. At Pivotal we do that at 9:06 every morning, everyone just knows that's when they show up for that. This is team members, interested parties, again, we may learn things from yesterday, if there was a process change for handling incidents, or making sure if there is new documentation that someone contributed that everyone knows that's there. Again, good chances to improve awareness. As always, we're doing this to make sure we improve our throughput, fix blockers, if there are any things that are causing problems we can fix those, but also keeping the team well-informed on what matters most, which is delivering a great service to our customers. Later on on Friday, we want to package our code for release. So this is about having our application in a ready-to-deploy state, packaging it up, and this may involve our CI process, our Continuous Integration process, the on-call engineer may be partially responsible for some of this, and this matters in DevOps because of fast flow, of constantly making sure that I've got things in a ready-to-go state. It's so important to get software in the hands of our customers, because that's really the end of that idea to production case. Now, again, it's not ever done because I'm collecting metrics and improving it, but the end of at least the initial flow is getting that into production, and packaging that up is a big step. A package is the result of a successful CI pipeline, so it creates, really, the bill of materials for your deployment. The result of a successful CI pipeline means all the tests have passed. You may be able to do continuous delivery, meaning you actually have a package that's production-ready at any time. You may even do continuous deployment, which means you're actually deploying each change to production. Shrinking batch size is important, so it's important that the CI pipeline is constantly keeping in a green state so that you always have production-quality code ready to be packaged or ready to be delivered, and there may a special process you have that does a final packaging for production, potentially, but you want to make sure that at least your regular process is always running and you always have a good state. There's multiple ways to package an app, so depending on where you're going here, if you're going to a platform like, let's say Cloud Foundry, I just need raw code and a manifest. I might not need anything else to bundle it up, I just need the code itself. I may have compiled apps, plus scripts that are part of that. Or I could have containers, the result of a CI pipeline, and I deploy those sorts of things. So the way your app is packaged could be binaries, could be raw code, could be a container, could be embedded into a gold VM image. Lots of different ways you could choose to package that in a repeatable way. But what's really important is making sure you have all the pieces. So it can be easy to think of just code as being the part of your package, but this should include infrastructure scripts if needed. Are you opening up new networks, are you putting this onto a virtual private network in a cloud? Do you have certain loading scripts that you have to have for your database or even changing your schema? Are there smoke test components where you're deploying a piece that's going to be introducing new metrics and running some tests from outside in? Are there some third-party agents for things like monitoring or log collection that are part of the app that need to be included? Do you have some environment variables that you're changing? Maybe each environment has a different connection string and those get loaded as environment variables or a config store, you want to make sure that's included, and the overall deployment script should be there, and all of these things should be in source control. There should be no extra steps that require a release engineer to jump onto a box and fiddle with settings or turn something on. Everything that needs to be deployed as part of that app should be part of that package. And then finally, you want to ensure that the package can be recreated. Manually assembling a deployment package will lead to errors, we've all probably done that in our careers, where we have big complex deployment procedures that involve pulling the app code together, running a set of processes, and it's always prone to mistakes, because you may leave out a step, it may not be entirely repeatable because you're relying on people assembling sometimes very complex components. You don't want that. The same package should go through all the different environments in the same way to ensure no new conditions are popping up that you can't anticipate. So a few tips for getting started besides those. Document the current process to find non-automated steps. You may surprise yourself as you get started, that there's things you might have left out of your automation because those were things that you just always did. Maybe it's in your destination infrastructure, maybe it's something up front. You want to actually walk through that current process to make sure everything you do as part of deployment is either automated or at least really well-known, so it's not something that's just tribal knowledge. You might want to consider investing in a package repository or a binary repository. There's a few different tools out there that are package management systems or package managers, and they act as a bit of a bridge between a build system and a release system and so you can store these binaries, have a history, and deploy from there. There's some good tools that might be worth investigating for yourself. I guess this could be, again, somewhat controversial, but I believe you should only be using containers as the output of continuous integration, not at the input, so your CI system should be potentially creating containers, container files, as the output. Your developers shouldn't necessarily be assembling containers themselves and looking for images. Ideally this is something where dependencies are _____ pulled during a CI integration, standardized image or pulled as part of a CI process, and then you end up with a great container image at the end of it versus having developers doing it and introducing a lot of variables. So containers can be great, but are they an upfront design choice, or are they something that happens as the result of a build process? Hopefully it's as you integrate things and you're doing things through standard images, pulling dependencies the right way, and the output of that could be a container, or again, it could be code that goes to a platform like Cloud Foundry.
Deploying Application Updates to Production

Our next step on Friday is to actually deploy the application update to production. This is all about getting value in the hands of our customers. It's the culmination sometimes of having that initial idea that you want to then deploy as software, but it's just one hopefully boring step in the process. Who is involved? Well, the deployment pipeline hopefully does all of the work, and you may have the on-call engineer at least monitoring this process, but hopefully everything is done through automation. Why does it belong in DevOps? Well, it's the great close, it's the thing that's the last part of your delivery pipeline is getting that thing into production hands. It's an important part of completing the cycle and then being able to start over again as you collect feedback on what you've deployed. Shipping often for better outcomes, that's what it's about, but it should be practiced so heavily that it become boring. You want teams that practice it so many times it's just a non-event. The longer the interval between deployments the worse the outcomes, so I want to make sure that I have short intervals between deployments, and the team isn't scared to make changes. We've probably all worked on systems where we were terrified to actually update them, because we didn't even know if we could bring them back online again. Nobody even rebooted them. Instead, you want deployment to be something that even the newest person on the team could do without being completely freaked out. There's a few different deployment techniques you could follow to minimize downtime, and one option is to take downtime, and say, when we do deployments there's a couple of hours where it's going to be offline. That does happen, that's not ideal. You can do things like green/blue or blue/green deployments where it's a release technique where I try to reduce downtime by having two identical environments, call them blue and green, or red and black, whatever you'd like to do. At anytime only one of those environments is live. The live environment serves all the business traffic. So when I do deployments, though, I deploy to the alternate environment, let's say green, and when I make sure everything looks good, I simply switch all the traffic over to green, and now blue becomes the standby and so I simply have these environments I can switch between, and I can do this arguably with zero downtime, which is very exciting. You also may hear about canary releases. This is a technique for reducing some of the risk of adding new software versions in production by slowly rolling out that change to just smaller subsets of users. So you may launch in a certain geography first and see how that works, and then expand that, or have 10% of your users using the new servers, and then when that works you start to roll more people in there. The idea is being able to slowly find out what's going on. The canary refers to canary in the coal mine, you're getting early indicators. If something is wrong you can roll back, and if something is fine you can keep going. And, of course, you can also use things like feature toggles and deploy the code continuously. It's typically known that products like Facebook have most of the features they're releasing over the next few months already in production, they just have to flip them on with feature flags. You may ship the code constantly, but then marketing or other departments can actually choose when to expose that to their end users with configuration feature flags. Now databases can be trickier, since they're not stateless, so as I have schema changes I may wrap those in services. I may use some sort of way to make sure old clients are working, and new clients can take advantage of schema changes, so there's definitely some techniques you'll want to observe for dealing with databases. It's not as simple as having a bunch of stateless web apps, but you can still work well with this. Finally, you want to make sure you have telemetry in place to measure the impact, positive or negative, of a release. You want to make sure you're monitoring the business and system metrics before and after. Did all of a sudden you see no negative system performance information after a deployment, but new customer sign-ups have absolutely cratered, or all of a sudden you're seeing no orders come in. So maybe everything is technically fine, but maybe you broke something in the user experience and all of a sudden customers can't actually use the system correctly. So make sure you're tracking not just the health of the operational component, but make sure you're tracking business metrics, hopefully visibly, so you can make sure nothing bad happened. A few extra tips for getting started. Try automated deployments with individual services before moving to entire systems. So if you start introducing automated deployments, it may not make sense to literally deploy everything for the first time, at one time. Instead, you may choose to say, let's have some services go, some of these stateless services where we don't have to worry about downtime the same way, we can do things like blue/green, maybe move up to some data intensive systems to figure out how that works. You may have a few different ways you introduce automated deployments to your systems, and you might want to do that incrementally. Blue/green is probably the easiest way to start, again, it's based on your environment, based on your team's skillsets, but this is a great way to have alternate environments be able to make sure you have some protection if you need to roll back, which is nice. Sometimes teams do want to roll forward, and you never roll back, you only simply add to those environments, which is great if you can do it, but it's nice to have an environment when you're getting started where if something just goes wrong you can point people back to a known good running environment. And as I mentioned, watch business and system metrics, super important. You want to make sure that everything is working from an entire perspective, not just no technical errors in your log files, but also making sure that the health of the business remains good, and hopefully improving based on the new software you've deployed.
Attending a Team Lunch

Well, it's midday on Friday, so we want to attend a team lunch. This is something that's really about getting together and team building, making sure that your team builds some relationship with each other as they're working together. And everyone gets involved here. This isn't just for team leaders, it's not carving people off to a separate table to collaborate about the mysteries of the world, instead it's about making sure that everyone has a chance to get to know each other a little better and build some trust. This matters for team building and trust, it matters for a sense of unity. You want that single purpose for the team, to at least feel like they're all chasing something similar and getting to know each other. So trust is a foundation of DevOps, an important piece of things, and building relationships within and across your teams. Now you may have fully virtual teams where this is difficult. You don't get together physically all the time, maybe it's once a year or a few times a quarter, but the goal is to build relationships between team members. Long-lived product teams, not projects, mean that relationships matter just a little bit more. Instead of having product teams where, hey, if I don't like these people or whatever, my project is over in a few months anyway, instead with a product team or a service team I'm actually focused on long-lived support here, and so I need to trust my colleagues, because I'm going to depend on them when there's an outage. I'm going to get tickets from them when I become the on-call engineer, a lot of things that matter. Now, what's important is everyone doesn't have to be best friends. DevOps isn't some sort of cult where everyone needs to hang out on weekends and become best buddies. That's not realistic. But instead, you want to at least have a lot of mutual respect for each other because these connections matter during a crisis, during retrospectives. Teams that know each other can come together when necessary and not have in-fighting and not have problems. Instead they can have honest communication, they understand each other's strengths, they know who can do what, who has what kind of skills, who is a good communicator and somebody who would take charge in an incident. That's good to know. Who's more introverted, but extremely smart that you might want to put on something for forensics to figure something out. Those are really important things. You want people to trust each other most of all. Now meetups to get to know each other better, that should be done by the team, but I also encourage the executives to be doing that. You want to demonstrate that not only can teams self-organize, but that there's an investment from those higher up in the organization in building team unity. A few other tips for getting started. So I would recommend set aside some budget that you can buy lunch for the team once a week. You should be able to invest in your team and demonstrate that, just saying hey work better together, but putting your money where your mouth is there. For meetings, use video conferencing if you do have a remote team, which is fine. I, myself, have a very remote team right now, and we make sure we use video conferencing for all of our conversations so we're actually looking at each other. We're not just faceless voices at the end of a conference call. Instead, you're getting to know people by at least even seeing a reaction, sometimes laughing together, having that relationship that doesn't come from people just on a voice call. So invest in video conferencing. It's easy nowadays, free with so many services, it's worthwhile. But what's important here is not forcing group socialization. So, again, it can accidentally come across as, hey, we all need to go bowling every night, we all have to force ourselves to be social. Everyone isn't into being social with their coworkers. They want to respect them, you want to work well together, but at the same time, you have your own life. So you may want to foster one-on-one lunches so small groups can get to know each other. Simply look at your team and understand the dynamics. Don't impose something that might not be a good cultural fit.
Cross-training Other Teams

Finally, we're going to wrap up our Friday with some cross-training, because it helps support personnel, it helps other teams know about your service. This is about taking a step back and saying, it's not just about local teams, it's about the global group, and how am I making sure everyone is aware of what's going on, specifically support and folks that may be interfacing and talking about your product to other people, keeping them well-informed so they don't get embarrassed or actually provide lesser service because they don't know about new things you've done. Who is involved? Anyone in the organization. This should be opened up to your company as a whole. Of course, it belongs in DevOps because it's about, again, continual learning. This may be something where you're learning about a new technology together, this could be teams demonstrating their work, this could be all sorts of things, but it's about being very transparent and helping teams learn. So share local learnings more broadly, "show and tell". Keep teams thinking about deliverables. After every release, potentially have every team do a "show and tell" where they demonstrate what they've built. Developers like showing what they're working on. When I would build something, I would like to be able to show that off. I've watched our teams being able to show off what they work on, and it's a very proud moment. So give your teams that forum where they can demonstrate what they're building, not only just to show the technical guts, but also demonstrate the new features, so someone who is selling it or supporting it or architecting for it now understands something new. Now, of course, this "show and tell" isn't just for software engineers. You may be showing off infrastructure features, you may be showing off a new bot that the support team built. You may be showing off a new product management technique that the team is going to be using for new sprint planning. So "show and tell" can be lots of different things. It's not just new software, it's not just new features, it could be new automation that the team collectively built with pairing, again, it could be ops oriented, it could be all sorts of things. The key is giving that platform to be able to demonstrate new accomplishments and things that other teams can learn from. It's particularly useful for front-line support engineers. So they should learn about new features the first time customer calls about it and says it's not working. You want to make sure that those people understand what's happening with the system, maybe new defects that could pop up or new scenarios that they have to be ready for. It might even be new support procedures because you added a new database that you might want to let them know that they can query if there's problems or even restart if there's an issue. As components change, as features change, you want to make sure your support people know. And also, again, useful for other stakeholders, so if you have other application teams, they might see your new feature or the new technology you introduced and want to use it themselves, and that's about sparking ideas with other teams. When new learnings are discovered locally, there has to be some way for everyone else to figure that out and benefit from it. You want to avoid duplication. There's no reason to have every team do their own flavor of the same thing. Avoid duplication of the technology. I want to have lessons learned so they don't go down the wrong path. At least learn from my pain and my application team so that you don't use the same thing. And then finally, sharing the result of experiments with new technology. I've, again, enjoyed watching developers and engineers over the last few years take some downtime during holidays or even in between major releases to try out some new technology and then use a luncheon learner or brown bag session to actually demonstrate this new thing. And maybe it becomes part of the system, maybe it doesn't, but you want to make sure there's this feeling and vibe where people know that they can learn, and that's okay. That they can take downtime and learn new technology, that they can try to introduce it if it makes sense, and not if it doesn't, but you want to make sure there's this feeling that we should be constantly learning. So a few extra tips for getting started. Ask teams to demo after they ship. Make this something that's just expected. And whether it's in person, or it's a video recording, or something, each team should expect to demonstrate what they've built after they've delivered it. Invite a broad audience outside the application teams. So, again, even beyond support, make sure that even salespeople might want to sit in on this because they're talking to customers every day about this technology. Record these things, make sure it's easy, that people can go look at these after the fact and play them. At Pivotal we do these all the time, and we have this giant archive where I can go back and listen to these replays to find out what has been built, how it works, and then I can go try it myself. So you want to make sure you don't force everyone to attend live things, especially if you have a global team. If you have distributed teams where time zones don't work, don't necessarily leave people out or make them feel neglected simply because they're not in the time zone with every other engineer.
Module and Course Summary

So in this particular module, we talked a little bit about packaging code, the value of having that come out of a CI process, deploying that application to production, such an important stage in the lifecycle of DevOps, but then continuing to build on the team collaboration, making sure that you're building trust with your collaborators, your teammates, cross-training, learning new technology, learning how to use the things that were just built, other ways to share lessons learned. Now, of course, you've made it through the whole course. Congratulations, I hope you enjoyed this. We talked about DevOps environments, and I hope throughout this course you saw these things, that they're optimized for throughput, that as you have a week in the life of DevOps you're focused on throughput, removing bottlenecks, a clear view of the entire deployment pipeline. This is where many enterprises go wrong, because each group has only a small view of the overall deployment. They think about their step, and when they're step is done they're done. Instead, in DevOps everyone has a clear sight of the entire deployment pipeline and their role in it, which is hopefully much bigger then just a small department. It's about a customer-centric definition of done. Hopefully you heard from me in many cases, the reason we do many of these things is because we're focused on the customer experience, we're focused on not just internal-facing success metrics, but customer-facing success metrics and what "done" looks like. Done is when the customer can use it, not when my QA process is finished. Small frequent software releases. We saw small batches as a theme throughout this course. How am I delivering frequently? And why? It's not just about delivering faster, it's about delivering things that add value and let me collect feedback to make a better service, that's the point. When I get things out faster, I can experiment faster, I can help our customers faster, I'm not doing massive releases that may be wrong, and it's going to take me too long to fix it. Feedback loops, one of the most important parts of DevOps. I want to get feedback from customers, from support people, from other stakeholders, from engineers, and I want to use that feedback. Just collecting it doesn't matter, I have to put it to use, and so that's a big part of, I think you saw, some of the meetings that we talked about in this course, and some of the other retrospectives, other ways to collect feedback and turn it into action. Automation is a big part of this. While DevOps is a lot about cultural change, and how people work together and improving that, automation plays a key part in making these things happen reliably. So you can have all the collaboration and trust in the world, but if I don't have automation I am going to still have throughput issues. And then finally, focusing on outcomes, not activities, all the things we talked about here. You may choose to do some of them, none of them, all of them, whatever it may be. The focus is the outcome. The outcomes should be business-centric, customer oriented, and it's not about a bunch of ceremonies or activities that say you're just doing something. Instead, it's are you reaching your objective, and if you're not, get rid of that process or step, don't do that particular activity. If it does add value, maybe double-down or add new ones. I hope you enjoyed this course, I certainly did. This is a great topic. We're going to continue to talk about DevOps at Pluralsight, and of course in the industry. If you have feedback for me, find me on Twitter at @rseroter or seroter. wordpress. com for my blog. Thank you for taking the time, and I look forward to chatting with you.
Course author
Author: Richard Seroter
Richard Seroter

Richard Seroter is the VP of Product Marketing at Pivotal, with a master’s degree in Engineering from the University of Colorado. He’s also an 11-time Microsoft MVP for cloud, an instructor for...
Course info
Level
Beginner
Rating
4.5 stars with 115 raters(115)
My rating
null stars
Duration
2h 53m
Released
27 Jan 2017
Share course


