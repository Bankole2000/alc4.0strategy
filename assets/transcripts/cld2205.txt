Architecting Data Warehousing Solutions Using Google BigQuery
by Janani Ravi

BigQuery is the Google Cloud Platform’s data warehouse on the cloud. In this course, you’ll learn how you can work with BigQuery on huge datasets with little to no administrative overhead.

Organizations store massive amounts of data that gets collated from a wide variety of sources. BigQuery supports fast querying at a petabyte scale, with serverless functionality and autoscaling. BigQuery also supports streaming data, works with visualization tools, and interacts seamlessly with Python scripts running from Datalab notebooks. In this course, Architecting Data Warehousing Solutions Using Google BigQuery, you’ll learn how you can work with BigQuery on huge datasets with little to no administrative overhead related to cluster and node provisioning. First, you'll start off with an overview of the suite of storage products on the Google Cloud and the unique position that BigQuery holds. You’ll see how BigQuery compares with Cloud SQL, BigTable, and Datastore on the GCP and how it differs from Amazon Redshift, the data warehouse on AWS. Next, you’ll create datasets in BigQuery which are the equivalent of databases in RDMBSes and create tables within datasets where actual data is stored. You’ll work with BigQuery using the web console as well as the command line. You’ll load data into BigQuery tables using the CSV, JSON, and AVRO format and see how you can execute and manage jobs. Finally, you'll wrap up by exploring advanced analytical queries which use nested and repeated fields. You’ll run aggregate operations on your data and use advanced windowing functions as well. You’ll programmatically access BigQuery using client libraries in Python and visualize your data using Data Studio. At the end of this course, you'll be comfortable working with huge datasets stored in BigQuery, executing analytical queries, performing analysis, and building charts and graphs for your reports.

Course author
Author: Janani Ravi	
Janani Ravi
Janani has a Masters degree from Stanford and worked for 7+ years at Google. She was one of the original engineers on Google Docs and holds 4 patents for its real-time collaborative editing...

Course info
Level
Beginner
Rating
4.6 stars with 13 raters(13)
My rating
null stars

Duration
2h 48m
Released
15 Oct 2018
Share course

Course Overview
Course Overview
Hi, my name is Janani Ravi, and welcome to this course on Architecting Data Warehousing Solutions Using Google BigQuery. A little about myself. I have a master's degree in electrical engineering from Stanford and have worked at companies such as Microsoft, Google, and Flipkart. At Google, I was one of the first engineers working on real-time collaborative editing on Google Docs, and I hold four patents for its underlying technologies. I currently work on my own startup, Loonycorn, a studio for high-quality video content. In this course, you will learn how you can work with BigQuery on huge datasets with little to no administrative overhead related to cluster and node provisioning. BigQuery supports fast querying at petabyte scale with serverless functionality and autoscaling. In addition, BigQuery also supports streaming data, works with visualization tools, and interacts seamlessly with Python scripts running from Datalab notebooks. We start off with an overview of the suite of storage products on the Google Cloud and the unique position that BigQuery holds. We'll see how BigQuery compares with Cloud SQL, Bigtable, and Datastore on the GCP, and how it differs from Amazon Redshift, the data warehouse on AWS. We'll then create datasets in BigQuery, which are the equivalent of databases and RDBMSs, and create tables within datasets where the actual data is stored. We'll work with BigQuery using the web console, as well as the command line, we'll load data into BigQuery tables using the CSV, JSON, as well as the AVRO format, and see how we can execute and manage jobs. We'll then study advanced analytical queries, which use nested, as well as repeated fields. We'll run aggregate operations on our data and use advanced windowing functions as well. We'll programmatically access BigQuery using client libraries in Python and visualize our data using Data Studio. At the end of this course, you will be comfortable working with huge datasets stored in BigQuery, executing analytical queries, performing analysis, and building charts and graphs for your reports.

Understanding BigQuery in the GCP Service Taxonomy
Module Overview
Hi, and welcome to this course on architecting data warehousing solutions using Google's BigQuery. Now BigQuery is Google's data warehouse, but there are a whole variety of data storage services that the GCP provides. We'll first start this module off by understanding where exactly BigQuery fits in in the GCP service taxonomy. As you're probably already aware, organizations collect data from a wide variety of data sources. For example, a company that runs an e-commerce site will collect statistics from its website, its mobile applications. Maybe it has an additional application to track its deliveries. Statistics and data from all of these will come together and be stored in a data warehouse, and this is where BigQuery comes in. BigQuery is GCP's data warehouse on the cloud. This supports both standard SQL and other SQL drivers such as ODBC and JDBC. There are a variety of ways to work with BigQuery. It offers an intuitive and easy-to-use web UI. You can use the command line, or you can access it programmatically. Client libraries are available in Python and most of the other common programming languages. The best part about BigQuery and its most powerful feature by far is the fact that it's serverless. There are no clusters that you need to set up, no nodes that you need to instantiate, no servers that you need to manage at all. BigQuery can be considered to be completely no-ops. BigQuery works in such a way that there is absolutely no need for you to set up indices on any of the data that you feed in here. The fact that BigQuery is serverless, and the machines on which data is stored and where you run your queries is completely abstracted away from you, means that BigQuery can offer autoscaling right to petabytes just out-of-the-box. In addition, BigQuery supports a number of advanced features. You can work with streaming data that is fed into BigQuery, and BigQuery gives you real-time analytics.

Prerequisites and Course Outline
Before we move to the actual course content, let's see what the prereqs are so that you can make the most of your learning. As far as courses are concerned, there are no required prereqs. This course does not assume that you're familiar with working on the GCP. However, if you're looking for other technologies on the GCP that might be considered prereqs to this course, here are some courses on Pluralsight that you can watch. Creating and Administering Google Cloud SQL Instances shows you how the relational database service works on the GCP. Architecting Google Cloud Storage Configurations will show you how the elastic storage service on the GCP works. Here are some skills that might be considered prereqs for this course. You need to have a basic understanding of how cloud computing works, the GPC, AWS or Azure, knowledge of any of these platforms will be helpful, and you should be very comfortable working with SQL queries. Querying and processing data in BigQuery is all in SQL, so you should be comfortable with SQL as it applies to an RDBMS. We'll start this course off with a basic introduction to BigQuery. We'll talk about where exactly BigQuery fits in the range of GCP services, we'll talk about how BigQuery is priced, and we'll compare it to other GCP technologies. We'll then move on to understanding the basic model of data storage within BigQuery. We'll talk about datasets, tables, and views, and we'll talk about partitioned tables. The module after that will focus on the advanced analytical queries that you can perform on BigQuery, we'll talk about how you can store data in nested and repeated fields, we'll see a range of aggregate operations in BigQuery, and also work with windowing functions. And finally, we'll see how we can access data stored in BigQuery programmatically. We'll work with Datalab notebooks, which is basically Jupiter Notebooks hosted on a VM in the cloud. This is the only part of the course where we'll do a little bit of Python programming, very little. All of the demos will assume that engineers in this organization, SpikySales. com, are evaluating BigQuery on the GCP. They're contemplating a move to the cloud. SpikySales is a hypothetical online retailer which specializes in flash sales of trending products. Traffic to their website and mobile app tend to be very spiky. At sale time, they have huge traffic with lots of visits, at other times it's relatively quiet, which is why cloud computing fits perfectly. They're interested in the pay-as-you-go model. They do not want idle capacity during off-sale periods. Data analysts and engineers working in SpikySales are very interested in BigQuery as an analytical data warehouse. They're especially drawn to the serverless operations that it offers. They know that they only need to pay for storage and queries, and not for servers that they may not always utilize. And they have built-in visualization using Google's Data Studio.

Transactional and Analytical Processing
When you're working with data, the kind of processing that you perform on this data can be divided into two categories, broad categories, transactional processing and analytical processing. For example, John is responsible for order management support. At SpikySales, he's responsible for tracking and delivering orders on time. Anna works for SpikySales as well. She is a revenue analyst. She is responsible for tracking and monitoring revenues for the company. This is what a typical workday looks like for John. He finds out that 20 deliveries to Kent, Washington have been delayed. The courier company there has had a computer outage, and they aren't able to make their deliveries on time. John then goes into the order management system and assigns these orders to another courier company in the same region. Here is what a typical day for Anna looks like. Anna's manager wants an update on last month's revenues for SpikySales, Last month was an unusually slow one, despite the fact that there were as many sales as in previous months. Anna pulls up data for the last five years to check for seasonal effects. John and Anna both work with data and both use data processing systems, but the kind of processing that they do is very different. John primarily performs transactional processing; Anna performs analytical processing on data. Let's quickly take a look at some of the differences between transactional and analytical processing. Transactional processing typically involves analyzing individual entries in any dataset. Analytical processing involves analyzing large batches of data. When you're working with transactional processing, you're typically working with recent data. Access to recent data from the last minute, the last hour, or the last few days is more important. Analytical processing involves accessing older data going back months or even years. Analytical processing is all about seeing patterns and extracting insights, which means we need data going back a long way. Transactional operations are mostly about updating the data, and it's important that updates follow asset properties. Analytical processing mostly involves reading data or querying data. Transactional processing typically requires fast, real-time access to data. Analytical processing is typically made up of long-running processing jobs. Transaction processing involves using data, usually from a single source, whereas with analytical processing, you'll access data that might be from multiple sources. As you can see here, the objectives for transactional and analytical processing are completely different, but back in the old days, when the data involved was small, both of these objectives could be achieved using the same database system. You could use a single machine or a single RDBMS server, and basically work with that machine to perform all of your transactional and analytical processing. You would have a single machine maybe with some backup. The data would be structured, well-defined, you'll access individual records or the entire dataset. This is small data after all. As the size of data collected and stored by organizations kept growing, and we ended up with big data processing, it was very hard to meet all of these requirements of transactional, as well as analytical processing, using the same system setup. As data sizes grew, processing needed to move to a distributed cluster which had multiple machines. The kind of data that we were working with also changed. Data could be semi-structured or even unstructured data. These big data solutions worked well for analytical processing because they do not offer random access to data, which is typically not needed when you're running long-running queries or jobs. Data would be replicated, and the propagation of updates would take time that was fine as well. Analytical processing places more importance to older data or trends. Extracting insights with analytical processing also involves access to data from multiple sources. These different sources might have unknown or different formats. When we talk about a huge amount of data, or big data, we typically speak of three Vs. The first V refers to volume, that is the shared amount of data. The second V refers to variety, the number and different types of data sources that we might need to work with. And we have the third V that stands for velocity. Data might come in a streaming format, it might come in very fast, or you might have batch operations that need to run quickly. At this point in time, we have different systems which work with transactional processing and analytical processing. Transactional processing is typically done on a traditional relational database management system. Analytical processing is performed using a data warehouse, and this is where BigQuery fits in. BigQuery is a data warehouse that is hard to tell apart from an RDBMS, which means analysts that have been working with traditional relational databases can move to BigQuery seamlessly.

Introducing BigQuery
At the end of the last clip, we described BigQuery as Google's data warehouse that is hard to tell apart from an RDBMS. The best way to really understand what BigQuery has to offer is to compare and contrast it to a traditional data warehouse. All data warehouses have been explicitly built to support complex analytical queries, and the same is true for BigQuery as well. Data that you store in BigQuery can scale to petabytes. Once again, this is not so very different from other traditional data warehouses. BigQuery, though, supports both reads, as well as updates to your data. Traditional data warehouses typically do not allow row level update, you can only read data from within it. One of the most important features of BigQuery is the fact that it's very, very fast. It allows fast, real-time access to your data, allows batch processing operations to run as well. Traditional data warehouses typically only support long-running jobs. You can have data from multiple sources stored in BigQuery, once again, not so very different from a traditional data warehouse. BigQuery allows you to work with streaming data as well. You can stream in data into BigQuery buffers and query it real-time. Traditional data warehouses tend to be more focused on batch processing operations. Another useful way to understand BigQuery is to compare and contrast it with a traditional RDBMS. BigQuery uses standard SQL, which means it's no different from working with a traditional RDBMS. BigQuery data scales to petabytes, traditional RDBMSs can typically store just terabytes of data. Unlike traditional relational database management systems, BigQuery does not offer transaction support or support for ACID properties. Another significant difference, which in fact is a huge advantage when you use BigQuery, is the fact that it's serverless. You don't have to instantiate compute nodes where you store your data. All of this is taken care of behind the scenes and abstracted away from the user. Relational databases are a classic example of using a server. You have to instantiate nodes, install the database, administer the database, and so on. When you're using BigQuery, you don't need to provision nodes to store your data off a compute, you don't need to set up indices on your data as well. RDBMSs on the other hand, have a heavy administrative overhead. Now that we've understood how BigQuery is different from a traditional data warehouse and a traditional RDBMS, let's talk about what features it has to offer us. The very first thing about BigQuery is the fact that it's serverless. You don't have to set up a cluster. There is no provisioning of compute nodes or storage nodes. This is a significant factor in BigQuery pricing. You don't have to pay for instances to be up if you're not using them. The fact that BigQuery is serverless means that there are some specific implications. The first is, BigQuery can support autoscaling out-of-the-box. It can expand to meet your processing, as well as storage needs without you having to explicitly configure anything. Serverless also implies that BigQuery is automatically configured for high availability. Data is replicated, and should always be available to you. It has a very high uptime. BigQuery has full support for the three Vs of big data. The first is volume, and BigQuery storage scales to petabytes of data. BigQuery is actually very powerful in terms of the variety of data sources it supports. It's fully integrated to all other technologies on the GCP. You can store data within BigQuery itself. You can also use BigQuery to query data that is stored on cloud storage buckets, BigTable, or even on Google Drive spreadsheets. Data can live elsewhere but can be queried from within BigQuery. This is huge. And finally, we have Velocity. BigQuery can support streaming data, and supports real-time queries on this data as well. You don't have to learn a new programming language or tool to work with BigQuery. BigQuery supports standard SQL, which is ANSI:2011 compliant. There are extensions built in to work with nested and repeated fields. In addition to standard SQL, which is what you typically use with BigQuery, there is an additional SQL dialect that BigQuery recognizes. This is referred to as legacy SQL. The original version of BigQuery only understood legacy SQL. The latest versions, though, have full support for standard SQL, and that's what is recommended for use. Even though BigQuery is serverless, it gives you some control over where exactly your data is stored. It has specific support for some locations. You can specify that you want your data within the US, or within the European Union. You can also store data in Japan, specifically in Tokyo. BigQuery is being actively developed, and it has many cool new features such as BigQuery ML, which is currently in beta. The objective of BigQuery ML is to bring machine-learning algorithms to where data is stored. It democratizes machine learning, making it easy for data analysts who know only SQL to build machine-learning models. We won't be covering BigQuery ML in this introductory course, though.

Choosing BigQuery
The GCP, just like other cloud computing platforms, offers a wide variety of options to store your data. Let's see what these options are, and let's see where BigQuery fits in. Now the data that you want to store would be unstructured data or structured data. With unstructured data, you typically want cloud storage buckets, that's the object store, or you might store them on persistent disks that are attached to your VM. With structured data, you have a choice as well. You can choose to store it in SQL storage or noSQL form. In the case of SQL storage, it could be for transactional processing or analytical processing. SQL storage is typically used for OLDB, online transactional processing, or OLAP, online analytical processing. A NoSQL store could be BigTable or Cloud Datastore. There is an asterisk here next to Cloud Datastore, because it's a document-based storage. Your data should be in the form of entities to store it in cloud Datastore. For transactional processing on the GCP, you have two choices. You can go with Cloud SQL, which is its relational database management service, or Cloud Spanner, which offers you ACID++ properties at scale, distributed across your graphical locations. If you want to perform analytical processing on the GCP, you'll use BigQuery. Now let's see the differences between Cloud SQL and BigQuery, and when you would choose to use one over the other. The first factor in your decision making is how much data you want to store. If your data is in petabytes, you'll choose BigQuery. Cloud SQL tops out at 20 terabytes of data. BigQuery, as we've discussed before, is serverless, you don't have to provision nodes or clusters. Cloud SQL requires node provisioning or cluster provisioning. Just like any other traditional relational database, Cloud SQL requires that you manually design schemas and build indices for your data. All that is not needed for BigQuery. BigQuery is highly available out-of-the-box. There is no explicit configuration setting. Cloud SQL has to be explicitly configured for high availability. BigQuery does not support transactional features or ACID properties. Cloud SQL does. Both BigQuery and Cloud SQL is used with a structured query language. Big Query follows weak schema enforcement. You specify the schema, but your data doesn't have to strongly adhere to this. In the case of Cloud SQL, though, there is strong schema enforcement as in the case of all RDBMSs. Let's now understand the differences and the similarities between BigQuery for analytical processing and BigTable. This is the columnal store on the GCP. BigTable is very similar to HBase, but far more powerful. Both BigQuery and BigTable can scale to petabytes of data. BigQuery is serverless, but BigTable requires you to provision nodes or clusters. BigQuery supports autoscaling, whereas with BigTable you have to explicitly design your cluster, disk type, and other features. BigQuery is typically used for data in a columnar format, which supports nested and repeated fields. BigTable supports data in a columnar format, four-dimensional columnar data model. BigQuery offers SQL storage, BigTable follows NoSQL technology. Accessing data in BigQuery is very, very fast. The latency is in the order of seconds. BigTable is faster though. The latency order is of milliseconds. You can update data stored in BigQuery, but writes are relatively slower as compared with reads. With BigTable, though, you have extremely fast reads, as well as writes. The cost associated with BigQuery are for storage and query processing, and you'll find that BigQuery is relatively inexpensive. BigTable, on the other hand, is quite expensive to use. We'll now compare BigQuery with the other NoSQL offering from the GCP, Datastore. BigQuery scales to petabytes of data. Datastore is more effective at terabyte scale. Both BigQuery and Datastore are similar, in that they are first serverless technology. There is no provisioning of nodes or clusters. BigQuery stores relational data. Datastore stores information in the form of documents, or entities which have attributes. BigQuery gives you SQL access, Datastore is a NoSQL technology. BigQuery supports analytical queries, Datastore supports hierarchal queries. That's because the entities themselves are stored in a hierarchal format. BigQuery is used for OLAP, that is online analytical processing. Datastore can also be used for OLAP, but in addition it also has transactional support. Transactional support is lacking in BigQuery, as you already know. It's often useful to compare BigQuery with offerings from other cloud computing platforms. Let's quickly take a look at the differences and the similarities between BigQuery and Amazon's Redshift data warehouse. The most significant differences between these two is the fact that BigQuery is serverless. There is no provisioning needed before you get up and running with BigQuery. With AWS Redshift, though, you have to set up nodes and clusters. AWS Redshift kind of works like BigTable in this case. Both BigQuery, as well as Redshift, work at very high scale. BigQuery offers autoscaling, though. You have no control over the number of nodes used to store and process your data. BigQuery automatically takes care of scaling for you. With AWS Redshift, you can scale, but you have to explicitly add nodes to your cluster. The operational overhead when you work with BigQuery is minimal to nothing. With AWS Redshift, because you're provisioning nodes, there is some ops involved, and cluster maintenance might be needed.

BigQuery Pricing
Before we use BigQuery on the GCP, it's important we understand how exactly BigQuery is priced. There are three components to BigQuery pricing. The first is the storage cost for your petabytes of data. The second is the cost of querying and processing your data. And lastly, we have certain operations that are always free in BigQuery. Let's first talk about what we have to actually pay for, storage and query costs in BigQuery. BigQuery costs can be divided into two components, storage and query costs. Storage costs differ based on whether your data is active data or long-term data. Data modified within the last 90 days is considered active data, and it costs more to store. Long-term data has lower storage costs. Query costs vary based on the amount of data you process with each query. These can be divided into on-demand queries, as well as flat-rate queries. For on-demand queries, you pay based on usage. For flat-rate, you have predictable costs from one month to another. Let's talk about active and long-term data in BigQuery, and how costs differ. Data which are stored in tables that have been modified in the last 90 days are considered active data. Data in tables that haven't been modified within the last 3 months is considered long-term data. Active data, as you might imagine, is more expensive to store, approximately 2 cents per GB per month. For long-term data it's about 1 cent per GB per month. Costs are 50% lower with long-term data. For both kinds of data, the first 10 GB that you store in BigQuery is absolutely free. Managing data as active and long-term is automatically done by BigQuery. When a table is edited, the pricing structure reverts to that of active data. When a table is not edited for about 90 days, the pricing automatically drops to that of long-term data. Query costs for BigQuery also differ based on whether your usage is on-demand or you're on a flat rate. Now if you and your team use BigQuery only sporadically, you might want to go with on-demand pricing, because it's based solely on how much you use and process data. Flat-rate pricing, though, is better for larger teams and for more predictable monthly costs. In on-demand pricing, the first terabyte of data that you process within a month is absolutely free. Over and above that, the cost is $5 per terabyte per month. For flat-rate pricing, the pricing is in terms of BigQuery slots. It's $40, 000 per month for 2000 slots; $10, 000 for 500 additional slots for that month. A slot in BigQuery is a measure of computational capacity and one slot is one unit of computational capacity. Now BigQuery automatically calculates how many slots it needs to execute a particular query. The more complex your query, the more slots it will occupy. Now that we've understood storage and query costs in BigQuery, let's talk about those operations which are always free. BigQuery doesn't charge anything if you want to load data into BigQuery, however, there is a slight wrinkle here; streaming inserts are not free. If you're loading streaming data into BigQuery, you'll have to pay additional charges. Other operations that are free are copying and exporting data from your BigQuery tables. If you want to delete datasets, or delete tables, views, and partitions in your data, these operations are free as well, as are metadata operations. Notice that all of the operations that are not actually processing your huge dataset are free operations. Streaming inserts are 1 cent for every 200 MB, however, only successfully inserted rows are charged. When you're working with BigQuery, there are several techniques that you can use to minimize the cost that you incur. The first thing is that BigQuery is a columnar store under the hood. We don't know how exactly the data is laid out, because that is Google's proprietary information, however, you can minimize the cost of processing by querying only that data which you require within a certain query. There are additional columns in there, leave those out. Every column of data is stored separately in BigQuery in a separate encrypted and replicated file. Each column that you add to your query, adds to your cost. If you want to just explore what the data looks like in a particular BigQuery table, use the table preview option that's available on the web console. Don't run queries just to explore your data. That will incur processing costs. Make sure you're always aware of costs when you're working on a cloud platform, because these costs can add up. You can also calculate the query price before you execute your queries. You can use the dry_run flag in the command line interface. You can use the query validator in the UI. You can always take a look at the GCP pricing calculator before you perform heavy-duty operations.

Logging into the GCP and Enabling APIs
Enough with the talk, let's get hands-on in this demo. We'll see how we can log into the GCP and access BigQuery from the GCP web console. Now the Google cloud platform is available at console. cloud. google. com, and you can log in with any Gmail account, or a G Suite account. So I'm logging in as spikeysales@loonycorn. com, specify your password, and you'll be logged in. This is the main GCP dashboard from where you can access all of your resources. Notice that we have a little card here for billing. Make sure you enable billing before you do the demos in this course. Your BigQuery processing should come within the free tier. You might incur some charges, though, when you're using a Datalab VM instance, for example. Any time you're working within the GCP, you'll be within a project. A project on the GCP is a logical grouping of GCP resources and services. A project is also a billing unit. Typically, a project is associated with a particular feature within your org, or maybe a team. You can change the current project that you're working on by clicking on this drop-down at the very top and choosing a project from the projects that have been enabled. On the top left here, you have the hamburger icon that gives you access to all of the services and products that GCP has to offer. This will bring up a navigation menu, and you can scroll down this menu and get access to any product or service. We'll start off with APIs and services, and enable those APIs that we need in order to perform the demos in this course. Click on ENABLE APIS AND SERVICES up on top, and this will bring up a search page where you can search for a particular API to enable. You can also scroll down this list until you find the API that you're looking for. For our current project, you want to enable the Google Compute Engine API, which will allow us to instantiate VM instances. Go to the APIs page and click on the Enable button. The API will be enabled for you. You can search for other APIs that you want to enable using the search box at the top. We also want to enable the Cloud Source Repositories API so that we could use Datalab Notebooks. Once this API has been enabled as well, we are now ready to use BigQuery.

Beta UI and Classic UI
Let's get to see the BigQuery web console. Using the hamburger icon, open up the Navigation menu. That will give you access to all of GCP's services. If you scroll down to BIG DATA, you'll find BigQuery. That is the first option on this list. If you're planning on using BigQuery often, you might want this to be more easily available, you can click on the pin icon there to pin the BigQuery menu item to the very top just under Home. Click on the BigQuery link, and this will take you to the BigQuery web console. This is what the UI looks like at the time of this recording. The beta version of the UI is currently available along with the classic version, and you can choose to use either of these. All of the functionality is not available in the beta version. For some things, you may have to switch back to the classic version. On the left navigation pane, here are some quick links that are very useful. You can see a history of all the queries that you've done in Query history. You can save certain queries and access them quickly in Saved queries. You can take a look at all of the jobs that you've run on BigQuery, such as importing and exporting data. The Transfer service is what you'll use for large data migrations into BigQuery. These are the quick links on the left nav pane. When we work with BigQuery on the command line, we'll use the Cloud Shell terminal window. Cloud Shell is an ephemeral VM that you can access using a terminal window on the browser itself. Cloud Shell comes with all of GCP's command line utilities preinstalled on that VM. Since all operations are not yet available on the beta version of the UI, here is a link that will take you to the classic BigQuery UI. Click on Go to Classic UI. This is what the UI used to look like previously. You can start writing queries by clicking on this big, red, COMPOSE QUERY button here. There are quick links here on the left navigation pane exactly like in the beta UI. The one differences here is the public Datasets. Public datasets on BigQuery are datasets that you can use for prototyping your models and your queries. The storage costs are absolutely free. They're stored on a public project, which can be accessed by anyone using BigQuery. You only pay to process this data.

Public Datasets
We start off here in the beta UI for BigQuery. Notice here that we do not have access by default to public datasets. We can get access by clicking on the navigation menu and going to Marketplace. GCP's marketplace here gives you access to a whole variety of products and services that can be used along with the Google Cloud Platform. But what we are looking for, though, is BigQuery public data. My search term is BigQuery dataset, and it brings up a number of public datasets that I can use here. I'm going to click on one specific dataset, Political Advertising on Google, and when I'm taken to the page for this dataset, I'm going to click on VIEW DATASET. You can see additional information on this data set right here in this overview page. VIEW DATASET will automatically allow you to view this dataset from within BigQuery's classic UI at this point in time. I'm going to go ahead and close the other tabs here and just keep this one classic UI open for BigQuery. And notice that I'm currently on the dataset page for political ads. This is the dataset that I had clicked on from the marketplace. If you switch back to the beta UI by clicking on the Try the new UI button on the top right, you'll find that the public datasets are not still accessible on the beta UI. Now in order to enable public datasets, you need to go to the BigQuery documentation and explicitly visit a link. On the main BigQuery documentation page, click on the link on the left navigation pane which says Using the Web UI, scroll down a little bit until you find a link here that will allow the public datasets to be displayed by default on your new beta UI. Copy over this link by clicking on the button on the top right here, and then paste this link into your browser window. This link will take you back to your beta UI, but this time you'll have public datasets enabled, and you can see the link for public data off to the left of your page in the navigation pane. BigQuery public data is the public GCP project that holds all of the public datasets. I'm going to click on the PIN PROJECT button here to the right of my screen so that this public data is always available to me. We can pin project, and BigQuery public data is now pinned to my BigQuery UI. If you expand this project, you'll see all of the datasets that are available. They have pretty descriptive names. Here is a good source for machine learning, or anything else that you want to perform. Within here, go to the samples dataset, a project on the GCP contains datasets. Datasets contain tables within them. We are going to query one specific table in the samples dataset, that is the shakespeare table. When you click on a table within BigQuery, the UI will automatically show you useful information about that table. Here you can see on this tab the schema for the shakespeare table. Every word from Shakespeare's works is present in this table, along with the word_count, the corpus where that word can be found, and a corpus_date field which gives us the year in which this particular work was published. If you click on the Details tab, you'll get information as to when this table was created, how many rows that table has, how much data does it store, and so on. Instead of running queries that are expensive, the Preview tab will allow you to explore data without pay8ing any additional charges. You can view almost all of the data in a particular table by clicking through the page links that is available at the bottom here.

Executing Queries
We are now ready to run our first query on BigQuery. We are going to query the shakespeare table within the samples dataset in the public data project. When you reference a particular table in BigQuery, it has to have this specific format, the project_name, then a dot, then the name of the dataset, then a dot, and then the name of the table, and this should be enclosed within backticks. For every query that you write using the web console, BigQuery has this nifty query validator that will tell you whether the the syntax of the query is correct. It'll give you tips on how to correct it if it's wrong. It'll also tell you how much data will be processed because of this query. That will allow you to estimate your query processing costs. Because we do a select star and don't specify individual columns within this query, the entire data in the table is queried. Click on the Run query blue button here at the bottom of the query editor, and that will execute your query and show you the results at the bottom of the screen here. By default, results are in a tabular format. You can get the JSON format as well by clicking on the JSON tab. And here is one entry in our table present in the JSON format. BigQuery performs a bunch of optimizations in order to most efficiently execute your query. If you take a look at the execution details, you'll see the various stages of operation of our query. This query was a fairly simple one with just two stages, the input stage and the output stage, and each stage is made up of individual tasks, Wait, Read, Compute, and Write. You can also see off to the very right the number of rows that were processed at each of these stages, the number of rows at the input of the stage and the output of the stage. All of this information you can use in order to optimize your queries. I'm going to go back to the other tab I have open here where I have the queries classic UI open. Let's compose a query in the classic UI. Click on the Compose query button, and let's put in another query on the query editor. This queries the same table that we saw earlier, the shakespeare table. We use the LIMIT 10 clause to display just 10 results from this table. Immediately notice the query validator down to your bottom right. You can see that there is a red exclamation mark here indicating that there's something wrong with that query. Now back in the old days, when the first version of BigQuery came out, it ran a particular dialect of SQL called legacy SQL. So in the classic UI, by default, we use legacy SQL. This is a non-standard SQL dialect. The recommended dialect that you should use now is standard SQL. Standard SQL is now the default in the new beta UI. Now because we use standard SQL in our classic UI, that's why you see this error over here. The default dialect in the classic UI is legacy SQL, and that requires that your table names be specified in a slightly different format. You can't use backticks, you can't use dots. Let's switch the execution in the classic UI to the recommended standard SQL format. Click on the Show options button here. This will bring up a form that has a number of configuration options for your query. There's this checkbox right here in the middle that says Use Legacy SQL, and it's checked by default. Uncheck this checkbox, and the query editor will use standard SQL. Click on the Hide Options button, and you'll now see that this query is valid. Executing this valid query is straightforward. Just click on the RUN QUERY button, and you'll see the results of running this query here at the bottom of the screen. If you're interested in this dataset, you can run a bunch of other queries. Select * EXCEPT word_count, select all columns except the word_count column. Run this query, and you'll see in the result that the word_count column is missing. If you want to know how many of Shakespeare's works are represented in this dataset, you can say SELECT COUNT DISTINCT corpus, and you can see there are 42 works of Shakespeare here.

Working with the bq Command on the Terminal
You can access BigQuery datasets and query data using the command line as well, using the bq tool that comes preinstalled on your Cloud Shell VM. Let's see how. We start off in the BigQuery beta UI page. This is what we'll use in this course from here on in. You won't see the classic UI again. Click on the Activate Cloud Shell button here on the top right, and this will bring up a terminal window on your GCP cloud. I'm going to click on this icon here to open up this terminal window on a new browser tab. The Cloud Shell VM comes preinstalled with all of the command line tools that you need to work with the GCP, including the bg tool, which you can use to work with BigQuery. If you run bq help, that will show you all of the options that you have available. Bq wait will allow you to wait for the particular BigQuery job to complete, bq version will show you the current version of BigQuery. Bq update will allow you to update a dataset, a table within the dataset, or a view within that dataset. Here are some other BigQuery commands that you can explore as well. Query is what you'll use to execute the query, rm to remove a table or a dataset. Let's first use the BigQuery ls command in order to list all of the datasets that we have available in this particular project, and we can see that we have no datasets created in our spikey_bq project. Let's run an ls command to see the tables that are available in the samples dataset in the public data project. So publicdata:samples, and bq ls is a command that we run, and you can see that there are a number of tables available here. We'll use the bq mk command on the command line in order to create a new dataset, which we will name spikeysales_dataset. This will create a new dataset under our current spikey-bq project. You can switch back to the web console here, and you'll see under the spikey-bq project we now have a dataset, spikeysales_dataset. Datasets in BigQuery are kind of like databases in an RDBMS. We'll talk more of what exactly datasets are in the next module. For now, they are just a container for our tables. With this dataset created, we can now switch back to the command line and run the bq ls command, and you'll see that this dataset is now listed. You can delete datasets from the command line as well. Run bq rm and specify the name of the dataset. It will ask you for confirmation. You can then say yes, and have the dataset deleted. If you want to view information about a particular BigQuery object, whether it's a table or a dataset, you can use the bq show command. If you want to view information about a table, make sure you specify the full path to that table, project, colon, dataset, dot, table name. Here is the metadata information on this table, which we have seen in the Details tab on the web console. We can also use the command line to run a simple query. Bq query is the command that you use, and then you specify your SQL query within quotes. This is a query to view ten rows from the shakespeare table, and here are the ten rows printed out to your console. And on this note, we come to the very end of this introductory module on BigQuery, which is the Google Cloud Platform's cloud data warehouse. BigQuery supports standard SQL, as well as ODBC and JDBC drivers. BigQuery is completely serverless, which means you don't need to instantiate clusters or servers in order to get up and running with BigQuery. BigQuery happens to be a very popular choice for a data warehouse because of its low administrative overhead. It's completely no-ops. You don't even have to set up indices for your tables. The serverless nature of BigQuery implies that autoscaling is available right out-of-the-box, and BigQuery can store petabytes of data that can be queried efficiently and fast. BigQuery supports streaming data and real-time analytics on streams. Working with BigQuery is also relatively inexpensive, which means it is a great choice to store your data. In the next module, we'll get hands-on with BigQuery, and we'll see how we can use datasets, tables, and views.

Using Datasets, Tables, and Views in BigQuery
Module Overview
Hi, and welcome to this module where we'll get really hands-on with BigQuery. We'll see how we can use datasets, tables, and views. We've already briefly described datasets in BigQuery. In this module, we'll get some more practice with them. Datasets are containers, and as far as containers go, they're very similar to databases in a traditional RDBMS. With BigQuery, you can assign permissions and ACLs information to datasets in order to define who exactly can view the data stored within. Datasets contain other objects, such as tables. Tables are what contain your actual data. Tables are made up of records. Tables in BigQuery are similar to tables in an RDBMS, except that you don't have the overhead of setting up indices or primary keys, and in addition, you can have tables which hold nested and repeated fields. If you want to query just a subset of data in a table, you can define a view. Views are a view of the data that is stored within a table. They are defined based on the query underlying the view. Views in BigQuery are not materialized unless you explicitly query for data from a view. The data associated with a view query does not take up additional space. It continues to remain in the underlying table which the view queries.

Datasets Tables and Views
Here is a big-picture understanding of the data model that BigQuery uses, and how data is laid out in BigQuery. We have datasets, which are containers, and datasets contain within them many tables which hold information. Tables are what hold our actual data. Tables are made up of records. Tables in BigQuery can be very large. They can be terabytes or petabytes in size. Sometimes you only want to work with a subset of data that's available in a table, which is where you'll create a view, which is one representation of the data that's in a table. Here are some formal definitions for these terms. A BigQuery dataset is a top-level container used to organize and control access to tables and views. You apply ACLs and permissions at a dataset level in BigQuery. Any table or view that you have must belong to a dataset, and as you've already seen earlier, a dataset is associated with a GCP project. A table in BigQuery lives within a dataset. It contains individual records which are organized in a tabular format in the form of rows. Each record is made up of columns. These columns in BigQuery are referred to as fields, and these fields can be more complicated. They can hold structured data that is nested fields or lists as well. When you create a dataset in BigQuery, you can specify where exactly you want the data that lives within that dataset to be located. This geographic location can be specified at create time, and once the dataset has been created, this location is immutable. You can't change the location of an already created dataset. You can specify that your datasets will live in a multi-regional location such as the United States or the European Union. Multi-regional locations span a large geographical area that covers several regions, or you can have your dataset located in a regional location. That is Tokyo in Japan at this point in time. The most important thing you need to remember about a dataset is that this is where you specify access control. The dataset level is where you assign permissions for who can view and query your data. You can also assign labels to logically group your datasets. You can also specify an expiration time for new tables that live within a dataset. We've already worked with public datasets in BigQuery. Let's define them here. Public datasets are stored in BigQuery where Google pays for the storage. These are in a project somewhere that Google completely manages and pays all the bills for. They are available for general usage via this shared project. What you as a user pay for are only the queries that you run on this public data. A view in BigQuery can be thought of as a subset of data from a particular table that is made available to users. What data is made available is defined by a SQL query that underlies the view. Whenever a user queries the view, the underlying query associated with the view, called a view-query, is executed. Views in BigQuery are non-materialized, which means the data associated with the view does not take up additional storage space. It continues to live in the underlying table, and this data is only retrieved when you execute queries on the view. There are several advantages to using views in BigQuery, and a huge one is the fact that your query complexity can be reduced if you use views instead of sub-queries when you query your data, the fact that your query complexity can be reduced if you use views instead of sub-queries when you query your data. Tables in BigQuery tend to be very large and may contain huge amounts of data. You may want certain panes to view only portions of this data, and this restriction can be applied using views. You can also create views in a separate dataset, and give that dataset specific permissions. These permissions can allow you to restrict access to certain portions of data using a view. Views also allow you to logically group your data. You can construct different logical tables from the same underlying physical table when you use a view. This goes some way towards reducing the cognitive overload that might result when you're working with data on a very huge table which has many fields and potentially millions of records. For example, in the spikeysales organization, different analysts are responsible for data associated with different geographic regions, such as the US, European Union, or Asia. Even if the sales data for the entire organization across all regions is stored in a single BigQuery table, analysts can be restricted to view only that data that they are working on, data from the US or data from Asia. This can be done using views.

Creating and Editing Access to a Dataset
In this demo, let's create a new dataset using the BigQuery web console. We'll also specify access control information allowing specific users read and write access to the tables that live within this dataset. Let's start off at the BigQuery web console. We'll be using the beta UI. We are within the spikey-bq project, as you can see on the left side of your screen. When you have your project selected on the left navigation pane, you'll have the CREATE DATASET button enabled. Click on this CREATE DATASET, and this will bring up the UI allowing you to create a new dataset. The ID of this dataset is going to be spikeysales_dataset. This dataset ID is case sensitive, must contain letters or numbers, cannot contain special characters, and must be unique within this project. If you have restrictions on where data can be stored, you can specify a multi-regional location like the US or European Union, and your data will always live within that geographic region. Your data will be spread across multiple regions in a large geographical area, or you can specify that your data lives in Asia. A regional location like Tokyo is what is enabled at this point in time. This might change in the future as a additional locations are added. For every dataset you can specify default table expiration times. You might want data in your tables to be around forever, or you might want to specify the number of days after which the data will be deleted. This will be the default value for all tables that you create within the spikeysales_dataset. Click on the Create dataset button here, and in a couple of seconds you'll get the message at the bottom left of the screen which says your dataset has been created, and you'll find here under spikey-bq, that's our project name, we have the spikeysales_dataset. If you click on the dataset name in the left nav pane, you'll find metadata information available on the web console. Here it tells you when the dataset was created, where the data is located, when the dataset was last modified, and so on. If you want to update the description, you can do so using the web console as well. Click on the description link, specify the new description, and click on the Update button. We'll update the permissions associated with this dataset using the command line. Activate Cloud Shell, and let's first view the metadata information for this dataset using the bq-show command. We want this metadata information to be displayed in JSON, which is why we specified the format as prettyjson, and we store this metadata information in a JSON file called dataset_spikeysales. If you click on this pencil button here to the top right of your Cloud Shell, that will launch the code editor on the GCP. This is a recently launched feature that allows you to edit configuration files and even code files from within your browser window. You don't have to switch back to your local machine. When you click on code editor, it will give you a graphical view of the folders and files that live in your Cloud Shell home directory. and you can see that we have this here, dataset_spikeysales. json. This is the file that we just created. Click on this file, and you can see metadata information about our dataset in the JSON format. This also includes access permissions that are associated with our dataset. We know which roles and users have permissions to read and write to tables in our datasets. You're going to edit this here, you're going to add in a new user called cloud. user@loonycorn. com, which has the role writer. We can write to tables and read from tables in our dataset. Here we are looking at a specific use case where we assign writer permissions to an individual user in our organization. You can assign permissions to users, groups, or even to the domain as a whole. This data file will be automatically saved in your home directory, and with this update in your JSON file, you can run a bq command in order to update the permissions on your dataset. Call the bq update command, specify the source json file, which contains our updated permissions, and apply it to the spikeysales_dataset. Once the permissions associated with that dataset have been updated, we can call bq show and see the latest permissions. You can see that the cloud user has read and write access to the tables within this dataset. We now want to validate this. Permission-related information is not yet available in the beta UI for BigQuery at the time of this recording, so I'm going to switch over to the classic UI in order to ensure that the cloud user indeed has write access to this dataset. In the classic UI, if you click on the little drop-down next to the name of the dataset, you'll find an option which says shared dataset, and this is the option I'm going to select. This will pop up a UI, and you can see here within this UI that cloud. user@loonycorn. com can edit this dataset.

Verifying Access to Datasets
We are back in the beta UI for BigQuery here. Let's verify that cloud. user@loonycorn. com indeed has access to tables within the spikeysales_dataset. We first need to create a table here. I'm going to create a very simple test table. We'll talk more about table creation in the clips that follow. Select the dataset on the left navigation pane. you'll have the CREATE TABLE option. Click on it, and it will bring up the Create Table UI. I'm just going to call the Destination table customer_test_table with two fields within this table, customer name and customer_id. If this is going a little fast for you, do not worry. In the clips that follow we'll talk about all of the these options that are available during table creation. We see that a customer_test_table has been created here. I'm going to open up a new browser window and go to the main dashboard for my GCP project, and from within here I'm going to sign in with a new account. Click on the Add account in the menu that pops up when you click on the user icon to the very top right of your screen. I'm going to quickly sign in as cloud. user@loonycorn. com. I'll specify the username and password to check whether cloud. user has access to the BigQuery dataset that we just created. Click on the BigQuery option from the left navigation menu. This will take you to the web console. Within here, you won't get any notification indicating that you have access to this dataset, but you can run queries on the customer_test_table, and these queries will be executed successfully. If you click on the Run query button, you'll find that the query returned no results, but it executed successfully. Cloud user clearly has access to the customer_test_table within our spikeysales_dataset. I'm going to close this browser window and switch back to my original account, spikeysales@loonycorn. com, select the customer_test_table and click on the DELETE TABLE button. This is how you delete tables from the UI. It'll ask you for confirmation. Specify the name of the table and hit Delete. This table will now be deleted.

Creating and Querying Tables
In this demo, we'll see how we can create and use tables in BigQuery. We'll see how we can create a table with source data from another table. we'll also see how we can copy tables. Click on the spikeysales_dataset on the left navigation pane. We're going to create a new table within this dataset. Click on the CREATE TABLE link that's available here. This will bring up the UI that we've seen before. We'll now study the various configuration options that are available in this UI. You have the option to specify whether the table that you want to create will be populated with source data. I'm going to create an empty table to start off with. We'll talk about populating data within tables as you create them in the next module. The table that we are going to create here is the destination table, and you can specify the project where you want the table to be located, the dataset within which the table should be located, and whether it's a native table or an external table. The native table option that we've chosen here is when the data that you're going to store within this table lives within BigQuery and not on some external source. We'll look at configuring these in other demos in this course. Let's specify a name for this destination table. I'm going to call it the special_customer_table. You can specify a schema for this table at the time of table creation. You can choose to specify the schema in a JSON format, in which case you'll choose the Edit as text option, or you can simply click on the Add field button here, which allows you to specify individual fields, the name and the corresponding data type associated with that field. And click on Add field, and you can see there are three components to field specification. The first is the name of the field or the column. This field is called customer_name. You then specify the kind of data that this field will store. We have chosen string here. You can say bytes, integers, floats, date, time, Boolean, and a number of other options. And the third specification is for the mode of the field, whether values in this field can be null or are they required values. There are three specifications for Mode. It can be a nullable field, a required field or a repeated field. A repeated field is a field that is a list. We'll make customer_name a required field, which means it can't have null values. Let's go ahead and add a new field here by clicking on the Add field button. This is a new field to store customer_email. It's of type STRING, and its mode is NULLABLE, which means it can have null values. We'll add two more fields here. The next is customer_city, again, of type string and nullable. And the next is customer_contact, which is of type integer and nullable. You're now ready to click on the Create Table button and wait for this table to be created. This newly created table will now be available under the spikeysales_dataset. There you see it, the special_customer_table. Click on this table in order to view additional information, such as the schema of this table. This is the schema that we just specified. If you click on the Details tab, that will give you metadata information on the table that we just created, such as the size of the table, it's 0 bytes, it holds no data at this point in time, the number of records in the table, when the table was created, and so on. The Preview tab shows you that the table is currently empty. There is no data to explore. We can use the query editor to write an insert statement that allows us to insert records into this table. We'll insert into the spikeysales_dataset. special_customer_table. Notice that we specified the dataset name, then a dot, and then the name of the table. Because we are working within the spikey-bq project, we don't need to specify the project name here. We want to add a record that will populate all four of the fields in our table, and the values for these fields are James, james@yahoo. com, San Francisco, and his phone number. Notice the query validator off to the bottom right. Because we are inserting data into the table, this query will process 0 bytes when run. Remember, inserts into tables are free. You only have to pay for storage. Click on the Run query button, and execute this insert query, and you'll soon see an update at the bottom of your screen. One row was added to our newly created table. If you click on the table now and view the Preview tab, you'll see that the table has one row. This is the record that we just inserted into this table. If you want to view all of the tables that live within a particular dataset, you can do so using a SQL query as well, select star from spikeysales_dataset. __TABLES_SUMMARY__. This is a special table that is automatically created within our dataset, which holds a list of all the tables available. Execute this query, and here you can see we have exactly one table here, the special_customer_table.

Creating Tables from Other Source Tables
We'll now create a table in BigQuery and populate it with data from another table. We start off in our public data project within samples. We are on the shakespeare table here. Here is the query that we want to execute on the shakespeare table. We want to select all records with a limit of 5. This will query the entire table, so you'll be charged for querying the entire table even though you want just five records. This is something to watch out for in BigQuery. Click on the More option here, and within that you'll find an option called Query settings. This is where you can specify that you want this data to be populated into a destination table. This opens up a configuration dialog for the query itself, which allows you to specify how you want this query to be executed. We want the results of the query to be populated into a destination table. I click on the Set destination table checkbox and specify the name of the destination table within my project. I'm going to call it the shakespeare_table. You can specify different options as you write data into this table. You can say you only want to write data in if the table is empty to begin with. You can append this data to the table, or overwrite the existing contents of the table. We'll go over the Write if empty option. You may not realize it because the query is so fast, but the size of the data in your table can be huge, it can be in petabytes. If you want your destination table to work with such large data sizes, you can click on this checkbox which says Allow large results. There is no size limit. I'll leave it unchecked for now. My source table is pretty small anyway. BigQuery jobs can run in interactive mode or batch processing mode. In interactive mode, your query will execute right away, and that's what I'm going to choose here. I don't want my job to run later on at some point in time. Cache preferences allow you to specify whether you want to use cached results to populate your destination table. Once you run a query, BigQuery behind the scenes caches the results of that query, and this option will allow you to specify whether you want to use cached results. This option is not enabled here when we're reading from a public dataset, so I'm just going to leave that be. Let's move on and hit Save in order to create and populate our new table. Our query settings are now configured. We can now execute this query to create a new table and populate it with data from the shakespeare table. There is something interesting to observe here. We query all columns of the shakespeare table, which means all of the data within that table will be processed, even though we're interested in only five records. Let's go ahead and execute this query. The query results will be displayed right here within your browser. Your new table has been created and has been populated with these results. You can explore this newly created table under your spikeysales_dataset. What we're going to do here, though, is run another query to overwrite the data that is present in this table. We'll query the shakespeare table as before, but this time we're going to ask for a thousand records. Let's open up Query settings once again. The destination table is the same shakespeare table that we created earlier, but this time we want to overwrite the table with these thousand records. This will completely delete the five records that existed in this table earlier, and overwrite it with a thousand records that we've specified. Notice that this query, once again, processes the entire data within the shakespeare table. The limit clause has no effect on the amount of data processed. That purely depends on the number of columns you query. Here we are querying all columns. Let's execute this query and populate the shakespeare table; overwrite its existing contents. Overwriting an existing table's contents is a pretty major task, which is why BigQuery asks you for a confirmation. I'm going to hit OK here, and this will overwrite the original table that was created with five records. If you see the results of this query, at the very bottom right you can see that there are a total of a thousand records because of our limit 1000. You can explore the shakespeaere table within your spikesales_dataset and confirm that it indeed has 1000 records. The entire table has been overwritten. We'll do something else here, and continue on this screen. Let's just select word and corpus from the shakespeare table. We're limiting the number of columns that we want to query. And now you'll notice that the amount of data processed by this query is much smaller. It doesn't process the entire table, it only processes 3. 62 MB of data. This is because we've limited the number of columns that we are querying. Limiting the columns or fields that we query is a technique that you you can use to minimize your query processing costs. Let's click on the Run query button here, and you'll notice something. The query settings that we had specified earlier applies to this query as well. We are within the same session with the same editor. The query settings that we had configured applies to this query as well, so make sure you're aware of this as you're executing multiple queries in a session. This query is now complete. Our existing shakespeare table has been overwritten with these new records with just two fields, the word and corpus field. Let's go take a look at the spikeysales dataset, which has the shakespeare table which we've written to and overwritten several times by now. Notice the latest schema of this table contains just the word and corpus fields. This was the last query that we executed that overwrote the contents of this table. You can create a new BigQuery table with source data from an existing table using the COPY TABLE command. This will copy over the entire data that exists in the current table. This is the destination table, shakespeare_table_copy, and the source table is the shakespeare_table in my current dataset. Click on Save here. This will start the process of copying the table. Any time you are bringing data into a BigQuery table or exporting data from a BigQuery table, BigQuery will run a job under the hood, and if you click on Job history here on the left navigation pane, you can see the status of the current job. Job history will show you all of the jobs that have been executed in BigQuery. You can see that the latest job involved copying from one table to another. You can see below here other jobs that I have executed while playing around with BigQuery. If you click through a particular job, you'll get additional details on what exactly that job did. This was a copy job from a source to a destination table. The newly copied table is not visible within my spikeysales_dataset. I'm going to click the Refresh button on the browser, and once I hit Refresh you can see that the newly created shakespeare_table_copy is now present. You can explore this table if you want to in order to validate for yourself that it's indeed a copy of the original shakespeare table in our dataset.

Creating Views
So far, we've worked with datasets and tables. Let's see how we can create and use views in BigQuery. Now let's say in the spikeysales organization the customer support team needs to be able to view customer name and customer email information, but they are not allowed to view the location of the customer. You might have a view which selects just the customer name and customer email from the special_customer_table. Let's execute this query and view the results. There is just one customer here. Notice that we have an option just below the query editor which allows you to save this query as a view. Click on the Save view button, and this will allow you to name your view. I'm going to call this the special_customer_view. Click on Save, and you have a newly created view within your spikeysales_dataset. You can see the little view icon to the left there, indicating that it's a view, and not a table. You can click on the view and see that the schema information contains just the customer name and customer email. The other columns are not present. If you click on the Details information, that will give you additional information on the view. If you scroll down to the bottom here, you have the query that's associated with the view. You can edit this query as well. Using the edit query button that you can see on screen here, you can click on it and change the query that is associated with this special_customer_view. Here are some other options available with a view. You can copy a view, delete a view, and export the data associated with the view. In our demo, we'll pick the first option here to query a view. Remember, this view only has customer name and email information, so when you select from this view these are the only two fields that you can specify. I'm just going to say select star for now, and you'll see that I get the customer name and email information for the one customer that exists within the table underlying this view.

Authorized Views
All of the orders in the spikeysales organization is present in one BigQuery table, however, analysts for different regions, the US, Europe or Asia, only have access to that data that corresponds to their region. We can achieve this using authorized views in BigQuery. Authorized views is a standard pattern that can be used in BigQuery to control access to who sees and can access what data. Remember that permissions in BigQuery are applied at a dataset level, so a combination of dataset-level access control along with views will allow you to set up authorized views. Let's see how this works. Let's imagine a hypothetical scenario here where we have two datasets, Dataset A and Dataset B. Within Dataset B, we have a table which contains the order information across the entire globe for spikeysales. We want analysts who are based in the US to be able to view only those records which are associated with the US, analysts in Europe should be able to view Europe's specific information, and analysts in Asia should be able to view Asia's specific information. We have one BigQuery table with all of the data. We'll use authorized views to restrict access to data. This view will be present in a different dataset, Dataset A here. Let's see how this works. Our users are data analysts who should be able to query view information. We'll assign the curated role bigquery. user to all users at the project level. Both of these datasets are present within the same project. This bigquery. user role ensures that for this particular project users can only access the datasets that they have created. They can't access datasets that have been created by other users in the same project. The spikeysales order management information is present in one table, which is within Dataset B. This is private data. No one except executives at the very top level can view the order information for the entire world, so we'll house this private data in a self-contained dataset with special permissions that allow only high-level executives to access this table. Analysts who can access the order data for their specific region will use a view that has been created in a separate dataset. You can imagine that we have a separate dataset for each of the regions, one for the U. S., one for Europe, and one for Asia, and we create a view within each of these datasets that queries the subset of records which are accessible to analysts in that region. That way analysts in Asia can only view those records where the region is Asia. We only need to authorize the view that's present in Dataset A to access the source dataset, that is Dataset B, which contains our entire orders table. And on this note we come to the end of this module, where we worked with datasets, tables, and views in BigQuery. We saw that datasets are containers very similar to databases in RDBMS, except that you specify permissions at a dataset level in BigQuery. Datasets contain other BigQuery objects such as tables. Tables are what hold your actual data. Tables contain records. You can define views over tables. Views are defined by the query that they perform on the underlying table. These views are not materialized unless we actually execute a query on this view. In the next module, we'll see how we can import data into BigQuery and work with partitioned tables.

Getting Data in and out of BigQuery
Module Overview
Hi, and welcome to this module on Getting Data in and out of BigQuery. When you're moving storage and compute to the cloud, an important aspect of this move is the migration of your data. BigQuery has very easy interoperability with cloud storage, you can simply upload your data in CSV, JSON or AVRO formats to cloud storage, and then load this data from cloud storage into BigQuery. When you're working with huge data that is time based, it's often useful to partition your tables for better performance. You don't have to query an entire table, you can simply query a specific partition. BigQuery makes it very easy for you to set up and work with time-based partitioned tables, and we'll see how to do that in this module.

Uploading Data to GCS Buckets
Engineers and analysts in the spikeysales organization want to migrate some of their e-commerce data to BigQuery in order to try it out. They have their data in the CSV, JSON, and AVRO formats, and they want to first load these files to cloud storage buckets from where they can migrate into BigQuery very easily. We start off in the main dashboard for our spikey-bq project. I'm going to use the hamburger icon on the top left in order to access the navigation menu. I'm going to access cloud storage buckets, which is present under Storage and Browser. Cloud Storage buckets on the GCP offer elastic storage for objects. This object-based storage is very similar to AWS's S3 or Blob Storage on Azure. Cloud Storage follows a PSU use policy. You can create buckets and store objects within them. You only pay for the storage that you use, not for what you allocate. There is no explicit allocation required. We don't have any cloud storage buckets here. Use the Create bucket button in order to create our first bucket. The bucket can be thought of as a container, and it should have a globally unique name. This name should be unique across all GCP projects in the world. It's good practice to prefix your bucket name with the name of your organization so that it's easy to remember and it's easy to get unique names this way. The storage class of a bucket determines how much you pay for storage. Multi-regional storage basically means that your data will be present across multiple regions and replicated. The cost for multi-regional buckets are also the highest. You can also have regional buckets where data is located in a single region. Nearline and Coldline storage is for archival storage. I'm going to go ahead and create this bucket, and once creation is complete I'll click on the Upload file button in order to upload files from my local machine. Within the Downloads directory on my local machine, I have three files, data. avro, data. csv, and data. json. I'm going to upload all three files, and I'm going to do it one by one, but you can upload all three files in one go if you want to. Go ahead and upload all three files. You can see here that all three files are now available within my spikey_bucket. Let's take a look at the contents of these files. Data. csv contains order information from the spikeysales org in the CSV format. You can see here that we have information about orders, ship mode, customer-specific information, the products they bought, how much they paid, and so on. Data. json contains geographic data in the JSON format. It contains the name of the region and lat/long information. The data. avro file is a little harder to read. This file format is not really suitable for browsing. This contains some ad-related information from the spikeysales website.

Importing Data from CSV Files on Cloud Storage
Let's see how we can create a table and load data into it using the CSV file that we uploaded to Cloud Storage. Here are the three files that we uploaded to Google Cloud Storage in the last clip. Let's switch to our BigQuery web console, and within the spikeysales_dataset let's click on the Create Table link. This will open up a dialog that is familiar to us. We're going to specify source data for this table though. Click on the menu there, and you'll see a bunch of options available. Within this, we're going to say that our source data will come from Google Cloud Storage. You can see that the UI has changed in order to allow us to specify the URL of the file which contains the source data. This is a GCS URL that starts with gs://. This is followed by the name of the bucket, that is spikey_bucket, and the name of the file within the bucket, data. csv. The file format is the next option. The default is Avro. You can click on the menu and choose the CSV option. This file contains some sample e-commerce data that we can play with. The destination table will be simply named. We'll simply call it e_commerce_data. Now I'm going to ask BigQuery to auto-detect the schema for this file. BigQuery has the ability to parse the file header for CSV files and then see the kind of values that are stored in each field and auto-detect schema. That's the simplest way to get data into BigQuery. Click on the Create table option, and wait for the job importing data into our BigQuery table to complete. You can see in the Job history here that we have one job running, and you can see that the job has now completed successfully. Click through, and you'll see from the job description that this involved loading data from a cloud storage bucket into a BigQuery table. If you look within the spikeysales_dataset, you'll see that we have a newly created table called e_commerce_data. If you click on the table, you'll be able to view the schema. Notice that the schema has been auto-detected. You can compare this with the header over in the CSV file, and you'll see that the schema matches. You can click on the Details tab here to view additional information on the table as to when it was created and how many records it contains. You can see that this table contains around 1200 records. You can use the Preview tab to explore the data. Remember, if you use Preview you won't be charged for querying the table, so Preview is a good way for you to see the kind of data that's stored here. We have the table selected in the left nav pane. You can click on Query Table, and BigQuery will set up the query for you. Now the query doesn't specify the columns that you want to query. That's up to you to define. Because our query is incomplete, columns or fields haven't been specified, which is why the query validator shows us a warning. When you're using the web console, you can specify the fields that you want to query by clicking on the field within the schema. Notice that in the schema, when I click on a particular field, that field will be populated in my query. I click on OrderDate, and my query now selects the OrderDate field from e_commerce_data. As we include OrderDate, notice that the query validator has updated. This is now a valid query which will process 9. 66 KB of data. Let's click on another column here. I'm going to now click on the customerName column that has been added to my query. You can see that the amount of data processed by my query has now increased to 27. 82 KB. Go ahead and hit the Run query button, execute this query, and you'll see the result right here within your browser. All of the data that you've loaded in from the CSV file on Cloud Storage is available to you now within BigQuery. The data has been loaded into BigQuery and is now stored in BigQuery in addition to the CSV file which still exists on Cloud Storage.

Configuring Additional Settings While Importing JSON Files
In this demo, we'll see how we can load JSON data from Cloud Storage buckets into BigQuery. This time we'll also explore some of the other configuration options available when we are importing data. Within the spikeysales_dataset that I have selected on the left nav pane, I'm going to create a brand-new table, and I'm going to use source data, which is located on Google Cloud Storage, exactly like we did earlier. This time, the GCS URL will point to the data. json file that's stored on my Cloud Storage bucket. I need to update the file format to specify that it's Newline delimited JSON. I'll leave the other options as is and specify a name for this table. I'll call it data_table_json. By default, BigQuery creates native tables. Native tables are when the data itself is stored within BigQuery. It's not stored in an external source. I'm going to have BigQuery auto-detect the schema for this table, and I'm going to expand the Advanced options menu here at the bottom. Within Advanced options, I can specify how I want to write data into this table. I can write to the table if the table is empty, I can append data to the table, or I can overwrite data. I can indicate whether I want the newly created table to be partitioned or not. We haven't discussed partitioning yet. I'm going to go ahead with the No partitioning option. When you're reading in data from real-world sources, it's quite possible that there are errors in individual records. You can specify the maximum number of errors that you can tolerate. You can enter the number of rows with errors that can be present when we read in data. I'll just put in 10 here. You can also have BigQuery be flexible in how it reads unknown values. Columns or fields which do not match the schema will be ignored and not loaded into BigQuery if you check this checkbox. I'm going to hit Create Table, and I'll take a look at my job history to see the latest upload of data into my BigQuery table. As usual, you can explore the schema and data for this table by clicking on the link on the left navigation pane. Here is the schema for this table that has been automatically detected by BigQuery. Clicking on this Preview link will allow you to explore the data stored within this table, our geographical data. Click on the Query Table link here to run a query on this newly created table. BigQuery will set up the query for you in the query editor. You can choose the schema to select those fields that you're interested in, and you can see how the amount of data processed changes based on the columns that you add. When we have just the name column, we process 1. 96 KB of data. When we add additional columns, such as the location column, we now process 3. 8 KB Of data. I'm going to add in another column here, that is the status column, and now we process 4. 45 KB of data. I'm going to execute this query, and here is the result right here on screen.

Creating External Tables with Data in GCS Buckets
In this demo, we'll see how we can load AVRO data from Cloud Storage, but this time we'll create an external table where the data in AVRO format is not stored within BigQuery, instead it continues to remain in the Cloud Storage Bucket. We begin the same way as before. We click on the Create Table link within the spikeysales_dataset, and I'm going to choose Google Cloud Storage for my source data. Instead of specifying the URL for your file directly, you can also choose to browse your Cloud Storage bucket. I'm going to click on the Browse option here. This will bring up a UI, and I can see the buckets that I have available. Spikey_bucket is what I'm interested in, and within that I'm interested in the data. avro file. Click on the Select button at the bottom in order to select this particular file, and now I'll move on to the table that I want to create. This time my destination table is going to be an external table. If you click on the Table type menu, you'll see two options, Native Table, that is the default, that's what we've been creating so far, and we can also create external tables, and that's the choice I'm making here. If you already have data in an external source, and you don't want to import this data into BigQuery, you can have it continue to remain in this external source, such as Google Cloud Storage buckets, or even Google Drive, and still query the data from within BigQuery. That's what an external table does. External tables reference data from an external source. I'm going to call this table data_table_external_avro. Note that we can't specify a schema. For external tables, the format of the external source defines the schema for the table. You can't explicitly specify a BigQuery schema. Click on the Advanced options and see what's available here. You can specify the number of records which can have errors. You can also choose to ignore unknown values. Click on the Create Table button and wait for this table to be created. Data_table_external_avro is an external table, and if you click on the link you can see a few differences in what we have available to view within a browser. You can see that there is no Preview tab for external tables. You can't automatically preview data that has been stored in an external source. The schema for this table has been auto-detected correctly and is available to us. You can click on the Details tab and see additional metadata information just like we did earlier. For external tables, BigQuery is unable to automatically detect the number of records and the amount of data in that table. We'll know this only when we execute a query on that table. There is extra information available here if you scroll down. We have external data configuration, which points to the source where the data is actually located, which happens to be the data. avro file in our spikey_bucket. I'm going to run a query here which selects three fields from the external table. I'm going to limit my query to return just 1000 records. BigQuery has no idea how much data it's going to process for the external table. Hit Run query, and you'll be able to observe the query state much longer to run when the data is stored externally. And here are the query results, and notice that this data is available to us from within BigQuery even though the data itself lives on Cloud Storage. If you want to take data from within BigQuery and save it to another destination, you can do so using the Save As. You can download this as JSON, CSV, you can save as another table, you can also save to Google Sheets. This is fairly straightforward, as easy as clicking the choice that you are interested in. You can try it out and see how it works.

Partitioning in BigQuery
Data warehouses store huge amounts of data, and if you have to query all of the records within a certian table, your queries will be inefficient, which is why you might need to partition data to improve performance. Partitioned tables are a special kind of table in BigQuery. The data within the table is divided into segments called partitions. It makes it easier to manage and query your data from within partitions. Unlike in a data warehouse such as Hive, where partitioning is more general purpose, BigQuery supports only two types of partitioning. It's less flexible than Hive here, partitioning based on ingestion time and partitioning based on a particular column which has time or date information. These are the two kinds of partitioning that BigQuery supports. As you can see, both of these are time and date-based partitioning. Ingestion time partitioning is specifically useful for streaming data such as log messages or files. The ingestion time for your data is the load time or the time at which data arrives at BigQuery. BigQuery will take care of loading data into daily date-based partitions. Let's say I upload data from a particular dataset today, and then another dataset tomorrow. BigQuery will automatically partition this data internally. When a table is partitioned based on ingestion time, BigQuery will automatically add a pseudo-column called _PARTITIONTIME. This column can be queried. It's a pseudo-column in that it doesn't really exist; it's a part of BigQuery metadata. Recent versions of the query also allow you to perform column-based partitioning where you can partition data that is stored in a particular column in your dataset. This column must be of type DATE or TIMESTAMP. Partitioning on other columns are not supported in BigQuery at this point in time. When your data is partitioned by the values in a particular column, there is no additional PARTITIONTIME pseudo-column that is added to your data. BigQuery also creates two special partitions called NULL and UNPARTITIONED. Records which have null values in the column where we've performed our partitioning will go into the NULL partition. Records which lie outside of the date range for our partition will go into the UNPARTITIONED partition. Partitioning of data is an internal BigQuery concept, and BigQuery completely takes care of partitioning your data. To improve query performance, everything is abstracted away from you. Sharding of data involves splitting up your data based on a particular value in a column. Sharding is an actual split where you store data in different tables. Let's compare and contrast partitioning data with sharding data. With partitioning, you split data into logical horizontal groups which are time-based in the query. With sharding, you can do the same. You split data into logical horizontal groups. Both achieve the same objective. Data partitioning, as you'll see in just a little bit, is completely managed and handled by BigQuery. You simply have to configure your table to be a partitioned table based on ingestion time or column-based partitioning. If you want to perform sharding using BigQuery, you have to manually set up the sharded tables yourself. With partitioning, BigQuery gives you improved performance, and takes away a lot of the overhead associated with tables. Metadata and access control is automatically applied to partitioned tables. With sharding, though, you have to individually configure the metadata and access control to the individual shards. Unless you have an explicit reason to shard your data, you should go with partitioning. It's far more efficient as performed in BigQuery. Sharding tends to be less efficient. Partitioning is less flexible though. You can't partition on any column in BigQuery, only on time and date-based columns or ingestion-time partitioning. With sharding, you can shard on any column value. Sharding tends to be more flexible. If you want to improve query performance in BigQuery by having it query data in smaller segments, you should prefer partitioned tables as opposed to sharded tables in BigQuery. Partitioned tables in BigQuery just perform better.

Creating and Querying Ingestion Time Partitioned Tables
In this demo, we'll see how we can create and use partitioned tables. We'll work with both ingestion-time partitioned tables, as well as column-based partitioned tables. We'll start off in the BigQuery UI within the spikeysales_dataset. We're going to create a new table that's going to be a partitioned one. Here we'll call the destination table e_commerce_partitioned. It's going to hold e-commerce data. I'm going to specify the schema for this table in a JSON format, so I'm going to slide over to Edit as text. Let's get the schema specification from the e-commerce_data table that we had set up earlier. I'm going to open up Cloud Shell here and get the schema for the table that already exists within our BigQuery. I use the bq show command with the --schema option. I indicate that the format in which I want to view the schema is prettyjson, and I specify the e_commerce_data table within my spikysales_dataset. And here is the schema for this table displayed in JSON format. I'm going to copy over this schema. This is the same schema which I'm going to use for the ecommerce_partitioned table. Switch back to the other browser window where I have our BigQuery open. We're in the process of creating the e-commerce partitioned table. I'll place the schema in here within this window. This is the Edit as text option. I'm going to click on Advanced options here and specify that this is a partitioned table. The default is No partitioning. Expand the menu, and I'm going to choose to partition by ingestion time. Observe that the query shows me other partitioning options that are available to me based on the schema that I've specified. The two date/time columns in my schema specification are OrderDate and ShipDate, and those are the other options available to me. This is how I do column-based or field-based partitioning. I'll just go with Partition by ingestion time, that is our first example, and I'll turn my attention to the next option here, the partitioning filter. Now the partitioning filter basically makes the WHERE clause that specifies the partition from which you want to query records compulsory. If you check on the Require partition filter, any query that you run on this data must specify the partition in the WHERE clause. It can't be left out, you can't query all of the partitions in one go. I'm going to leave this unchecked, and go ahead and create this partitioned table. The partitioned table has been created. When I click on the table on the left nav pane, the data for this table immediately tells me that this is a partitioned table, and here is the schema that we specified in JSON format. You can take a look at the details for this table. The details will also tell you this is a partitioned table, and it's partitioned on the field _PARTITIONTIME. The presence of this pseudo column should immediately indicate to you that this table is partitioned on ingestion time. For column-based or field-based partitioning, this pseudo column _PARTITIONTIME is not present in your table. If you click on the Preview tab here, this will show you that this table is completely empty at this point in time. We haven't loaded in any data yet. In order to load data into this partitioned table, let's query data from the e_commerce_data table. Click on the Query Table button and let's query 1000 records from e_commerce_data. We'll simply SELECT * from the e_commerce_data table. I'll specify within the query settings, that is available in the More menu, that I want the destination table to be our partitioned table. E_commerce_partitioned is the name of our destination table. I accept the other options, and go ahead and click on Save in order to load this data into the partitioned table. Now that I've specified my query settings, I just need to execute this query in order to populate my partitioned table. Observe something here just under the query editor. These tags give you quick info about the query settings that apply to the current execution of this query. Click on Run query and go ahead and populate our partitioned table. Once the query has run through, if you click on the partitioned table and click on the Preview tab, you'll see that this table now has data. This table has a pseudo column called _PARTITIONTIME, but you can't explicitly view the information in this column when you take a look at the preview. Let's switch back to our query editor, and see how we can run a query on a partitioned table. The SELECT * FROM table is exactly the same as before. The interesting details lie in the WHERE clause. Notice that you can reference the _PARTITIONTIME column within the WHERE clause. Our table is ingestion time partition, and we loaded all of the data in at one go, which means only the partition for the current date has data within it. I'm looking for all records that have been ingested at this particular date, 9 September 2018. I'll execute this query, and the results will be all of the data in my table. All of the data is currently present in the same partition because they all have the same ingestion time. I'm going to update the WHERE clause of our query to query a different date. Now PARTITIONTIME should be 2nd September 2018, and you can immediately see that the query validator tells us that this query will process just 0 bytes. There is no data in this partition. Execute this query, and you'll see that it returns no results. All of our data is present in a single partition. If you switch the WHERE clause back to September 3, you'll see that it now processes all of the data, all of the records in our table.

Creating Column Based Partitioned Tables Using the Command Line
We'll create a new partition table. This time we'll do two things differently. We'll use the command line, and secondly, we'll have this table partitioned on a particular field. We'll use the bq. make --table command here, and we specify that the schema of the table should contain an OrderID, an OrderDate, an order Quantity. We want this table to be partitioned on the OrderDate column, which we specify using the --time_partitioning_field flag. We'll call this table the order_data table within the spikeysales_dataset. Go ahead and hit Enter, and this will create the new table. We can now switch back to the web console. If you click refresh on your browser window, you'll be able to see this new order_data table within the spikeysales_dataset. Click on the order_data, and you can see that this is a partitioned table, and you can see the schema for this table as well. If you take a look at the table details, you'll find that this table has been partitioned by the OrderDate field, and it's partitioned by day. The PARTITIONTIME pseudo-column is only available for tables that have been partitioned based on ingestion time. It's not available in this table, and this pseudo-column cannot be used in queries to this table. This table is currently empty. Let's populate it using the bq command on the command line. We want to run a query using bq, and the results of that query will feed into this newly created order_data table. The --location flag specifies where we want the query to be run. Nouse_legacy_sql indicates that we're using standard SQL for our query. The destination_table that we want to populate here is the order_data table that we just created. This is a table that has been partitioned on the OrderDate column. We'll populate order_data with records from the e_commerce_data table. So we select the OrderID, the OrderDate, and Quantity from e_commerce_data. We'll query 1000 records from this table, and we want only those records where OrderDate is not null. Once the query has executed successfully, you can use the bq show command in order to see the metadata information for order_data. You can see that order_data now has 1000 rows. You can also see here that this table has been partitioned using the values in the OrderDate field. Let's query this order_data table from the command line itself using a bq query command. We want to select all those records where OrderDate is the 1st of November 2014. And you can see here that there is exactly one record in this partition, one order for this date. You can try a couple of other queries if you want to, but the significance and the performance improvement when you query partitioned tables will only be apparent to you when you have a huge dataset, a table with many, many rows. And on this note, we come to the very end of this module where we saw how we could import data into BigQuery from Google Cloud Storage. We saw that BigQuery works with CSV, JSON, as well as AVRO formats. You can also create external tables where the data continues to live in external storage but can be queried from within the query. We also saw how we could improve the performance of our queries by partitioning our table so that the queries run on a smaller dataset. We saw that there are two kinds of partitioning, ingestion time partitioning and partitioning based on a column which is of type, date, or timestamp. In the next module, we'll see how we can set up BigQuery with nested and repeated fields. We'll study some advanced analytical queries, including windowing functions.

Performing Advanced Analytical Queries in BigQuery
Module Overview
Hi, and welcome to this module where we'll see how we can perform some advanced analytical queries in BigQuery. We start off by understanding some complex field types that BigQuery can support. BigQuery supports both nested, as well as repeated fields. Nested fields in BigQuery are effectively like structs in programming language. In fact, we use the STRUCT keyword to create nested fields. This is a logical grouping of fields, such as city, state, and other information within an address. Repeated fields in BigQuery are effectively array fields. These are fields which contain repeated information or a list of values. BigQuery also supports subqueries which allow you to perform very complex analysis, and it also has support for windowing functions. Windowing functions are where you can partition and order data within a window and then perform an aggregation over the window.

Normalized Storage in a Traditional Database
When you work with data stored in traditional relational databases, the data is typically in normalized form. Let's understand what normalized storage means before we see how BigQuery is different. In a traditional database, the objective is to minimize redundancy, so data is stored in a very granular form. The same piece of information will be stored in just a single table, and other tables will reference this information. In an organization, if you have a relational database to store employee information, here are bits of information that we might want to capture, and we'll store each logical group in a different table, so you might have three tables to minimize redundant storage of data. You might have an employee details table, a table which holds the subordinates of a particular employee if he or she is a boss, and then you'll have the employee address table. Each of these three separate tables will hold different information about an employee. One will hold personal information, another subordinate information, and the third, addresses. Let's take a look at these tables in turn starting with the employee details table. All of the employees' personal information will be in one table, and each employee will have a unique ID that can be used to reference that particular employee. All employees who are managers will be present in the Employee Subordinates table. Employees will be referenced by IDs everywhere else other than the main Employee Details table. You might have a third Employee Addresses table, which holds contact information for a particular employee. Notice how logical groups of information that we have about an employee is split across multiple tables, and employees are always referenced using their unique IDs. This granular representation of data across multiple tables is called normalization, and this is how typically data is present in a traditional database. If you want to query information that spans more than one table, you'll have to perform a join operation. Let's say you want to query for Emily's department and all of the people who report to her. You'll have to perform a join operation across two tables using the employee's unique identifier. Remember the objectives of traditional database are different, which is why data is stored in a normalized form. We want to minimize redundancy and minimize our storage requirements. Remember, this space was expensive back in the day. We used foreign keys to ensure that joins we performed across multiple tables were valid. All updates to information was in one table or one location. There was no duplication of data. This made it easier for databases to provide transactional support and support for ACID properties.

Denormalized Storage, Nested, and Repeated Fields
BigQuery is a data warehouse, and not a traditional RDBMS. Data in BigQuery is stored in denormalized form. Let's understand what that means. Denormalized data basically means that data is compressed in such a way that all of the data regarding a particular entity is stored in a single table so that it can be read in one operation. Analytical data warehouses try to pack together as much information about an entity as possible so that they can be queried to extract business insights. Denormalized storage works for data warehouses, because this space nowadays is very, very cheap. Storage capacity on distributed clusters is no longer a constraint in the modern world. When we have denormalized storage, you don't have to set up foreign key or other constraints in order to make sure you reference data in a unique way. Denormalized storage works very well when most of the operations that you perform are read operations. They don't work very well if you want to perform a lot of updates to your data. When we store data in denormalized form, we are optimizing our data layout to reduce the number of disk seeks we have to perform when we read in data. We store data for an entity in one location, so all of the employee information will be present in a single table. We don't worry about redundancy. What we want to do is to minimize joins. Denormalized storage works very well for extremely large datasets. Join operations on extremely large datasets can get very expensive. That's what we want to avoid here. Let's say we want to query information about the employee, Emily, and all of her subordinates. We don't want to perform joins on multiple tables, instead, all of the subordinate information should be included in the same table that holds information about Emily. So we have a single large employee table. This will hold employees' personal information, as well as the subordinates who report to that employee. A single manager can have multiple reports, which means the subordinate information has to be in a list form, so there are multiple subordinates associated with a single manager. The subordinate field is a part of the larger employee table. It will be a repeated field of subordinate IDs. The same table should hold the contact information for individual employees as well. We can logically group the individual fields which corresponds to an address into a single field, which is of the struct format. A structure is a logical grouping of fields, which means city information and zip code information will be nested together into the same address field. In this way, the original employee data that was spread across three different tables can be compressed together into a single table, provided it supports repeated fields and nested fields. Now if you want to get all of the information pertaining to a single employee, you can do so with one read operation of this larger denormalized table. BigQuery has the capacity to support nested, as well as repeated fields. A struct is what creates a nested field. A struct is a logical grouping of data which can have data of different types. It can hold any number of fields or values, and each field is referenced by a name. BigQuery also supports repeated fields, which can be thought of as an array. An array is a collection data type. There is no fixed size to the number of values that can be stored in a repeated field, but all of the values in that field should be of the same type. BigQuery supports repeated fields of primitive types or collection of nested fields as well, so the type within the repeated field can be a record or a primitive type.

The UNNEST, ARRAY_AGG, and the STRUCT Operators
BigQuery has special operators that can be used with nested and repeated fields. Let's study those operators here in this clip. The first is the UNNEST operator that allows us to flatten repeated fields. We have the ARRAY_AGG operator, which does the opposite of UNNEST. It aggregates primitive data to create a new repeated field. And finally, we have the STRUCT operator that allows us to create a nested field by logically grouping together different columns. The UNNEST operator is used to split an array field into individual rows. Let's see how that works here with an example. Here a student with ID 1 has had three different addresses during the course of his or her study. Notice that the StudentId is a single value, and there are a list of addresses associated with that student. The UNNEST operator will flatten the list of addresses associated with this particular student so that each address is present as a separate record or row. Notice here that the StudentId goes from one record to three different records, and each record is associated with a separate address in the list. UNNEST is a flattening of repeated fields to have individual records associated with each repeated value. The ARRAY_AGG operation does the opposite of UNNEST. It aggregates individual records into a single array field or repeated field. Let's say we start off with three different records for a student, each associated with a different address. When we use the ARRAY_AGG operation on the address field, you'll end up with one record and a list of addresses. We go from three records for the student with ID 1 to a single record corresponding to the student, and his multiple addresses have been compressed into a single list field or repeated field. And finally, we have the STRUCT operator in BigQuery which aggregates individual columns or individual fields into a struct, or a nested field. If you group these field values together, let's say that the address information for a student was in three separate fields, the street in one field, city in one, and the state in the third. Using the STRUCT operator on street, city, and state, and aggregating it together into a single address field would give us us a logical group or a nested field. The original field names in the nested field can be accessed using the dot notation, Address. Street, Address. City, Address. State, as you can see here on screen. The three original fields now belong to a new nested field, the Address field.

Working with Nested Fields
In this demo, let's see how we can create nested fields in BigQuery. Within the spikeysales_dataset, let's create a brand-new table. We'll call this table the special_customer_details table. Let's define a few fields that will live within this table. We'll use the Add field button and specify a field for customer_name, which is of type STRING. We want this to be a required field. Go ahead and click on Add field once again and add a second field. This contains the customer's date of birth, it's a string field, and it's nullable. Go ahead and add a third field for addresses. An address can have multiple components. We'll specify this as a nested field or a record. The type of data that we're going to store here is a record, and for a record type you'll see that just next to the addresses field there is a plus button which allows us to add subfields into the address record. Click on the plus button and specify the components that make up the addresses record. The first is the status of the address, whether it's an active or an old address. Then click on plus once again and specify additional fields. We have a string subfield corresponding to the city of the address. We can go ahead and click on Create table. This will create a new table with a nested addresses field. click on the special_customer_details table on the left navigation pane, and if you notice the schema, you'll see how a nested field is represented. We have addresses as a record, and we have the status and the city fields within the addresses record, which are of type string. Let's insert one record into this table which has a nested field. We'll use the INSERT INTO command. We want to specify values for the customer name, date of birth, and addresses. We have John here, born in 1989. Notice how we specify the values for the record addresses. The values for the subfields within the addresses record are specified within parentheses. Execute this query. We get the results saying one row has been inserted. If you click on the special_customer_details table, you'll be able to preview the data that now lives within this table. Notice how address. status and address. city contains John's address components. For practice, let's run another inert query to add another record to this table. This is for Rachel, and notice once again we specify the components of the address within parentheses. Execute the query and preview the data, and you can see that the new record for Rachel has also been added. Let's see how we can query this table with nested fields. I'm going to query for customer_name and the addresses record. Remember, addresses is a nested field. Execute this query, and take a look at the results at the bottom of your browser. The status and city fields are named using the dot notation, addresses. status, addresses. city. You can use the same dot notation within the query as well. I want to select the customer_name, and only the city information. Notice how I specify addresses. city within my select statement. Execute the query, and here you can see the name of the customer and the city where he or she lives.

Populating Data into a Table with Nested Fields Using STRUCT
In this next part of the repeated fields demo, we'll start off logged into a Cloud Shell terminal window. We'll create a new table containing the same data which is present in our e_commerce_data table, but this time our new table will have a few nested fields. We start off with a JSON schema from another table, the e_commerce_partitioned table, which contains the exact same information as e_commerce_data, but it is a partitioned table. We just want the schema here, which doesn't change based on whether the table is partitioned or not. We use the bq. show command with the --schema flag, and the format is prettyjson. This will print out the JSON schema of the table. We'll copy over this JSON schema, and we'll use the schema in order to create a new table. We are now in the BigQuery web console. Within the spikeysales_dataset, I'm going to create a brand-new table. We'll call this table e_commerce_data_nested because it's going to contain nested fields, and within schema I'm going to choose the Edit as text option so that I can specify the schema in a JSON format. Now this is the original JSON format which contains no nested fields. All of the field values are flattened. I'm going to get rid of a few fields here so that I can put these values in a nested field. This contains the sales and order quantity, and this contains the OrderID and OrderDate. I delete these four fields, and create a new nested field called OrderInfo. Observe that OrderInfo is a nullable field of type record, and the subfields within the orderInfo are OrderID, OrderDate, Sales, and Quantity. We can now group the components of address into one nested field as well. Address_Country, City, State, Postal_Code, and Region will be put into one nested field called Address. Address is a nullable field of type RECORD, and within it it has subfields for country, city, state, postal code, and region information. Go ahead and create this table, and once the table has been created, you can click on e_commerce_nested and take a look at the schema. You can see here that OrderInfo is now a nested field, as is Address. OrderInfo contains four subfields, and Address contains five subfields. The remaining fields in this table are unchanged. They are the same as the fields in e_commerce_data or e_commerce_data_partitioned, and this table is currently empty, as you can see from the Preview pane. Let's use an insert statement to insert some data into this newly created table. We want to insert into e_commerce_data_nested, OrderInfo and Address our record fields. We'll query the data that we want to insert into this table from another table. The SELECT statement will be run on the e_commerce_partitioned table. Notice how the flattened fields are selected as is, the nested fields are selected as structs. The values in the fields for OrderID, OrderDate, Sales, and Quantity form one nested field, Address_Country, City, State, Postal_Code, and Region form another nested field, and they are specified as STRUCTs. The other individual fields are queried as is from the e_commerce_partitioned table. If you execute this query, this query will run and populate your nested table. If we view the Preview tab here, we can see how exactly the data is stored. Notice that OrderInfo is a nested field, and we use the dot notation for subfields within this nested field. The same is true for the Address nested field as well. Let's run a simple query on this table with nested fields. We want the OrderDate and OrderID information from the OrderInfo nested field, we want the entire Address record, we want Discount and Profit field information. Execute this query, and you can see the result right here on screen. OrderDate and OrderID are present as individual fields in the result. The address is still a record, or a nested field.

Working with Repeated Fields
Now that we've understood nested fields, let's see how we can work with repeated fields in BigQuery. We'll start off in the BigQuery web console. I had way too many tables under my spikeysales_dataset, which is why I've cleaned them up a little bit. We now have the e_commerce_data, e_commerce_data_nested, e_commerce_partitioned, and the OrderData tables. I've deleted the remaining tables, and now I'm going to create a brand-new one. This table will have repeated fields. I'll call this table the customer_order_details table, and I'm going to use the Add field button to add individual fields. The first field will be called customer_name. It's of type String, and it's nullable. The second field is the order_id, it's of type integer, and it's a required field. Within a particular order, a customer could have bought multiple products. This is what is held in the product_names field, which is of type string, but it's not a nullable field, instead it's a repeated field so that it can hold multiple products. Specify product_names as a repeated field, and click on the Create table button. A brand-new table will be created. Click on the table name in the left nav pane in order to view order details. Here is the Schema for the table. Notice that product_names is a repeated field. The Preview pane will tell us that this table is currently empty. Let's go ahead and add some data into it by using the INSERT INTO command. We want to insert into this table customer_name, order_id, and product_names information. Observe in the values that we insert into this table that the names of products are specified within square brackets. This is what denotes a list or a repeated field. Once the new rule has been inserted into our table, we can preview the contents of the customer_orders_details table, and here is what a repeated field looks like. We have one record here with a customer name and order_id, and there are four products within this particular order. Let's insert another record here; this time for the customer Rachel who has bought four products as well, keyboard, umbrella, curtain, and phone. These products are specified within square brackets. Execute the query, click on the table, and preview the results, and you can see that this table now has two records, one for John and one for Rachel. The record for Rachel, which did not exist before, has now been added. Let's run a simple query here to select the name of the customer and the product that he or she has bought. Click on Run query, and you can see the results right here in the lower part of your browser window.

Populating Tables with Repeated Fields Using ARRAY_AGG
In order to get some more practice with repeated fields, let's create a new table within the spikeysales_dataset. This time this table will have two repeated fields. We'll call this the customer_product_price table. Use the Add item link here to specify the schema for this table. The first is the CustomerID field, which is of type string and nullable, and we have the Products_Bought field, which is of type string, but is a repeated field. You can imagine that this contains a list of all the products that the customer has bought on spikeysales, and the price that the customer has paid for these products. Price_Paid is also a repeated field. This repeated field is of type float. Once we have all of our fields, we can go ahead and create this table. Click on the table name on the left, and let's take a look at the schema for this table. We have CustomerID of type string. Products_Bought and Price_Paid are both repeated fields. In order to populate this table, we'll extract information from the e_commerce_data table that we had set up earlier specifically. We want all of the products that the customer bought, and the price the customer paid for each product. We get this information using the ARRAY_AGG BigQuery operator, which allows us to aggregate individual records into a single list or array, or repeated field. Every product bought by a particular customer is a separate record in the e_commerce_data table. We perform the ARRAY_AGG operation on the Product_Name field from the original table. All of the products are aggregated as a list and saved in the Products_Bought field. We perform the same ARRAY_AGG operation on the sales field as well, which contains the price that the customer paid for individual products. We query this information from the e_commerce_data table, and we group this information by CustomerID so that we can perform our ARRAY_AGG aggregations. Group by CustomerID is required in order to get all of the products and the corresponding prices in the form of a list associated with a single customer. There are 641 unique customers in our e_commerce_data table. These have now been aggregated into the customer_product_price table. Let's take a look at what the data looks like. Here is the preview pane. There are customers here who bought just a single product. These are flat records, or so it seems, and you can see there are customers here who bought multiple records. The Products_Bought and the Price_Paid are both repeated fields. Here is another customer here who has bought two products from spikeysales. Click on the Compose query button on the top right in order to query this table with repeated fields. There is nothing really special about this query. We ask for the CustomerID, Products_Bought, and the Price_Paid. Execute this query, and here is the result right here at the bottom of the browser. Here are customers who bought just one product, and here are customers who bought multiple products. Products are available as a list associated with each CustomerID.

Using Nested and Repeated Fields Together
Nested and repeated fields are new features in BigQuery which you may not have encountered earlier if you're working with regular SQL. Let's see an example now of using nested and repeated fields together. In this demo, we'll run a new query on the e_commerce_data table. This is the one with all flattened fields. We'll write a query on this e_commerce_data table in order to find all of the customers who live in a particular city. We want additional information about a customer. We want the CustomerID, as well as the CustomerName, aggregated into a single nested field. So we create a struct called CustomerInfo which holds the CustomerID as well as the CustomerName, and we apply the ARRAY_AGG operator in order to create an array of CustomerInfo objects associated with every city. In order to perform this array aggregation, we have to group our data by city. We order by city as well. Execute this query, and let's take a look at the results. And here are the results on a per-city basis. We can see that spikeysales has three customers from the city of Akron, and here are their CustomerID and CustomerName information within the CustomerInfo nested field. Spikeysales has just one customer in the city of Allen. Let's write another query here. This time we want to aggregate all of the order information associated with a particular state. The order information contains the OrderDate, OrderID, the sales in that order, and the quantity of products in that order, and we want to aggregate this information into a repeated field associated with a particular state. In order to do this, we need to group by state. We order by address_State as well so that we can view this in an alphabetical order. You can see all of the orders that are associated with Alabama, and here are all the orders associated with Arizona. If you want to save the state-specific information, we can use the Save as link here. We can choose the option Save as table. You can also download this JSON as JSON. Because this table is fairly complicated with nested repeated fields, Download as CSV and Save to Sheets is not enabled option. We'll call this new table orders_by_state. Click on the Save button, and you can see that this new table will be created and your data saved. Click on orders_by_state in the left navigation pane, and here is the schema for this table. OrderInfo is a repeated field, it's also a record, and these are the subfields that are contained within every record in the OrderInfo field.

Using UNNEST to Query Repeated Fields
Let's run a few queries on the customer_product_price table that we created earlier. This is the one with two repeated fields. For every customer, we have a list of all the products that he or she bought on spikeysales, and the price that he or she paid for that product. In this query, we want to retrieve all of the CustomerIDs and the products that they bought, but we want to do this in a flattened manner. So we retrieve CustomerID and Products, but here the Products field is basically the flattened version of the Products_Bought field. The UNEST operator in BigQuery flattens the Products_Bought array into individual records, and associates each of these records with the CustomerID who bought the product. Go ahead and click on the button to execute this query, and you'll see the results here at the bottom. You can see that there are certain customers who bought just one product, and there are other customers who have bought multiple products. You can see that the value in the Products_Bought field has been flattened and converted to individual records associated with the CustomerID. Let's see another query here, this time with two UNNEST operations. You want to select the CustomerID, Products, and the Price. We get the products that a customer bought in flattened form by calling the UNNEST operator on the Products_Bought field. We then call the UNNEST operator on the Price_Paid field to get the price for each product in a flattened form as well. Execute this query, and you can see that the result is in completely flattened form. For every CustomerID, we have the product that he or she bought and the price that he or she paid for that product. Let's practice the UNNEST operation a little more. This time we'll try it on the orders_by_state table. Here is the schema of the table. You can use it to refresh your memory. For every state, we have all the orders that are associated with that state. Let's get the information about all of the orders associated with a particular state in flattened form. We query the Address_State and the OrderInfo. The OrderInfo comes from the UNNEST operation on the original repeated OrderInfo field. This query is valid, and it runs just fine, but it doesn't really do what you might expect. You can see that the order information hasn't really been flattened into individual records. It still remains as a nested repeated field. Let's try this query once again with a slight modification. We want to select the Address_State and Orders, and we've unnested the OrderInfo and aliased it as Orders, and now we'll see the flattening really happens. Here in the result you can see that the nested fields remain nested, but all of the repeated fields have been flattened into individual records. You can use the UNNEST operator and extract specific subfields from your Orders field as well. Here we want only the OrderDate. We've unnested OrderInfo as Orders and extracted only the OrderDate from here. Run this query, and here you'll see in the result the OrderDate associated with all of the orders from each state, Alabama here, Arizona is available here. You can scroll down and view the results in all of the states if you want to.

Aggregations
In this demo, let's see some of the aggregation operations that we can perform in BigQuery. BigQuery supports almost all of the aggregations that SQL supports, and some of these things should be familiar to you. We are going to run our aggregation queries on the e_commerce_data_nested table. This is the schema of the table. This contains nested fields for order information and address information. You can take a quick look at the data in order to refresh your memory. Let's start writing our queries here within the query editor. In our first example here, we'll get introduced to the ANY_VALUE function. This function will retrieve at random any record that meets our selection criteria. ANY_VALUE of OrderInfo will retrieve any order within the Address_State of California. Execute this query, and here is an order retrieved at random. This order is from the state of California. We can use the average aggregation in BigQuery as well. Here we want to find the average price of products bought by a particular customer, so we perform the average function on the OrderInfo. Sales field. When you perform aggregations, you need to group by the non-aggregated fields, which are CustomerID and CustomerName. We'll also order our result by average price. Run this query, and you can see the results at the bottom here, the average price paid by customers for products that they bought on spikeysales. Let's run another query here. We want to retrieve information about all the customers who have made purchases of over $100 from within our data. We use the COUNTIF function for this. COUNTIF will apply the condition that we have specified within brackets. Here the condition is the sales should be greater than $100, and it will alias it as NUM_OVER_100. This will give us all of the customers when they have bought products over $100 on spikeysales. We need to group by the non-aggregated columns, group by CustomerID, CustomerName, and order by the number of items over $100 that the customer has purchased. Execute this query, and you'll see that there are several customers who made purchases of over $100. Sanjit Chand here is one customer who has bought four such items. Let's see some examples of aggregation operations that create lists such as the STRING_AGG function. The STRING_AGG will create a single list of all the customer names in our table. Execute this query, and you can see that the result is a single list of all customer names. You can click on the More link here to view all of the customer names in a separate dialog. The ARRAY_AGG operator that we use to create lists or repeated fields is actually very powerful. Let's see some different ways in which we can use this. Here we've performed an aggregation on the Address_City column. This will give you a list of all of the cities where spikeysales has customers as a list, and this list will have all of the city names specified in quotes. You can click on the More link here and view the entire contents of this list in a separate dialog. By default, ARRAY_AGG will not eliminate duplicates. If you want to eliminate duplicates in your list of cities, you need to add the DISTINCT keyword as you see here on screen within the ARRAY_AGG operation. Running this query, will give you a list of all of the unique cities where spikeysales has customers. You can see that this list is a much shorter one. You can specify limit operations within the ARRAY_AGG function as well. Here we want to aggregate cities, but we want to limit it to just five cities, so we specify Address. City LIMIT 5. Run this query, and you can see that the resulting list has just five cities within it.

Subqueries
BigQuery allows you to specify subqueries in the FROM clause, as well as in the WHERE clause, and let's see some examples of subqueries. Here is a SELECT statement for the OrderDate, Product_ID, and CustomerName that uses a subquery in the FROM clause. We want to select r. OrderInfo. OrderDate. Notice how OrderInfo is a nested field and OrderDate is a subfield within the nested field. R is the alias that we give our subquery. We'll see that in just a bit. Take a look at the FROM clause. You'll see that we don't specify a table directly. Instead we specify a subquery that queries the e_commerce_data nested table. This is simple, SELECT * subquery, the alias to subquery AS r, and this is the r that we reference in the SELECT clause. We want the resulting data ordered by the OrderDate field in the ascending order. Execute this query, and you can see the result right here at the bottom of your browser. Let's try another subquery in the FROM clause. This time we'll use an aggregation. We want to find the sum of all sales within a particular segment. The subquery in the FROM clause specify the data that we are querying in the outer query. Here we want to only extract those records that the Address_State is equal to California. You can see SELECT * FROM e_commerce_data WHERE Address_State is equal to California in the subquery. We alias the subquery AS r, and use r within our SELECT clause. We are preforming an aggregation here, which means we need to specify a GROUP BY clause as well. We group by segment. Execute this query, and you'll get as the result the segment wide sales in the state of California. Here is a typical query that your analyst team might want to run. You want to find the total sales on a birth date basis. We select the Address_State, and sum up the sales information as Total_Sales. We know that spikeysales is very popular in the states of California and Washington. We're only interested in the Total_Sales data for these two states, so we GROUP BY Address_State, and we only display information for California and Washington, and this is what you see here in the result. But let's say the states that we are interested in is not from a fixed list; the list of states comes from another query. This is possible using subqueries in BigQuery. In this query, we retrieve the state and the total sales for that state, for all of the states that have same day shipments. So in the WHERE clause we specify that the Address_State should be in only those states where ShipMode is equal to Same Day is present. Group the result by Address_State, execute this query, and here in the result you can see that there are 23 states which have same day shipment, and here are the total sales in those states.

Windowing Operations
An important feature typically available on data analytics warehouses are window functions, and window functions are available in BigQuery as well. Window functions are very powerful functions which operate on logical groups of rows, and you can specify what those rows are. Window functions may be a little hard to understand at first, but they make very complex mathematical operations simple without needing many intermediate calculations or temporary tables that you need to set up. Analysts in the spikeysales organization often write queries such as these. What were the top-selling N products in each category over a particular week? Here the partition is one week, and the operation that we need to perform is to order the products by sales and then find the ranking. Analysts who work with supplier data might have to answer a query such as this one. What revenue percentile did this supplier fall into for this month? The partition here is one month, and the operation that we want to perform is to order the data and find percentile values on revenues on a per-supplier basis. When we have window functions, complex queries such as these can be expressed in a single query, and that's why window functions are so powerful. Let's see an example of how window functions work. Consider that this data belongs to an organization that runs a chain of grocery stores in different locations. We want to find that store which has the lowest price for a particular product. We will first take all of the data available in this table and perform a partition on the Product field. Partitioning is a logical operation that will group our data based on products. Bananas will be in one partition, potatoes will be in a separate partition. Once we have our partitions, the next step is to order within each partition all of the records based on the price for that particular product. This ordering of products by price is performed within each partition, so within the bananas partition we have an ascending order of bananas by price, and within the potatoes partition we have the same. Now we can find the store with the lowest price, and that will be the first record within each order partition. Window functions allow you to partition and order data within a partition. In addition, you can specify a window range, forming a window of records within a certain partition, for example, unbounded preceding and current row. Let's see how this window range works. The window function will be applied to every record within a partition. This is the current record within the partition, this is the current row, so only that record is included within the window. When we move to the next record, unbounded preceding basically means include all the records all the way to the beginning of the partition. That means the first and the second record will be included here. We have all of the records from the beginning of the partition up to the current row. When we move to the next record, we'll include all of the records from the beginning of the partition up to the current row, which is now the third row in this partition. And finally, when we come to the last record in this partition, we include all of the records starting from the beginning of this partition up to the current row, which is now the fourth row. This window range is applied to each partition in our table. When you use windowing operations in BigQuery, you can partition by values in any column, you can order by any column, and there are a number of different options available for the window range as well. Unbounded preceding and current row is just one example.

Performing Window Operations Using "Partition By" and "Order By"
In this demo, we'll see some examples of window operations in BigQuery. Let's try our very first window query. This is going to be a simple one on the e_commerce_data_nested table. We want to calculate the value of total sales on a per-segment basis. and we do this using a window function. This is the general structure of any window function. We have an aggregation operation, and then the OVER keyword. The actual window over which this aggregation operation is applied is specified within parentheses. This window can specify a partition and order within a partition, and a window range within that partition. Here we only use the PARTITION keyword. We want to partition our data by segment and find the sum of sales for each segment. Execute this query, and let's take a look at the result. We can see that we have multiple rows corresponding to the same information. We have the total sales for each segment, but we have one record for each order in our table. This is not exactly what we were looking for. We can fix our query a little bit in order to view the information a little differently. Here is the data for the Corporate segment, the total sales corresponding to Corporate, multiple records with the same information, and we see that the same is true for the Home Office segment as well. Let's fix our query here. We modify the SELECT clause so that we specify a Select Distinct. This will extract only the distinct records. The partition and the function over the partition remains the same as before. If you execute this query, you can see that we get the result with three records, the total sales for Home Office, Corporate, and Consumer. Let's run another query here that will track every customer within a particular state by total sales, by how much they bought on spikeysales. Here we have the individual fields, CustomerID, CustomerName, OrderInfo. Sales, and Address_State, and here is our window function. We want to perform a ranking operation over the partition that we have specified. The rank operation is over PARTITION on the Address. State, and within the Address. State partition we want to order by OrderInfo. Sales. Run this query, and let's take a look at the result, and you can see that within the state of Alabama we have rankings for individual customers. You can see that Irene Maddox has bought a total of $1800 worth of products. She is the customer with rank 1 in Alabama. We've partitioned our data by state, and ordered by sales, and applied a ranking function within Arizona. We have Susan who is at number 1. If you scroll down to the state of Arkansas, you can see that Sanjit Chand is at number 1 here. Just like the rank function, BigQuery has the percent rank function as well, which allows us to find the percentile rank. We want to use this query to find the percentile rank by total sales for every city in a particular state. We query the state, city, and total sales information, and apply a window function to find the percent rank. We call the percent rank function, we specify a partition, we partition by state and order by total sales in the ascending order within the state. In order to find the city by its total of sales for every state, we use a subquery. This subquery finds the total sales for a particular city by grouping by Address. State and Address. City. Execute this query, and let's take a look at the result. You can see that within the state of Alabama, cities have been ordered by their percentile rank. You can see that the city Auburn has a percentile rank of 0. Then the cities have gradually increasing percentile ranks until we come to Florence with a percentile rank of 1. For Arizona, the city Sierra Vista has the lowest total sales; it has a percentile rank of 0, and the city here, Tucson, has the percentile rank of 1 with the highest total sales within Arizona.

Windowing Operations Using a Window Range
Let's see one last example of a more complex window operation which uses a window range before we move on. This query is a slight modification of the previous one that we have seen. We're still looking at Address. States and Cities within states and calculating the total sales. We're ordering cities by the total sales, and then we want to perform a cumulative sum operation. You can calculate a cumulative sum in BigQuery using a window range. Let's see how to do that. We're selecting the state, city, and total sales. This is exactly the same as before. Our window function will use the partition, the order by, as well as the window range. We want to perform a sum operation on the total sales field. This is our aggregation. We want to partition by state and within this partition we want to order by the total sales field. We want cities in the ascending order of sales from the lowest sales to the highest, and then we specify a window range. range between unbounded proceeding and current row. This window range will apply the aggregation function in such a way that we get a cumulative sum of sales starting with the city within the state which has the lowest sales, and going up to the city with the highest sales. We extract this information from the same subquery that we ran earlier. There is no additional information needed here. Let's execute this query and understand how the cumulative sum works. Here are all the cities arranged on the basis of their Total_Sales within the state of Alabama. Notice the very last column here which has the cumulative sum. You can see that the value in the second record is the sum of the total sales in the first, as well as the second record from the beginning of the partition up to the current row. The cumulative sum in the third record is the sum of the first three records, the sum of Total_Sales for Auburn, Hoover, as well as Montgomery. And on this note, we come to the very end of this module where we studied advanced analytical queries that we can run on BigQuery. We saw that BigQuery supports both nested, as well as repeated fields. Nested fields are effectively generated using the STRUCT operator. They are a logical grouping of your field data. Repeated fields are like arrays in programming languages. BigQuery also allows you to run subqueries to perform complex analysis. BigQuery also has support for windowing functions. In the next module, we'll see how we can use Data Studio for visualizing data that's contained in BigQuery. We'll also see how we can access BigQuery programmatically using client libraries in Python.

Programmatically Accessing BigQuery from Client Programs
Module Overview
Hi, and welcome to this module where we'll see how we can programmatically access data in BigQuery from client programs. We'll first study Data Studio, which is a powerful visualization tool offered by Google. BigQuery has built-in integrations with Google's Data Studio, which helps you create very powerful and cool-looking visualizations very easily. We'll also see how we can set up Datalab notebooks from within GCP. Datalab is basically a VM instance that allows you to prototype and visualize your data using Jupyter notebooks hosted on the cloud. We'll write code using Python and Datalab, giving us programmatic access to BigQuery using Python client libraries.

Integrating BigQuery with Data Studio
In this demo, we'll see how you can visualize the data that you have stored in BigQuery using Google's Data Studio. Data Studio can generate some pretty nifty-looking charts and graphs. So what exactly is Data Studio? It's a tool used for interactive dashboard building and reports. This has built-in connectors to several GCP services including BigQuery. If you have your data stored across different services on the GCP, you can directly access the data there and visualize it using Data Studio. Data Studio also makes it very easy for you to share your reports and visuals within your team or organization. Data Studio follows the point-and-click pattern to build dashboards. It's very intuitive and easy to use. It's available at datastudio. google. com. Once you log in, this is the welcome page that you'll see. You can use this tutorial here to get started. This says Welcome to Data Studio. Take a look at the left navigation pane. Whatever function you want to perform, you can take a look at the tutorial here. We'll just plow ahead with Data Studio. We'll go ahead and create a new blank report and use it to test out different visualizations. I'm going to rename this untitled report to be called spikeysales_report, and this report will be automatically added to the Google Drive linked with your GCP account. If you want to just play around with Data Studio, here are a number of sample datasets that you could use, or you could connect to the data that you already have stored in BigQuery. Click on Create New Data Source at the bottom right here in order to set up a connector. This will show you all of the data sources that you can use with Data Studio. I'm just going to click on BigQuery here because we have our e_commerce_data stored within BigQuery. Our project is the spikey-bq project, and within the spikey-bq project we have the spikeysales_dataset, and we'll use the e_commerce_data table. Click on the Connect button on the top right. This will bring you to a page which will list all of the fields available in the e_commerce_data table and the corresponding data types. You can click on ADD TO REPORT to add this data source to our current report. Click on ADD TO REPORT here, and we have now connected to our e_commerce_data table. On the right navigation pane, the layout tab allows you to determine how exactly your visual will be displayed. If you want to change the look and feel of your graph, you can use the THEME tab. You can click on the chart icon in the top menu in order to create a new visualization. We'll choose a simple time series line chart. With the time series chart selected, simply draw on your report and the graph will automatically be displayed. Data Studio has automatically chosen a time dimension which made sense from our e_commerce_data datasets. The time dimension chosen by Data Studio was the ShipDate, and the metric is the record count, the number of orders for every ship date. Let's click on the metric to change the data that we want displayed. Instead of record count, I want to display sales information based on ship date, and my visualization is immediately updated. Pretty nifty, isn't it, and all this without writing a single line of code. This is a time series chart. Let's say we want a different time dimension. We don't want ShipDate, we want to view sales information based on OrderDate. Simply choose the OrderDate metric and our chart has been updated. You can drag the corners of the chart so that it's displayed in the size that works best for you. I'll make this time series visualization smaller and choose a different chart to display. This is a simple bar graph. When I draw out a visualization, Data Studio, once again, has automatically populated this with some data that made sense. The X axis contains the Sub_Category of our products, and on the Y axis we have the record count, the number of products sold for each Sub_Category. This, of course, may not be the data that you want to view. Click on the Sub_Category in order to change what we represent on the X axis. I'm going to go ahead and change the X axis to represent the state to which the order was shipped. My visualization is updated. I'm also going to change the Y axis to represent sales information by state. You can see from the bar graph here that the state of California has the highest sales, the state of New York comes second. If you're not really impressed with the theme of your graphs, you can change these using the STYLE tab. You can make your bar chart less boring by orienting it horizontally or you can switch back to vertical because that's easier to read. You can also specify how many bars you want displayed. I've chosen to display fewer bars here. I go from 10 to 8, and it's the default. There is a color palette available here. You can click on it and update the colors to match the colors of your organization. As you click around and make changes, your visualization is automatically updated. If you want to display labels on your X and Y axis, you can click on the checkbox that says show axis title on the X, as well as the Y axis. I think the white background for this chart is a little boring. I can choose the background color palette in order to update the background color. Now I have a bright yellow. You can select multiple charts by dragging your mouse over them, and you can hit the delete button in order to delete everything from this report. Once I have a clean report, I'm now going to try out a pie chart. Once again, Data Studio automatically populates the pie chart with data it thinks it is interesting. Then I mentioned here the Sub_Category and the metric is a Record Count. Click on Sub_Category, and you can change this to a different dimension. I'll choose Address_City here, and automatically my pie chart gets updated. I'm also going to change the metric to be Sales. And here you can see Sales by City are displayed in my pie chart. If you want to perform a different aggregation on a metric, and not the sum aggregation, you can click on the pencil icon there in order to change your aggregation. This pops up a window showing you all of the different aggregations that are available. As you select an aggregation, your pie chart will automatically update. I'm going to get rid of this pie chart and take a look at this really cool visualization, the Geo map. The Geo map will automatically plot location data from your dataset. Notice that Data Studio has detected that the Address_Country field is geographical information, and all of our orders in the spikeysales are based in the US. Based on how large your dataset is, the Geo map might take a while to populate, so you might need to be a little patient. Let's click on the Address_Country dimension and change the dimension to be Address_City. Notice that our Geo map has now zoomed into the United States. I'll wait a little bit for the data to populate, and this Geo map will automatically give me sales information based on region. You can see that the largest sales come from New York, California, and the state of Washington. When you use Data Studio, you have all of this cool information and visualization at your fingertips without writing a single query or a single line of code.

Connecting to Datalab
In this demo, we'll see how we can access data that is stored in BigQuery, programmatically. We'll use Datalab notebooks to write Python code, and we'll access information from BigQuery. Google's Cloud Datalab is a powerful interactive tool that is used to build machine learning models on the GCP. Datalab is a VM instance which has data exploration and visualization tools preinstalled on that VM. Datalab is software that runs on a special VM which runs Jupyter notebooks. Jupyter notebooks are an interactive browser-based shell which is widely used by data analysts, engineers, as well as data scientists to prototype ML models and visualize data. Working with Datalab is very cool because it has built-in connectors for other GCP services. It's automatically integrated with other services, and you can pull in data from other GCP sources. We start this demo off in a Cloud Shell window. This is where we are going to use the command line in order to create a new Datalab VM instance. Datalab creates the command. This command is automatically available within Cloud Shell. Bq-datalab-vm is the name of our virtual machine. GCP will ask you to specify the zone where you want this VM instance to live. I'm just going to select zone 50, us-west2-a. Datalab instances take a little while to spin up. You have to be a little patient. You can view the log messages and see where exactly you are at. Google automatically sets up an SSH connection to your Datalab instance. For this you need to generate SSH keys. Yes, I do want to generate keys, and when prompted for a passphrase, I'm just going to specify the empty passphrase for my RSA private public keypad. You need to do this process exactly once for each project. Once the SSH keys have been propagated, you can wait for your Datalab instance to be up and running, and then connect to it. The Web preview button at the top right corner of your terminal window allows you to connect to Datalab. I'm going to change the port to be 8081, because that's where my Datalab instance is running. Change and Preview, and you'll immediately get a new browser window where you connect to Datalab. If you've used Jupyter IPython notebooks, you'll find this interface very familiar. Datalab has automatically set up a few examples that you can try within the docs and notebooks folder here. We'll use a new notebook to write our Python code. Click on the notebook link here. This will open up a new browser window. You can change the name of your notebook from Untitled Notebook. I'm just going to call it bq-datalab. Click on Rename. We now have a browser-based interactive shell where we are going to write Python code.

Running Queries Programmatically
We'll first see how we can access data within BigQuery using built-in connectors that come preinstalled with Datalab. I'm going to set up a couple of import statements here to get the libraries that I need. I'm importing the google. datalab. bigquery library aliased as bq. I'll also import the open source pandas library that makes it very easy for me to work with tabular data. Use Shift plus Enter to execute the code that you write in each cell of this notebook. The results of your execution will be available right here on screen for you. Once I have these libraries imported, I can use the bq-magic command to invoke the BigQuery Datalab connector. %%bq will give me automatic access to BigQuery. The bq query -n request command indicates that I want to specify a query here. I want this query to be stored in the requests variable. And here is the query that I want to execute on the e_commerce_data within my spikeysales_dataset. The structure of the query is exactly like how you would run it using the BigQuery web console or the bq tool on the command line. I'm retrieving the OrderDate, Quantity, and ShipMode for all orders in the state of California and New York. This particular query has been stored in the requests variable. I can sample 10 rows from this query by executing this request. Bq sample --count 10 will sample 10 rows, and the query that I want to execute is the request query. And if you execute this particular cell, here are 10 rows from the table. Observe how seamless it is to work with BigQuery from within a Datalab notebook. If you check the type of the request variable, you'll see that it's a special class within google. datalab. bigquery called query. You can execute this query by invoking the execute function on the requests variable as well. The query was executed, as you can see from this job specification here, but we didn't really store the results anywhere. Let's see how we can execute a query and store the result in a pandas data frame. You can specify that you want the output rows of this query as a pandas dataframe by simply saying bq. QueryOutput. dataframe within your options. Call the result function after you've executed it, and then the df variable will contain your query results. There are 372 records within df, and executing the head function on your dataframe will give you the first five records. You can see the data types associated with each column in this dataframe. We have the OrderDate and the ShipMode that are objects. The quantity is of type int64. Once you have data in the form of a dataframe, you can apply all data manipulation and plotting techniques that pandas has to offer. Here I'm grouping my information by ShipMode and performing a sum aggregation on the quantity so that I can see the total sales on a per-ShipMode basis. You can see from the results here that there are 206 shipments which were shipped first class, 53 same day shipments, and so on. Pandas offers you visualization tools as well. You can call groups. plot. pie to view this data in a pie chart format, and here is the resulting pie chart. You can see that the vast majority of shipments are of standard class. you can ignore the warning that you see on screen here. That's because certain forms haven't been set up correctly on Datalab. Nothing to worry about as far as our demo goes. If you want to access BigQuery programmatically using Python client libraries, you have to first install them on Datalab using pip install. Call pip install --upgrade google-cloud-bigquery. This is the step that you'll perform in order to access BigQuery from any Jupyter notebook, even one that's running on your local machine, not just on Datalab. We'll import the BigQuery module from the google. cloud namespace, and use this module to instantiate a bigquery. Client and assign it to the client variable. Even with Python libraries, you can use magic commands from within your Datalab notebook. You have to explicitly load these magic commands by calling the %load_ext magic command. Loading these magic commands is a good choice to make because it will allow you to execute queries on BigQuery with minimal code. If you want to specify a query, you simply say %%bigquery. This will execute the query and store the results in a pandas dataframe. Here is the query that we want to execute. We want the CustomerID, CustomerName, and Sales data from California or New York. We want just 10 records. When you execute this query in the cell where you specify the bigquery magic command, you see the results right here on screen in front of you. The results are in a pandas dataframe. You can specify a variable. Here it is state_info to hold this dataframe as well. We specify a new query here to get the Total_Customers and Total_Sales by state and store it in the state_info dataframe. A summary of the query results will be displayed to you within your browser. This information is also present in the state_info pandas dataframe, and if you call the head function you can view a sample of the results. Once you have a dataframe, you can use the pandas visualization libraries to plot graphs. We use state_info. plot. bar to view a bar chart representation of Sales_data, as well as Total_Customers by state. Let's see a different way by which we can execute queries using the Python libraries. Here I have specified the same query within a query string. I'm now going to call client. query, pass in the query that I specified, and indicate that I want the results in a dataframe format. I'll store the resulting dataframe in the variable state_df. This is exactly what we did before, but with magic commands. This time we do it by specifying Python code. If you invoke the head function on this dataframe, you'll see that we do have the query results.

Summary and Further Study
And this brings us to the very end of this module. In this module, we saw how we could visualize data that we stored in BigQuery using Data Studio. We set up very complex visualizations without writing a single query or a single line of code. We then moved on to our next demo where we saw how we could programmatically access BigQuery APIs from within Cloud Datalab. And this brings us to the very end of this course on BigQuery, the data warehouse that is available on the GCP. If you found this course interesting, and you're interested in studying data warehouses and other cloud computing platforms, here are some resources that you might find useful. Building Your First Amazon Redshift Data Warehouse will talk about Redshift on AWS. Introduction to the Azure Data Lake and U-SQL will introduce you to the data warehousing solution that the Azure platform has to offer. If you want to learn more about other GCP technologies, here are some courses that you might find interesting. Creating and Administering Goggle Cloud SQL Instances will talk about Google's relational database service on the cloud. Architecting Google Cloud Storage Configurations will talk about how you can create and manage object storage using buckets on the GCP. And this is all I have for today. Thank you for listening.

Course author
Author: Janani Ravi	
Janani Ravi
Janani has a Masters degree from Stanford and worked for 7+ years at Google. She was one of the original engineers on Google Docs and holds 4 patents for its real-time collaborative editing...

Course info
Level
Beginner
Rating
4.6 stars with 13 raters(13)
My rating
null stars

Duration
2h 48m
Released
15 Oct 2018
Share course

