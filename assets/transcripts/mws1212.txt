Relational Database Design
by Hugo Kornelis

This course is for anyone who wants to understand relational database design, or data modeling in general. You will learn how to gather requirements, model them, normalize the model, and transform that model into a fully normalized relational...

In this course, you will learn all the skills required to design good databases. This starts with information gathering: how to find all required information, and how to ask questions without miscommunications. You will learn how to abstract the information gathered into a data model, how to normalize the data model so that your database will be free of anomalies, and how to transform the final, normalized data model into a relational database design - ready to be implemented.

Course author
Author: Hugo Kornelis	
Hugo Kornelis
Hugo is co-founder and R&D lead of perFact BV, a Dutch company that strives to improve analysis methods and to develop computer-aided tools that will generate completely functional applications...

Course info
Level
Intermediate
Rating
3.9 stars with 355 raters(355)
My rating
null stars

Duration
7h 33m
Released
6 Jan 2014
Share course

Introduction
Outline
Hello, my name is Hugo Kornelis and I am presenting this Pluralsight course on Relational Database Design. This is Module 1, the introduction. In this module, I will first give a quick overview of how the hardware used to store computer data has evolved over time. Data storage methods and paradigms have seen many changes, most of which who are influenced by the possibilities and the limitations of the hardware and we'll look at the evolution of database technologies next. After that, I will focus on the Relational model, which is the subject of this course. I will discuss the fundamentals of the relational model and explain the set of rules known as Codd's Rules that should be at the core of any relational database engine and that also influence many of the data modeling rules we will see throughout the rest of the course. I will close off the introduction by describing the differences between Conceptual, Logical, and Physical data models.

Development of Storage Hardware
The earliest electronic computers, which were built in the 1940's, used hardware for data storage that was already proven technology back then. Punch cards were already in use in the 18th Century. For example, the United States Census of 1890 was the first to use Punch cards and a tabulating machine. Because of this, the total time to tabulate the census was down to only 2. 5 years, significantly faster than the 7 years required for the 1880 Census. A punch card is actually very simple. There's a card made out of stiff paper, with holes punched into it. Information is encoded by punching holes in specific pre-defined positions. In order to feed information, either program code or data to a computer, the stack of punched cards was inserted into the reader, which will then process those cards one by one in the order inserted. If a program produced results, it has to be retained for future processing, they would be sent to another device that would punch the proper holes in empty punch cards, but punch cards had their problems. With a limit of 80 characters per card, most information did not fit on a single card, but had to be written to a series of cards, which then had to be read again later in the same sequence. So you had to take care not to trip and drop a card deck when moving the data between the computer and the storage area. This problem was solved by using punch tape. Punch tape uses the same basic idea as punch cards, punching holes in paper to store data, but instead of using a deck of cards, a long strip of paper was now fed through a machine that would either punch holes in the right locations or read the locations of the holes and translate it back into data for the computer to process. A lot more data would fit on a single reel of punch paper, limited only by the length of the paper strip used, but the downside of that was the reliability. Punching copious amounts of holes in a thin strip of paper and then winding that paper strip around a reel at high speeds results in huge risk of tear. The 1950's saw a revolution in data storage for computers. Though magnetic storage had already been used to record music since the beginning of the 20th Century, it was only in 1951 that magnetic tape was first used to store data. The magnetic tape is a long strip of plastic film coated with a magnetizable coating. This strip is then passed at great speed along the read/write heads that are able to detect if modify the magnetic field of this coding. Using plastic instead of paper and not punching holes in the tape resulted in a much better longevity of the medium and the information density of magnetic tape is much, much higher than that of punch tape. This, along with a much higher access speeds, revolutionize computing and storage. Thos familiar with music recording techniques in the 70's and 80's of the 20th Century will remember that the predominant techniques back then were the audio tape, which is just magnetic tape storage for music and the vinyl record. A major advantage of the vinyl record was that you could very easily skip over a song you didn't like or replay the one just played. Just lift the needle, move it a bit to the start of the song you want to hear, then lower it again to resume playing. With audio tape, you had to rewind or fast-forward the tape, which usually took a lot longer. Magnetic tape for computer data suffered the same problem. Programs had to process the data in the order in which it was stored because rewinding or fast-forwarding the tape was so much slower that most units didn't even support it. The next revolution came in the late 1950's that only really took off since the 60's. Spinning disks, first in the Hard disk later also in spinoffs such as the Floppy disk, was the first technology to enable Random IO. The term Random refers to the ability to access data in any order instead of just sequentially. Instead of coding the magnetizable material on a strip of film, it is now coded on a round surface that is spinning at high speed, while the read/write heads move over the surface to access the information stored there. Just as I used to lift the needle and place it directly on the part of the final record where my favorite song starts. This Random IO is still slower than Sequential IO. Its speed is determined by the time it takes the read/write head to travel to the right part of the disc, plus the time it takes for the disc to revolve until the sector or ______ or requested data starts is underneath the read/write head, but the speed difference was now long enough to make a Random IO a very viable option. Then, nothing happened for a long time. A lot of effort was invested in increasing capacity and speed of storage hardware while at the same time, decreasing size and price, but the basic techniques remain the same. Magnetic disc for storing data that had to be easily accessible and magnetic tape for storing data that did not need to be immediately available, such as archive data or backups. We had to wait in 1985 to see a really new storage technique for computer data hit the stage, optical discs. They were actually already invented in 1958, but there were not brought to market until 1980 and compact discs were released as an audio recording medium, and then we still had to wait another five years before the CD-ROM made its entrance on the stage of computers. In the years after that, the CD-ROM was superseded by read/writable compact disc, then DVD, and currently Blu-Ray discs. They all use the same basic technique. Optical discs have a lot in common with magnetic discs. They also encoded data on a round surface that spins at high speed, while the read/write head moves to the appropriate location, but the information is encoded using optical rather than magnetic techniques. This made it possible to pack the data much denser so that more data can be stored on the same size disc. But the higher density does not come without a price and in this case that price is the speed. Reading from optical disc is much slower than reading from magnetic disc and writing to optical disc is even much slower than that. As a result, optical discs have never been an alternative for magnetic discs, but they do form an interesting alternative to magnetic tape, for storage of archive data or backups. Solid Sate Drives, or SSDs, are the latest new developer in storage hardware. The technology has been used for several years already in thumb drive and consumer devices such as Smartphone's, but only recently have reliability capacity and pricing developed to a level where SSDs are a viable alternative for enterprise storage. An SSD does not have any moving parts. All data is stored on memory chips, usually flesh memory, a special kind of memory chips that retain their stored information even without power. Because there are no moving parts, access time is no longer limited by the time required to move a read/write head or to spin a disk to the correct position. Every location on the drive can be instantly accessed. This makes SSDs a lot faster than even the fastest magnetic discs, but perhaps more important than that, SSDs are the first ever storage devices that offer the same performance for random IO as for sequential IO.

Database Technologies
Nowadays, when people talk about a database, they usually refer to a Relational database, but that has not always been the case. In fact, the term database is defined, by the Merriam Webster English dictionary as, a usually large collection of data organized especially for rapid search and retrieval (as by a computer). The Relational model is one way to organize a large collection of data, but not the only one. In the earlier days of computing, there was only one way to organize data. Due to the limitations of storage on punch cards, punch tape, or magnetic tape, data had to stored as a single continuous stream of data. This model for storing is data is called the Sequential file format or the flat file format. A typical flat file would contain a series of records, which corresponds to individual cards in a deck of punch card. Each record would contain some information and in most cases, the information only made sense in the sequential context of the file. For instance, the record with generic information about a customer, could be followed by one or two address records, each containing an address. The processing program would know which customer the address belongs to by reading the records in order and remembering the last customer read. The structure of flat flies could easily become quite complex. For instance, the customer would not only have one or two addresses, they would hopefully, also place many orders and each order could encompass multiple product ordered records. Records that track the product ordered along with the quantity, the price, and the promise discount, if any. If the same company then also wanted to trick their product catalog and automatically watch stock levels, the second flat file would have to be added for the products. Each product would have one record with generic product information, followed by number of records for deliveries adding to our stock level, and then all the product ordered records for that product. These records are the same collection of records also included in the customers file, but in a different order, and the limitations of Sequential storage left, there's no choice but to accept this duplication and try to write program code to prevent, but also detect report, and repair any inconsistencies. When spinning discs became available and affordable, new storage models were designed to take advantage of the new ability to do Random IO with only a minor performance hit. The first new kid on the block was the Hierarchal database. Conceptually, this was not very different from the flat file model. The same data could still be processed in the same order, but now that order was no longer implemented by storing the records in that specific order, they could be stored anywhere on the disc. The logical order was imposed by so called pointer chains. Each record would store a few extra bytes, the pointer, containing the location on disc of the record that was logically next. This new model only actually developed into a hierarchy because more than one pointer was added to each record, but order record would not only have a pointer to its first product or order record, but it would also have a pointer to the customer that placed the order and the pointer to the next order record for the same customer allowing programs to process all orders for a customer without having to read all the details of the order. Some implementations would even add extra pointers to allow navigation in two directions, but since storage was still very expensive in those days, not everyone was willing to pay for those extra bytes. The advantages of hierarchal databases were clear. The extra navigational possibilities combined with the higher speed of spinning discs enabled businesses to create their first programs to read, act upon, and modify data in real-time, but there were disadvantages as well. The design of the database was very ridged. Modifications often required the database to be dumped to a flat file, dropped, recreated, and then rebuilt from that flat file and then all application programs also had to be modified to support a new design. Plus, data such as the product orders records, still had to be stored multiple times if it was part of multiple hierarchies. The next development the Network database solved that. This database used the same pointer technique as the Hierarchal database, but now even more pointers could be added to a record, so that it could be a member of multiple hierarchies. So each product ordered record would not only have pointers to their owning order in logically next product ordered record within that hierarchy, but it would also have pointers to their owning product, the product being ordered, and the logically next record within that hierarchy. This solved the problem of having to duplicate data, but at the price of added complexity into the design of the database, which of course was just as ridged as a hierarchal database. In 1970, Dr. E. F. Codd published a milestone scientific paper. He proposed a model for data storage that abstracted all storage internals, such as pointers or physical storage order, away from the users of the database. In his proposal, the model for data storage was based on Mathematical Set Theory. This paper laid the foundation for the Storage Model that is the most wide used now, the Relational Database. It took a lot of time before the Relational database actually started to make an impact on the market mainly because the requirements to storage capacity and processing power exceeded what was available in the 1970's, but once the hardware developments caught up, Relational technology took off to become the defector standard for generic data storage. I will discuss the underlying ideas and governing rules of Relational databases in the next section. Development of storage methods did not end with the Relational model. In later years, several other developments have been introduced. For instance, Object databases have been introduced in an effort to bridge the gap between the object-oriented model used in application programs and the Relational model used in a database. While Object databases did not become as successful as Relational database, they have their use in specific applications. A similar statement can be made about XML databases. As storage kept dropping in price and increasing in capacity, XML data representation that stores the definition of the data structure along with the data itself, gained popularity. As a method to exchange data between application programs, XML has become very successful, but it tends to create databases that are fully based on XML, have not been able to find more than niche market. At the time of creating this course, 2012, an exciting new development appears to be around the corner, Big data is probably the most popular buzzword of the year. As with all new developments in the computer industry, definitions are hard to find or rather easy to find and all different, but most definitions agree that Big data is characterize by extremely large amounts of data that is only loosely structured and often distributed. All of those three elements, the large amounts, the lack of a ridged structure, and being distributed are a challenge for Relational databases. Various companies have entered the market with database management systems specialized in handling Big data. We will have to wait for the dust to settle before we will know which companies come out as the winners, and what the long-term impact of Big data on the database market as a whole will be. We will also have to wait and see what, if any, new developments arise from the availability of the storage that has no performance difference between Sequential and Random IO either by using SSD or by loading and keeping an entire database in memory. At the time of writing this course, most effort appears to be put into optimizing Relational databases to profit as much as possible from these hardware possibilities, but who knows what new database technologies we'll see emerging a few years from now.

The Relational Model
The Relational Model is based on mathematics or to be more precise, on Set Theory and First-order predicate logic. That doesn't mean you have to be a mathematician to use a Relational database, but having some knowledge of these underlying concepts can help understand some of the typical behavior of Relational databases. However, none of the mainstream Relational databases are a true implementation of the Relational model. They all feature details the deviate from the original Relational model. In a Relational database, data is stored in one or more tables. Each table is a collection of rows and columns, however, in Relational theory other terms are commonly used. A table is called a relation, a row is called a tuple, and a column is often called an attribute. Data values can be stored at every intersection of a row and a column. Both the rows and the columns within each table are by definition in order. You cannot reference a column as the third column, nor a row s the 21st row. Columns have to be referenced by their name, which therefore has to be unique within each table and rows are referenced by one or more of their attributes that combined uniquely identify a single row. The attributes that are used for this purpose are called the primary key. This is one of the features that set Relational databases off from all other storage methods available at that time. There are still separate storage containers for separate kinds of data like customer order, and product ordered, but the relationships between them like which order was placed by what customer, are now no longer implemented in terms of physical storage. Instead, the connection is made only on a logical basis by adding the primary key of a customer in the storage area for an order. A second important difference is that the rules for Relational databases only describe the behavior of the database, not the implementation. So if in the records still choose to implement the relationship by order and customer using a pointer, as long as this implementation is handled transparently by the Relational engine, none are exposed to the outside world. You don't even need to know anything about how a specific database implements things and you would still be able to use it. Just as you don't need to know anything about car engines in order to drive one, however, just as with car engines, having some knowledge about the internals will help you to get more performance out of the engine. Dr. Codd formulated a set of rules that are now known as Codd's Twelve Rules, even though there are actually 13. There rules govern the behavior of Relational database management systems. I will not cover them all, but I will explain some of them. Rule 1, the Information Rule states that information can only be stored as values in a table not in any other way. Rule 2, the Guaranteed Access Rule states that every value must be accessible using a combination of a table name, a column name, and values of the primary key columns. Rule 3, Systematic Treatment of Null Values defines how missing data should be handled. Data values can be stored at every intersection of a row and a column, but this rule says that every such intersection can also store a special marker to indicate that there is no data value at this intersection. This marker should be independent of the columns data type, and it should be different from any value supported by the data type. So no magic values suggest 0 or -1. The database management system should treat these markers in a systematic way. Rule 5, the Comprehensive Data Sublanguage Rule states that the database management system must support a language for data and schema manipulation that can be used both programmatically and interactively. In all mainstream Relational database management systems this language is SQL the Structured Query Language. Rule 7, High-level Insert, Update, and Delete states that the language must provide the ability to insert, update, or delete data in whole sets at a time. Rules 8 and 9, Physical Data Independence and Logical Data Independence are related to shielding the application program from implementation detials so that implementations choices can be changed without requiring a change to the application code. Rule 10, Integrity Independence states that the database management system must support the ability to define rules that govern data integrity as part of the schema and that the database management system will then ensure that those rules are not violated.

Data Modeling
When you need to create a database to store information for a business, it is important to plan the database first. How many tables do you need? What columns go into each of those tables and how do they relate? Planning that is called database design or Data modeling and the result of that plan is called the Data Model, but models can be made at various abstraction levels, Physical, Logical, and Conceptual. A Conceptual data model or Conceptual schema is mainly intended to asses and formalize all the information that is relevant for the business. A Conceptual data model can be used as a starting point to later implement a database but it doesn't have to. Sometimes old-fashioned pen and paper are far superior to computers, but it can still be important to know and define the information that is available in the organization. A good Conceptual modeling language uses terminology that is as close as possible to the everyday language of the people in the business. This course will not cover Conceptual modeling. A Logical data model or Logical schema, defines how the business information should be stored in a database. In a schema that is designed for a specific database technology, for instance hierarchal, Relational, or object-oriented. If a Conceptual data model has been made, it will usually be used as a starting point for the Logical data model. In the remainder of this course, I will focus on creating a Logical data model for implementation in Relational databases. Finally, the Physical data model or Physical schema defines the exact implementation of the logical data mode, optimized for a specific implementation. The Physical data model should always be derived from the Logical data model. Sometimes the Physical data model will be equal to the Logical data model, but more often it will not. Creating the Physical data model and optimizing it to take advantage of the specific performance criteria for the various available Relational database management systems is beyond the scope of this course.

Summary
In this module, we have seen the Relational model in a historical perspective. We have seen how developments in storage hardware have enabled database technologies to move from flat file Sequential only storage through Hierarchal and Network databases, to the Relational model. We have also take a quick peek in our crystal ball by looking at recent developments that still have to prove they are long term appeal in a market. After that, we have taken a somewhat deeper look at the fundamentals of the Relational model by looking at its mathematical roots and at a set of rules that Dr. E. F. Codd proposed as the test of Relational databases. Finally, we took a quick peek at the differences between Conceptual, Logical, and Physical models and their places in the development cycle.

References
ER Modeling
Outline
Hello, my name Hugo Kornelis and I am presenting this Pluralsight course on Relational Database Design. This is Module 2, which covers Entity Relationship Modeling. In this module, I will start with a quick overview of the advantages and risks of Entity Relationship Modeling. After that, I will dive in and start describing all the elements you can encounter in an Entity Relationship diagram, Entity types, Attributes, Relationships, which come in various kinds, and Subtypes.

ER Modeling (overview)
There are many methods for designing a database and probably even more methods for presenting a graphical representation of the design. Almost all modeling methods use some variation of ER modeling, which is short for Entity Relationship modeling. I will use ER modeling in this course as well and for the graphical representation of the model, I will use the IDEF1x notation, which currently is very popular. In another module, I will describe several other ER models and you will find that most of the differences between the diagrams are superficial. There are good reasons for the popularity of ER modeling. When used in communicating with layman such as the subject matter experts that you're working with to create your model, but also future users of the database application you are designing, or the managers who are responsible for paying your bill, they are not ideal. Not everyone is able to think and communicate at the abstraction layer where database models are, but they are at least a lot easier to understand than some other diagramming techniques. Especially when used with a good tool, automatically hiding details when looking at a helicopter perspective of entire data model and then adding in more details as you zoom in specific subsections can be a great help in getting to understand both the broad picture, as well as the detials you're currently working on. Finally, it is extremely simple to convert a finished ER modeling into a logical database design and it is in fact even possible to start with a database that was already built and to reverse engineer that into an ER diagram that you can then use as a starting point to get to understand the database design, find and repair its flaws, and extend its functionality. However, there are also risks involved when using ER modeling. Because the diagrams are accessible enough that the mangers and end users can mostly understand them, IT professionals have fallen for the trap of having subject matter experts or managers sign off on an ER diagram which is silly, even though the subject matter expert or manager may get a generic understanding of such a diagram, he or she will not understand the details nor be able to verify that everything is entirely accurate. A data modeler asking for a sign off on an ER diagram and then blaming the customer if the finished database does not work as intended is like an architect asking you to sign off on the stability calculations for the house he designed for you, and then blaming you if the house collapses. Another risk with using ER diagrams is none of the available diagramming techniques can capture all the details an analyst may discover during his work and details that are not captured in a model often get lost and have to be discovered again. Finally, because there are so many different diagramming techniques with sometimes only minor differences, people with different backgrounds may think they use the same notation, even if that is not actually the case.

Entity Types
The term Entity, refers to everything be it a physical object, a person, an event, or an abstract notion that is relevant within the context of the information system being designed. So if I am creating a data model for my local Snooker Club, examples of entities that correspond to physical objects will be Snooker table 3, or the 4 bottles of Coke in stock at the bar. Examples of persons will be Mary or Dave both members of the Club. Relevant events will include the 2012 Christmas Tournament or my upcoming match against Mary where I get to defend my second to last position in the C League, and examples of abstract notions will be that same C League or Dave's January 2013 membership fee payment. All the examples I have named are individual entities sometimes also explicitly called entity instances. In data modeling, looking at individual instances may be a good way to ask the subject matter expert a question or to clarify consequences of model choices, but the model itself is concerned with a higher abstraction level. Entity instances, for which the same kind of information is gathered, are grouped together in a new concept the entity type or entity class. An entity type is a single concept to describe all instances of the same type. So, even though Mary, Dave, and I are very different we are all members of the Snooker Club. The Snooker Club tracks birthdates, membership fee payments, and met results for all three of us. So in a data model, we are all instances of the same entity type member. Unfortunately, many people tend to shorten the term Entity type to just Entity, which of course is very confusing because an entity is officially and entity instance. For my Snooker Club I already mentioned the entity type member. Similarly, all Snooker tables on the club are instances of the entity type table. Both the 2012 Christmas Tournament and the 2013 Midsummer Tournament are instances of the entity type tournaments and the A, B, C, and Junior Leagues are all instances of the Entity type League. In IDEF1x, an Entity type is represented by a rectangle, horizontally divided into two areas with the unique entity type name written on top of the rectangle. Some entity types are represented by a rectangle with rounded corners, I will explain the difference between those and the regular rectangle later in this module.

Attributes
Neither Entity types nor Entity occurrences are very interesting on their own, but occurrences become interesting by recording facts about them. So for one of our Snooker Club members, we might record facts like one our members is named Dave. Dave has phone number 555-0163, then, Dave was born on December 12, 1982. These fact contain information about the name, phone number, and birth date of a specific occurrence of the entity type member. Abstracting these facts to the abstraction layer of the model, we call each class a fact an attribute and each attribute is associated to exactly one entity type. For instance, the facts Dave was born on December 12, 1982 and Mary was born April 18, 1948 are of the same class. That effects about the birth date of a specific member of the Club. So in the ER model, those effects and all others of the same class are abstracted to the attribute birth date of the entity type member. In an IDEF1x diagram, attributes are represented by their name, placed in the rectangle that represents the entity type they belong to. Most attribute names are places below dividing line in the entity type symbol. Only the so called key attributes are placed above the dividing line. So, what is a key attribute? This is actually a concept we are all very familiar with. We all have been in situations where we wanted to identify a single occurrence in a group. For instance, can you give me that apple? No, not that one, the one just to the left of it looks tastier. Or, check out that girl over there, not the tall one with the long dark hair, but the on just to the left of her, isn't she great? So you all know how inconvenient it is if you cannot identify a single occurrence in a group of similar entity occurrences. In our Snooker Club, it's no different. We are a small club, so we are able to distinguish our members are name only, which is great. That makes Name the key attribute of members in our club, but a much larger sports club like a football club probably has many members named Dave and if one of them doesn't pay his membership fees, none of the other Dave's should get into trouble over that. So the football club will probably assign each member a unique number and use that to distinguish the members from each other. In their data model Name, will be a regular non-key attribute of the member entity type and member number will be the key attribute. There will not always be a single attribute that identifies unique occurrences of the entity type. In those cases, the combination of two or more attributes has to be used. The membership fee payment entity type is a good example of this. There is not a single membership fee payment occurrence for Dave, he has to pay his membership fee every month. There also is not a single membership fee payment occurrence for January 2013, as all our members have to pay each month, but there is only one membership fee payment for Dave's membership in January 2013. The combination of member name, year, and month identifies a single membership fee payment. So those three attributes are all key attributes for this entity type. The keys that are formed out of more than one attribute are called composite keys. It's also possible for an entity type to have more than one attribute or combination of attributes that could be used as the key. In those cases, one of these so called candidates keys is chosen to be the primary key. All other candidate keys become alternate keys. In IDEF1x, this is shown in a diagram by placing an abbreviation AK in parentheses after the attribute name, because there can be more than one Alternate Key, the letter AK are usually followed by a number. As you can see, the two I used to create the pictures for this course also adds a second number to indicate the order in which attributes appear in a component alternate key. This is actually only relevant only in physical data models, but there is no way to control how alternate keys are displayed by these too.

Relationships
No man is an island and no thing stand on its own. That is true in the real world and because the data model is supposed to model the real world, it is true in a data model as well. Entities relate with each other both at the instance level and at the abstractive class level. At the instance level, it is hard to tell the difference between an attribute and a relationship because they both appear as facts. For instance, Dave plays in the B league. The only difference between facts of this type and the facts we saw before is that in this case, the fact not only gives information about an instance entity type member, but it also relates it to an instance of entity type league. So we can say that the relationship is a class of facts that associate an instance of an entity type with another instance of an entity type. In an IDEF1x diagram, normal relationships are represented using dashed lines connecting the entity types that play a role in the relationship. So for Dave plays in the B league, a dashed line is drawn between the rectangles representing the member entity type and the league entity type. An important property of relationships in an ER model is their cardinality. A cardinality is a restriction on how often an entity occurrence may participate in the collection effects represented by the relationship. Members or our club, cannot compete in more than one league. So, since Dave already plays in the B league no other similar facts about Dave can be valid at the same time and the same applies to all members. No member can appear more than once in the collection of facts that this relationship represents. We call this the maximum cardinality. The maximum cardinality of entity type member in this relationship is 1. However, we can have other similar facts about the B league like for instance Mary plays in the B league. So the entity type league has a maximum cardinality of many in this relationship. Relationships with a maximum cardinality of 1 on one side and many on the other side are the most common relationships. They are called one-to-many relationships. In these relationships the entity type whose instances can participate in many facts is called the parent and the entity type whose instances can only appear once is the child. In IDEF1x, the parent entity type is visualized by placing a dot at the end of the relationship line that connects to the child entity type. You can read this dot s, the entity at the other end of this line, can appear more than once in this relationship. Conversely, the child entity type is marked by not placing at a dot at the parent end. The absence of this dot can then be read as the entity type at the other end of this line cannot appear more than once in this relationship. In addition, all key attributes of the parent entity type are added as normal attributes for the child entity type and they are marked FK for Foreign Key. The word key in this name can be confusing. They are not key attributes in a child entity type, but normal attributes that reference the key of another entity type, the parent. This Foreign Key attribute is how the relationship is implemented. We track who plays in what league by adding the league code in the storage area for a clock member. We cannot do it the other way around. If we wanted to add the member name in the storage area for a league, we would have to reserve room for a potentially unlimited number of members who play in that league. In addition to the maximum cardinality, relationships have a minimum cardinality as well. Our club has a few recreational members. They are free to stop by and use the tables when they fancy a game of Snooker, but they are not scheduled for the league competition matches. So for Jimmy, one of those recreational members, no fact of the type Jimmy plays in leagueâ€¦ exist. That means that the minimum cardinality for entity member in this relationship is 0. Looking at the other side of the relationship line, we see the entity type league and since a league without members playing in them makes no sense, the minimum cardinality for this entity type is 1. Notation of minimum cardinality in IDEF1x is different for parent and child entities. For the child entity type, a minimum cardinality of 0 is symbolized by placing a diamond at the end of the line that connects to the parent. Each instance of the child entity type is associated with 0 or 1 parent occurrences. This is also called an optional relationship. No symbol at the parent ends means that the minimum cardinality for the child is 1, each child occurrence has to be associated with exactly one parent. For the parent entity type a minimum cardinality of 1 is represented by placing a P for positive next to the dot at the child end of the line and a minimum cardinality of 0 is represented by leaving the dot as is. IDEF1x also supports special cardinalities. For instance, if we require all leagues to have exactly 10 members the symbol P can be replaced with the number 10 or if each league has a minimum of 8 and the maximum of 12 members the notation 8-12 would be used. Finally, for really odd requirements like every league has to contain an even number of members, a parenthesized number can be used. This is a reference to a note elsewhere on the page that explains the cardinality rule. Finally, to enable better understanding of diagram, one or two readings of the relationship are added to the line. The reading is the verb phrase of the facts represented by the relationship type. However, IDEF1x requires that the reading must be in parent to child order, although the child to parent reading can optionally be added as well. For the required parent to child reading, you may have to rephrase the facts. In our example case, the possible parent to child reading could be contains. The child to parent reading is of course plays in. Now the meaning end cardinality of a relationship can be reconstructed by following the line, reading appropriate symbols along the way. Each league contains one or more members or going in the other direction, each member plays in 0 or 1 league. One thing to be aware of when working in auto languages is that creating and using relationship readings and this way works great in English because most English sentences are built using the form subject, very phrase, object. Other languages may use different word orders or even insert the object and subject between parts of the verb phrase. In those languages, finding good reading for the relationships can be challenging.

Identifying Relationships
Let's now look at another relationship between entity types, member, and membership fee payment. At first glance, this relationship does not look very different from the one we saw in the previous section. There is a dot at the payment side of the line, so member is the parent in this relationship and payment is the child. that makes sense. The membership fee payment is payment for a single member only, but a member pays many membership fees, one for each month of his membership. There are subtle differences in the minimum cardinalities. No diamond at the parent side means that the minimum cardinality for the child entity type is 1, the relationship is not optional, not a surprise. The membership fee payment that is not for any member wouldn't make sense. The absence of a P at the child side means that the parent has a minimum cardinality of 0, we have members with no membership fee payments. This is for people who are often serving on the board for more than 15 years are awarded with a lifetime honorary membership, but theses difference are minor. What makes this relationship really different from the member plays in league relationship of the previous section is that in the child entity type, membership fee payment, the Foreign Key attribute is not a regular attribute, but one of the entity types key attributes. The key attributes of an entity type are what is used to identify individual occurrences. In this case, the Foreign Key attribute Name, which implements the relationship between payment and member is part of the key of the payment entity type. So, this relationship is required to identify individual payments. For that reason, the relationship is called an identifying relationship and this is shown in a diagram by using a solid line instead of a dashed line to represent the relationship. Most of the cardinality possibilities of a normal relationship apply to an identifying relationship as well and they are diagramed the same. There is only one exception; every entity occurrence must always be identifiable. so key attributes can never be optional, and because the Foreign Key that implements an identifying relationship is also a key attribute of the child entity type, this relationship can never be optional. The minimum cardinality of the child is always equal to 1, so that will never be a diamond at the parent end of the solid line, it represents an identifying relationship. Every entity type that is a child in one or more identifying relationships depends on those relationships to identify its occurrences. Such an entity type is called weak as opposed to a strong entity type that does not have any Foreign Key attributes in its key. In an IDEF1x diagram the weak entity type is drawn with rounded corners and a strong entity type is drawn with normal corners. Note that whether an entity type is strong or weak and whether a relationship is identifying or not depends on what we choose to use as the primary key of an entity type. As you see, all membership payments also have an attribute booking number, which corresponds to the booking number used for the payment in our bookkeeping application. This attribute is an alternate key. We could have chosen to use the booking number as the primary key instead, in which case, the combination of Name, Year, and Month would have been an alternate key. Now, the membership fee payment entity type no longer depends on the member name for identification. The entity type is now strong and the relationship it has with the member entity type is a regular, non-identifying relationship.

Special Relationships (1)
Most relationships are one-to-many relationships between two different entity types, but there are other kind of relationships as well and in this section we will take a look at them. To find good examples, we will have leave my beloved Snooker Club for a while and head to the office instead. As you see, we have entity types Employee and Departments. In the full model, there would be more attributes, but I want to focus on the relationships here. The one-to-many relationship Department employs one or more employees it's completely standard and doesn't need further explanation, however, each Department is also managed by someone, the boss. Adding this relationship immediately reveals something that may not have been obvious. There can be two relationships between the same two entity types. There can even more than two, as many as you need to properly represent reality, but let's look at the cardinalities for this second relationship. Every Department has exactly one manager. So, both the minimum and maximum cardinality are 1 and since managing a department is a fulltime job, no employee should every manage more than 1 department, the maximum cardinality for the employee entity type is not many, but also 1. Not every employee manages a department, so the minimum cardinality on this side of the relationship is 0. The relationship is not one-to-many, but one-to-one because the maximum cardinality on both sides is 1. In IDEF1x, we must decide which entity type will be the parent and which one is the child. In the physical model, the choice will be based on the consequences for storage requirements and performance. In the logical model, either choice is equally valid. If we choose to make departments the parent, the Foreign Key attribute is added to the child entity type, Employee. I cannot use the default attribute name, DeptCode because we would then have two attributes with the same name in single entity type and that is not permitted. So, I have to choose a different attribute name for this Foreign Key The relationship is optional, not every Employee manages a Department. So we get a diamond at the parent end. Every Department is managed by at least one and at most one Employee, in other words, by exactly one Employee. This is represented in the diagram by placing the number 1 next to the dot. So this is what the IDEF1x diagram will look like if we make Department the parent. We can also choose to make Employee the parent. In that case, the Foreign Key is added to the Department entity type. I could have accepted the default attribute name EmployeeName, but I have choose a more descriptive name Manager. Every Department must be managed by an Employee, the relationship is not optional, so there is no diamond at the Employee end. At the Department end, the letter Z for 0 or 1, is added to the dot to show that Employees may manage 0 or 1 department. The IDEF1x model looks like this if the Department is the parent in this relationship.

Special Relationships (2)
Another special kind of relationship, more common than one-to-one, but not as common as one-to-many, is the many-to-many relationship. For an example of this, let's look at the relationship between Employees and Projects. Most Projects in our company are being worked on by a team so the maximum cardinality of Project in this relationship is many. The minimum cardinality is 1 because at least one employee must be working on any project. Many projects take only a few hours per week, so Employees are often working on several projects at once. The maximum cardinality of Employee is many as well. Here, the minimum cardinality is 0, since not all of our staff is assigned to projects. In IDEF1x, a many-to-many relationship is represented by a solid line with dots at both ends. Even though the line is solid, not dashed, it is not an identifying relationship. The minimum cardinality of 0 or 1 is as always represented by omitting or placing the letter P at the opposite end. If needed, special cardinality notations, such as a range or single number, can be used too. The next special case the Recursive relationship is actually not that special at all. It just looks special and it can be confusing, but once you get your head around it, you'll see that it's not that different from other relationships. In our office, we recently had a small survey. Management wanted to find out who the most helpful Employees are by asking everyone to name the employee they considered to be the most helpful. This survey produced effects such as Paul think the most helpful employee is Carrie. This fact does not relate and instance of entity type Employee to an instance of another entity type, it relates an instance of entity type Employee to another instance of the same entity type and it doesn't even have to be another instance. An employee who considers him or herself to be very helpful, but who is not very modest, could fill out their own name in the survey. If we duplicate the Employee entity type in a diagram, we can simply follow the same steps as before to determine parent and child and to find minimum and maximum cardinality. You could only name a single employee the most helpful and not all employees return the survey, so one side has minimum 0 and maximum 1. Some employees were considered helpful by a lot of their peers, others were never mentioned at all, so the other side has minimum 0 and maximum many. So far, this looks just like any other one-to-many relationship that we have seen so far except that it connects two rectangles that both represent the same entity type. Unfortunately, an IDEF1x diagram allows only a single rectangle for each entity type, so we have to draw this relationship in a different way, as a line that connects the rectangle to itself, as shown here. As you see, the symbols at the endpoints all remain the same, as do the readings. A relationship such as this that connects two occurrences of the same entity type is called a Recursive relationship. The example I have shown here is a recursive one-to-many relationship, but recursive relationships can also be many-to-many. For instance, if the survey had asked us to name all the employees you consider to be above average helpful, the relationship would have been many-to-many. A recursive relationship can also be one-to-one, but this is very uncommon. However, a recursive relationship can never be an identifying relationship.

Special Relationships (3)
So far, all relationships we have seen represent classes or effects that associate two entity occurrences, but what about facts that associate three or even more occurrences? Let's assume Pluralsight wants to track what kind of devices are used by their members, to which specific courses. Their data model would of course have entity types for Member, Course, and Device. In their actual data model, they will have more attributes, but I have left them out to focus on this specific relationship. To track who used what to which course, the data model has to support a way to store effects such as Membera@b. c, watched Relational Database Design on a tablet. These facts form an associate between three entity occurrences, a Pluralsight member, a Course, and the Device. So we will need to add a relationship between those three entity types to our diagram. The relationships we have seen before between two entity types are called binary relationships. Relationships such as the one we need now between the three entity types are called Ternary relationships. The number of entity types participating in a relationship is called the arity or order of a relationship. Binary relationships have an arity of 2, Ternary relationship have an arity of 3. Higher arities are possible in theory, but you will hardly ever encounter them in real world data models. In all my years of data modeling, I think I have seen one single relationship with arity 4 and none with a higher arity. Unfortunately, IDEF1x does not support relationships with arity higher than 2. That means that we'll have to use a trick to properly model reality. This trick is no nominalization sometimes also called objectizing. This is actually a linguistic trick used when we give additional information about something that is not an object, but a verb. For instance in I failed to do my shopping in time, failed is a verb, but if I want to explain the cause, I transform it into the noun failure. My failure to do my shopping in time was caused by playing Halo 3. Failure is the nominalization of to fail. Any verb phrase can be nominalized, although the wording may sometimes be awkward. For instance the fact that Membera@b. c watched Relational Database Design on a tablet can be nominalized to the watching by Member@b. c of Relational Database Design on a tablet. With awkward wording such as this, it often pays off to look for a better phrasing. Changing the order of the instances and maybe even using completely different words. For instance, a@b. c's used for a tablet to watch Relational Database Design. You have to make sure that the better wording still represents the exact same meaning of course. You may recall that classes effects are represented by relationships or attributes, but classes of objects are represented by entity types. So by nominalizing the effects of this class, we have transformed it from a relationship into an entity type and because the relationship was impossible to represent in IDEF1x, we had to make this transformation. Occurrences of the new entity type are identified by the combination of a members Email, a CourseName and Device. So the entity type is weak and has identifying relationships to the other three entity types. Finding a good name for these artificial entity types can be a challenge. I have often seem data models where they were simply name by combining the names of the entity types that participated in the transformed relationship, MemberCourseDevice in our example. While this undoubtedly saves time and effort when creating the data model, it can cause a lot of problems ranging from lost time, to application errors during later phases and future maintenance. I suggest always investing the time and effort to come up with a good descriptive name. The reworded noun phrase can often be a good starting point. In my example, I think Usage is not a bad name at all. You will also find that it can be a challenge to find good verb phrases for the identifying relationships of these artificial entity types. Again, for a better understanding the model down the line and in later maintenance I still recommend to come up with the best you can.

Subtypes
All entity occurrences are equal, but some are more equal than others and you may need to capture that in your data model. In my Snooker Club, some members are more than just members. They do extra work for the club either by serving on the board or by being available for volunteer duties. If we want to track extra facts about those members such as position in the board or skills or a volunteer, we can just add those attributes and relationships to the member entity type, but that wouldn't be very clear. Looking at the model, you may think that we want to track skills and board position for all our members, which obviously is not the case. We can solve this by using subtypes also called categories or specialization. A subtype is an entity type whose occurrences are a well-defined subset of the occurrences of another entity type. Every volunteer is a member, but not ever member is a volunteer. We can start with all the occurrences of the member entity type, all Club members. Then filter out only those who are volunteer and form new entity type, volunteer out of this collection. We can already model these with the notations we have learned so far. Volunteer is a weak entity type because it needs to the key attributes of another attribute type, member, to identify its occurrences. The identifying relationship is one to zero or one. However, for this specific scenario, of a one-to-one identifying relationship a special symbol is used in IDEF1x a circle with one or two horizontal bars below it. The circle end connects to the supertype and the bars connect to the subtypes. This is called a subtype relationship or a categorization relationship. It is important to realize that in spite of the special symbol, this is still a one-to-one identifying relationship. Visual clues in the diagram for this are the round corners of the volunteer entity type and the Foreign Key indication in the key attribute name. With the subtype of volunteer in place, the many-to-many relationship with the skill entity type can now move to this subtype. Now, we immediately see that we only track skills for volunteers and as an added bonus, we can now also add a P to the skill end of this relationship to that every volunteer must have at least one skill. One of the differences besides the visual between drawing a normal one-to-one identifying relationship in a subtype is that the subtype symbol allows us to add a discriminator, usually an attribute of the supertype that defines which subtype a specific occurrence is. In our case, it is the member type attribute, member type of Have means that this member is a volunteer. This discriminator is added next to the subtype symbol. We can use the same method to create a subtype for board members. As you can see, the board member entity type connects to the same subtype symbol used for the volunteer subtype. This is possible because two conditions are met. The subtypes are mutually exclusive, we do not want to wear out our members so we do not permit board members to volunteer as well and they used a same discriminator. Of course, after adding the subtype for board members I can move the board position attribute from member to board member. We have already seen two advantages of the special subtype symbol, over using a normal identifying one-to-one relationship. The ability to add the discriminator and the visual clue that the populations of the volunteer and board member entity types are mutually exclusive, but there is a third one, the single horizontal bar under the circle defines this subtype relationship as incomplete. There may be members who are neither a volunteer nor a board member. In Snooker, there is no reason to distinguish males from females, but if there was, the data model would look like this. A second subtype symbol has been added because Club members for instance can be female and volunteer, the subtypes are not mutually exclusive and this second subtype symbol uses a double bar because it is a complete subtype of relationship. Every Club member will be in exactly one of the subtypes, Male or Female. Subtype relationships can have as many or a little subtype as required. We already saw a subtype relationship with one subtype, when we had already defined the volunteer subtype, but had yet to start with the board member, but there can also be three, four, or more subtypes. Subtypes can also be nested. If for whatever odd reason we also want to introduce a specific subtype for pregnant members, it would be a subtype of the female entity type, since only females can be pregnant. However, if we increase the level of Members one step further and create a subtype for pregnant volunteers, then, even though this is logically a subtype of both the female entity type and the volunteer entity type, IDEF1x does not allow us to model it that way. Entity types cannot have more than a single supertype. So in this admittedly weird case, additional footnotes would have be added to the data model to explain the relationship between the various subtypes.

Summary
In this module, you learned how to read and understand entity relationship models. We first saw an overview of the features that make ER models a good tool for Relational Database Design and we looked at the risks involved. After that, we covered all the elements of an ER model and the corresponding notation in an IDEF1x ER diagram. Entity types are collections of similar objects, persons, event, or other things. Entity types can be strong or weak. Entity types have attributes, collections of facts that carry information about occurrences of the entity type. Key attributes are used to distinguish one occurrence from another and non-key attribute give further into about those occurrences. Relationships are also collections of facts, but they associate occurrences of entity types with each other. All relationship are further defined between the minimum and maximum cardinality of each participating entity type. Some relationships are identifying or recursive. You can also encounter relationships with an arity of three or more because they are not support in IDEF1x. We learned how to transform such relationships into artificial entity types by nominalizing the effects represented by the relationship. Finally, we looked at subtype, entity types formed out of the subset of a population of another entity type. This is useful if specific effects need to be recorded for only that subset. Subtypes can be complete meaning that every occurrence of the supertype has to be an occurrence of exactly one of the subtypes or they can be incomplete meaning some that members of the supertype might not be included in any of the subtypes populations. Subtypes within the same subtype relationship are always mutually exclusive, however, an entity type may be the supertype in multiple independent subtype of relationships.

References
Gathering Information
Outline
Hello, my name is Hugo Kornelis and I am presenting this Pluralsight course on Relational Database Design. This is Module 3, where you will learn how to gather the information you need to create the data model. After the previous module, you should now be able to read and write the language of data modeling entity relationship diagrams. But before you can use that knowledge to draw a data model for a specific application, you will need to gather a lot of information. There are two main sources for this information, the people who are involved with the application like managers, subject matter experts, and future end users and documents describing their requirements for this application. In this module, I will first give an overview of the information you need and the sources you can use. After that, I will present the method you can use to scrape all the available information out of any written documentation you have at the start of the project. Then, in the last two sections of this module, I will give you some guidelines on how to interview people.

Introduction
People who design databases sometimes forget what it's all about. They think in entity types and attributes or in tables and columns, but those are all just tools. For the end users, the content of any database is a collection of facts, facts that they need in order to their job. I find that a lot of things we need to do to create a good data model are a lot easier if I occasionally return to the level of those facts. A data model determines two things, what types of facts a database can store and what facts or combination of facts are not permitted because they cannot be correct. Throughout this course, I will constantly use either individual facts or fact types, classes of similar facts, to demonstrate the process of data modeling. In real work, you will often be able to take bigger steps skipping over the fact types and going directly to entity types and attributes. That's okay, once you know how to tie your laces, you no longer have to make the bunny run around the tree and once you know how to create a data model, you no longer have to resort to the fact type level for everything. Just don't forget that the facts are the basis and that you can always resort to them if you have a tricky situation where you don't immediately see the right data model. Finding the right set of facts to store is very important. If the database is unable to store information that the end users need to do their work, the application is worthless. End users will either not use it or start using creative ways to work around the limitation, like storing a Social Security Number for a credit status in the fourth address line because most address only use three lines anyway, and that can go horribly wrong if the company updates its application software in preparation for the new international business model, but if the database wants to store facts that are not relevant, the application can also be a pain to use. A lot of optional fields on a data entry screen that are not really relevant for the users job, are an open invitation for more creative uses for them and don't even get me started on the nuisance of numerous mandatory entry fields, but constraints are just as important. Constraints are rules that ensure that facts or combination of facts that could never be correct are not permitted. Constraints ensure that the information stored in the container for birthdates of the member of my Snooker Club are actual valid dates, within a range that is reasonable for Snooker Club members, so no dates from the future or from the 18th Century. Constraints also ensure that we don't record two or more different birth dates for the same member and that the 30 year old member doesn't play in the Junior League. Data modeling starts by investigating the types facts that are required for the business. One obvious source for this investigation is the description of application to be designed, often called the Mission Statement, but this document usually is not complete and exact description. The Mission statement is intended to give a general overview of the application, at a detail level that is just enough for making estimates of the development effort. Descriptions of the data model in the Mission statement will be informal and vague, usually incomplete and sometimes even inconsistent. In the next section, I will show a technique that can be a great aid in clarifying all impreciseness and finding all inconsistencies. A second source of information are interviews. Most data modelers will schedule interviews with management and with designated subject matter experts either to get a generic overview of the application to be built or to clarify questions that come from processing earlier interviews or the Mission Statement. The benefit of interviewing management and subject matter experts is that these people are relatively comfortable discussing data design at the abstraction layer of the data model. The downside is that they are often too far away from the everyday reality to really know exactly what is happening on the work floor. For managers, that's a given. It is their job to oversee, and delegate, and leave the details to their staff. Subject matter experts usually know more, but still not everything. For example, when interviewing people for an insurance company, I found that the managers knew that the clients had to fill out a standard form when they wanted to file a claim and had a rough idea of the information on those forms. The subject matter experts knew exactly which fields were on the form and how customers were supposed to fill them out, after all, they were the people who had designed the form, but only the people in the data entry and claim processing departments knew how customers actually filled out those forms and what hoops they sometimes had to jump through to get the claim form entered and processed without having to get back to the customer at least three times for every claim. So that brings us to the third source of information, maybe the most important one, the actual end users of the application. The problem with interviewing these people is that they are not always able to think and discuss at the abstraction layer of a data model. Interviewing these people will only result in useful answers if we can find a way to converse with them at a level and in a language they are comfortable with. A great way to do this is to use concrete examples using notation forms they use in their everyday work and I have even found that most subject matter experts and managers benefit from using that communication style as well. The second step in data modeling wants the fact types to be stored are known, is to investigate what constraint there are on the population of those fact types. For this part, the same sources should again be consulted, Mission statement, interviews with managers and subject matter experts, and interviews with actual end users. In practice, you will often guard our information about fact types and constraints in a single interview or in a single pass over the mission statement. The third step in the process is actually creating the data model. This step included identifying functional dependencies and normalizing the data model, skills that I will discuss in the next module. In theory, no additional information is needed during this step, you can do this completely based on the information guarded in the previous steps. In practice, you will often find that you missed some information in a previous steps and need to ask follow up questions or you may decided to already start on modeling or normalizing while still gathering information. That is okay, as long as you make sure not to cut any corners by skipping steps and making unverified and undocumented assumptions.

Analyzing a Mission Statement
When you have to create a data model for a new application, there often already is a document that outlines how that new application will look. Those documents can have different names. In this course, I will use the name Mission statement. Obviously when you have such a document, you should lift as much information as you can from that document instead of boarding interview partners with questions you could have answered from the Mission statement. The Mission statement has a different target audience and a different goal from the data model and that results in differences in the description. A data model is a formal description of the types of facts stored and the constraints own those facts. A data design is complete or at least as complete as the chosen model method allows and consistent. Mission statements on the other hand, are always informal description, usually vague and incomplete and sometimes even inconsistent. They also describe much more than just the data model. They also include descriptions of the process the application should support, like invoicing, the human processes around the application, like checking that a supplied ID is valid, and justifications for the choice to create the application. It can sometimes be hard to extract the required information from a Mission statement. A very useful technique that always helps me analyze a Mission statement is to use highlighters in various colors to assign each sentence or even part of a sentence in the Mission statement to one of several categories. For data modeling the relevant categories are Fact types, Constraints, concrete Examples, and Other. If my role in a project involves more than only the data model, I will use some extra categories such as human process, automated process, project management, but those are outside the scope of this course. As an aid in understanding the Mission statements, I will often create the concrete examples to illustrate the text in the Mission statement. When needed, I ask a subject matter expert to check if those examples show a correct interpretation of the text. One of the most important things to get right in these examples, and hence also one of the most important things to have the subject matter expert verify is that you use the right identification for individual occurrences of entity types. Shown here is a small part of a fictional Mission Statement from my Snooker Club for an application to track scores and compute results during tournaments. I always start by reading the entire document from start to finish. This prevents me from making assumptions or writing down questions that are clarified later in the document. After the first reading, I take out my highlighters and start the second reading. The first relevant bit of information is a bit scattered through the text. During tournaments, members play matches against opponents. This describes a fact type. To clarify this part of the Mission Statement I would write down a few concrete examples such as during the 2012 Christmas tournament, Dave plays a match against Mary and then ask the subject matter expert not only if this example illustrates the highlighted part of the Mission Statement, but also if Dave and Mary are good examples of how to identify a single member in his world. I don't need to ask this for the 2012 Christmas tournament, since I took this example straight from the Mission Statement. The second bit of text is an example. If it were an example fact, it would also be a fact type, but in this case, it is just an example occurrence of the tournament entity type. The only thing I can and will do with this is to use this for some of the example facts I create to illustrate the rest of the text. Competing or actually competing member, describes the next type of fact. Apparently, not all members compete in each tournament so we need to track facts such as Dave competes in a 2012 Christmas tournament and Mary does not compete in a 2013 Midsummer tournament. The remaining three words in this sentence describe three constraints, each tells me that each competing member must be assigned to matches, three constrains the number of matches each player is assigned to, and different clarifies that competing member cannot play the same opponent twice in a tournament. For clarification, I would create an example that includes a member who wants to compete, but is not assigned any matches, an example where members are assigned two or four matches, and an example where a member plays the same opponent twice. Then comes a sentence that is not relevant for the data model. The scope of the Mission Statement is a database to track result, in that scope, the duration of the match is not important. The start of the next sentence is also irrelevant for the data model. For the data model, it is not important if the results are recorded after the match or three years later, nor whether they are entered directly on a handheld device, written on a piece of paper, or even carved in a clay tablet. How many frames each player has won describes a fact type that I can illustrate with examples such as, the match between Dave and Hugo in the Christmas 2013 tournament ended with the result 3-nil, and apparently, we then also need facts as, during the match between Dave and Hugo in the Christmas 2012 tournament, Dave had the highest break of 28 points and during the match between Dave and Hugo in the Christmas 2012 tournament, Hugo had a highest break of 9 points. Finally, a very subtle hint to a constraint is the absence of the letter S here, the singular form is used. This suggests that only the single highest break for each player is recorded, not all high breaks above a certain threshold. I would illustrate this by creating a fake score form that holds two different highest breaks for one of the players and I would definitely run this past the subject matter expert to verify that I am not reading too much into this.

Interviews
Interviews are an essential part of data modeling. Unless you are creating a data model for purely personal use with yourself as the only user, you will have to talk to people in order to find what information a database will need to store and what constraints you should define to prevent impossible data from being entered. Even if you have a great Mission Statement to work off, you will still have to interview people to verify your understanding fill in the blank spots. The soft skills you need to conduct an interview are out of scope for this course, but I provide you with some guidelines and give you some techniques that can help you with those interviews. Always adapt your level and language to the person you're interviewing. Level in this case, refers to the abstraction level. Managers and subject matter experts will often be able to understand the level of abstraction in a data model, though that is not necessarily always the case. End users may struggle with that level and be more comfortable with discussing concrete individual examples, but there will be exceptions here as well. Whoever you are talking to, always keep a close eye on sign that they are struggling with the abstraction level of the conversation and shift gears when needed. The concrete level is, in my experience, the safest choice as everyone is able to understand examples. When you find you do have to shift the abstraction level, do this in a respectful way, don't say or do anything that might give the impression that the person you are talking to is unable to meet some requirement that they fall short. Every person as their own skills, you, the data modeler, a supposed to posses the skills to work at the abstraction layer of the data model and to bridge the gap and talk with everyone. So if the interview starts off on the wrong foot and you have to shift gears, you, if anyone, are the one to blame. Adapting your language means that you should make an effort to use the jargon and notation forms that the interviewed person is used to using. This definitely means that you should not use terms such as entity type, relationship, attribute, constraint and so on. Those terms are the jargon of the data modeler, not of the person you are interviewing. You should be the one to adapt to his or her job and not the other way around, just as the architects designing your Holiday home will explain the structural satiability of the balcony in terms of maximum occupancy not in whatever units they use for their calculations. Make sure to use a language that ensures that the interviewee understands you, then use your own skills to ensure that you understand the interviewee. If you were given a Mission Statement, you can use it to help the interview move along. Take out the Mission Statement and the examples you created to illustrate the fact types and constraints you found in it, then as your conversation partner if they make sense and ask him or her to help you correct your examples if they don't. You can also ask the interviewee to check if parts of the Mission Statement you think are not relevant for the data model, are indeed not relevant for it. Another great help in interviews is to use concrete examples, either real or realistic. This works especially well when interviewing end users, but can also be use with subject matter experts and sometimes even with management. Ask the interviewee to bring some examples of the data they have to work with to the interview. If privacy guidelines restrict them from showing you actual data, ask them to make up realistic examples with fake data. When I worked for an insurance company, some of the best insights in the data model came from going over actual policy application forms and actual claim forms. When using concrete examples in an interview, always keep in mind that those examples, no matter how they look, are all representation of a collection of facts, just as the data in a database. When I interviewed a board member responsible for running the competition, tracking scores and calculating the rankings in the various leagues, she might bring a score form, such as this one, to the interview. In this case, I know the facts represented in this example because I am a Club member myself, but what if I was hired by the Snooker Club and had no idea whatsoever about this fine sport? Some people say that this is why data modelers should work only in areas, where they already know the business inside out, I disagree. Sure, it can help, but it can also hinder. If I make a data model for another Snooker Club, my knowledge of the game can help, but it can also cause me to jump to conclusions, thinking that every Snooker Club handles thing the same way as our Club and you do not need to have knowledge of Snooker to find out what facts are represented on this score form. That's what you are holding the interview for. You can simply as the interviewee to read you all the facts the form represents except you have to ask this in a way the interviewee will understand. I usually do this by asking him or her to pretend they get a phone call from a coworker who needs this information now and they have no email, fax, or Smartphone, only an old fashioned speech only telephone. This usually works very well. Sometimes the interviewee will start by reading facts about the layout of the form such as the League and the Data of the match are at the top of the form. For the purpose of designing the data model for the competition results, those facts are not relevant, but with some nudging you will eventually be read the actual information contained on the form. On October 3, 2012, Katie and Jim played a match in league C. This match ended with 2 frames won by Katie and 1 frame won by Jim. To verify that all information in the example is read, I again use my markers to color corresponding elements and ask for confirmation. If parts of the example are not colored, they are either irrelevant for the application I am designing the data model for or the reading is still incomplete. In the case of this example, the highest breaks and the frame scores are relevant, but I will leave them out for shortness sake. To transform this first reading into pure facts, I need to do two things, make sure each sentence stands on its own, and break up sentences into the smallest possible parts. Sentences cannot stand on their own when their referred back to the previous sentence. So in this case, This match in the second sentence needs to be replaced with an independent identification of the specific match. Sometimes, it is obvious how to make such a reference independent, but other times you need the help of the person you are interviewing, unlike in this case. If our club has only a single table and the opening hours are too short to play two matches, the matched played on October 3, 2012 will be a fine way to identify this match, but our club is in a lucky position of 7 Snooker Tables so this will not work for us. In a very large club with maybe 14 members playing in each league, everyone would play each other member only once per season, so the match between Katie and Jim would work, but again, not for us. We have only 12 players per league, so we play each competitor 3 times per season. In our case, the only correct identification of this match would be, The match played between Katie and Jim on October 3, 2012. Once every sentence can stand alone, I need to break them up into the smallest parts possible, but make sure they don't lose their meaning. There are no hard rules for this. Experience helps a lot. Luckily, errors made at this stage won't result in incorrect data models, though the result in extra work later. The word and is often a sign the sentence can be split. So in this case, the match result is not 1, but 2 facts, but note that that word and does not always result in splitting the sentence. The word and here, by Katie and Jim, does not mean we can split the sentence. Maybe, Katie was in the Club the whole day and played a match again Lucas as well. If you would split the sentences to read for instance, The match played by Katie on October 3, 2012, ended with with 2 frames one by Katie, we would not know if this was the match against Jim or against Lucas, so we would lose information.

Counter Examples
Another technique I often use in interviews is to use counter examples, modified versions of examples that have been used before. These can be very useful to verify whether specific constraints exists or not. During my analysis, I will eventually need to know if the highest break for each player in a competition match is mandatory and whether there can be multiple high breaks. If I am lucky, I already found these when going over the Mission Statement, but I don't always have a Mission Statement and even if I do have one, it is usually incomplete. So chances are that this is a detail not covered in this document. I'll need to interview someone when I want to find out if the highest break is optional or mandatory and whether there can be multiple high breaks, but asking it in these words is an open invitation to misunderstandings. My conversation partner will probably say that there definitely can be multiple high breaks, thinking of the two high breaks on any score form or even thinking of the complete stack of all score forms used in a tournament. Or they will say that there is only a single highest break, thinking of the extra price for the highest break of the entire tournament, but none of these replies would answer the question I thought I was asking, so this can introduce errors in the data model. To prevent that, I avoid the abstract communication level for these questions as well, using concrete individual examples instead and I presented those examples to the person I am interviewing in the form they are most comfortable. In this case, that would be a score form. I take an existing score form, modify it so that it reflects a situation I want to know about, then use this modified example in the interview to check if the score form such as this would be fact. So, to check if a highest break is optional, I simply modify the score form to not have a highest break for one the players. Then, ask the person I am interviewing if that would be allowed. This always works very well. I have even experienced people getting truly upset about the counter examples they were confronted with. Similarly, I would modify a score form to carry two highest breaks for a player to check if that is or is not allowed. When the interviewed person tells you that a counter example is not valid, always ask, why it is not valid. Sometimes you have accidently violated the completely different constraint then the one you think are testing. For instance, this example would not be valid even if the club does want to record multiple high breaks during the match, for the simple reason that the number used 180 is higher than the theoretic maximum break score. If this happens, make a note for future use, then modify your example so that you can use it to verify the constraint you are investigating, while no longer violating the constraint you accidently stumbled upon.

Summary
In this module, we focused on how to guard our information we need for creating the data model. A data model describes what kind of facts the database will allow the user to store, but it also describes constraints, rules that determine that specific individual facts or combination of facts are not allowed because they cannot possible be true. Creating a data model, requires us to know which fact types the end user will need to store. We must take care that all relevant information can be stored and we must avoid crafting a data model that allows or even requires users to store all kinds of information that is not relevant for the application we are designing. Creating a data model also requires us to know the rules that state that specific combinations of facts are not allowed to constraints. For finding the fact types and the constraints, we can use two sources, the document describing the application to be designed and built, I used the term Mission Statement for this, and interviews with managers, subject matter experts, and end users. You can lift information from a Mission Statement by assigning the contents to different categories and then creating concrete examples to illustrate the words in the Mission Statement. These examples help you get a deeper understanding and they can also be used to verify your interpretation. When interviewing, concrete examples are a great help. Reading information from actual or fake examples from documents used on the work floor, help you find the fact to store. Modifying examples is a great way to verify if specific constraints exist or not. If you happen to be familiar with the subject matter, it can save a lot time because you don't have to ask as much in interviews, but beware that this can also be a danger, if you are unaware of subtle differences between your current and your previous employer, you might build your data model on assumptions that are incorrect for your current employer.

Creating The Initial ER Model
Outline
Hello. My name is Hugo Kornelis and I am presenting this Pluralsight course on Relational Database Design. This is module 4, in which you will learn how to create a first version of the Entity Relationship Model. With all the information you gathered using the techniques from the previous module, you are now finally ready to start creating an actual entity relationship diagram. This is done in a series of steps. You first abstract all similar facts into fact types using a template with a placeholders to describe the form of these facts. You also investigate whether duplicates are allowed for each placeholder in the collection of all occurrences of a fact type. You can then use this collection of fact types to find what entity types you will need in a diagram. Each of those entity types will have an identifying fact type, which may or may not already be in your collection. To create the IDEF1x entity relationship diagram, you first put the entity types on the diagram. Once they are there, you can add their attributes and their relationships. Some fact types, are not represented as a normal entity type attribute or relationship, to represent them in IDEF1x, you need to introduce extra artificial entity types.

Generalizing Fact Types
In the previous module, you learned how to use the various information sources available to you such as the Mission Statement and the people you interview to find the information you need. You should now know exactly what facts need to be stored and you should also have a lot of information about constraints that tell you what combination of facts are invalid. Ideally, you also have examples to illustrate how facts are represented by the end users and some examples of invalid fact combinations to illustrate the constraints. Before we can use this information to draw a first draft of the data model, we must move away from the level of individual concrete facts to the more abstract level of collections of similar facts called fact types. So instead of discussing the individual facts, Dave was born on December 12, 1982 and Mary was born on April 18, 1948, we recognize that these are two occurrences of a class of similar facts, facts that give information about a member and a date, and we can describe all members of this class effects with a template, Member was born on Date. In this template, Member and Date are placeholders that indicates that in instances of this fact type, occurrences from the collection of all members and from the collection of all Dates should be inserted at these places. Finding the placeholders to use for the fact type templates isn't always that obvious. For example, for facts such as the match played between Katie and Jim on October 3, 2012 ended with 2 frames won by the first player, the template would not be the match played between Player and Player and date ended with Number of frames won by the first player. This is incorrect because the fact doesn't really give information about Katie, Jim, or October 3, 2012, it carries information about a specific match and it uses Katie, Jim, and October 3, 2012 to identify which match. So the correct template would be Match ended with Number of frames won by the first player. But do not conclude from this example that templates will always have two place holders for that is not the case. If we have facts like Dave smokes and Katie smokes, the template would simply be Member smokes, with only one placeholder and if we leave the Snooker Club for a minute and think back on one of the example I gave in the previous module for a fictional Pluralsight data model, we saw facts such as Pluralsight member a@b. c watched the course Relational Database Design on a tablet. These will be generalized to the template, Pluralsight member watched course on device, which has three placeholders. For all fact types with two placeholders, we need to know whether duplicates are allowed. You probably already know this from most fact types, based on the constraints that you already found, but if not, you many need to analyze the Mission Statement again or schedule another interview with a subject matter expert. For Member was born on Date, there should never be two occurrences of this fact type for Dave or for any another member. Members can only have a single date of birth. However, it is possible that besides Dave, one or more other members are also born on December 12, 1982. So a given date can occur multiple times in the population of this fact type, but any given member can occur only once. Another example of this situation is Table is used to play Match. Because a match is always played on a single table, but once that match is over, other matches can then played on the same table. In Member has signed up for Tournament, we obviously want there to be a lot of occurrences of this fact type for any given tournament and we also hope that most of our members will sign up for more than just a single tournament. So in this case, there can be duplicates in both the collection of Members and the collection of tournaments. The last possibility is when no duplicates are allowed in either population, as for instance in fact type Member has Email address. Our members can register only a single email address with the club and we do not allow multiple members to share the same email address. So neither a specific member nor a specific email address can appear in more than one of these facts.

Finding Entity Types
All the fact types we have found will be represented in the data model. In Module 2, we have seen that entity relationship model support two ways to represent fact types, as an attribute of an entity type or as a relationship between entity types. Whatever the representation, they always connect to entity types. We cannot diagram fact types without first having the entity types in the diagram. So we now need to find what entity types to add. For that, we look at our collection of fact types, after all, fact instances give information about entity occurrences, so those occurrences are bound to be used somewhere in the facts. Facts that are formed from the template Member was born on Date, use occurrences from two collections, Members and Dates. In this case, the collection of Members is an entity type and the collection of Dates is not. But how do you know that? That is actually one of the major problems you encounter when using entity relationship modeling. In many cases it will be obvious which collections are entity types, but not always and even when it seems to obvious to you, it may not be as obvious for someone else and it might even turn out to be wrong, but luckily, there are indications. One such indication is when occurrences from a collection participate in many different fact types. For our Snooker Club, we have a lot of fact types that include a member such as Member was born on Date, Member listed Address, Member plays in League, Member has signed up for Tournament, Member has Email address, and so forth. This is a good indication that Member should probably be an entity type, but beware, because this is not always the case. For instance, there are also a lot of fact types that include a date such as again, Member was born on Date, but also, Tournament is played on Date, Membership fee payment is due on Date, and Member reserved a table for solo practice on Date, and yet Date is not an entity type in a data model for my Snooker Club. An even better indication is given by fact types with two placeholders that allow duplicates in one and do not allow duplicates in the other placeholder. Examples we have seen of this kind of fact type are Member was born on Date and Table is used to play Match. For each of these fact types, the duplication rules allow us to conclude that the placeholder that does not allow duplicates represents an entity type. We already suspected Member to be an entity type because of the sheer number of fact types it participates in, this is now confirmed and we now also know that Match represents an entity type. We cannot conclude anything for the other placeholders though. Based on only these two fact types, we cannot say that Date and Table are entity types nor that they are not, we have to check all of fact types that. Fact types with two placeholders that do not allow duplicates for either placeholder such as Member has Email address, give us less information. For those fact types, we do know that at least one of the two placeholders has to reference an entity type, but we do not know which one and the other placeholder may or may not reference an entity type as well. In this case, since we already know that member is an entity type, we learn exactly nothing. More or less the same thing goes for fact types with two placeholders that do allow duplicates in the population of both placeholders such as Member has signed up for Tournament. Either both represent an entity type or one of them does and the other doesn't, but we have no way to tell which one does and in this case, there is even a special case where neither of the two placeholders references an entity type. I will discuss this special case later in this module. Word order of the sentences that form the facts is not an indication. You may find that at, least in English, most facts will be worded with the entity type at the start, but this is not rule you can always rely on. Dave was born on December 12, 1982 or December 12, 1982 is Dave's date of birth are two different ways to phrase the exact same fact and hence Member was born on Date or Date is Members date of birth are two different templates for the same fact type. Choosing a different template does not magically change Date into an entity type. After checking all fact types, using the guidelines and rules given, you should have a list of entity types. You may have to use gut feeling a few times, but don't worry too much about getting it wrong, we will have a better check later in the procedure.

Identifying Fact Types
Before actually drawing the first draft of our data model, I need to discuss a special class of fact types that I have not mentioned before, but that you need to be aware of. I call these the identifying fact types because occurrences of such a fact type identify instances of an entity type. All entity types always have an identifying fact type. However, this fact type is usually never explicitly used, it is only implied. Let's look at an example. The fact type Member was born on Date is not an identifying fact type, but it does imply one. If I tell you that Dave was born on December 12, 1982 I obviously tell you explicitly something about when Dave was born, but in doing so, I also implicitly disclose that someone name Dave exists and since I am telling you this within the context of the administration for our Snooker Club, it's safe to assume that Dave is a Club member. Likewise, that single fact about Dave's date of birth also implies that December 12, 1982 is a valid date. You probably already knew that, but that doesn't change the fact that the existence is also implied by naming this as Dave's birth date. So the single fact, Dave was born on December 12, 1982 actually implies two other facts as well, Dave is a member of our Snooker Club and December 12, 1982 is valid date. Those facts are identifying facts because they identify instances of the member collection and the date collection and because this implied identification of instances work so well, we usually don't bother to explicitly state these facts. However, sometimes identifying facts are explicitly stated. For instance, because the subject matter expert who reads the examples to us, chooses to do so, because even when it's not required it's still perfectly legal to do so and the domain expert may have felt the need to stress the existence of the occurrence before going on to give additional information and in some cases, explicitly stating the identifying fact is even required because there may instances of an entity type for which nothing other then their existence is known at this time. Because I identifying facts are represented differently in the entity relationship model, you need to learn to recognize them. Fact types that have a single placeholder in the templates are often identifying, but not always. You have to check what the facts mean. For example, Name is a member of our Snooker Club is the identifying fact type for the member entity type, but Member smokes is not, it is a fact that represents additional information about members that we can use the schedule smoking members to play on tables in the smoking area. Unless the data model requires an entity type, smoking member, then Member smokes is an identifying fact type for that entity type. Fact types with two placeholders in their templates can only be identifying fact types if duplicate entries are allowed for both placeholders. The placeholders can refer to an entity type and an attribute or they can both refer to an entity type. Very rarely, both placeholders might refer to an attribute. This is the special situation mentioned in the previous section. However, keep in mind that not all fact types with two placeholders that both allow duplicates are identifying. An identifying fact type must refer to exactly the same set of things that members of the identified entity type reference and you must also check the meaning of the facts. So, for instance, if we have a fact type like Membership fee term was paid on Date, which comes from facts such as Dave's January 2013 membership fee term was paid on January 10, 2013. The identifying fact type for the membership fee term entity type, must include references to a specific member, like Dave, to a specific year and month, January 2013 and no other references, but even though Dave requests not to be scheduled for competition matches during January 2013, satisfies that criteria, it is not the identifying fact type for this entity type because the words used in that fact indicate something quite different from the existence of a membership fee term for Dave in January 2013. If the wording of the fact type includes terms you are not familiar with, you should check this with a subject matter expert. All fact types with three or more placeholders can also be identifying fact types, but again, not all of them are. Like before, there has to be a match between the references and the facts and those in the entity occurrences, and the meaning of the fact type has to match. At this point, you can also check that for all non-identifying fact types, the templates should include at least one placeholder that references an entity type. If not, you need to add extra entity types. The identifying fact type for most entity types will not be included in your collection of fact types. You may want to add at this time, supplying your own reading. This is not really required, you can skip this, but in that case, you do have to spend a minute to think about their structure. Let's use an example again. The first frame of the match played between Katie and Jim on October 3, 2012 ended with 37 points for the second player. The template for facts of this type is Frame ended with Number of points for the second player, with frame a referencing an entity type for which we don't have an explicit identifying fact type. If we add one, it would probably read something like, A first frame was played in the match between Katie and Jim on October 3, 2012. The template for that would not be a Frame number was played in a match between Member and Member on Date, but A Frame number was played in Match. This implies that frames are not identified by just about any collection of two players, a Date and a Frame number, they are identified by a Match and a Frame number. I could add this fact type to my collection or I could choose not to, but in either case, I do need to be aware of its structure so that I can create my data model accordingly. That way, I will end up with a model that ensures that whenever a frame is entered in the database, the players and date used to identify it must refer to an actually match.

Entity Types
It is now time to start drawing the diagram. This is actually very simple once you have required the information. If possible, use a computer program that allows you to make drawings and to move elements around without breaking connections. You will often find yourself rearranging entity types in a diagram to minimize the number of crossing relationship lines. Specialized data modeling programs are of course ideal, there are other programs that will do nicely as well. If you have to use pencil and paper you probably best start with making a few very rough and simple first drafts before deciding on the layout for the final version to minimize the number of times you have to start over. You start by checking the list of entity types. The identifying fact type reveals, which ones are strong and which ones are weak. If there are one or more placeholders referencing other entity types, it's weak, otherwise it is strong. For the IDEF1x graphical representation, we start with the strong entity type. For every one of them, add the entity type symbol to the diagram and add a key attribute for every placeholder in the identifying fact type. So for the entity type Member, we would get Name as the key attribute. Most strong entity types have only a single key attribute, but there are exceptions. In this course, I have simplified the Snooker Club example. In reality, I wouldn't use only a FirstName for members of our club, even though we are not very big, we already have two members named Renee and two name Rob. So, a much more realistic example would have used a combination of FirstName and LastName to identify our club members. Next up, are the weak entity types. For each of them, add an entity type symbol to the diagram using the symbol for weak entity types. To find the key attributes, check the identifying fact type. For a placeholder that references another entity type add all of the key attributes of that other entity type, as key attributes for the weak entity type, mark them as Foreign Keys, and add the identifying of relationship to the diagram. In an identifying of relationship, the minimum and maximum cardinality of the child side are always equal to one, but the minimum and maximum cardinality of the parent side have yet to be determined. You can do that now or you mark the relationship symbol as incomplete and revisit this later. Placeholders that don't reference another entity type simply become normal key attributes. A special case of a weak entity types are subtypes. You can easily recognize them because they have an identifying fact type with only a single placeholder referencing another entity type, for instance, Member is a volunteer. Or you can recognize them by the cardinalities of the identifying fact type. For a subtype of relationship, this is always one to zero or one, whereas normal identifying of relationships are one-to-many. In IDEF1x, subtypes are represented using the standard symbol for weak entity types, but a special symbol is used for the identifying of relationship. Whenever a single entity type has more than one subtype, you will also have to think about their mutual relationship. One subtype may be a subtype of another subtype or they can be mutually exclusive in the subtype of relationship that is either complete of incomplete, or they can simply be independent. When the information you found so far is insufficient to determine the correct relationships between the various subtypes, you will have to go and find additional information.

Attributes
Once all the entity types are in the diagram, we can start adding representations for the fact types we found. Remember that there are various ways to represent fact types in an entity relationship diagram. So not all fact types are treated the same here. Identifying fact types are already in the diagram they are represented by the entity type the identify along with its key attributes and identifying of relationship if any. So you don't have to do anything for them. The first group of fact types you do need to add to the diagram are the fact types with two placeholders, one referencing an entity type and not allowing duplicate entries, the other not referencing an entity type. These will be represented as non-key attributes of the referenced entity type. The name of the attribute will either be the name of the placeholder or a combination of that name and part of the verb from the template to clarify the meaning of the attribute. Non-key attributes can be either mandatory meaning that for every instance of the entity type there must be an occurrence of this fact type, or optional meaning that there may be entity instances for which no occurrences of this fact type exists. In our Snooker Club, Date of birth is an optional member entity type because some of our members prefer not to disclose their age, but Email address is mandatory. All members must provide an email address because that's how we send the important information to our members. Unfortunately, some ER methods use a graphical representation that does not support a way to show whether a non-key attribute is optional or mandatory. IDEF1x is one of those methods. That doesn't mean that you don't need to know if non-key attributes are optional or mandatory if you use IDF1x, it is important, you will need this information later when converting the ER model to a relational database. So you should find out which non-key attributes are mandatory and which are optional. You can then represent this in the graphical representation by using your own Homebrew notation standards or you can add a separate document to the model for this information. Either way, you must make sure that within your organization, all data modelers use the same method for this and that everyone who ever reads the data model is aware of this standard. Don't forget to ensure that all new hires are given this information as well. For most attributes, you will already know if they are optional or mandatory. You may have to steal this information from the Mission Statement or you may have seen examples of documents where the corresponding factor was missing, which if course immediately implies that the attribute is optional, but if you don't have this information yet, you will have to ask someone. A good way to do that is to take a copy of a document where that fact is represented, remove it, then ask if this would be a valid document. So for my Snooker Club, I would take a copy of a membership card from the old cardbooks administration, remove the Birthdate and the Email address, then ask the subject matter expert if this would be valid. Her response was that it was not valid because the Email address is required for all members. After adding back the email address, she said it was fine, even though the Birthdate was still missing. Fact types with only a single placeholder that are not identifying fact types, will also be represented as attributes of the entity type referenced in their placeholder. These attributes will eventually be implemented using a Boolean data type or order truth-value because there is no actual data to be stored. Just a token value to signal that the corresponding fact is or is not present for the entity instance. This is often indicated in the data model by pre-pending that is prefix to the attribute name. So the fact type Member smokes will be represented as the attribute is_smoker. In the most pure form, this attribute is an optional attribute with True is the only allowed value. So the value True for Dave means that we know Dave is a smoker and the absence of a value for this attribute for Angie means that we have no information on whether or not she smokes, but many people find it easier to use a mandatory attribute that allows true and false. If you do so, be aware of a subtle change. Most people will interpret the value false for Angie as meaning we know that she is not a smoker, which is not the same as not knowing if she is a smoker or not.

Relationships
Fact types with two placeholders get both reference and entity type will be represented in the diagram as relationships. For each of these fact types, draw a relationship line between the entity type symbols that represent the entity types in the placeholders. In order to complete the relationship, we need to add various elements starting with the maximum cardinalities. They are easily found. We already checked if entity instances can occur in multiple fact occurrences, when we were deciding which placeholders are entity types. We can now use this information again. If duplicates are allowed for both placeholders the relationship is a many-to-many relationship. If duplicates are allowed for one placeholder only, the relationship is one-to-many and the entity that does allow duplicates is the parent, and if duplicates are not allowed for either placeholder, the relationship is one-to-one and either entity type can be the parent entity type. Add this information in the IDEF1x diagram and don't forget to add the key attributes of the parent entity type as Foreign Key attributes to the child entity type. Finding the minimum cardinalities of the relationship is very similar to finding whether non-key attributes are optional or mandatory, as shown in the previous section. In many cases, you will already have found the information in the Mission Statement or stumbled over it while doing interviews, but if not, you may have to pose additional questions to the subject matter expert or whoever your interview partner is. Again, the best way to find out is to prepare example documents where a specific entity instance does not occur in any occurrences of the fact type that the relationship represents. For the Snooker Club, when analyzing the Member plays in League relationship, I recall that I have already seen membership cards of recreational members who are not competing in any league. So I conclude a minimum cardinality of 0 on the member side. For the League side, I prepare a document by copying one of the existing league overview documents and erasing all the members. When I gave this to the subject matter expert, she told me, once she finished laughing, that a league without members simply made no sense at all. That allowed me to conclude that the minimum cardinality at the league side of this relationship had to be one. Some diagramming methods, such IDEF1x, allow special maximum cardinalities to be represented in the graph. That doesn't mean that you need to verify each and every relationship for these. When a business rules exists, that limits the amount of times an entity instance can take part in the relationship, it's almost always mentioned explicitly in the Mission Statement or in an interview. The last thing we need to add are the relationship reading. One of them can be found from the fact types template. For the reverse reading, you can either try to formulate facts with the elements in reverse order yourself or ask the subject matter expert for help.

Artificial Entity Types
By now, most of the fact types you have encountered should be represented in a graphical model. There are only two groups of fact types left, the first group of that is all fact types that have three or more placeholders in the templates, and the second group is that of fact types with two placeholders, one being an entity type and the other an attribute, where the entity type can be present in more than a single fact occurrence. Let's start with the last group. Fact types with two placeholders in the template with only one of them referring to an entity type and it is allowed for instances of that entity type to appear in multiple such facts. Let's look at an example. Members of our Snooker Club can reserve a table for recreational play or solo practice. The template for the facts that track these reservations is Member has reserved a table for Date. Member is an entity type and Date isn't. Obviously, a member can reserve a table on more than one date. It is also possible that multiple members reserve a table on the same date as long as there are enough tables left for all scheduled competition matches. This fact type is an example of a multi-valued attribute. The reservation date is an attribute of the member entity type, but it can have multiple values for a single member. Most ER modeling methods do not support multi-valued attributes and IDEF1x is no exception. This does of course that this does not mean that we have change the rules of the Snooker Club to fit in the limitations of the method we choose to represent the data model. Instead, we have to use a trick. This trick is called objectizing or nominalization. In linguistics, this means that a verb is transformed into a noun. The word nominalization itself is an example of this as it comes to the verb to nominalize. In data modeling, nominalization is the process of making an object or an entity type out of a fact type. That fact type then automatically becomes the identifying fact type of the new identity type. If we do this for Member has reserved a table for Date, a good name for the entity type we get would probably be Reservation. I call this an artificial entity type because we did not introduce it as result of analyzing the business requirements, but in order to work around a limitation of the IDEF1x diagramming method. If we had used a method that does support multi-valued attributes, we wouldn't have added this entity type. However, on the diagram this is no special way to represent artificial entity types. We know that Member has reserved a table for Date is the identifying fact type because the entity type was formed by nominalizing this fact type. This identifying fact type has two placeholders and one of them references another entity type. So, reservation is a weak entity type with an identifying relationship to member. The auto placeholder, Date, is not an entity type, so this simply becomes an additional key attribute. After verifying with the subject matter expert that a club can also have members who have never reserved any tables we can mark the minimum cardinality at the member side as 0. I have now introduced this member type as an artificial entity type, but note, that it could have just as well been introduced as a normal entity type during Mission Statement, analysis, or interviews. If reserving a table costs money and the Club tracks when this is paid, we probably already would have had a fact, Reservation was paid on Date. That would have caused us to introduce a normal, not artificial entity type reservation and then we would have recognized, Member has reserved a table for Date, as it's identifying fact type. The end result would have been the same diagram with the addition of the Paymentdate attribute of course. So the dividing line between artificial and normal entity types can be very thin. The last bunch of fact types we need to add to the diagram are those with three or more placeholders in a template. These can represent various situations like a relationship between three or more entity types, a relationship that has an attribute of its own, or a composite multi-valued attribute. None of these can be represented in IDEF1x, so I will not explain all of these concepts here, that will happen in later module, when I present a few other ER modeling and diagramming methods. For now, because IDEF1x does not support any of these situations and because we are currently only working on the initial version of the diagram, I will just use a sledgehammer approach that works for all cases. This may not immediately produce a completely correct result, but we will sort that out during the nominalization process that I will describe in the next module. For the initial diagram, you can simply use the nominalization approach that you also used to handle fact types representing a multi-valued attribute. I already gave an example of this process in Module 2, based on a fictional data model for Pluralsight. The fact type we used there had the template Pluralsight member watched course on Device, three placeholders, each referencing an entity type. So to nominalize this, we simply create a weak entity type with this fact type as the identifying fact type. The identifying fact type references three entity type, so there are three identifying relationships in the IDEF1x diagram and my assumption, as you can see in the diagram, is that Pluralsight's data model does support members who have not watched any course yet and courses that have not been watched, but that they only track information for devices that have been used to watch at least one course. For a real world job, I would of course have to bag those assumptions either from the Mission Statement or from an interview. Naming the artificial entity type can sometimes be a bit problematic. Some people simply like to name these entity types by smashing together all the placeholders used in their identifying fact type. For the first example in this section, that would have resulted in the entity type name Member Reservation Date, I don't know about you, but I think that Reservation describes it much better. For the Pluralsight example, finding a good name for the artificial entity type, does pose a bigger challenge. So I might want to go back to the subject matter expert and work with him or her to come up with a good name or I can review the documentation. When I first introduced this fact type in this course, back in Module 2, I explained that this fact type was introduced because Pluralsight wants to track what kind of devices are used by their members to watch specific courses. The key word in this sentence and hence also in the fact itself, is the verb, use. So while the word usage may be a bit too broad and fake for this entity type, I feel that DeviceUsage might be a very accurate description and it's definitely a lot better than Member Course Device, a name that tells me exactly what I can already see from the diagram, but nothing about the meaning of the represented facts.

Summary
In this module, you learned how to generalize the concrete example facts we used so far into fact types, abstract collections of similar facts. You used a template to describe fact type with a fixed part that conveys the meaning of the fact type and placeholders for actual values to which this meaning applies. Based on what placeholders do and do not allow duplicates in a collection of all facts in a fact type, you then found the first collection of entity types to represent in the entity relationship diagram. Every entity type always has an identifying fact type. This fact type can be used to explicitly announce the existence of an entity instance. In practice, this rarely happens because other facts about an entity instance imply its existence, however, if the identifying fact type does exist in the collection of fact types you found, you need to recognize it and if it doesn't, you should either add it or at least think about its structure. Creating the entity relationship diagram starts with adding all the entity types. You use the structure of their identifying fact types to determine if they are strong or weak and to find the key attributes. After that, all non-identifying fact types are also added to the entity relationship diagram. Most of them are represented as either attributes or relationships. However, fact types that represent a multi-valued attribute, the relationship between more than two entity types, or a relationship with an attribute, need a special treatment because those concepts are not supported by IDEF1x. In order to add them to a diagram, you have to use nominalization to transform those fact types into artificial entity types which then can be represented in the IDEF1x entity relationship diagram.

Converting to Relational
Outline
Hello, my name Hugo Kornelis and I am presenting this Pluralsight course on Relational Database Design. This is Module 5 in which you will learn how to convert an entity relationship model into a relational design and how to convert a relational design into an entity relationship model. This module covers the conversion between two representations of data design, the entity relationship diagram and the relational database design. After explaining why we need both representations and hence need to be able to switch between the two, I will how a few alternatives for representing the relational database design. The majority of this module will of course focus on the actual conversions. Both from entity relationship to relational and the other way around.

Different Representations
An entity relationship model is great for the design process. It enables the user to get a quick at a glance overview of the model. With experience, you'll find that you can read ER diagrams like a good book. And even though I do not recommend including ER diagrams in documentation for audiences other than developers and fellow data modelers, in practice they are often included in documentation or contracts and many decision makers have learned to interpret at least the important elements. However, they are not very useful when it comes to actually building the database. The elements in an ER model do map relatively closely to elements in a relational database, but the mapping is not one on one, as you will see in the rest of this module. So when it comes to the actual implementation, you probably want to hand the developers or the DBA a relational representation of the data model, either in addition to or in place of the ER diagram. So why not save on the conversions and do the entire design procedure in a direct relational form? I have actually already given the answer. Because an ER diagram is a very useful tool for giving a quick insight in a data model and the relational model isn't. That's why you should design and document your data model in an ER diagram, which you can then convert to a relational model for implementation. You will also find yourself frequently converting the other way around from a relational model to an ER diagram. The most common reason for that is when you inherit an existing database for which the documentation is non-existent or outdated. The actual relational design can always be seen by looking at the actual implementation in the database. Reverse engineering that design into an ER diagram can be a great way to get a good understanding of the underlying data model. When you use ER diagrams during design and modeling and want to hand the final design in relational form to the developers and DBA's, you will have to convert at some point, but when? Ideally, you want to convert only once when the design is final. After creating the initial model, you will first perform all the normalization steps, the subject of the next three modules, and only convert to relational when the design is final. That is in fact, my preferred sequence of steps. But the truth is that many of the normalization steps are easier to describe and sometimes even easier to perform on relational tables. For that reason, some people will prefer to convert the initial ER model directly to an initial relational model, use that as a starting point for the normalization procedure, and either constantly apply the changes made during the procedure to both the relational model and the ER diagram or simply convert the normalized relational design back to an ER diagram once the normalization process is completed. Finally, remember that most database implementations will make changes to the data model to tweak it for optimal performance on the target platform. These optimizations are vendor dependent, sometimes even version dependent and out of scope for this course. So I will limit this module to a straight conversion from the ER model to a non-optimized relational representation of the logical data model. When converting back from a relational to an ER diagram, you can choose to remove elements that are clearly purely performance optimization or you can optionally opt to keep them in, but remember that this introduces physical optimization in what is normally a logical data model.

Representing a Relational Design
Relational database designs can be represented in many ways. The very compact form is shown here. The simple listing of tables with their columns added in parenthesis. Underlined columns specify which columns are candidate keys, a concept I already touched upon in the previous module, and that we will investigate further in the next modules. The benefits of this notation are its compactness and speed of creating it. The down side is the lack of detail. There is no way to represent what columns are optional, what data types to use and how the tables reference each other. Sometimes dashed underlines are used to show which columns are foreign keys referencing another table. Though dashed underlines are also sometimes used to distinguish alternate keys from the primary key. The other extreme is the full DDL, short for data description language, the actual SQL statements that can be executed to create the tables. This form includes all the details, but it is very verbose and doesn't work for a human reader who wants to actually understand the model. Graphical representations are also possible. Many programs allow you to easily create representations of a relational model that look, for instance, like this. Tables are represented as rectangles, foreign key relationships as lines connecting the tables and details on keys, optionality, and data type of columns can be added as needed. As you can see, this notation is very close to the notation used in IDEF1X entity relationship diagrams and many other ER diagraming methods. This can be perceived as an advantage, as it allows someone familiar with one notation to quickly learn to understand the other notation. But it can also be dangerous because it tends to muddy the waters between two different notations. People using these and similar notations for their database design often cannot resist the temptation to use a single model for both design and implementation. Which means that the logical design will end up being influenced by implementation choices and by limitations of the relational model. This can hamper future maintenance and portability of the application. I personally like to use what I consider to be a very simple approach. Represent relational tables as tables and relational columns as columns. In this notation, I use arrows above the columns for primary and alternate keys. If needed, additional details on optionality and data types can be added in the columns and additional constraints can also be penciled in. Relationships can be represented by arrows or as a generic constraint or even left out completely if they would clutter the diagram too much. A benefit of this representation is that the model is immediately obvious to both technical and non-technical audiences. The ability to add simple data directly in a diagram can also be a major benefit.

ER to Relational: Entity Types
The first step of the conversion from an entity relationship diagram to a relational database design is incredibly simple. For ever entity type, a table is created. It doesn't matter if the entity type is strong or weak or if it's a sub-type. One entity type, one table. Naming the tables requires some thinking. The obvious idea of naming each table after the corresponding entity type is not a bad choice, but it's not the best option. This has to do with different naming conventions that come from different paradigms. An entity type represents an instance of a class and is therefore, usually given a name in singular form (player, match, payment etc. ) But a relational table holds the collection of all instances which is more obvious by using a name in plural form (players, matches, payments). In some cases, the singular name that represents the entire set is also a possibility. For instance, the entity type employee, when transformed in a relational table could be called employees, but it can also be named personnel or staff. This name change may sound superfluous and keeping the names the same does make it much easier to map between the two representations, especially when dealing with future changes. But ER diagrams tend to look strange and the relationship readings sound weird with plural entity type names. So the choice to keep the name the same, typically results in singular names for the tables, and that does also have its disadvantages. One of them is that using plural table names subconsciously reminds the database developers of the relational paradigm to write code that handles whole sets of data at a time, which usually performs better and is easier to maintain. Another reason is that SQL queries tend to sound more natural when table names or plurals or group names. And a final, quite practical reason is that reserved words in the SQL standard and in various (_____) SQL dialects, tend to be nouns in singular form, not in plural form. So a table name in plural form has a much lower chance to clash with a reserved word. In every table, we get one column for each attribute of the entity type. Naming these columns is easy, simply using the attribute name is almost always the perfect choice. If, in your entity relationship diagram you have chosen to add some representation of optional or mandatory attributes, you can immediately deduce deniability of the columns. Optional attributes results in nullable columns and columns generated from mandatory attributes get a NOT-NULL constraint. If this is not represented in the ER diagram, you will have to research the nullability of each column as part of the conversion. Data types are usually not represented in the ER diagram, but should be determined as part of the conversion to the relational model. In the logical model, the data type should preferably be represented in an implementation independent manner. I like to use a simple code representing the base type. For instance, N for numeric, C for character data and D for date or time data. Full details like the permitted range require precision, minimum and maximum length etc., would clutter the diagram too much. I include them in a separate appendix.

ER to Relational: Candidate Keys
When converting an entity relationship diagram to a relational data model, every key attribute and every combination of attributes that combine serves as a candidate key, will be converted into a relational candidate key and forced by a key constraint. There are two types of keys in relational databases. The primary key enforced by a primary key constraint is the best known. Codd's second rule that guaranteed X's rule states that ever table must have a primary key. It is not allowed to have more than one primary key on a single table, so if a table has multiple candidate keys, the others will become alternate keys, enforced by unique constraints. In practice, there isn't really that much difference between a primary key and an alternate key in a relational table. In the standard SQL language, a foreign key constraint, covered in the next section, references the primary key by default, but it can reference an alternate as well. And in standard SQL, a primary key must be on columns that do not allow nulls, a restriction that doesn't apply to alternate keys. The most relevant differences however, have to do with physical implementation and performance. Which can be difference for each specific database product. For a table with multiple candidate keys, the final choice of which one will be the primary key, should be made in the physical model, which is beyond the scope of this course. Depending on the chosen representation of the relational design, primary keys can be represented using a lock symbol or the letter PK for primary key in front of the columns that are part of the primary key. Or these columns can be underlined or covered with an arrow. Alternate keys are sometimes only specified in a separate appendix and not included in the graphical representation. When they are, the letters AKEY, for alternate key, UQ for unique, or just U and the number are placed in front of the columns. Or they are underlined with a dashed line or covered with a dashed arrow. It is also possible to use drawn lines or drawn arrows for all keys. A great way to stress that the choice of primary key is postponed to the physical modeling stage. In the full DDL representation of the logical database design, the keys are implied by the definition of the primary key or a unique constraints that enforce them. The actual conversion from the ER diagram to the logical database design is very easy. For every key in the ER diagram, check the attributes it is defined on, find the corresponding columns, and define the key constrained on them. How to decide if this key should be primary or alternate depends on the ER diagraming method used. When working with IDEF1X or another method that forces you to define one of the candidate keys as the prime key and the rest as alternate keys, you can stick to this choice when deciding whether a relational key should be primary or alternate. If you used an ER diagram method that allows multiple candidate keys without choosing, you might have to make a choice now. Just remember that the decision is not final, leave that for the physical database design.

ER to Relational: One-to-many Relationships
How a relationship in the ER diagram is represented in the relational design depends on the maximum cardinalities of the relationship. The easiest category of relationships to transform are all relationships with at most one maximum cardinality of many. This includes the very common One-to-many category as well as the quite uncommon One-to-one relationships. Note that the identifying relationships of a weak entity type are simply a special case of a One-to-many relationship. And the identifying relationship of a sub-type is a special case of a One-to-one relationship. For the conversion to a relational database design, these special cases are treated the same as regular One-to-many and regular One-to-one relationships. A One-to-many relationship in the ER diagram is represented in the relational database design as a foreign key constraint. This foreign key constraint, constrains one or more columns called the referencing columns in the table that was created from the child entity type and the constraint references the columns of one of the candidate keys of the referenced table, the table that is formed from the parent entity type. This foreign key constraint is a rule that instructs the database engine to ensure that every value stored in the referencing columns is a correct reference to an existing row in the reference table. As you can understand from this description, the foreign key constraint requires that there are referencing columns in the table formed from the child entity type. Because IDEF1X diagrams explicitly depict the referencing attributes for every relationship, these columns will already exist, so the only thing left to do is to add the foreign key constraint. Some other ER diagraming techniques choose not to include attributes for the foreign key relationship, figuring that these are already implied by the symbol for the relationship itself. When you work with such a diagraming method, you will have to first add the referencing columns, then add the foreign key constraint when converting One-to-many relationships. If the referenced table has multiple candidate keys, you can choose which one of them to reference, and hence which columns to add to the referencing table. Usually the primary key is used for this, but referencing alternate keys is allowed. One-to-one relationships, including subtypes relationships, are converted in the exact same way. When using IDEF1X or other diagraming methods that explicitly include the attributes that implement relationships, you already had to choose which of the entity types would play the role of parent and which would act as the child entity type. In diagraming methods that omit the attributes that implements the relationship, this choice has not yet been made so you should choose now. Either way, you must always remember that the choice you make is preliminary. Performance considerations relate to the vendor used to implement the database or to comment (_____) for the data, should determine the final choice in the physical data model. So you should always add a note describing this implementation option or maybe even supply two versions of the data model. As already mentioned, subtype relationships are converted to a relational database in the same way as normal One-to-one relationships. However, in the case of a sub-type relationship, the referencing columns will always be added to the table corresponding to the sub-type. The extra information associated with the sub-type relationships such as the discriminator, that mutual exclusiveness in whether the subtype relationship is complete or not, cannot be represented as standard constructions in the relational model. You should describes these as additional business rules in a separate appendix. For all One-to-many and One-to-one relationships, the minimum cardinality of the child entity type is represented in the relational database design through the nullability of the referencing columns. Optional relationships are implemented with referencing columns that are nullable. Conversely, referencing columns that implement a mandatory relationship will be defined as NOT NULL. A non-zero minimum cardinality of the parent as well as any special cardinalities, cannot be represented in standard way in the relational database design. You will have to specify these restrictions in a separate appendix and the database developer will have to write special code to enforce the rule. For database platforms that do not support deferred constraint checking, a feature defined in the NC standard for SQL, but not yet included in all major database platforms, any non-zero minimum cardinality of the parent is even impossible to enforce.

ER to Relational: Many-to-many Relationships
Probably the most surprising element of the conversion from an ER diagram to a relational database design, is that Many-to-many relationships that in the ER diagram look so very similar to One-to-many relationships, have a so completely different relational implementation. For every Many-to-many relationship in the ER diagram, you will have to add an extra table to the relational design. This table will have two foreign keys referencing the two tables that correspond to the entity types, connected by the relationship. For each of the foreign keys, pick one of the candidate keys from the reference table and add copies of its columns to the new table to implement this foreign key relationship. Finally, add a single primary key constraint over all the columns of the new table and you are done. Non-zero minimum cardinalities and special maximum cardinalities cannot be enforced with standard relational components. You will have to include those in a separate technical appendix for the application developers. For the name of the new table, many people simply combine the names of the two tables that were connected by the originating Many-to-many relationship. This practice will obviously cause problems when there are multiple Many-to-many relationships between the same two entity types. But even more importantly, this naming convention often fails to convey the meaning of the data in the table. A better way to name the new table is to use the reading of the relationship, optionally combined with one or both of the names of the connected tables. This gives the database developers who will have to write queries against the database, a much better idea of what facts are stored in the table. However, the best names may sometimes require you to really rethink what is stored in the table and then find a term that best describes the contents. Collectively, tables that implement a Many-to-many relationship are sometimes referred to as junction tables, linking tables, cross reference tables, or joined tables. When you encounter these names, keep in mind that they only tell you something about the ER diagram underlying the relational design. In the relational database itself, these terms are meaningless. The relational model only knows one type of table called table. Junction tables are not in any way considered to be special or different by any relational database engine. In the IDEF1X notation for ER diagrams, relationships can only be between two entity types, but there are other ER diagraming techniques that also allow relationships between three or more entity types. We will take a closer look at such relationships in Module 9. For the transformation from an ER diagram to a relational database design, you only have to remember that such relationships between three or more entity types are handled exactly the same as Many-to-many relationships between two entity types. They are represented as a table with a foreign key to each of the entity types connected by the relationship and with a primary key constraint on the combination of all columns.

ER to Relational: Other Constraints
In the previous sections, we converted all the elements of the ER diagram, plus some additional information into tables, columns and primary key, unique and foreign key constraints, but a relational database supports a few other types of constraints and you should add them as well. A CHECK constraint is a logical rule, expressed as a logical expression that may not be false for any row in the table. That sounds very versatile, and it is, but it is also quite limited. The expression that defines the CHECK constraint can apply to a single column or to multiple columns, but those columns must all be from the same table. You are not allowed to access other tables or even other rows from the same table in the CHECK constraint. So you can for instance, use a CHECK constraint to enforce that the total of FramesWon and FramesLost in a single match must never exceed five, but if the maximum number of frames played is not always five, but depends on the type of match, a CHECK constraint falls short because that information will be stored in a different table. Sometimes a CHECK constraint is so strict that only a single failure would be allowed for the constraint column. For instance, if all matches are over exactly five frames, the number of FrameLost can be deduced from the number of FramesWon by subtracting it from five. In such a case, instead of letting the end users keep trying until they hit the lucky failure or are letting the application layer compute the correct failure, we can tell the database to compute it. This feature is called a generated column, though the various database management system vendors sometimes have chosen different names for it, like derived columns or computed columns. Like a CHECK constraint, a generated column cannot access data in other tables or in other rows of the same table in its definition. An assertion is similar to a CHECK constraint, but more powerful because the limitation on accessing other tables or other rows from the same tables has been lifted. Almost every test that can be expressed as a query can be included in an assertion. However, assertions were not included in the original anti-standard for the SQL language that is used for most relational databases. It was added later and many vendors do not yet support this feature. You can still include them in the logical database design as a compact and standard way to document the business rules, but your database developers will have to write the code to enforce them. A DEFAULT constraint is called constraint, but is not really a constraint at all. It does not in any way constrain the information stored in the database, but it can help prevent integrity errors by allowing the database to provide a standard value for a column if no other value is specified. This may sound a bit like a generated column, but there are big differences. The most important difference is that a DEFAULT will be applied only when a new row is added and only when no value is explicitly provided. You can provide a different value, or you can modify the row later. For a generated column, there is no way to assign it a value that is different from its definition. Another difference between a generated column and a DEFAULT constraint is that the letter cannot address any other columns in the table, nor contain complex logic. The only things allowed are a constant value or a simple call to a built in function, such as current time stamp. Some typical examples of DEFAULT constraints would be a default CountryCode of NL for Netherlands, for members of our club, since we are a Dutch club and most of our members are local, or a DEFAULT of current time stamp for the column DatePlayed in the matches table. So that we don't have to explicitly specify the date if we enter the match result on the date the game was played. All the information you need to add CHECK constraints, assertions, generated columns, and DEFAULTS to your database model, will be found in a separate appendix that should accompany your ER diagram. The diagram itself will not contain any information to represent these concepts. Since there is no standard format for a technical appendix, there is no standard procedure to extract these relational elements out of it.

ER to Relational: Demo
Every year, at the start of the season, the board of our Snooker club faces the task of making a schedule for all matches. Not an easy task, for how do you schedule a match between a member who is unable to play on Mondays and a member who can only play on Mondays? What you see here is a simplified version of the data model they use for this task. It is not yet fully normalized as you will see in the next modules of this course, but it does provide for all the ingredients for a good demonstration for a conversion to a relational database design. The IDEF1X entity relationship diagram has six entity types, so we get six tables. For five of them, I choose a name that is the plural of the entity type name, although I do remove spaces to comply with SQL naming standards. For the table that corresponds to the day entity type, I decide that calendar is a better table name. The number of columns in each table is equal to the number of attributes in the corresponding entity type. In many cases, the column name is equal to the attribute name. Again, without spaces to comply with SQL naming standards. I did make a few minor changes, like replacing the column name date with match date to emphasize the role the date plays in those tables and I shortened some column names. For instance, using FramesWon instead of FramesWon Player1, this makes the model and the future queries more compact but less self-explanatory. Actually, the main reason I made this specific choice was to ensure that everything still fits on the screen. The ER diagram has no information about data types, but a good data designer will always supply additional information that can be used to determine the data types in the relational database design. I have depicted them in short form here. Details, such as the maximum number of characters used for player names and the fact that FramesWon and FramesTwo should be positive whole numbers, would clutter the diagram too much. They can either be determined later, when creating the physical model, or this information can be supplied in a separate appendix. Often, this is not a paper document, but a data dictionary or other automated repository. The final bit of information to the columns is for optionality. As you see, I have chosen to extend the ER diagram with a home brew notation for optional and mandatory attributes. This is not done for the key attributes because they are always mandatory. For every attribute that is marked as optional, I add an optional mark to the relational database design. I do not include a specific mark for mandatory columns. In this notation, columns are assumed to be NOT NULL unless marked optional. Most of the entity types in the ER diagram have only a single key so there is no choice. The primary key of every table will be on the column or composite keys on the columns corresponding to the attribute in that key. The player entity type is the only one with two keys. In the ER diagram, the player number attributes has been selected to be the prime key, and player name has been made an alternate key. I see no reason to change that now so I create a primary key constraint on the player number column and a unique constraint for the alternate key on player name. The One-to-many relationship player plays in league is transformed into a foreign key constraint. The referencing column is a league code in the players table. The league's table if the referenced table. The relationship is optional, which means that the referencing column is optional, but we already knew that from the explicit optional mark next to the corresponding attribute. The minimal cardinality of one for the league entity type, which states that there cannot be any leagues without players, can't be easily represented in the relational database design. This rule will be included in the list of business rules that the developers has to enforce by writing extra code. The One-to-many relationship competition match is played for a league, is also transformed into a foreign key constraint, the only difference with the previous one is that in this case the relationship is mandatory. So the foreign key column is NOT NULL, but again, we already knew that. The three identifying relationships of the weak entity type match two connect to player and one today, are all converted into foreign keys as well. The fact that these relationships are identifying, means that the columns constrained by the foreign key should be part of the primary key, but since we already added the primary key constraints in the previous step, we don't have to do anything special for the identifying relationships now. Of these three relationships, match is played on day is the only one with the maximum cardinality for zero for the parent. So this is the only one that will not introduce extra work for the database developer. The subtype relationship between entity types, match and competition match, is also converted into a foreign key constraint because the key of the match entity type consists of three attributes. A copy of all those three attributes has been added to the competition match entity type to implement the reference. This is not specific to subtypes. It will happen every time a relationship of any type or cardinality references an entity type with a composite key. These three copied attributes have already resulted in three columns in the competition matches table. These three columns combined are the referencing columns of the foreign key constraint. Obviously, the subtype relationship for the cup match entity type is handled exactly the same way. One relationship has not yet been handled, player cannot play on day. This relationship is Many-to-many, so it has to be converted to a table. Thinking about the meaning of the facts represented in this table, I chose to name it blocked dates. This table has foreign key constraints to the tables calendar and players because day corresponds to the entity types that are connected to the Many-to-many relationship. Both these tables have a single key, primary key, so each of the foreign keys is implemented with a single column with name and data type copied from the referenced columns. A single primary key constraint on all these columns combined concludes the conversion of this Many-to-many relationship. All elements that are visible in the IDEF1X diagram are now represented in the relational database design. But the technical appendix includes a few more details that I should convert as well. Our clubhouse is never open for normal matches during the weekend, so the day of week column in the calendar table can only be Monday, Tuesday, Wednesday, Thursday or Friday. Our cup matches are played over five rounds, called round one, round two, quarter finals, semi-finals and final. And though we can have matches scheduled, but not yet played, so no result is known, we cannot have partial results. Once a match is played, both FramesWon and Player1 and FramesWon, Player2 need to be specified. After adding these CHECK constraints to the relational database design, we are done. We will still have to normalize the tables because this designs violates the normalization rules as we will see in the next modules. And when normalizing, all changes made to the relational database design will have to be ported back to the ER diagram as well. That is the price we pay for the ease of normalizing on the relational tables. If we had chosen the slightly more complex strategy of normalizing the ER diagram, we would only have needed a single conversion for the final normalized design.

Relational to ER: First Draft
Once you know how to convert an entity relationship diagram into a relational database design, the conversion back from relational to ER is incredibly easy. Simply reverse the steps. There is only one problem, both entity types and Many-to-many relationship types have been converted into tables. So how do you know if a table should be converted back into an entity type or into a Many-to-many relationship? The answer is simple, you don't. Well unless you still have the original ER diagram, but then you obviously don't have to do this transformation. The answer to this dilemma is still simple. You first create a draft ER diagram by converting every table to an entity type. This diagram is already a correct ER representation of the data model. Technically, you don't need Many-to-many relationships in an ER diagram. You can always express them using an extra entity type instead. A Many-to-many relationship is simply an alternative way, often more convenient and closer to the intuitive mental model of reality to express the same data model. So after creating the first draft, you can check every entity type to assess whether it can instead be represented as a Many-to-many relationship and if that would improve the readability of the data model. Creating that first draft ER diagram from a relational database design is simple, for every table in the database, create an entity type. You can choose to give the entity type the same name as the table, or the singular form of it if the table is named as a plural. Next, find the primary key constraint of each table. For each column included in this constraint add a key attribute to the entity type. For all other columns, add a non-key attribute. If you use an ER diagraming technique that includes symbols for optional or mandatory attributes, then mark every attribute as mandatory that derives from a column with a NOT NULL constraint and mark all other attributes as optional. If there are any unique constraints in a table, find the constraint columns and then create an alternate key on the corresponding attributes on the ER diagram. CHECK constraints, assertions, DEFAULTS and generated columns cannot be represented in most ER diagraming methods so you will have to describe them in a separate technical appendix, or you could consider simply adding the relational database design as the appendix. That leaves only the foreign key constraints. They all get converted into a relationship. Typically a One-to-many relationship with the entity type that corresponds with the referenced table as the parent and the entity type that corresponds with the referencing table as the child. The maximum cardinality of the child is of course always one. The maximum cardinality of the parent is normally many, except when all columns of the primary key or all columns of a single unique constraint are included in the foreign key constraint. In that case, the maximum cardinality of the parent is also one. The minimum cardinality at the child's side can be derived from the nullability of the referencing columns. If they are defined as NOT NULL, the relationship is mandatory, depicted by a minimal cardinality of one at the child's side. Otherwise, it is an optional relationship minimum cardinality zero. The minimal cardinality at the parent's side is always zero because the relational model does not support any constructs to enforce otherwise. Except if you have a database that supports assertions and deferred constraint checking, or if application code is used to enforce such a rule. Unfortunately, the relational database design usually does not contain any clues to help you find a proper reading for the relationship. There is sometimes additional documentation that you can consult to find good readings, otherwise, you will have to consult the users or subject matter expert, or choose to omit the reading for now and fill them in at a later time. If all of the columns that are constraint by the foreign key are included in the primary key, then the relationship you created from it is an identifying relationship and the entity type that corresponds to the referencing table is a weak entity type. If the columns constraint by the foreign key correspond exactly to all the columns in the primary key, the relationship is a subtype relationship. It is usually impossible to determine the discriminator from just a relational model. You will have to review the full application code for that. The same goes for determining whether subtype relationships are complete or not and whether they are mutually exclusive or not if an entity type has multiple subtypes. If you use IDEF1X or another diagraming method that explicitly includes the attributes that implement the relationship, you are done now. These attributes have already been added as a result of the columns implementing the foreign key constraint. On the other hand, if you use a diagraming method that does not include these attributes, you will now have to remove them because you originally added them when converting all columns to attributes.

Relational to ER: Variations
In the first draft of the ER diagram you transformed every table to an entity type. Some of them may be better represented as a Many-to-many relationship. This freedom of choice does not apply for all entity types, many can only be represented as an entity type. Only those that meet certain conditions can optionally be converted into a Many-to-many relationship. The first condition is that all attributes have to be part of the key. In an IDEF1X diagram, that means that all attributes must be placed above the line. The bottom part of the entity type symbol has to be empty. The second condition is that the entity type has to participate in exactly two relationships. Both have to be identifying relationships and the entity type has to be the child entity type in both of them. If these two conditions are met, you can replace the entity type and its two identifying relationships with a Many-to-many relationship between the two parents. For the minimum cardinality of the two entity types participating in the new relationship, simply copy the minimum cardinalities from the two identifying relationships you just replaced. If the table corresponding to the entity type you removed has a descriptive name, you can often use that name to derive at least one of the readings for the entity type. Otherwise, you will have to go through the same hoops as for normal relationships to find a good reading. If you use a diagraming method for your ER model that supports relationships between more than two entity types, then the transformation rules can be relaxed. The first rule still applies without change, but for the second rule, the number of identifying relationships can now be two or more. The steps to convert an entity type that satisfies the rules to either a Many-to-many relationship or a relationship between more than two entity types are further exactly the same. But the technical transformation between entity type and Many-to-many relationship or vice versa, is not the hardest part. The problem that you will often struggle with is deciding when to apply this transformation and when to leave the entity type unchanged. Unfortunately, there is no simple answer to that question. Both versions of the ER diagram are correct. They are in fact, just two different representations of the same information, like synonyms in language. So though there is no rule on when you should choose which representation, there is the comforting knowledge that there is no wrong answer. Just pick the one that you feel best expresses the information that is being stored the database. You might even choose to switch between the two representations depending on the target audience. For relationships between three or more entity types, the same rule applies. Obviously, if you use IDEF1X or another diagraming method that does not allow relations between more than two entity types, then you have no other option than to leave these as entity types. This sole effect already demonstrates that this is never wrong to leave your model this way. But the diagraming methods that do support relationships between more entity types, do so for a reason. Obviously, their creators have seen situations where they felt a relationship better represents the meaning of the data model than an entity type.

Relational to ER: Demo
As a quick demo of the reverse transformation, let's assume that we have a relational database that looks like this and we want to model this as an IDEF1X ER diagram. For the first step of the first draft, I add entity types for each of the tables. Using the singular form of the table name for the entity type names. Every column that is included in the primary key becomes a key attribute in the corresponding entity type above the line. The remaining columns are all added below the line as regular attributes. I use standard IDEF1X notation for this demo, so I cannot represent what attributes are optional or mandatory. I will have to include that in a separate appendix to the documentation. Because of the unique constraint in the players table, I do add an alternate key to the player entity type. The foreign key constraint on the league code column in the players table, referencing the league's table, becomes a One-to-many relationship with league as the parent and player as the child entity type. The minimum cardinality of player is zero because the league code column in the players table is optional. Without inspecting all database and application code, I cannot determine a minimum cardinality for the league entity type, so I have to assume a minimum of zero there. I leave the reading out for now. I will fill those in later when I have a chance to chat with a domain expert. The foreign key constraint on the league code column in the competition matches table is handled exactly the same. The only difference is that this column is not optional, but defined as NOT NULL. So the relationship is mandatory with a minimum cardinality of one for the child. The foreign key constraints on the two columns in the blocked dates table are also very similar, except that they are on columns that are included in the primary key. So the resulting relationships are identifying and the blocked date entity type is hence transformed into a weak entity type. The exact same procedure also applies to the three foreign key constraints on the three key columns of the matches table. Having two foreign key constraints on different columns, but with the same referenced table, may look daunting at first, but if you just followed instructions you will always end up with the correct result. Multiple relationships between the same two entity types. The foreign key constraint on the cup matches table is on three columns. This is different from the previous scenario. In the matches table, we had two foreign keys on one column each, so we get two relationships, one for each foreign key. In the case of cup matches, we have one foreign key on three columns, so we get one relationship over three attributes. The rule is one relationship for one foreign key constraint, independent of the number of columns, so we get a single relationship and because the foreign key is exactly on all columns of the primary key, it is not a normal nor an identifying relationship, but a subtype relationship. The last foreign key constraint on the competition matches table is a similar situation. So this too will be converted into a sub-type relationship. It may be tempting to connect the competition match entity type to the existing sub-type symbol, but that would be a dangerous assumption. That notation would imply that competition match and cup match are mutually exclusive and we cannot conclude that from just the relational design that was our starting point. That is why two independent sub-type symbols are used and why they are both marked as incomplete sub-types. We have now transformed all elements of the relational design and the result is a correct IDEF1X representation of the data model. Although, it is still incomplete until we add the relationship readings and do some further research into the minimum cardinalities and into how the two sub-types relate. But it is not the only possible way to represent this as an ER diagram. One of the entity types, blocked date, meets the rules for the transformation to a Many-to-many relationship. All attributes are key attributes, it participates as the child entity type in two identifying relationships and it does not participate in any other relationships. That means that in addition to this diagram, we can create another diagram that is also correct, by converting this entity type to a Many-to-many relationship between the player and calendar entity types. Remember that this version is neither more nor less correct than the previous version. Both versions are correct, both represent the same data model and both are implemented as the exact same relational database. The only difference is in how well the model expresses the information that is stored in the database and that is very hard to assess if all you have been given is the relational database design, other documentation or talks with domain experts can help here.

Summary
In this module, you learned how to transform data models between a representation as an entity relationship diagram and representation as a relational database design. I first explored the merits of each representation, confirming the need for each of them and hence the need for the conversions. I then presented a few different methods that can be used to represent a relational database design. The actual conversion from an ER diagram to a relational database design, follows a simple series of steps. First, convert entity types to tables, attributes to columns and candidate keys to primary key or unique constraints. After that, all relationships are converted, One-to-many and One-to-one relationships, including identifying sub-type relationships get converted into a foreign key constraint. Many-to-many relationships cannot be represented as a constraint, they have to be converted into extra tables. Often, given fancy names such as junction tables or link tables, but just tables as far as the database is concerned. Finally, some other constraints are added that are not derived from elements in the ER diagram, but from the accompanying detailed information, CHECK constraints, DEFAULTS, generated columns and assertions. The conversion from a relational database to an ER diagram, takes place in two steps. In the first step, every table becomes an entity type, every column as attribute and every foreign key a One-to-many relationship. There will never be any Many-to-many relationships in this first version of the ER diagram. In the second phase of the conversion, we investigate which of the entity types can also be represented as a Many-to-many relationship. This results in alternative versions of the ER diagram. Equally correct, but perhaps better suited to support the intended use of the diagram.

Basic Normalization - Part 1
Outline
Hello my name is Hugo Kornelis and I am presenting this Pluralsight course on Relational Database Design. This is Module 6 Basic Normalization (Part 1). In this and the next module you will learn to fine tune the design you that you can be certain that your database will have no redundancies or other design flaws. Once you've followed all the steps from the previous modules, you have a data model, either as an entity relationship model or as a collection of tables and columns, that enables the database to store all the information the business needs, and that will prevent old data that would violate any of the identified constraints. But there may still be issues, glitches in the design that can cause unwanted side effects, like redundancy, a data model that allows you to store multiple copies of the same fact, or update, insert and delete anomalies. A data model that makes it impossible to modify effects independent of each other. To remedy this, you need to normalize the data model. After an overview of what normalization is why we need it, I will first describe what functional dependencies are. This is required because functional dependencies play a major role in the normalization process. I then explain the three most important normal forms, quite and imaginatively called First Normal Form, Second Normal Form and Third Normal Form. Interweaved with that description, I will show a method that you can use to make absolutely sure that you find all the functional dependencies so that you can be sure that your database design is perfectly normalized. Because of the amount of material covered, this information will be spread over two modules.

Why Normalize?
Sometimes the database design that looks okay at first sight may have some hidden problems lurking underneath the surface. One such problem is that of non-atomic values. This means that a value that is stored in a single column in the database is actually a combination of multiple values. This makes the database harder to work with because you will always need to use expressions to access the individual values in that columns and this can also have a major impact on performance. A database design suffers from redundancy if it allows multiple copies of the same fact to be stored. This is bad for several reasons. The obvious reason is that redundant storage of the same fact takes more disk space, resulting in a higher price tag and lower performance for the application. But the real problem is that once you have multiple copies of the same fact stored, it is almost inevitable that someday one copy of the fact will change and the other will not, leaving us with two conflicting versions of the truth. A different, more concealed form of redundancy is the storage of derived facts. If the result of all frames in the Snooker match are stored, we don't need to also explicitly store the result of the match as a whole, as this follows unambiguously from the frame results. So if we do store it explicitly, we are again wasting space and exposing us to the risk of storing conflicting data. There are a few misconceptions about redundancy. One of them is that storing the same value twice is redundancy. That's not true. If you track the hair color of your friends and two of them have red hair, you will need to store the value red twice. Okay, there are technical tricks using pointers or lookup tables that allow you to store the actual literal red ones and then have two pointers to that value, but since a pointer is a value too, you are now simply storing a different value still representing red twice. As I said before, a database doesn't store values, it stores facts. So even though the value red or the pointer representing that value is represented twice here, they are part of two facts. Jack has red hair and Jane has red hair and those facts are different. There is no redundancy in the facts and hence no redundancy in the database. The second misconception is that all redundancy is bad, again, not true. There may be valid reasons for deciding to store redundant copies of facts. Those reasons are usually performance related and often tied to optimizations for a specific make of relational database, so you would expect that kind of redundancy in the physical data model, which is outside of this course's scope. More often than that, you may decide to store derived data. Sometimes, this is functionally required because the data can be derived at the time it is entered, but not at a later time. And there are cases where the overhead of re-computing this data every time makes no sense at all. Consider your checking account. How long ago did you open it? And how many transactions are there on average each month? In theory, the current balance of your account can be derived by summarizing the thousands of transactions that have taken place since the day you opened your account. But given the obvious performance cost of such an approach, combined with how often your current balance needs to be retrieved, that would be silly. I don't think there is a single bank anywhere on the world that doesn't store the current balance of each account. If you decide to have redundancy in the data model, it should be controlled. You should clearly mark duplicated data as duplicated and derived data as derived so that during implementation of the physical model and the application, steps can be taken to prevent inconsistencies. The third problem that may haunt a database is that of insert, update or delete anomalies. This means that the database design makes it impossible to enter certain modifications in isolation, either other modifications have to be made at the same time or entering a single modification will automatically imply an unwanted other change. For instance, this design of a table for tournaments and their competing players shows two different anomalies. One is caused by the redundant storage of Dave's phone number. If Dave tells us that he has switched carriers, and now has a new number, and we update only one of the facts, we introduce an inconsistency. We have to update all copies of this fact to prevent this. Another anomaly in this design is exposed when Joanna decides not to play in the 2013 mid-summer tournament. We have to delete this row, but in doing so, we will also loose Joanna's phone number because she never signed up for any other tournament either. In order to avoid all those problems we have to normalize the design. This is done by carrying out a series of checks and changing the design if it fails a step. These steps are called the normal forms. These normal forms always apply to individual tables. So within a single database or a single relational data model, some tables may be Fourth or Fifth Normal Form and others are maybe only Third Normal Form. When people mentioned a normal form of a design or of a database, they actually mean the lowest of the normal forms of all tables. So if a database has 20 tables in Fifth Normal Form, Three and Fourth Normal Form and one in only Second Normal Form, we would say that the database is in Second Normal Form. Everyone agrees that all tables should always be normalized to at least Third Normal Form. There are different opinions on the relevance of the higher normal forms. I cover basic normalization up to Third Normal Form in this and the next module. And the module after that, I will pay some attention to the higher normal forms. Many people normalize their design after converting their entity relationship model to relational tables. However, because the ER diagram is the logical documentation, this means that any changes made to the relational table design as a result of normalization, should be ported back to the ER model as well. An alternative is to first normalize the ER model and only convert to relational once the design is fully normalized. The downside of this is that the description on the process becomes a bit more complicated. Instead of normalizing every table, we have to normalize every object in the ER model that will eventually become a table. In the case of an IDEF1X diagram, this means we have to normalize all entity types and all Many-to-many relationships. In my coverage of normalization, I try to cater to both groups. Most of my examples will show both the ER diagram and a table. In the explanation, I will usually just use the term table instead of entity type or Many-to-many relationship. This is only for the convenience of the description.

Functional Dependencies
In mathematics, a relationship between variables is called a function. If for each input, either exactly one output or no outputs can be computed. Similarly, we call a dependency between attributes functional if for each input, exactly one output or not output can be found. We say that the output attribute is functionally dependent on the input attribute or that it functionally depends on the input attribute. The word functionally is sometime omitted so if we're sloppy, we might say that an attribute depends on some other attribute. In the case of my Snooker club, birth date is functionally dependent on first name because if you give me a first name, I can look in our membership records and either tell you his or her birth date, or tell you that we have no member with that first name and hence I can't give you the birth date. The official terminology here is to call the birth date attribute the dependent attribute and the attribute it depends on, first name, that determinant. I try to use as little complex terminology and scientific jargon as possible in this course, but I will occasionally use these words to avoid long winded descriptions. Functional dependencies can be mutual, for this you can have situations where there is a dependency from an attribute to another and also a dependency between the same attributes in the other direction, but those are the exceptions, not the rule. Birth date depends on first name, but first name does not depend on birth date. If you give me a birth date, there might be two members who were both born that day, and in that case I could not give you one single first name. In a more realistic example of a club where multiple members might have the same first name, the functional dependency of birth date on first name does not exist. Instead, we find the functional dependency of birth date on the combination of first name and last name. And in yet another scenario, for an organization that can have thousands of members, even that wouldn't hold because there might be members with the same first and last name. These examples already demonstrate three important properties of functional dependencies. First, that they are not universal. Functional dependencies depend on the universe of this course, the world you are modeling. Always be aware of this when you make assumptions or when you think you know what functional dependencies there should be in a data model. The second property we have seen is that functional dependencies cannot automatically be reversed. If attribute A depends on attribute B, we cannot infer that B also depends on A. It could, but it doesn't have to. Finally, we have seen that attributes can be functionally dependent on either a single attribute or on a combination of attributes. Another important property of functional dependencies is that if an attribute X depends on attribute Y, it also depends on any superset of Y. In plain English, this means that for my club where birth date depends on first name, birth date also depends on any combination of first name and one or more other attributes. That makes sense when you think about it. If you already know that any first name will either give you one birth date or a not found result, then surely a combination of that same first name plus another attribute can never return more than one birth date. A consequence of this is that a dependency on a combination of attributes can sometimes be reduced by removing one or more of the attributes from the determinant. In my Snooker club example, the dependency of birth date and the combination of first name and last name, can be reduced to a dependency of birth date and only first name. The more realistic club example, this same dependency cannot be reduced. Any functional dependency that cannot be reduced is called a full dependency. Also, by definition any attribute depends on itself and on any superset of itself. This is obvious, if you give me a first name, I can always tell you exactly what first name you just gave me. And if you give me extra information like a first name and a last name, I can still return exactly one first name to you. This is called a trivial dependency. For the process of normalization, functional dependencies are extremely important, but only if they are non-trivial and if they are full. In other words, we will be working on the set of all functional dependencies of an attribute on one or more other attributes that cannot be reduced. Finding those dependencies can be hard. Sure, most functional dependencies stick out like a sore thumb. They are not the problem, the real problem is finding those pesky few dependencies that are not obvious. In this and the next module, I will present the method that, though tedious, guarantees that you will find all functional dependencies. This method combines finding the functional dependencies with the first normalization steps. So I will explain this method in parts in between explaining the normal forms. Since this method is tedious and time consuming, I do not recommend using it in all cases. Most functional dependencies are so obvious that you will easily recognize them. In practice, you would use this method only to check functional dependencies if you have reasonable doubt, or if you must make absolutely sure that you don't miss anything. Beware of functional dependencies are that caused the derivability. For instance, if you have an entity type sale with attributes Gross Amounts, Tax Rate and Tax Amount, the tests in this method will tell you that Tax Amount is functionally dependent on Gross Amount and Tax Rate. Technically this is true for any given combination of Gross Amount and Tax Rate, exactly one Tax Amount is (____). But because this is a derivation rule, you should not apply the normal normalization rules. If you do, you would end up with a separate table that lists the Tax Amount for each possible combination of a Gross Amount and a Tax Rate, and though that would not violate any normal forms, it would create an unusable database. In this situation, I would simply remove the derived attribute, Tax Amount, because it can easily be computed on the spot when needed, but when you choose to keep a derived attribute in the model, beware that they can cause these special functional dependencies that sometimes have to be treated differently from other functional dependencies.

First Normal Form
A table is said to be in First Normal Form if it has a key, a column that all other columns depend on and every column stores only atomic values. There are two ways for values to be not atomic. The first is when a column or attribute is actually a composite attribute, a combination of two or more attributes. For instance, if instead of first name and last name I had a single attribute full name, it could be a violation of First Normal Form. Why it could be a violation not is a violation because that depends on how the data will be used. An attribute that is composite for one application may be atomic for another. If all I ever do with the first name and last name is stick them together and print them on an envelope for a mailing, I might as well store them combined. However, if I store them combined and find myself frequently having to cut out individual parts, for instance, to personalize a mailing by including the first name, or for sorting the member list on last name, I clearly have the need to use these two attributes independently and they should be stored in separate columns. And taking it a step further, if I built a data model for a psychologist who is researching the correlation between behavior treats and the first syllable of the first name, then the first name attribute itself would not be atomic and should be split into at least an attribute for the first syllable and an attribute for the rest. Violations of First Normal Form by composite attributes are very easy to fix, just break up the attribute into atomic parts. The second case of non-atomic values is called a repeating group. We see an example here, the column phone number originally was used to store the phone number where each member can be contacted. This used to work great, but not anymore. People have home phones, business phones, cell phones, children live some of the days with their father, other days with their mother, etc. so now for many members the phone number column contains a comma separated list of all the phone numbers where we might be able to reach them. This is a classic example of a repeating group, a column that stores multiple similar values instead of just one. Now this is almost always a violation of First Normal Form. Again, why almost always and not always? Because again, it depends on what the data will be used for. If the psychologist I mentioned before extends his research to cover all the syllables in the first name, the first name column would be a repeating group made up of one or more syllables. But for most other applications first name is a perfectly atomic attribute and no other repeating group at all. There is also a special case that is often called a repeating group, though it actually isn't. Instead of having a single attribute and trying to squeeze multiple values in it, this pattern adds multiple copies of the attribute. This is technically not a violation of First Normal Form but it's still a bad design that can result in problems at a later time. However, in this case, the final call depends even more on the exact circumstances. If we may have to store an undetermined number of phone numbers then this design is definitely wrong. No matter how many columns I design, there will always one day be a member who has just one phone number too much and a lot of space will be wasted for all the members who have only a single phone. But if we know for sure there is a fixed upper limit and that limit is low enough to not be (_____), the situation changes. If we only allow our members to specify at most one home phone number and one cell phone number, the design with two attributes for these two numbers is perfectly fine. In cases like this, the best thing to do is to create two versions of the data model and let the developers and DBAs pick the one that works best for them. The remedy for both repeating group patterns is to create a new entity type that has all the key attributes of the original type, plus the attribute that was repeating, also as a key attribute. Of course, the first repeating group pattern can also be converted into the second if the maximum amount of failures is limited and fixed, but remember that this design is often frowned upon and for good reasons. So make sure you only choose this version if you have good arguments for it. The first requirement of First Normal Form there has to be a key in each table, could be considered redundant because it is already required by Carl's Second Rule, but it is a welcome reminder. For me, it is a good prompt that I should now put in the effort of identifying all the candidate keys of the table. The term candidate key is given to each attribute or combination of attributes that can be used to identify individual rows in the table and hence could be chosen to be the key. A second requirement for candidate keys is that no proper subset of its columns is a candidate key itself. The candidate key has to be a minimal set of columns that identifies individual rows. During the normalization process, the term candidate key is often used in a somewhat sloppy way. When a combination of attributes is known to identify individual rows in a table, but not yet confirmed to be minimal. In tables that have more than one candidate key, one of them will eventually be chosen to be the key, called primary key in the relational model and the others will become alternate keys, constrained to be unique. But that choice will be made at a later time, for now, all of those candidate keys will be treated as equals. However, the IDEF1X notation requires one candidate key to be made the key and all other to be made alternate keys, so I have to make a choice and keep in mind that this is just a preliminary choice to satisfy the need of the diagram notation and the tools I use. I will make the actual conscious decision much later. The most reliable way to identify all candidate keys in a table uses functional dependencies. An attribute is a candidate key if every other attribute in the table depends on it and similarly, a combination of two or more attributes is a candidate key if the combination is not a superset of another candidate key and every other attribute depends on either that combination or a subset of it. Let's check out an example. Here we see an entity type with six attributes, A, B, C, D, E, and F. Attributes B, C, D, E and F all depend on A. Attribute E also depends on B and F depends on D. Finally, attributes A, D, and F depend on the combination of B and C. We immediately see that A is a candidate key because all other attributes depend on it, B is not a candidate key, E depends on it, but A, C, D and F don't. However, the combination of B and C is a candidate key. A, D and F depend on that combination and E depends on the only which is a subset of the combination. You see in this example that there is a mutual dependency between the two candidate keys. That is always the case in tables with more than one candidate key, it is a logical consequence of the requirement that all other attributes must be functionally dependent on the candidate key.

Find Functional Dependencies 1: Are Candidate Keys Minimal
We need to identify all non-trivial full functional dependencies for every object in the ER model that will be represented as a table in the relational database. For an IDEF1X diagram that means each entity type and each Many-to-many relationship. There is a procedure for finding all functional dependencies that is easy but long winded. It uses a table with a column for each of the attributes. For an entity type, that includes all key and non-key attributes, including those that are used to implement a relationship, marked as foreign key and IDEF1X. For a Many-to-many relationship, that includes all the key attributes of the entity types the relationship connects. You already know at least one candidate key for the table. If the table comes from an entity type, the combination of all key attributes is a candidate key. If the tables comes from a Many-to-many relationship, the combination of all attributes is a candidate key. Sometimes you have already discovered other candidate keys as well. All candidate keys discovered so far should be marked. How you do that is a matter of preference. A very common way, especially in shorthand notation of a table, is to underline the candidate keys. Sometimes a drawn line is used for the primary key and a dashed line for alternate keys. My preference, especially in the tabular format I am using for this procedure is to drawn an arrow over the candidate key columns. This notation is easier to read when there are overlapping multi-column keys. I sometimes also use these arrows to represent candidate keys in the shorthand notation of tables and columns. By definition, a column or a combination of columns is a candidate key when all other columns in the table depend on it. To save a lot of space and to keep things tidy, I don't explicitly write out the functional dependencies implied by a candidate key. And if at any point I find out that there is a column or set of columns that all other columns in the same table depend on, I will add the candidate key marker and remove all notes I had for the corresponding individual functional dependencies. The first step of the procedure is to verify that all candidate keys are minimal. We do that by following a simple procedure. Single column keys are minimal by definition, so we only have to check multi-column or composite candidate keys. The procedure is executed once for each such key. For the composite candidate key under consideration, we try to populate the table with two rows in such a way that the first left most column of the key is different and the other columns of the key are the same. Here you see that I use a combination of a letter or numonic and a number to depict different or equal values for an attribute and that I use question marks for sales where I don't care what, if any value goes there. Remember that all the values here actually represent facts. Our task is now to find a population, a set of facts that satisfies all business rules already known and that includes this pattern. More rows may be added as long as there is at least one set of two rows that shows the required pattern. In many cases, we will already have such an example in our collection of valid examples. If not, we have to create it. If that succeeds, we need to represent those facts in the notation that the subject matter expert is familiar with, verify if he or she considers this a valid example and if not, check if that is because of the pattern we are testing or for another reason. If it turns out to be impossible to create an example that includes the pattern to test and that is deemed valid by the subject matter expert, we can conclude that the first candidate key column depends on the other candidate key columns, either by a normal functional dependency, or because of a derivation rule. A logical consequence of this is that all columns in the table depend on those other candidate key columns. The original candidate key was not minimal, it should be removed and replaced by a new candidate key that does not include the left most column. Regardless of whether we found a functional dependency or not, we always have to repeat this same procedure for each of the columns in the original candidate key. So I now do the same procedure with a difference in the second column and the other key columns equal, then the third and so on until I reach the last column. The order in which you do these tests is not relevant since you have to do them all. I always work left to right to make sure I do not accidentally forget a test. If no functional dependencies were found during the entire procedure, the original candidate key was already minimal. If there were one or more dependencies found, the original candidate key has now been replaced by one or more new candidate keys, all including one less attribute. If these new candidate keys still include two or more attributes, the procedure has to be repeated for each of them because they might still not be minimal. If the table has other composite candidate keys as well, they too, need to be tested. Once this procedure is finished for all candidate keys in the table, you can be sure that they are all minimal. The original candidate key of an entity type that was created as an artificial entity type, is often not minimal. That is a result of what I described before as a sludge hammer approach. Just stow all attributes in the candidate key of the first draft and rely on this procedure to sort out the mess. In entity types that started as first class citizens, you will sometimes also find that you can reduced a candidate key, but not quite as often. In tables that are formed from a Many-to-many relationship, the original candidate key should be minimal. That doesn't mean you can skip this procedure. However, if you find you have to replace the original key with a smaller one, you should use this as a sign that your entity relationship model is flawed and improve that, either before continuing or after completing the normalization process. In these cases the best approach is usually to reverse engineer the entity relationship model from the relational tables after completing all normalization steps.

Find functional Dependencies 2: Missing Candidate Keys
The next step in the procedure to find all functional dependencies is to check if we have not missed any candidate keys. Luckily, candidate keys are easy to find because by definition, all attributes depend on a candidate key, we can never have two different rows with identical failures in all columns of a candidate key. So any valid example with duplicate failures for one or more attributes is sufficient to prove that those attributes, and their combinations, are not candidate keys. Conversely, if it's impossible to create a valid example with such a population, that attribute, or combination of attributes, has to be a candidate key. To test for missing single column candidate keys, we try to populate the table with two rows, with equal values in the first column that is not part of any candidate key. All other columns are irrelevant, but all known business rules must be observed, so obviously there has to be difference in at least one column of each of the already known candidate keys for this to be a valid combination. It's irrelevant which and how many of the candidate keys columns are different. Extra rows may be added as required as long as we end up with a population that includes this pattern, satisfies all known business rules and that represents an example that the subject matter expert deems (_____). If we manage to create such an example, or if we already have one, the column with the duplicated data is not a candidate key. However, if we can't create any valid example with this pattern no matter what we try with the columns that are irrelevant, we must conclude that the tested column is a candidate key and mark it as such. In either case we then repeat this test for the second non-key column, then the third and so on, until all non-key columns have been tested. As before, using a fixed order helps to prevent accidentally skipping one. After testing all single columns that could be a missing candidate key we have to test all combinations of two or more columns that could be a composite candidate key. Combinations that include all columns of another candidate key should not be tested. Because of the inclusion of the candidate key, we already know that all other columns depend on this combination and we also know that this is not a full dependency. So we are not interested. Combinations of columns that all are included in a single larger candidate key can be skipped as well. We already verified that all candidate keys are minimal so we don't have to test these again. This means that for the tests and combinations of two columns, which we'll do first, we only have to consider combinations of two non-key columns, combinations of a non-key column with a column that is part of a multi- column key and combinations of two columns that are part of two different multi-column keys. To ensure that I don't forget any combination, I work systematically. I start with columns one and two and check if they meet the criteria for testing, if they do, I test the combination. I then repeat this for columns one and three, then one and four and so on. After the combination of the first and last column I continue with the combination of columns two and three, then two and four and so on, you get the idea. The actual test for these combinations is the same as for single column candidate keys. Try to populate the table with two rows, with the same value in the two columns to be tested and whatever you want in all other columns, as long as no known business rules are violated. If we are unable to create such an example that the subject matter expert considers valid, the tested combination is a candidate key, and because we already tested the individual attributes, we can be sure that this candidate key is minimal. After testing all combinations of two columns, we have to do the same for combinations of three columns. Again, excluding those combinations that include all columns of another smaller candidate key, or that are themselves fully embedded in a larger candidate key. And again, we know when we find a candidate key that it has to be minimal. After that come combinations of four columns, then five and so on. This may sounds like an awful lot of work, but you will find that in many tables you pretty quickly reach the point where every possible combination of columns always includes all columns of a candidate key, at which point you can stop testing. However, most databases also include a few tables that have dozens of non-key columns and no combination of those columns qualifies as candidate key. For those tables, this procedure can indeed force you to check an astronomical number of permutations. Here, you can save a lot of time by using an alternative approach. In this case, you start with a combination of all columns for which you expect not to find any candidate keys and test that complete combination. If you can create or already have a valid example that is represented in the table by two rows with the same value in all those columns, then you know that this combination is not a candidate key. And that immediately implies that none of the possible subsets of this combination can be a candidate key, saving you a lot of time and effort because now none of these combinations need to be tested anymore. However, combinations of one or more of the columns you just tested with one or more other columns still may be a candidate key and hence have to be tested. You can of course, choose to test another combination of many attributes to quickly weed out most of the remaining possibilities. If it turns out not to be possible to create this example, then the combination might be a candidate key, but that's not sure yet, it might not be minimal. The procedure now is to first assume a candidate key on all the tested columns, but then check if it's minimal using the test procedure described in the previous section. This can be very time consuming since it almost always turns out to be a far from minimal candidate key and the procedure lets you do a lot of work to shave off the extraneous columns one by one. So that is the tradeoff you make, try to save a lot of time on the test for missing candidate keys by combining attributes you assume to be non-key with the risk of having to do a lengthy extra test to find the minimal candidate keys if your assumption was wrong. Or you can choose a mixed approach where you first check if any of your existing valid examples happens to have a pattern with multiple rows with identical values in one or more columns. These column combinations and all their subsets can't be candidate keys. You can then use the normal procedure, starting with individual columns and then moving on to combinations of two, three, etc., but you can now not only skip all columns and columns combinations that are a subset or a superset of an existing candidate key, but also all combinations that are a subset of any combination known not to be a candidate key. This of course requires that you carefully track not only the candidate keys you found, but also the combinations you already ruled out, as you will see in the demo.

Demo: Verify that Attributes are Atomic
To see this first step of the procedure in action, let's return to one of the examples I used earlier, this form, with the results of a competition match. In Module 3, I used this form to demonstrate how to make the subject matter expert read the information from an example and how to transform that reading into facts. For shortness sake, I only read part of the example in Module 3, but let's assume that after I finished recording the module, I did go on to work on the rest of the example, and that I later also found that the person who runs the competition not only collects and tabulates the data from these forms, but also adds two extra columns showing how many competition points each of the players has earned in the match. After analyzing all the facts coming from reading those examples, I created the first draft of the IDEF1X ER model that looks like this. Note that I added entity types for player and league so that I can represent the foreign keys, but they are just stops, they are very incomplete. Finalizing these entity types is out of scope for this demo, we will focus only on the competition match entity type. I want to make sure that I get it completely right, so let's follow the procedure to find the functional dependencies and to properly normalize this entity type. Before I can even start the actual procedure, I must check if all attributes are atomic. That check consists of three parts, check for composite attributes, check for multi-valued attributes and check for multiple copies of an attribute. Most of the attributes in this entity type are obviously not composite. The only exception is the date attribute which might have to be split into its components, year, month and day, if these components are often accessed individually. So if the club frequently creates overviews of the best players per month, or the worse player on every third day of the month, I should consider splitting this attribute. In our (_____) club we only make overviews of all the competition games played so far, so there is no need to split this attribute. And in practice, a date is often considered a special attribute that most designers will still not split even when its components are sometimes individually accessed for reporting purposes. Two attributes are multi-valued, Frame Scores Player 1 and Frames Score Player 2. The example form I have shows that three frames have been played, so we need a comma separated list or something similar to record all the frame results. One possible way to fix this is to create separate attributes for each of the three frames' results as shown here, but that is not a very good way to fix this. The examples I have all show three frames, but how representative are they? In fact, matches in the A League are always played over five frames, so this data model cannot even represent these matches. The correct way to fix this is to create a new entity type, competition frame, to store the results of one single frame. Every competition match can then be associated with three, five or any other number of frames as needed. Checking for repeated attributes I see that Frames 1 Highest Break and Competition Points are all repeated for each of the two competing players, it is possible to move these into a separate entity type. But this situation is different from the frame results we saw before. In Snooker, the number of frames is not fixed, matches can have from three frames for Club Snooker, all the way up to 35 for the final of the world championship, but the number of players is fixed, a Snooker match is always between two players and that cannot be changed without changing the rules so much that it wouldn't be Snooker anymore. So this definitely meets the criteria that the number of players is both fixed and low, meaning that the original design is not wrong. Both are valid alternatives and both should be normalized and presented. For this demo, I focus on the original design only.

Demo: Verify that Candidate Keys are Minimal
Now I know that all attributes are atomic, it's time to start to check if all candidate keys are minimal. I start by creating a table with a column for each of the attributes in the entity type. In this table, I mark the candidate keys I have identified so far. In this case, I only have one candidate key at this time, the combination of attributes, Date, Player 1, and Player 2 that was marked as the key of the entity type when I created the first draft of the data model. For each of the tests to come, I have to populate the table with two or more rows. I can use made up data only, but I believe in avoiding unnecessary work, so for the first row I'll simply use the score form I already have and copy its data in the first row. Since this is from an example given to me by the subject matter expert, I can expect that this row in itself is valid. I will now add a row such that the first column of the candidate key is different, that other columns of the candidate key have the same value and the remaining columns can be anything I want as long as they don't violate any known business rules. I then have to modify the original example, or create a new one to represent this data in the representation form the subject matter expert is familiar with. In this case, that means creating a new score form for the second row and presenting the two forms as a set. Note that even though the columns for the frame results are not in the table, I still have to populate some data there, otherwise, the example would be rejected because of the missing frame results and I would not get the information I actually need. I then give these two competition score forms to the subject matter expert, making sure that she notices that these are for the same two players, but on different dates. She grins and tells me that the second form by itself is already wrong, so she can't really evaluate the combination. The highest break of 180 is impossible, in Snooker, the theoretic maximum is 155, and even that will never happen in our club. Also the numbers for frames 1 on the form contradict the individual frame results. I make a note of this, as this is important information about business rules that I will need later, but at this time I am trying to find out if the same two players can play matches on different dates. So I change the second row of data in the table to correct these errors, transform it to a score form and hand the set of forms to the subject matter expert, again ensuring she knows this is the duplicated names. She then tells me that this version is okay. We don't have very many players in each league so everyone has to play each of the other players three times in a full competition. Data like this is fully expected. This means that the combination of just Player 1 and Player 2 is not a candidate key for this table, but it is too soon to conclude that the original candidate key is minimal, there are still the other subsets to check. For the second check, I again start with the original example as the first row. This time I create the second row with a different value in the second column of the key, identical values in the other key columns and whatever I like in the remaining columns. Because I am lazy, I use the same values as before in those remaining columns, but I still have to ensure that this does not violate any business rules I already know. I create a representation of this example in the notation the subject matter is familiar with and present it to her. She tells me that this combination of competition score forms can never be valid. I ask her why and she says that it's because Jim can't play two matches on the same day. The Snooker club is only open from 8:00 until 11:00 p. m. and the average competition match takes two hours, so club members can never play two competition matches on the same day. This explanation helps me understand that the rejection of this example is not because I accidentally violated some other business rule, but because any combination of Date and Player 2 can only be associated with at most one Player 1 value. Player 1 depends on the combination of Date and Player 2, which in this case also implies that the candidate key on Date, Player 1 and Player 2 is not minimal, it can be reduced to a candidate key on only Date and Player 2. The original candidate key should be removed, but not immediately. I still have to test the last of three possible three subsets of this key. I used the same procedure as before, now with a difference in the third and last of key columns, no difference in the other columns and any data (_____) in the rest. After presenting this in familiar form to the subject matter expert, I am told that this example isn't valid either. Not a surprise as I already knew that no one can play two matches on a single day. In a real world situation, I wouldn't even have bothered the subject matter expert with this example, I only showed it here to demonstrate the procedure. The symmetry in this example is typical for this example only and will not occur in most other situations. Based on the subject matter expert's rejection, I now conclude that there is also a candidate key on Date and Player 1 and I can now remove the original candidate key as I tested all possible subsets and demonstrated it not to be minimal. This would also be a good time to make the appropriate changes in the IDEF1X model, instead of just a single key of three attributes, I now have two keys of two attributes each. The IDEF1X notation forces me to pick one as the key and make the other an alternate key, but the final choice on that should not be made until we are ready to actually implement the database because this choice can effect performance in ways that depend, among other things, on the chosen relational database management system. The choice made here is purely preliminary. It is important to remember that while I may have proven that the original candidate key was not minimal, I have not proven that the two replacement keys are. They too, might be not minimal, so I have to repeat the procedure for each of the two new keys. Let's start with the key on Date and Player 1. I start with a population of two rows with a difference in the first of the key columns and no difference in the other key column, but this time I do not need to create a new example for that. I already have an example that contains this pattern. I created it for the first test of the original candidate key and the subject matter expert told me it is fine. So I don't need to create a new example, or consult the subject matter expert again, I can immediately conclude that there is no candidate key on only the Player 1 attribute in this entity type. For the second test of the candidate key on Date and Player 1, I do need to make a new test, since I do not yet have a valid example with a different value for Player 1 and the same value for Date. I did have an example with this pattern that was not valid because it also had the same value for Player 2. But now I can put anything I want there, so let's see what happens if I put in a different value there, represent the population of the table as a set of two score forms, and consult the subject matter expert. Luckily, she smiles and says that this is okay, we have seven Snooker tables at the club, so it is perfectly normal to have several competition matches between different players going on at the same time. I conclude that Player 1 does not depend on Date and hence Date by itself is not a candidate key. The candidate key on Date and Player 1 combined is minimal and can remain. But I also have to test the second new candidate key on Date and Player 2. Luckily, I can finish this test very quickly, for the first test with different Date and equal Player 2, I can again fall back to the valid example I also used for the first test of the original candidate key and the first test of the candidate key on Date and Player 1, since that example also includes this pattern. Date does not depend on Player 2 so Player 2 by itself is not a candidate key for this table. Finally, for the last test, I need to use a pattern with different values for Player 2 and equal values for Date. And for this I can reuse the example I used for the second test of candidate key on Date and Player 1. I know that this is a valid example, so there is no candidate key on Date only. Of course, I already knew that from testing the candidate key on Date and Player 1 so I could even have skipped this test. Candidate key on Date and Player 2 is minimal. All candidate keys are now checked and known to be minimal. This part of the procedure is finished.

Demo: Check for Missing Candidate Keys
For the test for missing candidate keys I choose to use the mixed procedure. So I do not immediately start making new examples, instead, I first investigate all valid examples I already have, both from the initial fact readings and the ones I made while checking if candidate keys are minimal. This is the first correct example I made when testing for minimal candidate keys. Upon reviewing the population, I see that there are duplicate values in four columns, Player 1, Player 2, League and Highest Break Player 2. Since this is a valid example, I now know that the combination of these four columns is not a candidate key, which also implies that none if its subsets can be a candidate key. I mark the combination of these four columns as not being a candidate key so that I remember that I don't have to test this combination or any of its subsets. You can use different ways to mark this, in this case I choose to use a different color since using crosses as I did before would clutter the screen too much as I start to rule out more combinations. The second valid example from testing minimal candidate keys has duplicates in a different combination of columns, Date, League and Highest Break Player 2. So I mark the combination of these three columns as not being a candidate key too. Since I was able to reuse these two examples over and over in the previous demo, these are all the valid examples I have found so far in the course. In a real situation, I would probably have started my analysis with more examples and I will use those too. So to make this demo more realistic, let's just assume that I also found this valid combination of score forms at the start of the project with these accompanying entries in the spread sheet of the competition leader. I copy the facts represented in this example in the table for easy reference, allowing me to see that the same values appear in a combination of four columns, Frames 1 Player 1, Frames 1 Player 2, Competition Points Player 1 and Competition Points Player 2. So this is the third combination of columns I can mark as not being a candidate key. Let's assume that all other existing examples I have either have no duplicate values or have duplicate values only in column combinations that are subsets of the combinations already marked. So this is all I can conclude based on existing valid examples, and now I have to test all remaining possibilities and I will start that procedure with single column candidate keys. Most columns cannot be a candidate key, either because they are part of a candidate key known to be minimal, or because they are part of a combination of columns that is known not to be a candidate key. The only column that needs to be tested is Highest Break Player 1. So I try to populate the table with two rows, such that both have the same value for Highest Break Player 1, and no known rules are violated. The easiest way to do this is to start with an existing valid population and change one of the Highest Break Player 1 values to be equal to the other one. After representing this in her familiar notation, I consulted the subject matter expert, who tells me this is okay. From this I can conclude that there is no candidate key on the Highest Break Player 1 column by itself. However, because I started from a valid example that already had some duplicates and didn't have to change any other columns after introducing the duplicate Highest Break Player 1, this new valid example actually has duplicate values in five columns. So I can now also conclude that there is no candidate key on this combination of five columns. Adding the appropriate marker will save me a lot of tests on combinations of two or more columns. Since this marker implies that none of its subsets can be a candidate key, the markers of no candidate key on Highest Break Player 1 and no candidate key on the combination of Frames 1 Player 1, Frames 1 Player 2, Competition Points Player 1 and Competition Points Player 2, are now implied by this new marker so they can be removed. I now have to test combinations of two columns. With two candidate keys already known, and three combinations ruled out, it is not immediately obvious which combinations still need to be tested. So the best I can do is to systematically look at each possible combination and I will do that left to right. The combination of Date and Player 1 is already known to be a candidate key so I don't have to test it again. The same goes for the combination of Date and Player 2. The combination of Date and League is a subset of a group of columns known not to be a candidate key so I can skip this combination too. The combination of Date and Frames 1 Player 1 however, is not a superset of a candidate key, not a subset of a candidate key, nor a subset of a combination that was ruled out as a candidate key. This means that I will need to test this combination. I start with the population of the last used valid example, modify it so that there are duplicate values in the two columns being tested and present this to the subject matter expert. She thinks this is just fine so I now know that the combination of Date and Frames 1 Player 1 is not a candidate key. And like before, my testing has introduced a larger combination of columns with duplicate values. So I immediately rule out the candidate key over that entire combination and all its subsets. The markers that are implied by this new marker can safely be removed. Because of this new no candidate key marker I don't have to test the combination of Date and Frames 1 Player 2, or the combination of Date and Highest Break Player 1. The combination of Date and Highest Break Player 2 is ruled out by another combination and the new marker again rules out the combinations of Date and Competition Point Player 1 and Date and Competition Points Player 2. The combination of Player 1 and Player 2 is also ruled out because it's a subset of a combination known not to be a candidate key. The same known non-key combination also rules out the combination of Player 1 and League. But the combination of Player 1 and Frames 1 Player 1 does need a new test. Like before, I start with the population of the last valid example and change it to include the pattern we are looking for with two rows with the same values for Player 1 and Frames 1 Player 1. However, by making this change I have introduced a violation of a business rule I already know about. The candidate key on Date and Player 1 is violated by this combination of two rows with the same value and all the columns of this candidate key, so I can't use this example for my test. I of course, still have the option to create a new example from scratch or I can pick another valid example to modify, but in this case I choose to resolve the candidate key violation by simply changing one of the two date values. The example now conforms to all known business rules so I can present it to the subject matter expert. She tells me this is not okay. Each score form on its own could be correct, but the combination cannot. It is tempting to now immediately conclude a candidate key on the combination of Player 1 and Frames 1 Player 1, but that would be a mistake. There are duplicated columns in the example I presented in the subject matter expert, so it is possible that the actual candidate key that is being violated is on a larger combination of columns like Player 1, Frames 1 Player 1 and Highest Break Player 1, or even on a different combination like Player 1 and Competition Points Player 2, or maybe a completely different business rule was violated. So I ask the subject matter expert for an explanation and she points out to me that players cannot switch leagues during a competition season and since we build a database for a single season, Katie can never play matches both in the B League and in the C League. A player is always associated with exactly one league. So I stumbled upon a functional dependency I was not testing for, league depends on Player 1. I make a note of this dependency so that I don't have to repeat this test later, but I still need to test the combination of Player 1 and Frames 1 Player 1. I find if by changing the value for league in one row, I can resolve the violation of this business rule without introducing violations of other known rules. So I make the corresponding change to the example as well and consult the subject matter expert. She agrees that this combination of score forms could be valid. This does of course mean that there is no candidate key on the tested combination, but since there are more duplicated columns, I immediately conclude that there is no candidate key on the combination of all of them as shown here. I quickly check to see if any of the existing no key markers can now be removed, but that is not the case since none of them is a complete subset of the new no key marker. I will not show the remaining tests with the same level of detail, but I will quickly walk you through the last steps of the procedure. All remaining combinations of Player 1 with another column are a subset of either this new non-key marker or one of the other known key markers, so they don't need to be tested. The first combination that I do need to test is Player 2 and Frames 1 Player 1. The test population I use results in a valid example so I add a non-key marker over all duplicated columns and remove the non-key marker that is now redundant. I can then skip a lot of combinations, but I do have to test the combination of Frames 1 Player1 and Highest Break Player 2. The example is starting to get extremely unlikely with two matches with the exact same results, but it is possible so I get a new longer non-key maker. Two other non-key markers are now redundant and can be removed. All remaining two column combinations are a subset of this new combination, so I move on to the three column combinations. All those involving both Date and Player 1 or Date and Player 2 are a superset of a candidate key and can be skipped. But the combination of Date, League and Frames 1 Player 1 must be tested. When I again use the last used example and make the two date values equal, I now have two completely identical rows, which of course violates the two already known candidate keys. To solve that, I change the values for Player 1 and Player 2, now the candidate keys already known are not violated. This corresponding example is again unlikely, but possible, so I again add a new non-key marker and remove the non-key markers that are now redundant. I check the remaining combinations of three columns and then all combinations of four or more columns, but every combination I try is either a superset of a candidate key, or a subset of one of the known non-key combinations. In other words, all tests for this part of the procedure are now complete and I can be 100% sure that I have found all candidate keys. In reality, based on my experience and my knowledge of Snooker, I would have expected that I wouldn't find any extra candidate keys. So I would probably have taken the shortcut of immediately verifying the two long non-key combinations you see here. That would have taken me only two tests to verify that there are no missing candidate keys. But if my assumption had been wrong and there was a missing candidate key after all, I would have to do a lot more work to find the exact columns for the missing candidate key.

Summary
In this module you started to learn the two skills that arguably are the most important for anyone involved in data modeling and database design. Identifying functional dependencies and normalizing the data model. An initial data model can suffer from problems such as redundancy, or modification anomalies. Fixing that kind of problems is the goal of normalization. There are a lot of normal forms, but only the first three are generally considered an absolute requirement for a good database design. Often, knowing the value of one or more attributes is sufficient to know exactly what, if any, the value of another attribute is. This is called a functional dependency. Since normal forms are defined in terms of functional dependencies, you must know all functional dependencies in order to normalize your design. First Normal Form dictates that every column has to store only a single value. Composite attributes are not allowed and neither are repeating groups. Multiple copies of the same column are often also considered a repeating group and can be an indication of a bad design. First Normal Form also implies that every table has to have at least one candidate key and that all other columns in this table depend on this key. The next module will continue the lesson on finding functional dependencies and normalizing database design by covering Second and Third Normal Form.

Basic Normalization - Part 2
Outline
Hello, my name is Hugo Kornelis and I am presenting this Pluralsight course on Relational Database Design. This is Module 7: Basic Normalization (Part 2). This module continues the process of fine tuning the design of a database that we started in the previous module. At the end of the previous module, we had a design in First Normal Form and we had made a start with finding some of the functional dependencies. In this module, we will continue our hunt for the rest of the functional dependencies and further normalize the design. First to Second and then to Third Normal Form.

Second Normal Form
For a table to be in Second Normal Form it has satisfy two requirements. The first is that it has to meet all the requirements of First Normal Form. The second additional requirement is that attributes that are not part of any candidate key must depend on the whole key. That is, they may not depend on only a part of a candidate key. The second requirement of Second Normal Form only applies to attributes that are not part of any candidate key and these attributes can only depend on part of a candidate key if there is at least one candidate key on two or more attributes. So if a table has only single attribute candidate keys, or if all attributes in a table are part of a candidate key, you can skip the test on Second Normal Form for that table. If it is in First Normal Form it is automatically also in Second Normal Form. Let's look at an example again. In the previous module I introduced an entity type with six attributes, A, B, C, D, E, and F. Attributes B, C, D, E, and F all depend on A, Attribute E depends on B, F depends on D and A, D, and F all depend the combination of B and C. We already established that there are two candidate keys A and a combination of B and C. One of the two candidate keys is on more than one attribute. So we need to check for violations of Second Normal Form. Attributes A, B and C are all part of a candidate key and can thus be excluded. We do need to check the other three attributes. Attribute D depends on A and on the combination of B and C. Those are the candidate keys, not subsets of them, so no problem here. Attribute F also depends on A and on the B, C combination, but additionally also depends on D. Since D is not a subset of any of the candidate keys, this too is not a Second Normal Form violation. However, Attribute E depends on A and on B. B is a subset of the B, C combination. This is not allowed in Second Normal Form so this table is not in Second Normal Form. Once you know that a table is not in Second Normal Form, you need to fix it. That is actually pretty simple. First you create a new table, an artificial entity type in the ER diagram that holds all the attributes in the offending functional dependency. So both the dependent attribute and the determinant. If more attributes depend on the same determinant, they all go in this table. The attribute or set of attributes that is the determinant in the offending functional dependency is a candidate key for the new entity type. As with all artificial entity types, naming it can be a challenge, and though unusual, this new entity type itself could violate Second Normal Form, so you will need to check for that. Once the new artificial entity type has been added, all the dependent attributes of the offending functional dependencies can be removed from the original table. The determinant is not removed, but becomes a foreign key that implements a relationship between the original and the new entity type. This will always be a One-to-many relationship with the new entity type as the parent. But all permutations of minimum cardinalities on both sides are possible. If the composite candidate key that they dependent attribute belongs to is an alternate key in the IDEF1X diagram, the relationship will be a normal one. If it is the primary key, it will be an identifying relationship and the original entity type becomes a weak entity type. After removing Attribute E from our example entity type, there are no other functional dependencies of a non-key attribute on a subset of a candidate key, so it is now in Second Normal Form. The new entity type should still be checked, however, since it only has a single column candidate key it can never violate Second Normal Form.

Find Functional Dependencies 3: Dependencies on Subsets of Keys
Once we know that all candidate keys are minimal, it is time to systematically look for all functional dependencies on a subset of a candidate key. This includes all violations of Second Normal Form plus a few extra functional dependencies. The procedure is very similar to the method we already used to verify that our candidate keys are minimal. For this step we again only have to consider composite candidate keys, so you can skip it for tables that have all their candidate keys on single columns. You can also skip this step for tables that have one single candidate key on all columns, which should include all tables formed from Many-to-many relationships. Finally, if a table has exactly one composite key and single column keys on all other columns, you can also skip it. For the remaining tables, you will have to do some tests, and you will in fact have to do them for each composite candidate key. So for a table with two composite candidate keys, you have to perform the procedure twice, once for each composite candidate key. For the key under consideration we populate the table with two rows, with different values in the first column of the key and identical values in all other columns of the key. We already know that this is possible because this population was used to verify that the candidate key is minimal. However, we now also try to ensure that there are different values in one of the columns that is neither part of the candidate key we are testing, nor a single column candidate key. Optionally, you can combine these tests, making a single example with differences in multiple such columns. That can save time if there are no functional dependencies, but if there are you have to do extra work to find out exactly which of these columns is or are functionally dependent on the subset being tested. If it is impossible to create an example that includes the test pattern, satisfies all currently known business rules and that is deemed valid by the subject matter expert and if this is because of the required pattern and not because of an accidental violation of another business rule, then the column with the difference is functionally dependent on the subset of the candidate key that has no difference. This can be a normal dependency or a derivation rule and we need to make sure to find out which of these cases applies. Do not yet try to resolve violations of Second Normal Form, we will do that later when all tests are concluded. Just make a note of the dependency you found and continue testing. Once we tested all columns we need to test we also need to test for dependencies on other subsets of the same candidate key. So now we need to repeat the process for all except the second of the candidate key columns, then all except the third etc. And then you have to do the same for all the other multi-column candidate keys in the table. Once this procedure is completely finished, you know all functional dependencies of columns on part of a candidate key, but they may not be full dependencies. If for instance, the table has a candidate key on four columns, and a non-key attribute is functionally dependent on not three, but only one or two of them, the procedure will not find that dependency. Instead you will find implied dependencies on all combinations of three columns that are a superset of the actual determinant and a subset of the key. So if we find multiple dependencies within the same composite key and with the same dependent attribute, we have to check if these are actually caused by a dependency on a common subset. This is only possible for a subset for which every possible superset has been found. To clarify, let's say we found these three dependencies. All possible subsets of three columns out of A, B, C, and D that include A are there. So it is possible that these three are all caused by a single dependency on A only. However, all three column subsets of A, B, C and D that include both A and B are included too. So another possibility is that there is an actual fully dependency of E on A and B combined. Note that this implies only two of the three dependencies we found, so there has to be one extra dependency in this case. Similarly, we can also have a full dependency on A and C, or on A and D, but that's not all, there can also be combinations. For instance, the three dependencies we found can also be implied by these two full dependencies, one on A and B and one on A and C. Are any of the other combinations of two, two column dependencies, or even all three? None of the determinants overlaps with any of the others so these three full dependencies can all coexist. However, based on these three dependencies found, we do know for certain that there will not be a dependency on for instance, the combination of B and C, or on B only, even though B is a common subset for two of the dependencies found. Because if there had been a dependency of E on B and C, or on only B, we should have found the implied dependency of E on the combination of B, C and E. We have not found such a dependency, so obviously E cannot depend on B, or on the combination of B and C. If you find multiple dependencies with a common subset, and all supersets of that subset are included, you still have to verify if a dependency on that subset actually exists. And you have to do that for each such subset. For each such test you create one more example with the same values in only that subset, and different values in all other columns of the candidate key and in the dependent column. If this is impossible, the dependent column is indeed dependent on the subset you tested and the dependencies on its superset are not full and can be removed, otherwise it was just coincidence. Keep looking for possible subsets until all are either confirmed or ruled out and you can be sure that all remaining functional dependencies are full. Once you know every full dependency of a column on a subset of a candidate key, you need to deal with them, but only with normal dependencies. Those caused by a derivation rule can be disregarded for now. For the remaining dependencies, there are three possible situations. The most common is where the dependent column is a non-key column, that is it is not part of any candidate key. This would be a violation of Second Normal Form so you will need to remove this column from the entity type using the procedure described before, create a new entity type with the determinant of this dependency as the key and the dependent attribute as a non-key column, then remove the dependent attribute from the original entity type. If multiple non-key attributes depend on the same subset, they can all go to the same new entity type, but if multiple non-key attributes have dependencies on different subsets, they go to separate new entity types. It is far less common to find that the column that depends on a subset of a candidate key, is itself part of another candidate key, but it can happen. Since this is not a violation of Second Normal Form, you don't have to do anything about this, just make a note so that we can deal with the issue later. The last situation is extremely rare, but I am including it for completeness sake. If a non-key column depends on a subset of a candidate key and also depends on another subset of that same or another candidate key, you can choose to model it in different ways, but they have their problems. The most logical way is to create two new entity types for each of the two dependencies and remove the dependent attribute from the original table, but in this case one of the new entity types is actually redundant and you could populate the entity types in a way that would violate the dependencies. If you instead create just one of the two entity types, you get rid of the redundancy, but you expose the data model to a different kind of inconsistency. A third alternative, simply leaving all attributes in the original entity type and accepting the Second Normal Form violation would not be my first choice. But you could run into a situation where it has the least problems. The final choice depends on several factors, including how the data will be used and whether there are also dependencies between the two subsets that the non-key attribute depends on. Once you have handled all the functional dependencies you found in this section, you can be sure that both the table you tested and all new tables you created during this process are in Second Normal Form.

Demo: Dependencies on Subsets of Candidate Keys
Let's continue our demo from the previous module with a check for Second Normal Form violations and other dependencies on a subset of a candidate key. After fixing the First Normal Form problems, the competition match entity type now has two candidate keys on two columns each and a bunch of attributes that are not part of any candidate key. This means that I cannot skip this table for the test on dependencies on subsets of keys. Since there are two composite candidate keys, I'll have to walk through the test procedure twice. I'll start with the key on Date and Player 1. I have to create populations with two rows, with a different value for Date, identical values for Player 1 and different values in the columns that are neither part of this key, nor subject to single column key. Now I may already have such examples so I first check my stack of previously used examples. In this case, I find four examples with the desired pattern that were accepted by the subject matter expert. Here is the first one, I created this example during the procedure to verify if the candidate keys are minimal and the subject matter then told me this example is okay. It has the required pattern in the columns of the candidate key I am testing and it has different values in several of the other columns, Frames 1 Player 1, Frames 1 Player 2, Highest Break Player 1, Competition Points Player 1 and Competition Points Player 2. For all these columns we can conclude that they do not depend on Player 1 by itself. We cannot draw any conclusions yet for the three remaining columns, Player 2, League and Highest Break Player 2. The second example that has the required pattern and is valid was made while testing for missing candidate keys. Based on this example, we can now conclude that both Player 2 and Highest Break Player 2 are also not dependent on Player 1. The last two examples are incomplete in this course because I made them when quickly walking through the last part of the check for missing candidate keys, but that doesn't really matter since they don't help me anyway. They are the same as the second example, but with identical instead of different values for Player 2 and for Highest Break Player 2. None of the valid examples I have at this time includes the desired pattern for Date and Player 1 and different values for the League column. This isn't really a surprise, during the test for missing candidate keys I already discovered, by accident, that League is functionally dependent on Player 1. If I had not stumbled upon that, I would have needed to test for this, but now I don't need to test since I already know the outcome. Since all non-key columns are now checked, I am ready with the first pattern of the first composite candidate key. The second pattern to test for this first candidate key has identical values for Date and different values for Player 1. I again start by searching my stack of previously used examples and I now find three examples. The first example which comes from my tests for minimal candidate keys looks very much like one of the examples I used for the first pattern. That is no coincidence, both out of laziness and to minimize the chance of examples being rejected because of nonsensical data, I always make new examples by changing an existing example as little as possible. So I often will find examples that look very similar. Anyway, the columns of the candidate key under consideration have the required pattern in this valid example and there are different values in Player 2, Frames 1 Player 1, Frames 1 Player 2, Highest Break Player 1, Competition Points Player 1, and Competition Points Player 2. So none of these attributes depend on that. The second example with the required pattern, which I made when checking for missing candidate keys, has different values in the two remaining non-key columns, League and Highest Break Player 2. So those attributes do not depend on Date either, and I don't even need to look at the third example anymore. That concludes all the tests for the first candidate key, but I still have the second one on Date and Player 2 to deal with. The procedure is the same as before, I first test the pattern that has different values in Date and identical values in Player 2 and I again start by checking all the examples I already have. The first valid example I find that has the required pattern, happens to be the same example I also used for the first pattern of the first candidate key. Because it has duplicates in both Player 1 and Player 2 I can use this example here as well, and because it has different values for Frames 1 Player 1, Frames 1 Player 2, Highest Break Player 1, Competition Points Player 1, and Competition Points Player 2, I now know that all those columns do not depend on Player 2. Now I have more examples with the required pattern, but let's assume that this is not the case so that I can demonstrate that part of the procedure as well. I still need to verify three columns, Player 1, League and Highest Break Player 2. These three are not subject to a single column candidate key and examples considered so far didn't show me whether they depend on Player 2. I can test them using three different examples, all with the required pattern in the Date and Player 2 columns, different values in the column being tested and whatever I like in the rest. But I decide to try and save some time and effort, so I make a new example with different values in all those three columns. I check business rules, transform into the familiar notation of the subject matter expert and then ask for her opinion. She immediately objects because, as she puts it, the idea that Jim will ever play in the B League is simply too silly for words. This is a situation that you will often run into when discussing made up examples with a subject matter expert because they live very close to the data, they tend to check if the example is correct, not if it could be correct. Sometimes you can avoid that by replacing names of existing people, products or whatever with made up names. If I had used Carlos in this example, who isn't a club member, she would not have objected based on his skills or lack thereof, but luckily I managed to explain to my subject matter expert that I only want to know if an example cannot be valid, not if it should not be valid. Based on this understanding, she now tells me that the individual score forms could in theory be valid, but the combination is not. When I ask why, she patiently explains again that a player cannot play in different leagues during the season. She had already told me so when she rejected an earlier example where the Player 1 player was in two different leagues and she is disappointed that I am unable to understand that this rule also applies when a player is named second on the score form. From the explanation by the subject matter expert, I conclude that League is functionally dependent on Player 2. Her explanation also suggests that the other columns were no problem, but I need to make completely sure. After all, sometimes a subject matter expert mentions only one problems with an example, the one she considers the worst or sees first and doesn't notice other problems anymore. So I fix the problem with the two leagues by changing the example switch that Player 1 and Highest Break Player 2 still have different values, but the league column now has the same value on both rows. As expected, the subject matter expert now no longer objects, confirming my suspicion that these two attributes are not dependent on Player 2. The second pattern to test for the second candidate key has identical values for Date and different values for Player 2. Luckily, this final test can be completed pretty quickly. The first existing valid example with the desired pattern tells me that these six columns do not depend on Date only. The second example shows the same for the remaining two non-single key columns. After finishing all the tests, I have found two functional dependencies of an attribute on a part of a candidate key. League depends on Player 1, and League depends on Player 2, both are dependencies on a single columns so I don't have to check if these are actually caused by dependencies on the subset of the determinant. At first sight, this appears to be the rare situation where a single attribute depends on two different subsets of candidate keys, but in this case it isn't. Player 1 and Player 2 both represent instances of the entity type player that just happens to participate twice in the match entity type. So there actually is only a single functional dependency of League on Player that I found twice. The method to fix it would normally start with creating a new entity type with Player Name as the key and League as the non-key attribute. But in this case, we already have an entity type with Player Name as the key, so I simply add League as an additional non-key attribute in the existing player entity type and remove League from the match entity type. This moves the foreign key relationship we had between Match and League, to one between Player and League, which requires me to rethink the relationship readings and reevaluate the minimum and maximum cardinalities. The result of that is shown here.

Third Normal Form
The table is in Third Normal Form if it meets two requirements. The first is obvious, it has to be already in Second Normal Form. The second requirement is officially worded as every non-prime attribute must be non-transitively dependent on every superkey of the table. I will not even try to explain the technical terms in that definition, instead I will give you and easy way to check. A table is not in Third Normal Form if there is an attribute that is not part of any candidate key and that depends on an attribute or combination of attributes that is not a candidate key. Violations of Third Normal Form can occur in four basic patterns. The simplest is a table where one of the non-key attributes depends on another non-key attribute. A slightly more complex variation of this is a dependency of a non-key attribute on a combination of two or more non-key attributes. The third variation is a dependency of a non-key attribute on a combination of one or more attributes that are part of a composite key and one or more attributes that are not part of any composite candidate key. And finally, the fourth possible pattern, though you'll probably never actually encounter this one, is a non-key attribute that depends on two or more attributes as a part of two or more candidate keys, optionally combined with further non-key attributes. Let's return to our example from the previous section. After changing the model to fix Second Normal Form, we now have an entity type with attributes A, B, C, D, and F. Attributes B, C, D, and F all depend on A, F depends on D and A, D, and F all depend on the combination of B and C. Candidate keys are A and the combination B and C. The dependency of F on D violates Third Normal Form. This is an example of the first pattern, a single non-key attribute that depends on another single non-key attribute. The fix is very similar to the fix we used for Second Normal Form violations, create a new artificial entity type for all attributes that are either determinant or dependent in the offending functional dependency, making the determinant a candidate key in the new entity type and combining all dependent attributes if there are multiple Third Normal Form violations that share the same determinant. Then remove the dependent attribute from the original entity type, but leave the determinant there, as a foreign key it implements the relationship with the new entity type. Because the determinant is not a candidate key, this will not change a strong entity type to weak, but if the entity type already is weak, it will of course remain so.

Find Functional Dependencies 4: Verify Third Normal Form
To check Third Normal Form, we need to check for dependencies of non-key attributes on attributes that are not a candidate key. That can include dependencies on a single non-key attribute, or a dependency on a combination of attributes, key, non-key, or both, as long as that combination as a whole is not a subset of any candidate key. That would have been a violation of Second Normal Form and we already tested for that. And as long as that combination as a whole is not a superset of any candidate key because in that case, the dependency wouldn't be full, but implied by the dependency on the candidate key. For completeness sake, we also test for similar dependencies where the dependent column is part of a candidate key, even though that does not violate Third Normal Form. The test is carried out in two parts. The first part covers dependencies on single non-key attributes. You can skip this part of the test for any table that has no non-key attributes. That is, every attribute is a candidate key or part of a composite candidate key. The second part of the test then obviously covers dependencies on combinations of two or more attributes. You can only skip this part of the test if all candidate keys of the table are on a single attribute and there is at most one non-key attribute, or if at most one of the candidate keys is on multiple columns and a table has no non-key attributes. The procedure is again very similar to what we already have done before, populate the table with two rows using a pattern that would be invalid if the functional dependency we are looking for exists, then try to find or construct a population that includes this pattern. Start with the first column that is not apart of any candidate key. Populate the table with two rows, having the same value for this attribute and a different value in all other columns. Usually you will be able to find or create a valid example with this pattern and conclude that none of the other attributes depend on this attribute. However, if this fails, you know that at least one of the other attributes does depend on this one, but you still need to find out which of them. Columns that are a subject to a single column candidate key can never be the dependent column so you can cross them off, but that usually still leaves you with several candidates. The reason the subject matter expert gave to rejecting the example is usually a very clear indication. When in doubt, the safe way to find the dependent columns is to keep testing examples with the same value in the column being tested and different values in one of the possibly dependent columns and whatever you want or need to satisfy the known business rules in the remaining columns. If such an example is rejected, the column with the difference is a dependent column. But beware, it might be dependent on other columns that happen to have the same value, so make sure to understand why the domain expert rejects the example and do not stop with this test after finding one dependent column. There may be more that all depend on the non-key column currently being tested. For each dependency you found, you have to verify whether this is a normal functional dependency, or the result of a derivation rule. If it is not a derivation rule and the dependent column is not part of any candidate key, the dependency would violate Third Normal Form. To fix this, you create a new entity type with the tested column as the key attribute and all dependent columns as non-key attributes and remove the dependent non-key attributes from the original table. For dependencies that are caused by a derivation rule, or for any dependencies where the dependent column is part of candidate key, no model change is needed, as they are not violations of Third Normal Form. However, we do need to know this information so remember to make a note of all these dependencies. We can also exclude the dependent column from the rest of the test, just as if it were moved to a new table. Just remember that other dependencies you might find on the same determinant, or on the superset of it, could be transitive, caused by an actual dependency on this dependent column. Once done with the first non-key column and after changing the model if needed, repeat the test for the second non-key column, then the third and so on until all non-key columns have been verified. That concludes the first part of the test, but it doesn't stop there. After checking and if needed fixing, all dependencies on single non-key columns, there can still be dependencies on combinations of two or more columns that are not a candidate key. So the next step would be to repeat the same procedure for each combination of two columns that could still be the determinant for a non-key attribute, then combinations of three columns and so on. Combinations to test include combinations of two or more non-key columns, but also combinations of non-key columns with columns that are part of a multi-column key, or even combinations of only columns that are part of different multi-column keys. Every combination of columns that is not fully included in a single candidate key, nor include all columns of a candidate key should be considered. In practice, checking every possible combination of columns, especially in tables that have a lot of columns can be very laborious. Luckily, you usually can cut enormous corners here. If you have some knowledge of the subject matter, you will probably be able to rule out many combinations you would otherwise have to test. For instance, you can probably safely assume that in the customer entity birth date does not depend on credit card number and gender does not depend on the combination of shoe size and loyalty card number. So you would not include those and many similar options in your tests. However, you should be aware that this involves assumptions, so be prepared to change a model if it turns out that you happen to be working in a domain that has other rules than what you are used to.

Demo: Verify Third Normal Form
At the end of the previous demo I had found and corrected a Second Normal Form violation. The model now looks as shown here. This will be the starting point for my checks for dependencies on non-key attributes. I will first handle the non-key columns, one by one, starting with Frames 1 Player 1. The pattern I am interested in is a combination of two rows with the same value in this column and different values in the other columns. As usual, I will first check the valid examples I already have. The first example I come across that is valid and has two rows with the same value for Frames 1 Player 1, is from my tests for missing candidate keys. It doesn't have different values in all other columns, but it does show differences in Date, Player 1, Player 2, Highest Break Player 1 and Highest Break Player 2, so these columns do not depend on Frames 1 Player 1. However, the other three columns, Frames 1 Player 2, Competition Points Player 1 and Competition Points Player 2, all have the same values in these two rows so we cannot draw any conclusions for them yet. I have a few more valid examples with two rows with the same value for Frames 1 Player 1, but none of them have different values for the columns I still need to test. That can be a sign that there may be a functional dependency, but it can also be just a coincidence, a side effect of my testing method where I always make new examples by minimally changing an existing example. The only way to know for sure is to try if I can create such an example. Here is my first attempt. The first row is still the same, but in the second row I now change the values for the three columns I still need to test. This example satisfies all business rules I already know so I transform it into the subject matter expert's notation. The individual frames results and the League are not in the table, but I do of course, have to include them on the examples to make them valid. I remember from an earlier demo that the individual frame scores always have to match the numbers of frames won for both players, so I make sure to satisfy that business rule when making my examples. The subject matter expert says she cannot judge this combination of score forms because one of the forms in itself is invalid. All matches in the B League are over three frames, so the form for the match between Tony and Jack is incomplete, and besides it is also impossible for a player to have a highest break that is higher than the points they scored in any single frame. My first attempt to fix this is also rejected. Yes, there now are three frames played and the highest break is not higher than any single frame score, but the third frame shows a tie. In Snooker, the frame never ends in a tie, if both players have the same score when the last black ball is spotted, the black is re-spotted and play continues until one of the players scores points, either by potting the black or because the opponent makes a foul. I now have a problem, I need to make an example with two rows with the same value for Frames 1 Player 1 and different values for Frames 1 Player 2, but I cannot do that because the business rules say that the total of Frames 1 Player 1 and Frames 1 Player 2 must always be three. So for each value of Frames 1 Player 1, the value for Frames 1 Player 2 is dictated. However, there is a solution, one that I would have found if I had not constantly reused the same example with minimal variation, or if I had paid closer attention to what the subject matter expert told me. She said matches in the B League are played over three frames, that goes for the C League as well, but the A League plays five frames. So all I need to do is promote Tony and Jack to the A League to create an example that does have the same value for Frames 1 Player 1 and different values for Frames 1 Player 2, as well as all the other columns I still need to test. I still need to verify it though. Often when the subject matter expert reports a violation of a business rule, they don't check if there are other problems as well. So even though I expect the combination of the duplicated Frames 1 Player 1, with the different Frames 1 Player 2 to no longer be a problem, there still may be issues with the other columns I changed. And indeed there is, this example is again rejected by the subject matter expert. She tells me that a player who wins two frames always gets 12 competition points and that they get 20 points for three frames won. These numbers may change in the next competition year as the board is still experimenting with them to try to make the competition as exciting as possible, but within every season the number of competition points will always be directly based on the number of frames of won. I correct the example and present it again to the subject matter expert and this time she finally accepts it as a valid example. I can now conclude that Frames 1 Player 2 and Competition Points Player 2 are apparently not dependent on Frames 1 Player 1, but I still have not been able to prove this for Competition Points Player 1, and based on what the subject matter expert just told me, I know I will never be able to create an example with two rows with the same Frames 1 Player 1 and a different Competition Points Player 1 because the competition point are directly related to the number of frames won in a way that will not change during the competition season and the database I am designing will always be used for a single season only. If you look just at this year's competition, you could consider this dependency the result of a derivation rule. But even though only a single season will be populated in a database, that application will hopefully be used longer than that. Next year the club can just clear out the data and start adding information about the new season. If the board decides to change the number of competition points for winning one frame to seven next year, I do not want to have to change the application code. So this is a normal functional dependency after all, not a derivation rule. Since this is a functional dependency where the dependent attribute is not part of a candidate key, this is a violation of Third Normal Form. To fix this, I add a new entity type for the two attributes that are involved with Frames 1 Player 1 as the key attribute and Competition Points Player 1 as the non-key attribute. The later attribute is then removed from the competition match attribute and replaced with a relationship to the new entity type that is implemented by the Frames 1 Player 1 attribute. This fixes the Third Normal Form violation. This new table is where the club can specify the points to use for each match result throughout the season. We could already have had this table if at the start of the procedure when I asked the domain expert to bring examples of the relevant information and read their contents, she would have given me the table she uses to look up the competition points to assign. Very often normalization the data model is just another word for adding facts that were overlooked during initial analysis. The next attribute I need to check for dependencies is Frames 1 Player 2. Based on the understanding I by now have of the universe of this course, the subject matter I am investigating, I suspect that Competition Points Player 2 depends on this attribute the same way Competition Points Player 1 depends on Frames 1 Player 1. In fact, I already know this. When I first made this example, the subject matter expert told me not only that I had to correct Competition Points Player 1 based on Frames 1 Player 1, but also that I had to do the same for Player 2. So I know that Competition Points Player 2 functionally depends on Frames 1 Player 2, another violation of Third Normal Form. Like before, I can fix this by creating a new entity type with the determinant and the dependent attribute and replacing the dependent attribute in the original entity type with a foreign key, implemented by the determinant. The Third Normal Form violation is now fixed, but I have introduced new redundancy. Remember that both Frames 1 Player 1 and 2 and Competition Points Player 1 and 2 were repeating groups. A single combination of functionally dependent attributes was included twice and I have now created two entity types for that. This is incorrect, I should have created a single entity type and connected it to the match entity type with two relationships as shown here. I still need to check if any other attributes depend on Frames 1 Player 2. I expect not and this valid example that I already used before in this demo confirms that this is the case. It has two rows with the same value for Frames 1 Player 2 and different values in the remaining columns, except Frames 1 Player 1. Promoting Tony and Jack to the A League again enables me to easily make an example where Frames 1 Player 1 is different as well. To save some time I will not show the tests for the remaining attributes Highest Break Player 1 and Highest Break Player 2. These tests reveal no new functional dependencies. So now I can start testing for functional dependencies on combinations of two columns. I don't have to test the combination of Date and Player 1 because it is fully included in a candidate key. It is in fact exactly equal to a candidate key. Remember I only have to test combinations that are not completely included in a candidate key, nor contain all columns of a candidate key. Combinations that are equal to a candidate key fail on both conditions, so I can skip the combination of Date and Player 2 for the same reason. I do need to check the remaining combinations of Date and another attribute, Frames 1 Player 1, Frames 1 Player 2, Highest Break Player 1 and Highest Break Player 2. I will not show those tests in this demo. None of them reveal any new dependencies. I also have to test the combination of Player 1 and Player 2. Both attributes are included in a candidate key, but they are two different candidate keys. They are not both part of one single candidate key, nor is one single candidate key fully embedded in this combination, so a test is required. Based on this valid example that I made when testing for minimal candidate keys, I can conclude that none of the attributes Date, Frames 1 Player 1, Frames 1 Player 2 and Highest Break Player 1 depend on the combination of Player 1 and Player 2. Another example that I use to check for missing candidate keys, but didn't cover completely in the demo, shows that the same goes for Highest Break Player 2. The combination of Player 1 and Frames 1 Player 1 is interesting. I quickly find this example that shows that Date, Player 2 and Highest Break Player 2 do not depend on this combination. A small modification to the example shows that Highest Break Player 1 also does not depend on this combination. That leaves only Frames 1 Player 2 to be tested. I already know that the only way to get two rows with the same value for Frames 1 Player 1 and different values for Frames 1 Player 2 is to have one match in the A League and the other match in the B or C League, but in this case I also need to have the same Player 1 in both rows. And the dependency of League on Player 1 implies that both rows have to be the same league. So I can never make an example with the same Player 1 and Frames 1 Player 1, but different Frames 1 Player 2. I can conclude that I have found another dependency, Frames 1 Player 2 depends on the combination of Player 1 and Frames 1 Player 1. Well, actually it depends on the combination of League and Frames 1 Player 1. The dependency I found is a transitive dependency caused by League being dependent on Player 1, but since I already moved League to another table, this transitive dependency is all I have and I still have to deal with it. In this case, the dependency is actually a derivation rule, as Frames 1 Player 2 can simply be calculated by subtracting Frames 1 Player 1 from either five or three, depending on the League of the competing players. That means that I don't have to change the data model, but I do need to make a note of this derivation rule. I can now also exclude Frames 1 Player 2 from the rest of the test, but if I find more dependencies, I do need to check if they are transitive. For instance, if a later test would show a dependency of the combination of Player 1, Frames 1 Player 1 and Highest Break Player 1, I need to check if it's not actually a dependency on the combination of Frames 1 Player 2 and Highest Break Player 1. This does not come up though. In the remaining tests, which I have not included in this demo, no more dependencies are found. And that concludes all tests for this entity type. I can now be certain that the competition match entity type is completely normalized up to Third Normal Form. Before I can make the same statement for the entire data model, I have to check all other entity types and Many-to-many relationships, not just the ones that were originally in the model, but also all extra entity types that were introduced as a result of normalizing the original data model. But that is beyond the scope of this demo.

Summary
In the previous and this module you learned two skills that arguably are the most important for anyone involved in data modeling and database design. Identifying functional dependencies and normalizing the data model up to Third Normal Form. An initial data model can suffer from problems such as redundancy or modification anomalies. Fixing that kind of problems is the goal of normalization. There are a lot of normal forms, but only the first three are generally considered an absolute requirement for a good database design. Often, knowing the value of one or more attributes is sufficient to know exactly what, if any, the value of another attribute is. This is called a functional dependency. Since normal forms are defined in terms of functional dependencies, you must know all functional dependencies in order to normalize your design. First Normal Form dictates that every column has to store only a single value and that this value has to depend on the table's key column or key columns. Second Normal Form adds the additional requirement that non-key columns should not depend on a part of the key, but on the complete key. Third Normal Form specifies that non-key columns must not depend on other non-key columns, but only on the candidate keys. If you are in need of a pneumonic aid for these normal forms, how about this famous one. Every non-key attribute must provide a fact about the key, the whole key and nothing but the key, so help me Codd. In the next module, we will take a look at the higher normal forms and add de-normalization.

References
For further reading you can use this link to visit the Wikipedia article about the subject of normalization, which also includes many links to articles that provide more depth on normal forms and to various related subjects. Another interesting read is a paper by Phillip A Bernstein where he presents and algorithm that can be used to generate a database key schema that is guaranteed to be 100% Third Normal Form. It does require that you already know all functional dependencies. I did not cover this algorithm because I personally favor my own method that integrates finding functional dependencies and normalizing in a single procedure, but if you are interested in another approach, this is the most commonly used formal normalization procedure. 1

Higher Normal Forms
Outline
Hello, my name is Hugo Kornelis and I am presenting this Pluralsight course on the Relational Database Design. This is Module 8: Higher Normal Forms. In this module you will learn about normalizing your database beyond Third Normal Form. In the previous module, I covered normalization up to the Third Normal Form, but database science has defined many more normal forms often collectively called the higher normal forms. Many data modelers and many teachers as well will tell you that Third Normal Form is enough for practical applications and that higher normal forms are of academic interest only. Others respond that this statement results from a lack of proper understanding of the higher normal forms. In this module I will describe and illustrate all the higher normal forms so that you can be your own judge on this. At the end of the module I will also briefly cover the subject of denormalization and I will finish the module with some standard patterns that you can use to solve issues that find no direct answer in the normalization rules.

Boyce-Codd Normal Form and Elementary Key Normal Form
In the previous module, you have seen the Third Normal Form requires that a non-key attribute may not depend on the determinant that is a subset of a candidate key or not candidate key at all. In other words, every non-key attribute must depend on exactly all columns of each candidate key and on nothing else. However, columns that are part of a candidate key are not similarly restricted. For single key columns that is not relevant. If a single keyed column depends on part of a key, or on a non-key determinant, then all columns in the table depend on that determinant, making that determinant a key itself, but when composite candidate keys are involved, things change because a subset of such a key can have its own dependencies. This is where Boyce-Codd Normal Form, or BCNF for short, comes in. Boyce-Codd Normal Form is a step beyond Third Normal Form because it does away with the exception. In Boyce-Codd Normal Form every determinant of a full functional dependency has to be a candidate key, or in other words, every attribute, both key and non-key has to depend on each candidate key. Our Snooker Club occasionally rents out the clubhouse to host commercial events. People can just play on our tables or they can be assisted by some of our club members in various functions. Some of us are available as coach, others as referee, and yet others can do the catering. We don't train club members for more than one function, and the events are small enough that a single volunteer per function is always enough. Here you see an excerpt from the planning for these events. This is the corresponding IDEF1X entity relationship model. In the events staffing entity type, member is functionally dependent on Event Date and Position because there will never be two member performing the same duty on the same date. There is also a functional dependency of Position on Event Date and Member because a single member can't perform two duties at the same time, but that dependency is not full. A member cannot perform different duties on any date, we all have a single specialization. So the actual dependency is that Position depends on Member only. The entity type has two candidate keys, Event Date and Position is obvious, the second candidate key on Event Date and Member follows from the non-full functional dependency of Position on this combination. The key cannot be reduced to just Member, Position depends on Member only, but Event Date does not. There are no non-key attributes so by definition neither Second nor Third Normal Form can be violated. The entity type is in Third Normal Form, but the dependency of Position on Member only means that the entity type is not in Boyce-Codd Normal Form. That's not just an academic observation, a result of this BCNF violation is that this schema allows me to violate the known business rules because not all functional dependencies are enforced. For instance, this population violates none of the keys so the DBMS will allow it, but the functional dependency of Position on Member is violated. Mary now has to referee on the fourth of June, but she is also scheduled to be a coach on August 12th, impossible because members are trained for a single job only. This problem can be remedied by creating a new entity type for the attributes involved in this dependency, leaving the determinant in the original table as well to implement the foreign key relationship. However, this is not really an ideal solution because I can now easily accidentally populate the event staffing table with, for instance, two referees for a single event. A better alternative is to not remove the position attribute from the staffing table and make sure that the foreign key is defined on the combination of Member and Position, instead of on Member only, but most modeling tools do not support this and most currently available relational database management systems require tricks or manual code to implement this, and obviously this also introduces redundancy in the database. This example shows an important property of Boyce-Codd Normal Form. There are situations where it is impossible to derive a schema in BCNF. Please do not think that this is the rule, there are also situations where it is perfectly possible to achieve BCNF, just not always. That's where Elementary Key Normal Key or EKNF comes in. This normal form sits halfway between Third Normal Form and Boyce-Codd Normal Form. Where Third Normal Form requires only non-key attributes to depend on every candidate key and Boyce-Codd Normal Form requires this for all attributes, Elementary Key Normal Form has the same requirement only for some key attributes. To be exact, only for so called elementary keys. The difference between elementary and non-elementary keys is quite subtle and beyond the scope of this course. It's also not really relevant to know this. Elementary Key Normal Form will not solve all problems. For instance, the example I used before that violates BCNF does satisfy the rules the of EKNF, but that does not make this good schema. I have already shown that it's possible to violate the functional dependency of Position on Member. In spite of the redundancy, this alternative schema, which also satisfies the EKNF rules, is a much better solution. So what is the relevance of Elementary Key Normal Form then? Simply put, EKNF is the highest normal form that is guaranteed always achievable. Conversely, BCNF and all higher normal forms are assumed to be not always achievable. Further, there is also no known algorithm to produce a BCNF representation based on a set of functional dependencies, even if one is achievable. For EKNF, such an algorithm does exist. The synthesis algorithm the Philip A. Bernstein devised to generate a Third Normal Form schema for a given set of functional dependencies has been proven to actually always generate a schema in Elementary Key Normal Form.

Fourth Normal Form
Fourth Normal Form can be difficult to grasp so let's start this section with an example. To celebrate the 25 year anniversary of our Snooker Club, we are currently organizing a special event. For a whole day members and guests are invited to come to the clubhouse and play fun games, all played on the Snooker table and all using a variation on the official Snooker rules. Some members have volunteered to host some of these games. And since some variations require some special attributes to be attached to the Snooker tables they are played on, we decided to assign each variations to one or more tables. As you can see from the sample data, there are no functional dependencies in this table at all. So there is a single candidate key over all columns and the table satisfies Boyce-Codd Normal Form. And yet the design might be incorrect. Whether or not it is depends on the actual information represented in this table. You may recall from earlier modules that information in a database is actually a representation of facts. So let's look at what facts are represented here. The first row of the table can be read as variation Pooker is hosted by Mary on Table2, but is this sentence a single fact, or a combination of facts? Maybe Mary can elect not to host Pooker on all the tables where it is played. In that case, this sentence does indeed represent a single fact. Or maybe Mary has just agreed to host Pooker on whatever table and Table 2 has been prepared for this variation regardless of the host. In that case, the sentence can be split in two independent facts, Variation Pooker is hosted by Mary and Variation Pooker can be played on Table 2. In that case, the design does have issues. For example, the fact that John hosts Sneaker is recorded three times and if we assign a second table to Quick Snook, we have to add not one, but two new rows. We can try to fix this by representing the facts in a more dense way. For Pooker, that works fine because the number of tables just happens to be equal to the number of hosts, but for the other variations, you see that this method leaves us with problematic cells. So this is really not a good design. This is an example of a violation of Fourth Normal Form. The official definition of Fourth Normal Form is based on so called multivalued dependencies. An informal definition of a multivalued dependency is that given the value of the determining column or columns, you can determine all possible values for the dependent columns regardless of the values of any other columns in the table. As you can see from this definition, a functional dependency is actually just a special case of a multivalued dependency. One where the collection of possible values for the dependent columns never exceeds a single value. But there is an important difference, a functional dependency where columns A and B determine columns C and D, is completely equivalent to two functional dependencies where A and B determines C and where A and B determine D. However, if there is a multivalued dependency where A and B combined multi-determine the combination of C and D, no multivalued dependencies of C or D individually on the A, B combination are implied. By definition, any given set of columns in a table is always multivalued dependent on the combination of all remaining columns. This is called a trivial multivalued dependency. Those are irrelevant for Fourth Normal Form. The multivalued dependencies that are actually a functional dependency are also irrelevant because they are already handled by the lower normal forms. They can be easily recognized because in Boyce-Codd Normal Form the determinant of each functional dependency is a key or a superset of a key. All other multivalued dependencies are forbidden in Fourth Normal Form. Or to use a more official description, a table is in Fourth Normal Form if it is in Boyce-Codd Normal Form and additionally has no non-trivial multivalued dependencies where the determinant is neither a key nor a superset of a key. The first condition is actually superfluous because functional dependencies are a special case of multivalued dependencies, the second condition already implies all requirements for BCNF. Going back to the original example, if splitting the reading to variation Pooker is hosted by Mary and variation Pooker can be played on Table 2 was correct, then there are some non-trivial multivalued dependencies. For instance, for the variation Pooker, the collection of hosts is always equal to Mary and Dave independent of the table. The other variations behave the same so the variation multi-determines the host, and regardless of the host Pooker can always be played on Tables 1 and 2. This holds for all tables, so there is also a multivalued dependency of Table on Variation. To fix this violation of Fourth Normal Form, you have to use separate tables for all combinations of columns that participate in a multivalued dependency. So I would have to split this single table into two new tables, one for the facts linking variations to tables, and another one for the facts that link variations to hosts. The previously identified multivalued dependencies are now trivial. So this design conforms to Fourth Normal Form. In the corresponding IDEF1X entity relationship diagram, the two tables are now represented as relationships. Because all attributes in the original entity type were references to entity types and not attributes. Now what if the original reading variation Pooker is hosted by Mary on Table 2 cannot be split because Mary might choose not to host Pooker on Table 1. In that case, there are no non-trivial multivalued dependencies. For the variation Pooker, the collection of hosts is equal to Mary and Dave on Table 2, but to Dave only on Table 1, and looking at the tables, Pooker can be played on Tables 1 and 2 when Dave is hosting, but Table 2 only when he is taking a break and Mary is on call. In this situation, the original single table design does not include a non-trivial multivalued dependency, so Fourth Normal Form is not violated.

Fifth Normal Form
Let's take another look at the example of the previous section. We have seen that if hosts are assigned to variations and variations are assigned to tables, without the option for Mary to refuse hosting Pooker on Table 1, the table violates Fourth Normal Form and has to be split. We have also seen that in the other case when Mary can elect not to host Pooker on one of the tables where it's played, the table is in Fourth Normal Form, but there is yet another possibility. What if hosts cannot refuse to host a specific variation on the table, but can choose to not work at a table at all? So for instance, Mary could choose to work on Tables 2 and 3 only. In that case, these two rows would be removed. In a table with three columns, there are a total of six possible non-trivial multivalued dependencies. The population left after removing these two rows shows that none of them occurs here. You might want to pause the video and check this for yourself, or you can just take my word for it. So this table does still not violate Fourth Normal Form in this scenario. However, if you think back of the requirements, you may realize that we are dealing with different types of facts that have been combined in a single table. In the previous section, we had a group of facts assigning hosts to variations and another group of facts assigning variations to tables. And then we either did or did not have a third variation. If a host is allowed to exclude a specific combination of variation and table, we didn't represent the exclusions, but the combinations that were left, making the other two fact types redundant. In this section, we ruled out those exclusions, but did allow hosts to exclude tables entirely. Again representing this in reverse as a list of tables where the host does work. If we had started with a domain expert, reading as examples of these three types of facts and then followed the procedure outlined on Module 4, it would have resulted in three tables. Each representing one of these three fact types. We wouldn't have gotten the single three column table, but if we want to present information that way, we can always construct it simply by joining the three two column tables. Note that this would not have been possible if Mary did have the option to host Sneaker on Table 1, but not host Pooker on the same table. For completeness sake, here is the same data model as an IDEF1X entity relationship diagram. Again, using relationships to represent the new tables. The problem in the original design is caused by yet another kind of dependency, Join Dependency. The definition of Join Dependency boils down to this. There is a Join Dependency if you can split a table into two or more new tables, such that joining those new tables back together always results in the original table being exactly reconstructed. A different, less formal way to explain a Join Dependency is that the attributes represent independent facts. If you split a table in several new tables and one of them is equal to the original, joining them will always result in the original, so that Join Dependency exists by definition. That's why such a Join Dependency is called trivial and not considered for normalization purposes. Another easy way to split the table such that it can be reconstructed by joining is to include a candidate key of the original table in all new tables. We say that these Join Dependencies are implied by a candidate key. These two are not relevant for Fifth Normal Form. Finally, there is one last special type of Join Dependency. Any Join Dependency based on splitting a table in two new tables is equivalent to two multivalued dependencies where the columns that are in both tables multi-determine the remaining columns in each of the tables. To be in Fifth Normal Form a table has to already be in Fourth Normal Form and in addition there shouldn't be any non-trivial Join Dependency that is not implied by a candidate key. The first condition is not really required because multivalued dependencies are actually just a special case of Join Dependencies so the second condition completely implies Fourth Normal Form. Because it is based on Join Dependencies, Fifth Normal Form is also known as Project-Join Normal Form, or Projection-Join Normal Form.

Domain/Key Normal Form
The previous normal forms were all defined in terms of dependencies. Domain-Key Normal Form or DKNF for short, comes from a completely different angle. As the name implies, Domain-Key Normal Form is defined in terms of domains and keys. We already know what a key is. A domain is simply the set of all values that are permitted for an attribute. A third relevant concept for the definition of Domain-Key Normal Form is a constraint. The term constraint describes any rule that governs the population of a database, but in the context of DKNF, only constraints that govern the population of a single table are considered. A table is in Domain-Key Normal Form if every constraint is implied by the keys and domains. In other words, only the domains and keys suffice to ensure that every constraint is observed. It has been proven that any table that is in Domain-Key Normal Form can never violate the rules for Fifth Normal Form or any of the lower normal forms. DKNF is very useful because all relational database management systems have the ability to enforce both domains and keys simply be declaring them. Many other constraints take more effort to enforce, often requiring you to develop extra code. If your database is in DKNF, all constraints are automatically enforced by the domains in the keys so there is no need to write extra code. That benefit is very important because having to write code makes a project expensive. It's not just the time to write the code, it will also have to be tested, errors need to be fixed, and future maintenance costs will also increase when there is more code to maintain. Unfortunately, it's not always possible to achieve Domain-Key Normal Form. A classic example is that of a table that has to contain at least 10 rows. A very simple constraint, yet impossible to achieve with just domains and keys. When it is possible to achieve Domain-Key Normal Form, this often involves introducing extra tables for specific subsets of the population. The downside is that those extra tables will increase the amount and complexity of all code that uses the data. There are ways to minimize the impact for application programmers, for instance, by using views, but remember that views are code too. So the real choice is not between less code or more code, but between less code for constraints and more code for querying, versus more code for constraints and easier querying. As an example, this is a table that stores Snooker match results. At first sight, and thinking back of the previous modules, it looks as if the result column is not atomic, but if we never access the individual numbers of frames won by a player, Result can be considered an atomic attribute so it doesn't violate First Normal Form. You might also think that the league column violates Second Normal Form. However, for the matches used in this example a member can sometimes play a match in another league than his or her normal league. So the League column here is not dependent on a single player, but truly an attribute of the match. Second Normal Form is not violated. With only three Leagues and the low number of frames we play in our matches, the domains for League and Result only allow these values, but just like our normal competition matches, matches in the A League are always over five frames, while the B and C League play three frames only. This introduces a constraint that cannot be enforced by domains and keys only, so this design violates Domain-Key Normal Form. The solution in this case would be to create two tables, one for the results of the A League matches and a second table for the results of the B and C League matches. Now we can define domain rules such that the constraint on League versus number of frames played can never be violated. The revised design is in Domain-Key Normal Form.

Sixth Normal Form and Optimal Normal Form
The term Sixth Normal Form can sometimes be a bit ambiguous. Domain-Key Normal Form, the subject of the previous section, has sometimes been called Sixth Normal Form as well, until yet another new normal form was described and officially dubbed Sixth Normal Form. The name Sixth Normal Form is actually very appropriate for this normal form since it's only a small modification away from Fifth Normal Form. Remember that Fifth Normal Form only allows non-trivial Join Dependencies that are implied by a candidate key. Well, Sixth Normal Form doesn't. Whether implied by the key or not, non-trivial Join Dependencies are never allowed in Sixth Normal Form. This means that every Sixth Normal Form table contains all the columns of a candidate key, plus at most one other column, never more. An often claimed advantage of Sixth Normal Form is that it eliminates NULLs from the database. In other normal forms, the absence of a fact is represented by the special NULL mark. And many developers have suffered from the less intuitive consequences of using NULLS. In Sixth Normal Form, because no independent facts are every combined in a single table, you can represent the absence of effect by simply not including the corresponding row. Unfortunately, as soon as you start combining data from the various Sixth Normal Form tables for your queries, the NULLS will return unless you use database management systems that use a different query language specifically tailored for this purpose. This is well beyond the scope of this course. A logical consequence of this is that any database design in Sixth Normal Form will contain an enormous amount of tables, and that most of the queries you would typically execute in these databases will have to join many of these small tables together. The database management systems currently in mainstream use are not optimized for these storage and query patterns. Another problem with Sixth Normal Form is that it can become far from trivial to enforce common constraints. Here is an example. Transactions in our bank account are listed on the bank statement that we receive weekly. Every transaction can be identified by a combination of the bank statement number and a sequence number. Some of the transactions on a bank statement are payment of a membership fee. The membership fees are identified by the member name and the year of membership. Some members pay their yearly fee in a single payment, others choose to pay in monthly or quarterly terms, so the relationship between transaction and membership fee is One-to-many and because not all transactions are from membership fees, the relationship is optional. In Fifth Normal Form the table for transactions would look like this. All mainstream relational database engines allow you to enforce the relationship shown in the ER diagram by declaring a foreign key constraint on the combination of these two columns, referencing the table for membership fees. In Sixth Normal Form, the transactions table would have to be split. Remember a candidate key and at most one other attribute, so now the references to member and to year are distributed over two tables and the relationship can no longer be enforced with a standard foreign key constraint. Proponents of fact based modeling methods, such as object role modeling or NIAM, have proposed to normal form that they, modest as always, have called Optimal Normal Form or ONF. This normal form sits halfway between Fifth and Sixth Normal Form. In Optimal Normal Form every fact type in these modeling methods becomes a table. For the examples shown here, this would mean that columns such as Amount and Transaction Date are still split over individual tables, but the member name and year of a membership fee payment would not be split. In Optimal Normal Form relationships that reference a composite candidate key can still be implemented as a standard foreign key constraint.

Denormalization
A fully normalized database offers many benefits, especially in a database that supports online transactions processing or OLTP. The benefits of avoiding insert, update and delete anomalies are of course obvious, but in most cases, a normalized database design is also very easy to program against and usually results in good performance and limited locking and blocking issues in the OLTP system. However, online analytic processing, or OLAP, and reporting can see a performance decline because the process of normalizing the design has spread the information they need over multiple tables that now have to be joined and aggregated to produce meaningful results. Especially when large numbers of rows are involved, joining tables and aggregating lots of rows can become a major performance bottleneck. In order to speed up the performance of reporting an OLAP, databases that are used to support these workloads can be denormalized. Note that a denormalized database design is not the same as an unnormalized database design. In an unnormalized database design, you have no clue of the actual dependencies and the possible anomalies. In a denormalized design some normalization steps have the deliberately and purposely been undone. The resulting anomalies are well known and can be catered for in the application logic. Denormalizing and designing for reporting an OLAP workloads is beyond the scope of this course so I will limit myself to just a short summary of the most popular denormalization techniques. Storing the result of a query is a technique that can be used completely programmatically by creating an extra table and enhancing the application logic to ensure that modifications in the base tables are reflected in the table with the stored query results. Several major database management systems offer this functionality out of the box, usually called materialized views or indexed views. This relieves you of the task to translate modifications of the base table into the stored query results. The technique is often used to combine data that is normalized into separate tables, but often queried together, or to pre-aggregate data to the level where it is usually reported. Star schemas, also called fact dimension models, are commonly used for data warehouses. They get their name because the typical form of such a model in an ER diagram is somewhat shaped like a star. One so called fact table in the middle, usually holding millions of rows in strongly denormalized form and referencing a large number of smaller dimension tables. A more advanced version of this model where dimension tables can also reference other dimension tables is called a Snowflake schema. OLAP cubes are, as the name suggests, specifically designed to support Online Analytical Processing. OLAP cubes store and pre-aggregate data on various aggregation levels. OLAP cubes are almost always built on top of a Star or Snowflake schema to support the analysis of large amounts of data, the field of business intelligence or BI. For those who wish to know more about this field of computing, Pluralsight offers a wide range of great courses on the subject of BI. A more academic approach to the subject of denormalization is the so called Non-First Normal Form. In the original relational model an attribute is only allowed to store a single atomic value. In this modification an attribute can also store a complete table, so in Non-First Normal Form you would have tables nested inside tables. None of the major relational database systems currently available on the market have support for this. Some of the so called No SQL databases claim to support Non-First Normal Form, but I disagree. A Non-First Normal Form database would still have to adhere to a rigid schema and a No SQL database typically has a very loose schema. Another type of database that comes close to the idea of Non-First Normal Form are the so called multivalued databases. These can be thought of as supporting nesting single column tables only. Despite being around for a long time already, multivalued databases have failed to come even close to the market share of relational databases.

Standard Solutions for Non-standard Patterns
Normalization covers a lot of possible design issues, but not all. You will always find yourself running into situations not covered by the rules of normalization In those cases, you may have to improvise. However, some of these situations are common enough that I want to discuss them briefly here. To understand the referencing range pattern let's return to my Snooker Club. Currently, the league a player competes in is determined by a promotion or degradation at the end of each competition season. But that will change, next year we introduce a ranking system. Every match a member plays, either in competition, or in a tournament, can earn them ranking points. The amount depending on the type of match, the level of the opponent and of course the result. The total of all ranking points earned in the last 12 months results in the ranking score of a player and the ranking score determines the league they play in. This is obviously a functional dependency where a player's league depends on his or her ranking points. You could even call it a derivation rule, but the ranking system is new for our club and we may need to tweak things. If all members end up in the same league, we will have to change the threshold values until the distribution is satisfactory. So we decide not the implement this as a derivation rule, so that we can change the thresholds without having to touch the code. This entity type violates Third Normal Form. The attribute League depends on the non-key attribute Ranking Points. If we follow the standard normalization procedure, we fix that by introducing a new entity type in a diagram, resulting in a new table in the relational model. This new table can be used to determine League based on Ranking Points. But it doesn't do this very efficiently. With the current threshold values we will need 500 rows for the C League, 1500 additional rows for the B League and who knows how many rows for the A League, and that is only when we assume that the ranking score of a player is an integer value. If fractions are possible too, we would need an infinite amount of rows in this new table. The solution is easy if you just start with the actual examples of the data and take it from there. The scrap of paper listing the relationship between Ranking Points and League is information too. So we can apply the normal procedure of reading the information, transforming to facts, analyzing the facts, and then adding them in the entity relationship model. The result will be like this. We now have a table with a single row for each range of points and candidate keys on both start and end of the range. Ranges should never overlap in tables of this kind, though gaps may or may not be allowed depending on the actual problem area. They are not allowed in this case. We have a third candidate key on League in this specific scenario, but there can also be situations where multiple ranges resolve to the same value. If a range has no gaps, you can choose to include only the lower bound, or only the upper bound of each range in the implementation, or you can include both. In the latter case, you must make sure to add constraints to enforce that they are no overlaps and no gaps. If you have to implement a range that does allow gaps, your only option is to store both the lower and upper bound and to add constraints to prevent overlaps. For the referencing table, the simple approach is to just keep the referencing value as it already was, but this means that you cannot enforce the relationship with a foreign key constraint. As a result, it is possible to enter values that don't form a proper reference. For instance, because they are below the lowest value, or above the highest value, or because they fall in a gap. You can prevent some of this by adding an extra column in the referencing table that stores either the lower bound or the upper bound of the referenced range to implement the foreign key relationship. This prevents some, but not all possible inconsistencies. This pattern can also be valuable to make it easier to join a referencing row with the referenced data. If you are serious about preventing inconsistencies you will have to add two columns to the referencing table for both the lower and the upper bound. You can then define a single foreign key constraint on the combination. This may require some trickery because a foreign key normally references a candidate key and the combination of lower and upper bound includes two candidate keys. You can now also include a check constraint to ensure that the referencing value in the table falls between these bounds. This constraint is not a pure domain rule, so this design can never be in Domain-Key Normal Form, but most mainstream database management systems do allow you to specify this restriction with only a minimum of user supplied code. The second pattern I want to show is for storing historic data. When designing for OLTP, Online Transaction Processing, we often care about current values only and don't bother with history. In a data warehouse, this is quite different. That's why design methods for data warehouses include several standard patterns for dealing with changing data. Data warehouse design is not the subject of this course, but keeping track of historic data does come up in OLTP databases too. The classic example is a price list. When the price of a product changes, it should affect future orders, but orders already placed should continue to use the price as it was when the order was placed. Even if you don't save a lot of history in the database, you will still have to deal with questions about and corrections to orders used in the previous price in the days just after a price change. A very simple approach to this problem is to add attributes for the previous price and the date of the last price change. If you never have to work with data other than one price change, this pattern will suffice. However, in most cases you will that this is not sufficient. A pattern that allows you to keep the entire history is to add a column ValidTo to the table. That column will contain the last day that the data and the row was valid. For the row that described the present, the current row, the ValidTo column can be left empty using a NULL marker, or you can use the highest date value supported by the database management system. To prevent violations of the constraints that implement the candidate keys, you should add the ValidTo column to every candidate key. At the risk of stating the obvious, note that by solving the historic data problem, I have now introduced a referencing arrange problem. Suppose I have recorded the sale of the product Standard snooker ball set on August 12, 2013. There may be multiple prices for that product, each valid during a specific date range, and the correct reference should be to the row that was valid for a date range that includes August 12, 2013. For ease of handling the data, the actual implementation in the database will often add a view on top of the table that exposes only the data in the current row, leaving out the ValidTo column. So that the developers can used that view as if it was the original table before adding the logic to handle history. But it is also possible to reverse this design, that is, create two tables, one for the current data and one for only historic data. Optionally combined with a view that exposes the union of these data sets. Both approaches have their pros and cons. Whatever the chosen implementation, the downside of this approach is that a lot of storage space is wasted. When only a single value changes, a copy of the full row is made, including all columns that have not been changed. Obviously a normal form like Sixth Normal Form or Optimal Normal Form that creates lots of small tables, instead of fewer tables with more columns, reduces the amount of overhead involved with storing historic data in this pattern. That is why these normal forms are often considered ideal when working with temporal databases. Databases that are designed specifically to handle the challenges of data and sometimes even schemas that evolve over time.

Summary
In this module I covered all the so called higher normal forms. Elementary Key Normal Form and Boyce-Codd Normal Form build on Third Normal Form by disallowing some or all functional dependencies of key columns or non-key columns. If a table is in BCNF, every functional dependency is always on a candidate key, but BCNF is not always possible. EKNF can always be achieved at the price of allowing some key columns to depend on a non-key. Fourth Normal Form imposes an additional restriction on the design related to multivalued dependencies. A non-trivial multivalued dependency is only allowed if the determinant is a key, which implies that the multivalued dependency is actually a functional dependency, technically a special case of a multivalued dependency. Fifth Normal Form takes this another step further by introducing the so called Join Dependency. We saw that multivalued dependencies are a special case of Join Dependencies and this implies that a functional dependency is also a special case of a Join Dependency. In Fifth Normal Form, functional dependencies are allowed, but all other non-trivial Join Dependencies are forbidden. Violations of Fourth and Fifth Normal Form tend to occur more often when working with an attribute based modeling method. If you use a fact based modeling method instead, you will violate these normal forms less often because people intuitively tend to read facts in a way that already conforms to Fifth Normal Form. This is not a guarantee though, so even when using a fact based modeling method you still have to be wary of possible violations of these normal forms. Sixth Normal Form takes this yet another step further. In Sixth Normal Form, all non-trivial Join Dependencies are forbidden. This typically results in a design with a large number of small tables and may make it very hard to implement relationships as foreign key constraints, or to enforce some other constraints. Optimal Normal Form is very close to Sixth Normal Form, but does allow some Join Dependencies when the columns originate from a single fact type. Domain Key Normal Form is usually considered lower than Sixth Normal Form, but could just as well be placed next to it because it uses a very different approach. However, it has been proven that DKNF implies Fifth Normal Form so it can be definitely considered as higher than Fifth. In DKNF, dependencies play no role, the only rule is that every constraint has to follow logically from only domains and keys, a requirement that in practice can be impossible to achieve. In databases that are built primarily to support Online Analytic Processing and reporting (_____), some of the normalization steps are sometimes undone. Redundant storage of data is the price to pay for increased performance. This process is called denormalization. Some problems cannot be solved by simply applying the normalization rules. Two of these are the referencing range problem and the historic data problem. I have shown a few standard solutions that can be used to tackle these problems.

References
More information on all steps of normalization can be found on the Wikipedia article about normalization, which includes many links to article for further reading. For more information about Business Intelligence, and designing for data warehouses and reporting, you can visit the Business Intelligence section of the Pluralsight course library.

Other ER Methods
Outline
Hello, my name is Hugo Kornelis and I am presenting this Pluralsight course on Relational Database Design. This is Module 9, the final module in which I will give you a broad overview of the many alternative methods available for diagraming your entity relationship models. In this module we will investigate the many alternative representations that are used in various ER diagraming methods. Entity types may use specific notations for weak or strong entity types, or for subtypes. Attributes can be omitted completely, depicted in a compact way or very elaborate, and some methods even support special types of attributes. Relationships can also be depicted in a variety of ways. Identifying and subtype relationships may have a specific notation and the variations for specifying cardinalities appear unlimited. Plus, all methods have their own ideas of what types of relationships are or are not allowed, but despite these differences, once you see through the graphical differences most methods are actually all quite similar.

Many Methods
If you search for methods to represent an entity relationship model, you will see that there are many alternatives available. In this course I have used the IDEF1X notation. Not because I like it, I don't, but because it currently appears to be the most popular method. But if you are serious about a career in data modeling, it pays to know some other methods as well, for you will be confronted with them. Unfortunately it is impossible to cover all diagraming methods used for ER models. There are literally dozens of them available and that is just official methods. There are also lots of variations, usually created by combining elements from various other methods. Some of these are designed by companies who want their own house style, others exists as a result of individuals who develop their own personal habits and then pass them on to their successors. However, despite the differences, all these methods have a common base. They are all designed with the aim to illustrate data models that are created using an entity relationship based modeling method, so they will all have to include elements to depict the basic elements of these models, entity types, attributes and relationships. Instead of completely covering a random selection of available ER diagraming methods, I will give you an overview of how all these methods commonly represent these basic elements and what variations and alternatives are available in some of these methods. You will see some impressive differences, but you will most of all see that despite the different representations and flavors, the underlying concepts of all these diagrams are very similar. Once you see that, you will find that you can adapt to different diagraming styles with far less trouble.

Entity Types
For some reason I never really understood all entity relationship diagraming methods, choose the rectangle as the base form for representing entity types. Okay, there are differences. The height have be less than the width or more than the width, corners may be square or rounded and the name can be inside or outside the rectangle, but the basic rectangular form is always used. The distinction between strong and weak entity types is not represented in all ER diagraming methods. When it is, weak entity types are often depicted with a double border versus a single border for strong entity types. Though there are other ways to show the difference as well like for instance, in IDEF1X. Subtypes probably have the largest variation. Some methods have no specific notation for them at all. Representing them as normal entity types. Others use a notation similar to IDEF1X, using a normal entity type symbol, but specific symbols on the relationship to show that it is a subtype relationship and perhaps to visualize other properties like mutual exclusiveness or completeness of the subtype relationship. But there are also methods like for instance, Oracle designer that represent a subtype by including its entity symbol within the supertypes entity type symbol. This representation has some great advantages. It is probably the most intuitive visualization of the subtype concept, as it clearly illustrates that the subtype includes a subset of the instances of the supertype. When a supertype has multiple subtypes, it is also very easy to represent them in a way that clearly shows whether these subtypes are mutually exclusive or not. And a set of subtypes that is complete, can be illustrated by ensuring that the subtypes combined cover the entire width of the supertypes rectangle. But the downside is that the diagram can become cluttered and hard to read when the subtypes themselves have subtypes. And once you start adding relationships and attributes the diagram becomes an incomprehensible mess very fast.

Attributes
The simplest possible way to represent attributes, is to not represent them at all. And there are in fact ER diagraming methods that do exactly that. When using (_____)and ER diagrams, this is a complete data model. It is clear that this can never be used as the sole documentation for a database, and that is not the aim. The philosophy underlying the (_____) method is that all details should be stored in a data dictionary and the diagram only serves to give a very broad overview of the database as a whole. Adding a simple list of all attributes in the rectangle that represents the entity type is a very popular method. There are several variations in how additional properties are visualized. Key attributes can be placed in their own section underlined or decorated with a key marker, and some methods use symbols to distinguish optional from mandatory attributes, where as other methods choose not to expose that property in the diagram. There are also methods that use explicit symbols for attributes. The relatively unknown method manager ER diagraming style uses boxes with double lines for all entity types and boxes with single lines for attributes. The symbols used by this method to show the cardinalities of relationships are also used on the lines between entity type and attributes, to show if an attribute is optional or mandatory and to visualize candidate keys. A much better known diagraming method is (_____) ER. This method is popular in academic circles because of its rich syntax. But not quite as popular in business because of the large diagrams this rich syntax produces. In a (_____) ER diagram, attributes are depicted as circles, connected to their entity types with a simple line. Attributes that are part of the primary key are underlined. (_____) ER also supports a notation for composite attributes. Attributes that consists of multiple components, but are treated as a single unit. When converting to the relational model each of the components will become a separate column, but if the composite attribute is optional, you should add a constraint to insure that either all these columns are empty, or all are filled. Another feature that is unique to (_____) ER diagrams is the multivalued attribute. As the name says, this is an attribute that can have multiple values, so in this sample data model we can register more than one phone number for a club member. You can only directly implement this in the database if you use a multivalued database such as Pic. In standard relational databases, multivalued attributes will always have to be represented as additional tables. Likewise, the corresponding design in any other ER diagraming method would use a weak entity type for the phone numbers.

Relationships
Relationships are typically represented with one of their two base forms, either a line connecting the participating entity types, or a specific relationship symbol that's connected to all the related entity types. This symbol is very often a diamond, though other shapes can be used as well. Not all methods have a specific representation for identifying relationships. Those that do have a great variety of ways to represent this property, ranging from very obviously visible such as doubling the border of the diamond that represents the relationship, through something more subtle, like the IDEF1X distinction between drawn or dashed relationship lines, to something extremely subtle, such as adding a very small cross bar on the parent side of an identifying relationship. Methods that represent subtypes as rectangles inside the supertype usually don't explicitly include the subtype relationship. It is implied by the relative position of the entity types. Other methods do include this relationship. Almost always with specific symbols, these symbols are different for each method and often allow for variations to represent whether a set of subtypes is complete or not, and whether they are mutually exclusive or independent. As you have seen in this course One-to-many and One-to-one relationships will always be implemented in the relational database design by adding referencing columns to the table that corresponds to the child entity type. In many ER models, these referencing columns are also included as attributes in the child entity type and in some methods, like IDEF1X, even explicitly marked as the foreign key. But there are also methods that choose not to include explicit attributes for the foreign key columns. Some would say that this makes the model harder to understand, other say that it removes redundant information and hence makes the model simpler. An intriguing compromise is used in (_____) ER. Here, the foreign key attributes are normally left out, but for identifying relationships they are included. Underlined with a dashed line to distinguish them from normal key attributes. The more important properties of any relationship are its cardinalities. Every side of the relationship has two cardinalities, the minimum and the maximum cardinality. Sometimes collectively called a cardinality pair. These cardinalities can be represented in various ways. Some methods use more or less arbitrary symbols, such as dots, diamonds and letters in IDEF1X or single and double lines in (_____) ER. Other methods simply add a textural representation of the cardinality pair to each end of a relationship symbol. In these cases, minimum cardinality is normally represented as a 0 for optional or one for mandatory. Maximum cardinality is represented as one, or as a letter to represent a maximum cardinality of many. Typically, the letters M and N are used for this, which is why One-to-many and Many-to-many relationships are sometimes also called One to N, or N to M. To confuse things even further, ER methods all have their own idea on where to represent these cardinalities. A symbol for a maximum cardinality of many on the left side of a relationship can mean that an instance of the entity type on the left side can be related to many instances of the other entity type. Or it can mean that many instances of the entity type on the left side can be related to a single instance of the other one and the same confusion exists for the minimum cardinalities. I have seen methods that put both cardinalities on the same side, both on the far side and methods that have the minimum cardinality on the same side, but the maximum cardinality on the far side. So in order to correctly interpret the relationships in an ER model, you not only have to know what each symbol means, but you must also know where the method draws them.

Crow's foot Notation
In the previous section I did not include a family of symbols that is used by the majority of ER methods for cardinality, collectively known as Crow's Foot. This is not an ER diagraming method, but a generic name for a method to depict cardinalities using symbols that more or less intuitively express their meaning. This method gets its name from the symbol used for a maximum cardinality of many, which resembles the foot print of a crow, although there are also variations that use a rectangular form of the same symbol. Probably stemming from the time when many printers were unable to print diagonal lines. When the Crow's foot is placed on a relationship line like this, you can think of it as a line that spreads out from a single occurrence of Project to multiple occurrences of Employee. So a project can be worked on by multiple employees. There is no Crow's foot on the other end of the line, so a single employee can apparently work on only a single project. To make it perfectly clear that we didn't forget to put in a Crow's foot, but left it out intentionally, some methods replace the Crow's foot with a cross bar. Unfortunately there are other methods that use the same cross bar symbol to mark a relationship as identifying. So again, you really have to understand what method is being used and what home brew variations might apply, before drawing conclusions from and ER diagram that is handed to you. Within the Crow's foot family, minimum cardinalities can be represented in many ways. One way is to add a small circle on the line, this circle can be thought of as breaking the connection, or you can think of it as the letter O for optional. This circle here implies that some employees are not working on any projects at all, and a circle on the other end shows that there can also be projects with no staff assigned. As with a Crow's foot, a minimum cardinality of one can be either represented by just omitting the circle, or by replacing it with a cross bar. Another way to represent minimum cardinality is by drawing the relationship with a drawn line for minimum cardinality one or a dashed line for minimum cardinality 0. This is also quite intuitive. The dashed line suggests that not all occurrences of the Employee and Project entity types have to participate in this relationship, but it can get confusing when the two ends of a relationship have a different minimum cardinality because again, methods disagree on which side of the relationship to use. So the relationship shown here might mean that some employees don't work on a project, but all projects are being worked on, or it might mean that all employees work on a project, but some projects are not being worked on. You need to know the method used in order understand the meaning of this symbol.

Special Relationships
A part from the differences in notation for relationships, methods can also differ in the kinds of relationships they allow. A few methods do not allow Many-to-many relationships in the ER diagram. They have to be mimicked by using an extra entity type to implement this relationship. If you think back of how Many-to-many relationships are represented in the relational database design, you will probably see why this choice has been made by these methods. Without Many-to-many relationships, the mapping between relational database design and entity relationship diagram, is much simpler, almost one on one, but the downside is that you lose expressiveness in the diagram. Not using Many-to-many relationships is maybe okay if the ER diagram is made one for an audience of developers and DBAs who will have to implement the model in their database. But if you also include the ER diagram in the documentation, or use it to discuss the data model and the information represented, I would personally much rather use a method that does allow me the option to use Many-to-many relationships if I feel that they better represent the information. The number of entity types that a relationship connects is called the Arity or the grade of the relationship. Relationships between two entity types are called Binary or grade 2, with three entity types, the names change to Ternary or grade 3 and so on. Lots of ER diagraming methods, such as IDEF1X only allow Binary relationships, but some methods allow higher grades as well. These are usually the methods that use a specific symbol for the relationship, such a symbol can easily be connect to all participating entity types. In this diagram, the ellipse represents a relationship that is formed from the fact type, Employee reports on Project to Department. Cardinalities in relationships of higher grade can be challenging. The minimum cardinality is usually not the problem. The minimum cardinality of one for project means that every project must appear in at least one occurrence of the relationship. In other words, every project is reported on by at least one employee to at least one department. But there is no such restriction for employee or department, so there can be departments that don't receive any reports and employees that don't write reports. But what about the maximum cardinality? Does the maximum of many (_____) employee side mean that an employee can report on several projects? Or that she can report to several departments? In a few reports on multiple projects, do these reports all go to the same department or to multiple departments? Well, no, the actual meaning of this many symbol is that every combination of a project and department can be reported on slash 2 by multiple employees. If all maximum cardinalities are many, the net effect is that all combinations are possible, and in my opinion, that is the only valid way to use relationships of higher grades. Unfortunately, some methods also allow higher grade relationships where one or more ends of the relationship have a maximum cardinality of one, as shown here. The maximum one at the department side means that every employee project combination can be reported by slash on to only a single department. In other words, employees can report on multiple projects and projects can be reported on by multiple employees, but if employee X reports on project Y, it will always be to a single department. That exact same information can be modeled in a much more natural way by adding an entity type to represent who reports on what projects. You can think of this as a Many-to-many relationship, transformed into an entity type. And then add a mandatory relationship to specify what department these reports go to. Every relationship of higher Arity with at least one maximum cardinality of one, can be deconstructed in this exact same way and the result is always a data model that leads to less confusion and better represents the structure of the information stored in the database. So please, if you use an ER method that supports higher grade relationships, never use them with maximum cardinalities other than many on all sides, and even if the maximum cardinalities are all many, you should still ask yourself a few questions first. Remember the previous module on higher normal forms? Most violations of Fourth and Fifth Normal Form I have seen come from including a higher grade relationship that should not have been a single relationship. Are you sure you didn't make that mistake? Then think about the confusion caused by the cardinalities. Are you sure you used them as intended by the method? Will you be able to understand their meaning if you revisit this model a year from now? And are you sure your colleagues understand these cardinalities the same way you do? If you cannot answer all these questions with a firm yes, then you should avoid using relationships of a higher grade. They are never needed in an ER diagram and the alternative notation as an entity type causes less confusion and less errors. A final special notation I want to show is used only in (_____) ER. This method allows attributes to be attached to relationships. When the relationship is One-to-many, then this is technically equivalent to attaching the attribute to the child entity type. For a Many-to-many relationship the equivalent would be to transform the relationship into an entity type and then attach the attribute to that. This is how all other ER methods would represent this. And this is how I would represent it even when using (_____) ER diagrams, but other people may use this notation, so you need to know the meaning.

Summary
There are lots of different methods in use to diagram entity relationship models, and they all use different symbols and sometimes even allow or dis-allow different constructions. Entity types are usually represented as rectangles. Weak entity types and subtypes may or may not be represented in a different way. Attributes are sometimes completely omitted, sometimes included as a list within the entity type symbol, or sometimes represented with their own symbols attached to the entity types. Properties such as optionality or candidate keys can be represented in a variety of ways. Relationships are usually represented either as lines between the entity types, or as a diamond or other symbol that is connected to all participating entity types. Identifying and subtype relationships may or may not have their own dedicated symbols. Cardinalities can be represented with symbols with numbers and letters, or with one of the many variations of the Crow's foot notation. Variations that are close enough to look the same, yet different enough to be able to cause enormous confusion. Many-to-many relationships and relationships between three or more entity types are only permitted in some ER Diagraming methods. But they can always be expressed in an alternative way by introducing an extra entity type. All these different representations can make it hard to understand data models. The best thing you can do is to first make sure that you get very proficient in just one method. Then, when confronted with a data model in another diagraming style, transform the different notations to the style you are familiar with. You will first do this on paper, but with time and experience, you will learn to do this in your head. Looking at the data model, you can mentally visualize the corresponding data model in your familiar notation and use that mental image to understand what the data model represents. Once there, you can call yourself an expert data modeler. That concludes this Pluralsight course on Database Design. Now go out there and apply your knowledge because only practice makes perfect. Thanks for watching.

Course author
Author: Hugo Kornelis	
Hugo Kornelis
Hugo is co-founder and R&D lead of perFact BV, a Dutch company that strives to improve analysis methods and to develop computer-aided tools that will generate completely functional applications...

Course info
Level
Intermediate
Rating
3.9 stars with 355 raters(355)
My rating
null stars

Duration
7h 33m
Released
6 Jan 2014
Share course
