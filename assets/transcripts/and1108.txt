Overcoming Common Android Performance Barriers
by Omri Erez

Android is the most widely-used mobile OS in the world. This course teaches you how to investigate, evaluate, and improve common performance hazards related to memory, UI rendering, and more using the latest analysis and profiling tools.

At the core of any Android application is hidden performance glitches that cause the user experience of your application to be poor. In this course, Overcoming Common Android Performance Barriers, you will learn how to monitor, detect, and optimize performance hazards that will significantly improve the experience of your users. First, you will learn about memory hazards and memory leaks. Next you'll explore poor network and power consumption, as well as poor UI implementation. Finally, you'll learn how to lift all of them using the latest Android profiling tools. When you're finished with this course, you will have a foundational knowledge of investigation and enhancements of performance threats that will help you as you move forward to perfect the user experience in your application.

Course author
Author: Omri Erez	
Omri Erez
Omri is a software engineer and award-winning Android developer. His first introduction to programming was learning C at the age of 12. While starting his bachelorâ€™s degree in Information...

Course info
Level
Beginner
Rating
4.9 stars with 38 raters(38)
My rating
null stars

Duration
2h 4m
Released
13 Jan 2017
Share course

Course Overview
Course Overview
Hi, everyone. My name is Omri Erez, and I'd like to welcome you to my course, Overcoming Common Android Performance Barriers. I've been a professional software developer now for over 10 years focusing on the Android platform, and I'm super excited to present you with this course. Developing amazing Android applications is not a simple task, which involves mustering a complex set of skills. One of the most important and challenging ones is to analyze our code for performance weaknesses, and eliminate them in order to deliver our users with a superior experience. In this course, we are going to learn together how to analyze, detect, and optimize common performance issues in our applications related to the following subjects, memory consumption and memory leaks, network consumption, UI and GPU rendering performance, and battery usage. By the end of this course you will understand the basic concepts which drive performance issues, use advanced analysis and profiling tools, and learn how to hunt, optimize, and eliminate performance hazards in your applications. This course is especially designed for beginners. Before starting this course, you should have basic knowledge of Android development, and Android Studio. I hope you'll join me on this exciting journey to learn about Android performance with Overcoming Common Android Performance Barriers course at Pluralsight.

Analyzing and Understanding How Your App Is Using Memory
Introduction
Hi, I'm excited to warmly welcome you to this course. My name is Omri Erez. I assume that you choose this course either because you want to improve your Android Apps Performance or you desire to learn about Analysis tools to help you to eliminate performance issues in the future. With no further delay, let's start. In this module we are going to learn how to detect, optimize and avoid common problems related to your app's memory consumption. We will take a look at multiple examples which demonstrate poor memory management caused by our application. So, why is it important to be aware of our application's memory utilization? Generally speaking, RAM or random access memory is an expensive resource in any software environment but it's even more expensive where the memory limit is low and being constrained by the operating system. This is the case on the Android operating system. It's constraining all applications' memory and can even shut down our application if it has poor memory utilization. In order to understand how Android is handling memory, we need to examine the component in charge of freeing unused memory. What's garbage collection? Garbage collection is a form of automatic memory management. The garbage collector or GC attempts to reclaim garbage or memory occupied by objects that are no longer used by our application. Luckily for us, the Android garbage collector will do most of the work for us. In order for the GC to be able to reclaim the object's memory, it is not allowed to be strong referenced in our code. If the GC fails to reclaim the object's memory, a memory leak will occur. This particular problem is discussed in the next module of this course. As we allocate more objects in our application, a periodical garbage collection will occur. The more objects we allocate in our app, the more GC events will be invoked. Doing a GC event, all threads are suspended. Frequent GC events can cause you UI hiccups, AKA our application's UI is not responsive. Our job as developers is to allocate as less objects as we can and avoid frequent GC events in order to provide our users with a smooth experience.

Optimizing Loops
The first issue we're going to talk about is optimizing loops. We use loops frequently. This is why it's important to be aware of flow performance looping limitations as it can make a real difference in terms of memory usage. First, let's talk about the term I will use often, memory footprint. A memory footprint refers to the amount of main memory that a program uses or references while running. I will talk about a method, iteration and line of code memory footprint. The short explanation about the demo project structure. For each module in this course, we will examine a demo application I prepared. You can find the correct demo module by simply looking at the module name. A module name that contains the word before, represent the demo application before our optimizations and after represent the demo application after we optimize the all issues. Our demo application contains one activity with three floating action buttons, or FABs. A click on each of the FABs will invoke a low performance code execution block. For each action we will examine our application's memory behavior using the memory monitor. We will identify the problem, optimize it and compare both memory behaviors of before and after our optimizations. Let's open our demo application in Android Studio. Let's open M2 before module. The main class of this module is the M2 before activity. As I stated before, this activity contains three FABs. For this part of the demo, we will use the memory monitor. You can see the memory monitor by clicking the Android monitor button on the bottom of the screen. Here we can see a running chart of our app's memory usage in real time. Make sure to choose the right package name in order to see the relevant information. When clicking on the yellow FAB, our run loop method will be invoked. After each time the loop was executed, we introduce a small delay. This method contains a poor performance loop implementation. Let's click on the FAB and watch the memory monitor. Oh, oh, we can see something strange. After clicking on the FAB, our memory graph went wild. Every time your app's memory graph looks like this, you know you have a memory problem. This is how we can identify a situation of frequent garbage collection events. Every time that the graph has a falling edge, a GC event occurred. As we learned before, we should avoid a situation such this one in order to make sure our code doesn't contain poor memory utilization. It means that our code is allocating a high number of objects in the short period of time and the GC reclaims their memory. If we were to increase the number of loop executions and comment out the slip-delay method, the execution of the loop will be much faster and we would see something similar to that. It's called a memory churn. This is an indicator for a poor memory behavior as well. Another indication can be found in our LogCat. If you see multiple LogCat entries that contain the text sweep GC freed, this is a sign that the garbage collector is working overtime because of our application. Let's examine our implementation to check what caused our memory problem. Here we have a loop which will run 4, 000 times. It's a simple loop. The actual loop implementation is inside the poor performance loop method. After the execution was finished, we will measure the execution time. Inside our poor performance loop method we can see there is another loop which runs 20 times. Here we have a string array of length 20. It contains some numbers. On each iteration we will take the string value, create a new double object with it and then add it to our ArrayList of type double. In order to solve our little problem, we have to understand where are the objects and references allocated and the size of all objects in memory. Using the allocation tracker, we can found out both. Let's open our baselib module and open our M2 base activity. This is the superclass of all our activities. Let's try to measure the memory footprint of our simple allocation method. We will add two breakpoints which will define the code block we want to measure. We will run our application in debug mode and wait until the first breakpoint is invoked. After it's invoked we will click the start allocation tracking button, resume our program and wait until the second breakpoint is invoked. Then we will click the stop allocation tracking button. You will see a report similar to this one. Congratulations! You just finished to record your first allocation tracking. It's pretty simple, right?

Analyzing the Allocation Tracker Report
In this report we can see the method stuck trace and their allocations done by our code. It can be a little bit tricky to find the right allocations. If we expand the fifth item, we can see all allocations done in our simple allocation method. We can see that at line 41, one instance of the object class was created and its size is eight bytes. The object class is a superclass of all Java objects. These eight bytes contain the object reference, some housekeeping bytes for the object itself and the object header. As you can see, the allocation tracker is giving us a pretty accurate picture of what's going on. I'm saying pretty accurate because it's not 100% accurate. If we take a look at line 43, the allocation tracker tells us that we allocated one string object and one double object. This is obviously wrong. We can see in our code that our double object was allocated at line 43 but the first string object was allocated at line 46. Respectively, all string allocations in the allocation tracker are shown one line later. Maybe you wondered why we cannot see the allocations from line 44 and 45. This is because the allocation tracker is not tracking primitive types. In addition, we can see that between lines 46 and 49 we allocated four strings, all with different length but when we look at the allocation tracker report, we can see that they all have the same size and it's 24 bytes. Surprising, huh? Not really because the string object has a memory overhead of 16 bytes and in addition at that eight bytes for its characters. The Java virtual machine is allocating memory in chunks of eight bytes. This is why up to four characters, all strings has the same size. Just as a reminder, a character in Java has a size of two bytes. We can verify my claim if we look at line 49 in our report. Here there was a string allocated and its size was 32 bytes. This refers to line 15 in our code where we allocated the string which has a length of five. The virtual machine needed an additional eight bytes in order to store the fifth character. If we were to allocate the string with a length of eight characters, it will have the same size in memory. Now let's go back to our loop and examine our allocations. For each iteration of our loop, we are allocating a string array of length 20 and a double object. Let's try to calculate the memory footprint of one iteration of our loop. Per iteration, we have 20 strings, multiply 24 bytes plus an additional 16 bytes for the double object, equals 496 bytes. In reality, the memory footprint will be a little bit higher as the string array and ArrayList introduce a small memory overhead. Let's try to calculate our run-loop method memory footprint. Roughly 500 bytes per iteration, multiply 20 iterations, multiply 4, 000 calls equals 40 megabytes. That's a lot of garbage to be collected. The source of our problem is the fact that we are allocating a high amount of short-living objects. In each iteration we are allocating new objects and then their garbage collector is trying to reclaim their memory. We should avoid that and try to minimize our object allocations. Let's see how we can optimize our loop implementation without changing its functionality. Let's open our M2 after module and open M2 after activity. At our new implementation, you can see that instead of allocating a new string array for every iteration of the loop, we just defined a final static double array which contains our numbers. If you think about it, we don't really need the string array, as our array contains only numbers. In addition, we defined the length of our array in our ArrayList as constants. Another optimization we did is to define the ArrayList length. This will minimize the memory overhead of the ArrayList add function, as well here the allocations will be done only once. If we run our code again and click on the FAB, we can see that the GC events are gone. We can still see an increase of around 1. 2 megabytes after our code is executed and this is the new memory fingerprint of our loop. If we measure one iteration memory fingerprint, we will see that it is 320 bytes and this is because we are adding 20 doubles into our ArrayList. 16 bytes per double, multiply 20 equals 320 bytes. 320 bytes multiply 4, 000 loop runs equals 1. 28 megabytes which is corresponding to our memory increase after our code was executed. It is still a high amount of garbage to be collected considering the fact that we are not doing anything useful with our loop but it is 30 times less memory before and after optimizations. Another benefit we got from optimizing our loop is a shorter execution time. If we compare the execution time of the before and after optimizations, we can see that before our optimizations, the loop took 47 microseconds and after, only 19 microseconds. After optimizations our loops' memory footprint is around 30 times less and it is two and a half times faster. That's a serious performance boost. Now imagine that by keeping in mind these simple rules, your loops implementations can be up to 30 times more efficient in terms of memory. Remember, if you're iterating a small dataset, these optimizations are less important as the number of references and objects would not be so high but it's still better to keep these good practices in mind and be strict with your memory allocations, as every byte counts.

Creating a Million Strings
At the previous demo of this module we analyzed the problem related to poor performance loop implementations. These performance flaws are not relevant only for loops. In this section of the module we will examine a similar situation where we create a huge amount of strings. As I stated before, our job as developers is to allocate as less objects and references as possible and try to avoid frequent GC events in order to improve our app's performance. Let's take a look at the table I prepared. We will refer to the object memory overhead as OMO. All sizes in the table are in bytes. Let's compare creating a primitive type with creating and autoboxing Java object. For example, if we define a float primitive type it will take four bytes in memory but if we define a float object instance it will take 300% more than its primitive equivalent and that would be 16 bytes. Same sizes will be allocated also with an int and an integer object instance. In case of a double primitive type, its object equivalent will take 100% more memory and that would be eight bytes, compared to 16 bytes. The string object is special because there is no equivalent primitive data type for a string. The obvious conclusion from this table is that it's more memory efficient to use primitive types over autoboxed classes, such as float, double or integer. Let's focus on strings. Strings are immutable in Java. Once created, they are closed for change. That implies that the wrong usage of strings can cause a lot of garbage collection. In the following demo we will update a text of a TextView every 10 milliseconds and measure the memory footprint of this operation. We will record our app's objects allocation using the allocation tracker, analyze it, optimize it and compare the results. Let's go back to our M2 before activity in Android Studio. When clicking on the green FAB, our start RAM usage method will be invoked. When executed it will start a new thread and update the text in our TextView every 10 milliseconds and we will see our app's memory usage on the screen. Let's click on it three times and watch the memory monitor. Can you recognize the problem? As before, we can see that multiple GC events occurred. Now, if you want to find out how many bytes were allocated for classes related to your package name java. lang including all strings and floats allocated, is that possible? If you guessed yes, you are right. Let's do it. Let's run our code in debug mode. Start allocation tracking, click the green FAB, wait until the TextView is not updated anymore and click stop allocation tracking and again, the allocation tracking report will appear. Instead of using the group-by method report, we will choose the group by allocator report in order to find out how many string-related objects were allocated. If we expand the java. lang package we can see the following objects. We can see that the number of objects allocated from the java. lang package name is roughly 11, 500 and their total size was around one third of a megabyte, which is 26% of all allocations. In addition, we can see that 5, 500 strings were allocated and their total size was 116 kilobytes. Another interesting observation is that our code is using, internally, classes related to the StringBuilder class, although we don't use it explicitly in our code. This, of course, costs us some memory as well. Let's take a look at our code. First, we are calculating the use memory bar application, as well as the total free memory of the system. Then, we are refactoring it to have a megabyte representation. Finally, we are setting our formatted string using the set text method of the TextView class and we are repeating this operation 500 times. If we go back to our report, we can see that we're creating internally a huge amount of strings. So, how can we optimize this performance fiasco? We will perform the following optimizations. We will extract constants where possible, induce the StringBuilder class to optimize our string creation count. Let's talk about the StringBuilder class. The advantages of the StringBuilder class are, it is faster for operations related to string modifications and it is more memory efficient. This class will help us to minimize the memory footprint of our code. Let's take a look at our optimized code. Open the M2 after activity in our demo. Here we can see that we extracted some constants with the megabyte divider and our original string. Next, we initiated a new StringBuilder instance with our string and the string variable for our formatted floats. In addition, we defined the reference to the runtime object and some help variables. This will contribute to a faster execution time. The important part comes after storing the memory values at line 45. Here we parse the float value into a string and extract the first four characters. Next, we will use the delete method from the StringBuilder class in order to delete the respected characters and insert the new formatted string. We will repeat this operation for the free run value as well. Let's run our code and record again the memory allocations. If we take a look at the total amount of java. lang objects, we can see it's much lower, as well as its total size. Let's compare the results. Before our optimizations, our code created around 10, 000 string-related objects and after, only 2, 500. That's four times less. In addition, the size of all string-related object was around 300 kilobytes and after, only 88 kilobytes. That's a serious improvement. If we take a look at the java. lang objects, we can see similar results. Before our optimizations, their total size was 333 kilobytes and after our optimizations, only 137. We improved our code's memory footprint by 59%. That's a serious boost and only by using the simple StringBuilder class. Simple, huh? And I have news for you. This is not the most efficient way to perform this operation. If you try to implement the same function using char arrays, you could even minimize more our code's memory footprint. This is the deal with performance optimizations. I guess that what I'm trying to say is when correcting performance issues, you should determine which problems have the highest payback. In other words, which issues will deliver the greatest performance increases for the time you invest in correcting them.

Loading Images
The last issue we are going to talk about is image loading. We will learn how to minimize our imagery memory footprint and load images efficiently. An important part of all Android apps is high-quality imagery. Most apps that provide an immersive experience contain rich imagery. The problem is that imagery is expensive in terms of memory and even more expensive if not loaded with consideration. The Android operating system limits the amount of memory that can be used by bitmaps. In extreme cases, it can even throw an out of memory exception and state that the bitmap size exceeded the virtual machine budget. As developers we should avoid this situation in order to make sure our users are happy. Let's investigate how expensive is image loading. In general, the image size varies and depends on the size of the image and the format of its pixel. For simplicity, let's assume that our image's format is ARGB8888, which implies that our pixel size in memory is four bytes. Let's do a small exercise. If the width of our image is 1, 280 pixels and the height of our image is 850 pixels, each pixel needs four bytes in memory, what will be the size of the image be in memory? 1, 280 pixels multiply 850 pixels multiply four bytes per pixel equals around 4, 300, 000 bytes, hence 4. 3 megabytes and this is only for one image. As you can see, images require a significant amount of memory, so it is important to know how to load them efficiently. Let's verify our calculation using our demo application. We will load an image into an ImageView, dump our app's Java heap and detect the size of the image in memory. Let's go back to our activity in Android Studio. When clicking on the red FAB, we will load an image into an ImageView by inputting our image's resource ID into the set image resource method. Our image is inside the drawable folder. Let's open it. As you can see, the dimensions of our image are the same as in our calculation. Let's click on the red FAB and watch the memory monitor. The memory consumed by our application is greater than before. In order to verify our calculation, let's dump our app's Java heap by clicking the dump Java heap button. You will see a similar report. This table represents a snapshot of the memory consumed by our application. All objects allocated in our code can be found here. That's a lot of information to process. For the sake of simplicity, we will concentrate on the relevant columns in this report. The retain size column represents the size of memory that all instances of this class is dominating and the heap count represents the number of instances in the selected heap. At the right section of our report, we can see in detail all different instances of type byte array and their size in memory can be seen on the dominating size column. On the bottom section of our report, we can see the reference tree and see exactly where the byte arrays were allocated. You probably guessed right and you can see that this allocation was done in order to load our image into the ImageView. One byte array was allocated and its size is 4. 35 megabytes. That verifies our calculation from before. If our app contains a high amount of images, you can imagine that the size of 4. 3 megabyte per image is unsustainable. So how can we optimize it? If we think about it, it doesn't make sense to load the original size of the image into a small ImageView because the scaled-down version of the image will be sufficient, it would not affect the quality. Our first optimization is to scale down the image to fit the container size. In addition, we will cache the image in memory in order to make sure our image is allocated only once, if we use it multiple times. We have two options. We can go and implement a scale-down function and the memory cache ourselves or we can take advantage of the fact that Android is open source, save time and use an image loading library to do it for us. There're multiple options out there and they're all rich in functionality. I encourage you to check the relevant documentation in order for you to choose your favorite. For our demo, we will use a library named Glide. Let's jump back to our demo application. As you can see, Glide's API are pretty simple. You get a Glide instance by inputting the app's contents into with. Then co-load and input our imagery source ID. This method support multiple inputs. We scale down the image by calling the overwrite method and input it with our ImageView dimensions. Next, we call the into method and input it with our ImageView. After running our code again and dumping our app's Java heap, we can see the following report. We can see that now the image size in memory is only 2. 71 megabyte. After our optimization, we can see that the image size in memory was around 50% lower. Pretty impressive, right? And it took us only five minutes and three lines of code.

Summary
Let's summarize this module. So, what have we learned? First, we learned how to engage with Android Studio's memory monitor and its features. Second, we learned how to identify and solve memory hazards using the real time memory chart of our app. Third, we used the allocation tracker and learned how to record object allocation for a certain operation. As our app's memory has a limit, every byte counts. Last but not least, we learned how to dump our app Java heap and look for specific object sizes in memory. In particular, we learned why it is important to be aware of our application's image size and memory. The more comfortable you get with the memory monitor features, the better you will understand the memory behavior of your application and master hunting memory hazards. Remember, application memory utilization is a very important issue and need to be addressed in your everyday development. In the next module, we will continue to use the memory monitor features for identifying memory leaks. Thank you for watching and I will see you soon in the next chapter of our adventure.

How to Avoid Memory Leaks in Your Implementation
Introduction
Hi, I'm Omri Erez and welcome to the next module of this course, How to Avoid Memory Leaks in Your Implementation. In this module, we're going to learn how to detect and eliminate memory leaks in our code. We will take a look at multiple common scenarios where our code can cause memory leaks. Let's start. What is a memory leak? A memory leak is a side effect in which our application persistently retains an object's memory. It occurs when our code doesn't release allocated memory, even after it is not needed anymore. But wait. In the previous module, we learned that the garbage collector takes care of reclaiming unused memory and that the Android system is a managed memory environment, right? That you see he's doing his job in the background and we don't really need to interfere with his work. Does it mean that we can sleep well at night and not be bothered with possible memory leaks in our code? Not really. Unlike Superman or other super heroes that have special powers and can do everything, the GC has limited abilities. There is a misconception I hear often among young developers who think they can just ignore this subject but it is actually really important to know the GC limitations. It's crucial to remember that the GC can identify an object is unused only if it's not hard referenced in our code. First, we have to understand the different life cycles in our application. Our application life cycle begins after the user launches our application. From that moment onwards, the decision of when to kill a certain component is made by the operating system or by an explicit command in our code. Our app contains various components. We will talk about two main components, activities and services. The important bit to remember here is that we have absolutely no control on the different life cycles. The Android runtime can decide to kill our activity or service even if we did not call the activity's finish method or the stopService method explicitly in our code. For example, if we call the finish method from the activity class, the Android runtime will try to destroy the activity then the GC will try to mark it for removal and reclaim its memory. Let's dive deeper into the GC process. To understand how the GC works, we need to review the beginning of the garbage collection process. Every GC within starts at objects we call GC Roots. The GC starts to visit all object reference from a root object. After the GC finished to visit all nodes, all objects which the GC did not visit can be marked for removal, and these are the objects which will be garbage collected. On Android we have multiple types of GC Roots. For example, static variables and functions are defined as GC Roots. As well as references on the stack and threads which are referenced in our code. Another type of GC Roots is Java Native Interface objects and memory. More GC Roots types can be found on the Android documentation. During this module we will concentrate on the first two groups.

Static References
The first issue we're going to talk about is static references and how they can leak an activity instance. In general, static references will be held in memory until our application is destroyed. Let's take a look at the simple example. We have an object A which contains two objects inside of it, B and C. In addition, object B contains an instance of object F. We assume that all objects have the same life cycle which means that the amount of time they live in memory is the same. When the time comes and the GC decided to clean some garbage, it will check which objects are not used anymore. Let's assume that now the GC wants to reclaim the memory of object B. If object F does not hold a strong reference to B, they can both be garbage collected. It's the only one who needs F is object B. Now, let's take a different scenario where object A needs to be garbage collected. Which objects will be collected? All of them, right? Because both objects B and C are contained by object A. When will the GC have a problem to collect object A? If for example at the time when the GC occurs, object C still holds a reference to object A which implies that object C lives longer than object A. Exactly at that moment, a memory leak will be born. The next time our system will want to create an additional instance of object A, it will have to allocate new memory for it. And unfortunately, the old instance of object A will still be in memory. The same will happen if object B has a static reference to object A. If the operating system will decide to kill object A, it will be leaked and all objects will stay in memory. As we learned before the amount of memory allocated for our application is limited and if our code creates more and more objects which accumulate in our heap, sooner or later the OutOfMemoryError will be thrown. Think about the objects as the bananas on the truck. If we load the truck with more bananas, eventually its tires will blow up. The truck is our memory heap. If you experience a situation where your application crashed because a random OutOfMemory exception, this is usually a good indicator for memory leaks. Let's see how a similar situation happens in practice. Our demo application contains a Welcome Activity with multiple buttons. When clicking on each button, it will start an Android component in particular, an activity or a service. Each component contains a possible memory leak. In order to trigger our memory leak, we will finish our activity by clicking on the screen. Then initiate the manual GC event and dump our Java heap in order to check if our activity is still in memory. Let's open our demo application in Android Studio and run our before module. When clicking on the static reference button, our static ref leaking activity will start. If we watch the memory monitor we can see an increase of around one megabyte which corresponds to the memory overhead of our activity. Next, we will click on the screen to close our activity and trigger a manual garbage collection event by clicking the Initiate GC button. Normally this is the point where the garbage collector will collect the activity's memory. But if we look at the memory monitor, we can see that even after clicking the Initiate GC button, our memory consumption is constant. A similar situation can indicate on a memory leak but we're software engineers and like accurate data so let's go ahead and dump our Java's memory heap and try to hunt the memory leak.

Analyzing the Java Heap Dump Report
If we take a look at our report, we can see that our activity is still in memory and that its retain size is around one megabyte. In order to find the reasons for the leak, we have to take a look at the reference tree. Here we can see that our mActivityContext member in our MyStringHelper class is holding a reference to our activity, and its depth in the reference tree is one. Same happens for our mLeakingView. Both of them are defined as GC Roots and their depth in the reference tree is zero which means that they are the top of our tree. This icon indicates a GC Root. Let's take a look at our code to analyze our problem. Our activity's code is very simple. When our activity is created and the onCreate method is called, we are assigning the root view of our activity to the mLeakingView static reference. mLeakingView is static. This is why its life cycle is greater than our activity's life cycle. And when the operating system tries to destroy the activity, mLeakingView still holds a reference to it. The second reason for our leak is our MyStringHelper class. It's a Singleton. We assign a reference to our activity by calling its setActivity method. If we take a look at our MyStringHelper code we can see that our MyStringHelper instance is static which implies that its life cycle is greater than the activity life cycle. Another interesting fact is that our activity was leaked because of MyStringHelper even though mActivityContext is not static. As you probably understood by now, the source of our memory leak are static references to our activity which have a longer life cycle than the activity itself. If we were to draw a simplified chart that represents our reference tree, it will look something like that. The GC can't mark the activity for removal because mLeakingView and our MyStringHelper instance are holding a reference to it. So how can we avoid our memory leak? In case we have to have a reference to the activity, please make sure to clear it before the onDestroy method is called. The better solution is to follow these best practices. We should avoid static references to activities, views or other Android components in order to make sure that all our variables and references has the same life cycle of the activity. In addition, as the rule of thumb, prefer using the application context when possible as it's bounded to our application's life cycle but not to our activity's life cycle. Let's go back to our demo application and open our after module. If we take a look at our new activity code, we can see that instead of using a static reference to our view we made it non-static. In addition, we input our MyStringHelper instance with our application context instead of our activities context. These changes will make sure that both references have the same life cycle as our activity. And when the GC decides to clear the activity from memory it will have no problems to do it. Let's run our code again to check if our activity is still in memory after we close it. After initiating the manual GC event, we can see that some memory was cleared. In our Java heap dump we can see that our activity is no longer in memory. This is it, we won the battle against the memory leak. Our activity is now memory leak free.

Anonymous Inner Classes
In this section of the module we will talk about how can anonymous inner classes can create memory leaks in our application and how to avoid it. First, let's talk about what is an anonymous inner class in Java. An anonymous inner class is a class declared without its class name. It's declared and initialized at the same time. Let's take a look at some examples. Usually we would use an anonymous inner class if we want to override a method of a class or override methods from an interface. It's important to remember that an instance of an anonymous inner class will hold a reference to its outer class or enclosing class. A quick explanation about our demo. We will launch an activity which contains a handler. The handler is implemented as an anonymous inner class. Let's open our demo activity. On our onCreate method, we will send an empty message to our handler which will be delayed by 20 seconds. Then we will click on the screen, finish our activity and trigger a manual GC event as we did before. When we look at our report we can see that our activity instance is still in memory. Uh-oh, it smells like a memory leak. We can verify that our handler is implemented as an anonymous inner class by looking at its class name. We can see our activity name following a $1 sign. This is an indication for an anonymous inner class implementation. In order to understand the reason for the memory leak, we need to talk about the handler and how it works. When our application starts, the Android runtime creates a looper object for our application's main thread. The looper object implements a simple message queue. When a handler is created like in our activity it's been associated with the looper's message queue. And when we send a message to our handler it will be added to it. When the looper is ready it will process our message. Until that point of time, the looper will hold the reference to our handler. In our activity the following will happen. Until the looper has processed our message which is approximately 20 seconds after we sent it, the looper will hold the reference to our handler and our handler of course will hold the reference to our activity because it's implemented as an anonymous inner class. This is why if we try to close our activity before the message is processed and the GC will try to reclaim its memory, it will be leaked. In order to avoid a memory leak, we have to make sure to remove the message from the handler before the onDestroy method is called. If we open our activity in our after module and repeat the same operations as we did before, we can see that our Java heap no longer contains our activity, and again our activity is memory leak free.

Non-static Inner Classes
At the previous example we saw how a usage of an anonymous inner classes can create a memory leak. In this section of the module we will examine a similar situation that occurs when using non-static inner classes. What is an inner class. An inner class is a class defined within another class. It exists within the instance of the outer class. Important to remember, like anonymous inner classes a non-static inner class will hold a reference to its outer class. Like before, our activity contains a handler. We will post a MyRunnable instance into the handler with a time delay. MyRunnable is implemented as a non-static inner class. This time we will take an extra step forward and use a tool called StrictMode. StrictMode is a set of APIs which notifies the developer on accidental mistakes. It is useful not only to detect memory leaks but for other areas as well. We will concentrate on memory leaks. StrictMode will bring these mistakes to our attention by notifying in the logcat or by even crashing the app. It depends on our configuration. In order to use it, we will define a VM policy, define which types of leaks we want to detect and define a penalty or the way in which strict mode will notify us. Let's jump to our code. We will open our inner class leaking activity. In our onCreate method we can see that we define the VM policy as follows. For this demo we are interested of detecting only memory leaks related to activities. If you check additional APIs from StrictMode, you can see that we can detect the different kind of leaks. In addition, we defined a penalty log policy which means that StrictMode will notify us in logcat with an error message. Here we create an instance of MyRunnable and post it to the handler with a delay of 20 seconds. Let's open our activity by clicking the inner button and click on the screen in order to close it. Next time we open our activity, we could see an error message in logcat. Our error message is from StrictMode and it states that we have an instance count violation. Currently we have two instances of our activity although we're supposed to have only one instance. This is an indication of a memory leak. Let's force a GC event and dump our Java heap. In our report we can see that we have two instances of our activity in memory. In addition, we can see that our MyRunnable instance is holding a reference to our activity. This is why it cannot be garbage collected. Let's focus on our MyRunnable class. It's a non-static inner class and it holds a reference to our activity. In addition it needs a reference to our activity in order to print its component name. So how can we achieve the same functionality without leaking the activity? It's actually pretty simple. We will transform our MyRunnable class to be static and use the WeakReference class. Static inner classes in Java won't hold a reference to their outer class. Two sentences about the WeakReference class. It will hold the reference to an object until the GC wants to reclaim its memory and will return null if the object was already reclaimed. Let's jump to our code in our after module. Here we can see that we made our MyRunnable class to be static which will make sure it won't hold a reference to our activity. In addition, we created a WeakReference as part of our MyRunnable class. When the time comes and now a run method will be executed first we will check if the reference is still available and if it is, we will execute our code. If we run our code again and do exactly the same operations as before, we can't see any error message in logcat. If we were to dump our Java heap we could see that our activity instance is no longer in memory. Our activity is no longer infected with memory leaks.

An Automatic Approach: LeakCanary
Until now we examined multiple scenarios where our code contained memory leaks. Maybe you wandered by now if there is any smarter way for doing this process of finding memory leaks in our code. In this part of the module, we will introduce a new tool which gives us the opportunity to monitor memory leaks on different objects and we will examine another example where we leak a service instance. In previous examples, we detected memory leaks by using the StrictMode tool and by examining our Java heap dump. I encourage you to train yourself to analyze your Java heap dump as it is very useful for other tasks and not only for detecting memory leaks as we experienced at the previous module of this course. But as our application development is getting more and more complex with time, we should use some automated tools to make our development process more time efficient. There is a great tool developed by Square, Inc. called LeakCanary. We can integrate it really fast into our project and automate the process of detecting memory leaks. We will specify which objects we want to watch. LeakCanary will automatically monitor these objects for memory leaks. Only this is performing the same manual steps we did until now and notify us with a notification if one of the watched objects was leaked. Our demo will contain a service with a memory leak. We will start the service called the stopService method, wait for LeakCanary to notify us and examine the results. Let's open our demo code and open our MyApp class in our before module. In order to use LeakCanary, we have to initialize a RefWatcher in our onCreate method of our application class. Now, let's open our MyLeakyService class. This is our service and it contains a memory leak. As you can see in our onCreate method, we initialize the timer instance and schedule a TimerTask which contains an infinite why loop. After the TimerTask is started the run method will run infinitely. And will hold the reference to our service even if the operating system is trying to kill the service. Smells like a memory leak, right? If we take a look at our onDestroy method we can see that after we call the super onDestroy, we get our RefWatcher from our application class and tell LeakCanary to watch our service instance. Let's click our start service button, wait two seconds and click the stop service button. After a short period of time, we would see a pop up from LeakCanary which states that it is dumping our memory. After LeakCanary finish to process our memory dump we would see a notification in the notification bar. When we look at the notification, it states that our MyLeakyService has leaked around one megabyte. If we click to open for more details we can see the cause of our leak. Here we can see that our timer thread is referencing an anonymous subclass of TimerTask and it has leaked our MyLeakyService. Pretty simple, right? As you can see by using tools like LeakCanary, we can save a lot of time that we spend on manual detection of memory leaks. I personally integrate LeakCanary in all my projects. Let's summarize our module.

Summary
First, we learned that memory leaks are bad, that our application has a limited memory space and that memory leaks will eat our application's Java heap. Second, we looked at multiple common scenarios of how our activities can be infected with memory leaks and how we can cure them. Third, we learned how to analyze our Java heap dump and identifying memory leaks by examining our reference tree. Last but not least, we learned how to make the process of finding memory leaks more efficient and how to move it to happen in the background using tools like StrictMode and LeakCanary. The bottom line is that memory leaks are sometimes transparent to us developers and that we have to be aware of their existence and how to avoid them in order to make sure our implementation is memory leak free. Thanks for watching and see you soon in the next chapter of our exciting journey.

Network Is Expensive: Learn to Sharpen Your App's Network Usage
Introduction
Hi I'm Omri Erez and welcome to the next module of this course. Network is Expensive: Learn how to Sharpen your App's Network Usage. While in the previous modules, we focused on the memory monitor memory hazards and leaks, in this module we will focus on the Network Analysis tool and how to optimize our app's web traffic consumption. Let's take a look at some statistics. It is no secret that the worldwide data traffic is increasing in an accelerating pace. By the year 2021 the world wide data consumption will be around 40 exabytes. One exabyte is a gigabyte squared. That's a huge amount of data right? Let's examine the price of one gigabyte in some countries in Asia. In Singapore, one gigabyte of mobile data costs $7. 11. The minimum wage in Singapore is around $4 which means that the user needs to work almost two hours in order to pay one gigabyte. Similarly in Thailand one gigabyte costs around three hours of work. In Malaysia almost four. If we take a look at some less developed countries in Asia we see the following. In the Philippines the average person needs to work five and half hours in order to pay this one gigabyte. In Vietnam six hours and in Myanmar eight hours. A full day of work to pay one gigabyte of mobile data. Crazy right? These statistics is relevant for other parts of the world as well. The obvious conclusion from this statistics is that network is expensive. As Android developers we have users from all around the world and we have to be aware of the fact that some of them are living in less affordable countries. If our application will have a high usage of network with no consideration, it will lead to a high number of uninstalls.

The Network Monitor
It is important to be aware of our application's network consumption. There are various tools we can use in order to do that. In this module we will concentrate on one tool which is available in Android Studio. The Network Monitor on Android Studio adds up the amount of time it takes for the device to transmit and receive kilobytes of data per second. We will examine a demo application I prepared. All the data presented in this demo was taken from the open NASA APIs which can be found on this page. Our demo application contains three tabs. The first tab contains astronomy picture of the day. We will present the daily image with some description. The second tab contains multiple images from previous dates and the third tab contains a satellite image of our current location. We will use this demo application for the following modules as well. Let's open Android Studio and run our app. We will look at the Network Monitor in order to examine our application's network consumption. While looking at the Network Monitor we can see our application's data usage on the Y axis and the timeline on the X axis. The first couple of spikes on the graph correspond to the loading of the astronomy picture of the day and the first images in our second tab. Let's navigate and watch the Network Monitor. As we scroll across up and down, we can still see that our application is consuming data although the images are static. This is a bad behavior right there. In addition we will simulate our location by using a pre-defined file I prepared. Let's open the emulator options and load our GPX example file which can be found at the root folder of our demo application. Then we will press play and look at the Network Monitor. As you can see, our emulator's location will be changed roughly every two seconds. If we look at the Network Monitor we can see that every two seconds a new image is loaded. Although some of the locations are the same and every time we are loading the same image from the network. Now imagine that our user is navigating through the app and changes his location frequently. By doing these simple operations our application is consuming a high amount of data. If we open our earth tile fragment in our before module we can see that we are starting a new AsyncTask in order to load an image from the network. And we are doing that for all images. In addition, all image loading in the app is done in the same way. Let's go back to our Network Monitor report. The orange area represents the amount of kilobytes our app consumed. During the time period of around one minute our app consumed roughly 10 megabytes. And only a small part of the data consumed is for unique images. This is unacceptable and way too high. If I were to download this app from the app store I would probably note this after a couple of minutes that this app is eating my mobile data. And I will uninstall it directly. So how can we optimize the data usage of our application? We will make sure to load only new images from the network, use our old friend Glide to cache images in memory and on disk and create a memory cache with the image URL associated with a new location. Let's open our after module in Android Studio and open our upward fragment. Here we can see that we simply load images using Glide. Glide will catch them for us automatically. We will do the same in our recycle view adapter. In addition if we look at our earth tile fragment we can see that we are loading the images using Glide as well. The only difference is that here we initiated a memory cache that will contain the image ID from the API and associated with the image URL. In this way, we make sure to load only new images for a new location. It is necessary because Glide will use the image URL as its key for its cache. If we run our code again and watch the network monitor we can see that our application fetches from the network only new images. The interesting part is that after all images are cached, our Network Monitor is empty and if we scroll again our list, our app is not consuming any more data. If we do the same location simulation we can see a similar situation. We can see that only for a new location an image was loaded from the network. Next time our user will be in the same locations all images will be cached and our application wouldn't consume any more network data. Our goal is to get the Network Monitor to be as empty as we can. And load only new data and cache it locally.

Summary
What have we learned? First we learned that mobile data can be expensive and that we need to think about our users around the world. Second we learned how to use the Network Monitor analyze our app's network usage and detect a misbehavior of our application. Last but not least we learnt that it is a good practice to cache imagery on memory and on disk as it can significantly minimize our app's data consumption. Remember we want to try to minimize the orange area on our Network Monitor graph and to load only new data from the network in order to make sure our users are happy and won't uninstall our application because of a heavy mobile data consumption. Thanks for watching and I will see you soon in the next module of this course.

Optimizing Your App's UI Rendering Performance
Introduction
Hi, I am Omri Erez and welcome to the next module of this course, Optimizing Your App's UI Rendering Performance. In this module we are going to inspect multiple situations where our code is making the Android system a hard time to draw our layouts on the screen. In particular, all performance hurdles that we are going to talk about in this module are related to three main subjects. The UI thread, our view hierarchies and GPU performance. We will learn how the UI thread works and how to detect situations where the UI thread is blocked. In addition we will review the process of how Android is drawing our layouts on the screen and how to detect low performance hierarchies. Last but not least we will talk about the GPU and how to optimize our app's GPU rendering performance. It's crucial to understand that one performance hole might not be the end of the world, but when accumulated, many performance issues together can lead to poor user experience, to a high number of uninstalls of our application and to bad reviews in the app store.

The UI Thread
The first performance issue we are going to talk about is related to the UI thread. We will examine code that illustrates the problem, then learn how to detect similar situations using one of Android Studio's analysis tools. Let's start with a question. Is the UI thread the same as the main thread? Despite what I sometimes hear from developers after asking this question, the answer is yes, they are one in the same. Our application is running by default in it's own process. After the user launches our application, the UI thread starts, this single threaded module is crucial for us developers to understand. Ignoring the UI thread concepts can lead to poor performance and poor user experience. So how does the UI thread actually work? Almost all Android developers I meet knows the mantra of don't block the UI thread, but many don't really understand what's happening under the hood. By default the following is performed on the UI thread. All global system events are performed on the UI thread. For example, if we register a broadcast receiver in our manifest, the on receive callback will be performed on the UI thread. The same happens with application specific events, like broadcasts we send in our code and activity life cycle callbacks that we all know. Another group of events that are performed on the UI thread by default is all user input, and user input callbacks. In particular, all callbacks from touch and on click listeners that we register in our code are performed as well on the UI thread. You might be surprised but if you have a service, it is also by default running on the UI thread. A lot is happening on the UI thread, right? But the most important task of the UI thread is to draw the UI. This is where it got it's name from, pretty logical right? After I repeated the words, the UI thread around 20 times in 1 minute, let's dive into it's core task. The process of drawing the UI on the screen. As I stated before, the main objective of the UI thread is to draw and refresh the UI. It draws a frame on the screen in millisecond intervals. In general, frames are drawn on the screen in a frequency of 60 hz, or 60 frames per second. This number varies between different devices. For simplicity, let's assume it's 60 frames per second, which means that we developers have around 16. 6 milliseconds between frames. In other words, this is the time that we have to execute any additional code in our application. Without interfering the process of drawing the UI on the screen. This implies that every time we perform an operation which takes more than 16. 6 milliseconds to execute, the choreographer which is in charge of this applying rendering task will skip a frame. Let's take a look at some code. At this code snippet we can see the onCreate method which is part of the activity life cycle. As we learned before, it will be executed on the UI thread. For the sake of this example we will call a method from the Math package in order to put some strain on the processor. Let's assume the execution of blocking the UI method takes 180 milliseconds. How many frames will be dropped? The 180 milliseconds needed by our code divided by 16. 6 milliseconds per frame equals 10. 8 frames, or rather, 11 skipped frames, so the execution of our method results in 11 skipped frames. From our user's point of view, it means that our app's UI isn't smooth, and our users can experience a laggy UI, or a non-responsive UI. The extreme case of a blocked UI thread is the infamous ANR, or the Application Not Responding dialogue, which asks the user if he wants to kill our application. Users don't like ANRs, you can image that if our application results in a high number of ANRs, it will almost always mean that the user will eventually uninstall it. According to Google Research, the most common user complaints are related to performance issues like laggy UI and ANRs, so how can we analyze our code to detect similar situations of a blocked UI thread? Before we answer that, let's jump to our demo application.

The UI Thread Demo
We will use the same application from the previous module. In our application, we will scroll the list, detect a blocked UI thread situation, find the source of the problem and optimize it. Let's open our demo application in Android Studio. Let's scroll the list of our images. When we scroll there is a fun animation. You can see that after scrolling, we can see the animations but they're not smooth, they are very laggy. Uh oh, we have a problem. This is a typical situation of a blocked UI thread. And an indication that the UI thread skips a big number of frames. Another indication can be found on our logcat. If you see multiple entries that contains the text the application may be doing too much work on it's main thread, you know you have a problem. In our case every time we scroll the list, around 30 frames are skipped which is half of the frames per second. This is unacceptable. Now after we recognized the problem, let's see how we can recognize the source of the problem in order to fix it. Luckily for us, Android Studio offers an analysis tool which can help us to find the source of our problem. This tool is called Systrace. Using Systrace we can record an operation in our application and it will produce a two dimensional chart for us, and a report of execution times of all methods in our code. Using the report of Systrace, we can analyze which operations were done on the UI thread or on background threads, and to understand how long it took them to execute. Let's see how we can use systrace in practice. In order to use Systrace, open the DDMS Console. By Opening the Tools menu and under Android, open the Android Device Monitor. Under Devices, choose a device and your package name. Click the Start Method Profiling button, here we will choose the second option of Trace Based Profiling. This means that Systrace will capture the execution of all methods in our code with no exceptions. The first option will do a similar profiling but on a specified sampling frequency. Which means that Systrace will capture the method stack trace only at the specified frequency. In order to start the profiling we will click the OK button. Perform the operation in our application, and click the Stop Method Profiling button. You will see a report similar to this one. Here we can see a chart with a timeline on the X axis and the sequence of method execution on the UI thread. Let's concentrate at our table. Here we can see that the first couple of entries are related to Android internals. We will concentrate on the first column, inclusive CPU time, in percentage, this column represents the sum of the time percentage spent in the function itself. As well as the sum of the times for functions that it calls. After approximately 10 entries, we can see something suspicious, here we can see that our image list fragment is using the CPU for around three quarters of the total time, this is way too high. If we expand this item, we can see that our load images from assets method inside our fragment is the reason for that, and it's been executed on the UI thread. The total amount of time spent at this method is over 4 seconds, now we know something is wrong. In order to verify Systrace's claim, we can debug our application to check on which thread this method is being executed on. Let's go back to our image list fragment and add a break point inside our own bind view holder. Exactly in the same place we call the load images from assets method. Then we will attach our debugger and scroll our list. After our break point was reached, we can see in the debugger section that this block of code is being executed on the main thread, or the UI thread. So Systrace was right, you can see that we call the load images from a Assets method inside our setUpdateListener callback which is by default performed on the UI thread. If we expand the dropdown menu we can see all the threads in our application. Using this method you can always check on which thread your code was executed on. If we open our load images from Assets method we can see that we are loading a list of images from our assets folder and this is being done on the UI thread. This is the reason why our animations are laggy and not smooth. The UI thread is busy in executing our method and it's skipping frames. In order to fix our problem, we have to move the execution of our load images from assets method into a background thread. As Android developers, we have multiple ways to achieve that but they are outside the scope of this section. One way of doing that is using a handler thread. Let's open our after model and open our adapter to inspect our solution. If we take a look at the constructor of our adapter we can see that we create a handler thread, then we create a handler and provide it with the looper of our background handler thread, this will make sure that the handler message method will be called on the background thread and not on the UI thread. Our new handler will run our problematic code from before, every time handler message is called. And the handler message method will be called every time we send a message into the handler. As we do inside our onBindViewHolder method. So we get to the same functionality with one major improvement. Our code will be run on the background thread, and won't block our UI thread. Let's debug our code and verify that our code block is running on a background thread. As you can see under the Debug section, our code is called from MyCoolBackgroundThread so our goal was achieved. If we run our code again and scroll our list we can see nice smooth animations, which indicates that we solved our blocked UI thread situation. If we were to profile our code again using Systrace we would see the following. We can see that our load images from Assets method is called from a background thread, and therefore not keeping the UI thread busy. As you can see, using Systrace can be very useful in order to detect the reason for a blocked UI thread. I encourage you to dive deeper and get comfortable with the systrace report as it can be a huge contributor for improving your application's user experience.

Optimizing Layout Rendering Performance
In this part of the module, we will learn how the Android system draws our views on the screen. Talk about why it's important to be aware of our app's layout rendering performance and how we can optimize it. Every amazing Android application contains beautiful and complex layouts. The more complex they get, the harder it is for the Android operating system to draw them on the screen. Why do we need to care about our layouts' rendering performance? As we learned in the previous module, the origin country of our users can vary. In some countries, the newest Android devices are three or more years old. These users will be the ones to have a bad user experience when using our application if we aren't aware of our app's rendering performance. Let's talk about the rendering process. It all starts with our XML layouts. These are being processed by the CPU and translated into primitive types which a GPU can understand. These types are sent to the GPU using the OpenGL-ES APIs and finally the GPU is in charge of drawing them on the screen. In practice, we need to remember that complex inefficient layouts will result in longer rendering time which will directly effect on the user experience of our application. Let's dive deeper to understand the view hierarchies in our application. How Android draws our views on the screen? This process contains three stages. Measure, layout and draw. Let's take a look at an example. Here we have a simple layout, if we were to represent this layout in tree view, it will look like that. At the top of our tree, we have our relative layout, which is our root view. It has two children, a text view and the second relative layout which also has two children, a button and an image view, the first stage is the measure phase, it's a top down traversal of our tree. During this stage, each view measures it's dimensions and it's a recursive process. In addition, a parent view might call measure on it's children more than one time. It varies between the different layouts. For example, a relative layout will call measure at least twice on each of it's children. The result of this process is that every view has stored it's measurements. Let's simulate the measure pass on our tree. The first view to be measured is our image view, and then our button. After all these children were measured, the next to come is our relative layout, next, our text view will be measured and finally, the root view of our tree. After the measure phase is finished, the layout phase will start, this operation happens once per view. During this phase, each parent is responsible to position all it's children. The result of this phase is that all views are positioned using the information from the measure phase. If we go back to our tree it will happen in the following sequence, the layout phase will be first called on our text view, then on our button and image view. After the layout phase finished with its children, next comes the turn of our relative layout, and finally our root view. The final stage of this process is the drawing phase. It happens after the layout phase is finished, draws the views on the screen and draws parents before children. The result of this phase is that our layout is visible on the screen. In our example, the first to be drawn is our root view, then the text view, continuing with the relative layout and its children, the button and the image view. Let's jump to the demo. In our demo we have an activity with the same layout from our example. This layout contains simple custom views. I added logging at the entry points of onMeasure, onLayout and onDraw. We will count the operations needed for all the different phases, optimize our views tree, count operations needed after and compare the results.

Optimizing Layout Rendering Performance Demo
Let's open our drawing layouts model and open our activity before XML layout. Here you can see exactly the same layout as in our example. If you open one of the custom views I defined, we see the following, all views are actually printing normal. The only addition are the overwritten, on measure, on layout and onDraw methods. These are the callbacks to the phases we discussed before. Let's run our code, our layout will be drawn on the screen and we will take a look at the entries at logcat. In our Logcat report we can see all the onMeasure, onLayout and onDraw operations. If you compare the sequence of the onMeasure operations, it complies with our example from before. The children view were measured before their parents. Interesting to see is that image view 1 and button 2 which are children of a relative layout were measured multiple times. As we mentioned before, a relative layout will measure it's children at least two times. In our layout, relative layout 3 is a child of relative layout 5, we have a situation of nested relative layouts which fires a chain reaction of onMeasure operations. In order to draw the simple layout, the Android system needed around 25 onMeasure operations. Obviously we want to try to minimize that. After the onMeasure phase is finished we can see that the onLayout operation was called once per view, as we discussed before. And finally, the views were drawn on the screen, parents first and then the children, which verifies our claim from before. If we click our button, we can see the same behavior. The text of our button will be changed, and the Android system needed to trigger the onMeasure operation on most of our views. You can imagine that in real apps where the layouts are much more complex, it can cause a catastrophic effect. The more operations needed by the Android system to draw a layout, the more time it will take. As we learned before, our frame time or our UI thread time is limited, and we want to minimize that. As it can make a real difference in terms of user experience and performance. So how can we optimize our view hierarchy? In general, there are several ways to do it. First, we want to avoid deep and complex view hierarchies, and keep our view trees as flat as possible. Second, we need to use relative layouts with consideration, as it is very expensive in terms of drawing layouts on the screen, and finally, we want to avoid using relative layouts as our root views, because every change in one of its children will fire a chain reaction of measurements. Now let's run our code with our activity after XML layout. As you can see, the same layout was drawn on the screen. But if we open the definition of our activity after layout we can see the following, our root view is a linear layout, and it has three children. If you think about it, we don't really need to use relative layouts in order to get the same result. Interesting to see, if we look at our logcat entries we can see that the Android system needed only eight measure operations, that's a huge difference from before. In addition, if we click our button, it needed only two measure operations. If we compare the before and after, we can see the following before our optimizations, the Android system needed 26 measure operations, 5 layout operations and 6 draw operations which makes a total of 37 operations. To draw our simple layout on the screen. And after, only 16. We optimized the number of operations by over 50%. That's huge. We see a similar improvement for after clicking the button. Maybe you wondered why here the number of draw operations is relatively high. This is because of the famous ripple effect. Internally the Android system will draw the button multiple times in order to create the ripple effect. Just by being aware of the rendering performance of the different layouts and following these simple rules, you can significantly improve the user experience of the users using your application.

Using the Hierarchy Viewer
Until now, we learned how the Android system draws our views on the screen, and how to follow simple rules in order to prevent implementing low performance layouts. In this part of the module, we will learn how to use one of Android Studio's analysis tools that will help us to figure out what are the performance weak links in our view hierarchy. In manual detection and prevention of low performance layout definitions is useful when inspecting simple layouts, but inefficient when looking for the overall layouts rendering performance. Exactly because of that reason, Android Studio offers a tool called the Hierarchy Viewer. This tool lets us take a snapshot of our view hierarchy and inspect the performance of the different layouts. The Hierarchy Viewer dumps our views tree, gives useful indications on the measure, layout and draw execution time, and provides a relative performance for each view. Please note that for security reasons, this tool is working only with emulators. Let's jump to Android Studio and open the Hierarchy Viewer. Open the Tools menu and under Android, choose the Android Device Monitor. Click the Hierarchy View tab and choose our running activity then click on the blue Tree button. Here we can see a dump of our complete views tree. Starting from the left, our root view called decor view and the ending on the right side with the deepest views in our tree. If we zoom in and click our DecorView, we can see a graphical representation of our view. Our DecorView contains 18 views. If we navigate to the right side of the tree, we can see our concrete layout. When inspecting complex layouts, a more efficient way to navigate the tree is through the Layout View. Then we just click on one of the elements and it will be selected on the tree. In order to get some more useful information on our chart, we will click the button with the three round circles. You will see something similar to that. You can see that each child view has now three dots. In addition, our parent view contains now timing information regarding the measure, layout and draw phases. But more interesting is inspecting the dots. The computation of the color of the dots is done in a relative way. The red dot represents the draw process of our layout rendering, the middle dot represents the layout phase, and the right dot represents the actual execution phase. We will concentrate on the first two dots. The color of the dots is determined by the relative performance of the view. Imagine that we sort all the views by their rendering time. Green means that the view is at the top 50%, and the yellow means that the view renders faster than the bottom half of the other views. Now, if you have red dots, you know you have a problem. Red means that this view is among the slowest. In our case, relative layout three has a low performance, and you already know why. Because it's nested in another relative layout. If we run our code using our optimized layout and use the Hierarchy Viewer, we can see our optimized layout and almost all the dots are green. If we compare our two root layouts from before and after, we can see that we improved our layouts measure time by approximately 250%, that's a huge improvement. In addition we improved our layout phase time by 50% and our drawing time by 30%. Remember, every operation counts, the less time the Android system needs to draw views on the screen, the more time we have for other tasks, and eventually, less frames will be dropped. Until now, we talked about the UI thread, how to perform a manual optimization of our View Hierarchy, and detecting inefficient layouts using Hierarchy Viewer. Aren't you excited to reveal the missing parts of our puzzle? I know I am.

GPU Overdraw
After we focuse on the software side of the layout rendering process, we will talk about GPU overdraw and how we can minimize it. What is GPU overdraw? GPU overdraw is referred as the number of times the GPU needs to render a pixel on the screen. On the perfect scenario, the GPU will need to draw every pixel only one time, but in reality it's not the case. When activated, GPU overdraw shows us where our app is doing more rendering work than needed. And it helps us to find problematic views. Our demo contains an activity with a simple layout. We will activate the debug GPU overdraw, optimize our layout and compare the results. Let's open Android Studio and open our GPU overdraw model. We will run our code and activate the debug GPU overdraw by going into the Settings menu, and inside the Developer options, scroll down to the hardware accelerated rendering and click on Debug GPU Overdraw. Then we will click on show overdraw areas. Looks funny, right? Let's open our app again and see the results. So what is the meaning of all these colors? The colors are a hint of how many times the GPU needed to draw a certain pixel on the screen. Starting with the blue color which indicates the pixels were overdrawn once, green twice, and bright red 3 times. The darker red color indicates an overdraw of 4 times or more. If your app has a lot of red areas, you really want to start to optimize that, because your app is doing a lot of extra rendering work. So what is the source of the GPU overdraw? Let's open Android Studio and open our Activity GPU before layout. This layout looks pretty standard, but if we look at the details, we can see that we're setting the background color to be white in four different layouts. And this is the reason why the GPU needs to render the pixels more than one time. Actually, the default background color of the window is white, so there is no need to set it again and again. Let's open our activity GPU after layout. Here we can see exactly the same layout, the only difference is that here we are not setting the background color. Let's run our code and see the difference. That looks much better, no red areas. Most of the screen has its original color, it means that these areas weren't overdrawn by the GPU. As you can see, we weren't able to get rid of 100% of the overdraws, but that's fine. If we take a look at our button and image view, we can see that the GPU had to draw these areas twice in order to show them on the screen. If we compare the before and after, we can see that we saved the GPU a lot of extra work. Ideally, when the GPU overdraw is activated, you want to see the original colors of your layout, because then it means that no GPU overdraw occurred. Remember, the more extra work you can save from the GPU, the more your users will benefit out of it. As every pixel counts.

The GPU Monitor
In this final part of the module, we will touch the tip of the iceberg of GPU monitoring. Please notice that this is an advanced topic which is mostly relevant for game developers. As in games, the GPU usage is much higher. Until now, we learned how to detect a blocked UI thread, how to do a manual optimization of the view tree and how to use the Hierarchy Viewer. In addition we reviewed how to optimize GPU overdraw. The last piece of our puzzle is profiling GPU rendering. Before we continue, I encourage you to take a short period of time and to open our demo NASA app before model, and try to optimize the GPU overdraw and our view hierarchy. After you're finished, you can take a look at the after model in order to see the optimizations I made. Think about it as a small exercise to apply the optimizations we reviewed until now. What is basic profiling of GPU rendering good for? In general it gives us a quick look on how your UI performs and it's very useful in order to detect extreme performance issues related to UI rendering. We will review one example, we will open our NASA demo application before our optimizations, activate the GPU monitor, scroll our image list and analyze the findings. After we have a basic understanding of the GPU monitor findings, we will run our demo application after our optimizations, perform the same operations and compare the results. Let's open Android Studio and run our before model. Then we will activate the GPU Monitor by clicking the Android Monitor button and under the GPU section, click the pause button. On the Y axis we see the amount of time in milliseconds it takes the GPU to execute and process the different commands, and eventually to draw our frames. And the X axis is just a running timeline, similar to other monitors we reviewed until now. Next, we will scroll our image list and look at the graph. You could see something similar to that. Maybe you already noticed by now that the GPU monitor graph has two static lines, the green line indicates the 16. 6 milliseconds required in order to achieve a framerate of 60 frames per second, sounds familiar, right? The red line represents exactly half, AKA 30 frames per second. In general, if your graph is under the green line, it means that your application is drawing 60 frames per second without skipping any frames. And if the peaks of the graph are near the red line, it means that your app is skipping frames and draws roughly only 30 frames per second. Usually a good indicator for a problem is that if the peaks of the graph are between the red and the green line, or over the red line. If this is the case, you know you have an extreme performance problem that you want to optimize. In our case you can see that most of the peaks of our graph are way over the red line, which means we have a problem. Because of our blocked UI thread issue we covered in the first part of this module, and our application is skipping a high amount of frames. We can see that the dominant in colors are related to animation ad Vsync delay. Let's focus on the animation. If you remember, the first example in this module, where the problem with a laggy animation, and this exactly corresponds to the image we see. Now we will do exactly the same with our after module and compare the two GPU monitor reports. Let's run our code and scroll our image list. When looking at the GPU monitor we see a totally different picture, here we can see that now most of the area under the graph is under the red line. This is already much better. The more you optimize your app's GPU rendering performance, the lower will be the peaks in our graph. As you can see the GPU monitor has nine colors, we covered only a small part of them. The draw, the measure and layout, and the animation. The other commands are related to more advanced topics and they are outside the scope of this course, but I encourage you to keep optimizing our demo NASA app and compare the GPU monitor output before and after your optimizations.

Summary
So what have we learned? First we discussed the UI thread, how it works, how it can affect the user experience of the users using our app, and how to use systrace in order to find a situation of a blocked UI thread. Second, we learned how the Android system draws our views on the screen, and about the measure, layout and draw phases. We saw how using the Hierarchy Viewer can help us to optimize our view hierarchies and recognize low performance view structures. Third, we talked about the concept of GPU overdraw and learned how to use the debug options in order to optimize it. Last but not least, we learned how to use the GPU monitor and how to analyze particular operations in our application. Remember, the more you will optimize your app's UI rendering performance, the less complaints you will get from users that have an average user experience. In the next module, we are going to talk about how we can optimize our app's power consumption. Thanks for watching, and I hope to see you soon in the next module of this course.

Detect and Refine Battery Draining Features in Your App
Introduction
Hi, I'm Omri Erez, and welcome to the final module of this course, detect and refine battery draining feature in your application. In this module, we will analyze power consumption of our application, use some analysis tools, and check new APIs in order to reduce it. John is a heavy Android user, and he has downloaded your application. But before he went to sleep, he forgot to connect his phone to the charger. After John woke up in the morning, he tries to reach his phone, and he see the following image, empty battery. Uh-oh, I'm sure this image is familiar to you. One of the most requested features on Android is extending the battery life. Starting at around 2014, Android users can see exactly how much battery your app consumed. And in order for them to uninstall your application, they need only two clicks. I have news for you. Users will uninstall our application if they see our app is consuming too much power, as their battery life is very important to them. In general, we have a couple of obvious usual suspects when it comes to power consumption. It turns out that when the screen is on, the biggest consumer of power is the display. And when the screen is off, it's mostly the CPU, which is used for processing, and the radios, mostly the network and the GPS. We will concentrate on optimizing our app's battery consumption when the screen is off and the user is not present. Google has done a lot of great work in this area. And in Android 6, they introduced doze mode, and app stand-by mode. The main objective of these modes is to increase the device's battery life time. The impact of our battery optimizations will have the greatest effect on users which have pretty much of one of devices where doze mode and the app stand-by are not supported. For these users, we will try to minimize the usage of the network, the CPU, and background work.

Doze Mode and App Stand-by
Let's dive a little bit deeper into doze mode and app stand-by mode. Those mode is supported on devices that have API level 23 or newer, AKA starting on Android 6. It will batch all background activity of all apps on the device, and it will kick in only if the device is not charging and was stationary for a certain period of time. It all starts when the user is active on the device, is using some apps. The blue bars represent some activity. At some point, if the user is not using his phone anymore, the screen will turn off. From that moment onwards, the Android system is using some sensors in order to check that the device is stationary. If the device is stationary for a certain period of time, doze mode will kick in, and this will happen only if the device is not connected to a power source. During doze mode, the device will ignore any wake locks from your application, will restrict it from using any network, GPS and Wi-Fi scans. in addition, doze will reschedule any background jobs set by your app, any syncs, and alarms. It will reschedule them to the next maintenance window. After the device has been in doze mode for a while, doze will schedule a maintenance window. This window is a short period of time where all background work and syncs will happen. All apps will perform their background tasks during this maintenance window. In that way, the radios are accessed in a controlled batched manner, which eventually saves a lot of battery consumption. After all tasks are finished, the device will go back into doze mode. This sequence will reoccur until the user is present and the screen is on. Google had introduced another battery-saving mode, which called app stand-by. The scope of this mode is per app, and it will kick in if nothing of the following happens, the user has not launched the app, the app has no foreground service, and the app has no notification in the notification tray. The restrictions of this mode are similar to the doze mode restrictions. If your app has entered a app stand-by mode, it will have no network access, and the old jobs and syncs will be rescheduled to the next maintenance window. This mode will end as soon as the device is plugged into a power source, or the user is using your app again. Let's take a look on the simplified comparison between the modes. While doze mode is global, app stand-by is per app. In general doze will be triggered if the user is not using the device for a long period of time, and the app stand-by will be triggered if the user has not used the respected app. Android N includes enhancements for doze mode and app stand-by. I encourage you to check documentation in order to understand these extensions. There are some modern APIs which lets us developers to schedule background tasks. The JobScheduler APIs, which are available from SDK level 21 and above. And it doesn't require the Google Play Services. Another API, which is available for our use, is the Firebase JobDispatcher, which supports SDK level nine and above. This one requires the Google Play Services. We will concentrate on the JobScheduler APIs. Let's talk about our demo. We will use the JobScheduler APIs in order to schedule a periodic background task. In this task, we will fetch the current user location, and send it to a server via an http request. Let's open Android Studio and open our before module. Our module contains two classes, our main activity and our job service. In our activity, we will ask the user for the required permissions in order for us to be able to fetch his location. If he granted the required permissions, we will schedule a job. As you can see, the JobScheduler APIs are pretty simple. Here, you can see that we set the component to be MyJobService Then we use the JobInfo Builder class in order set the parameters for our background job. We defined that the job should be executed on any network type, will not require that the device would be on charging mode, and will be executed periodically every five seconds. Please note, if your tasks involves downloading of a high amount of data, maybe it's better to restrict it to happen only when the Wi-Fi is connected. Using these APIs, it's possible for us to configure the tasks to happen only under certain conditions. This is a very powerful feature. Finally, we will call our JobScheduler service and schedule our task. Let's open our MyJobService class and look on our implementation. Here we have our onStartJob method. This method will be called when our task is executed. Here, we get the reference with the location manager, then set a criteria for our location requests, and request the location update using the requestSingleUpdate method. When the location is available, we will start an AsyncTask. And on a background thread, we'll send the location via get the http request. Nothing complicated here. If we run our code on a real device, we would notice that our battery is drained pretty fast. So, how can we examine our app's battery usage. At 2015, Google introduced an analysis tool, which called Battery Historian. This tool helps us to inspect-battery related information and events on an Android device running Android 5 or newer. In order to install Battery Historian on our machine, we will download the Docker Toolbox from the Docker download page. Then we will install it. After the installation is finished, we will open the Docker Quick Start Terminal, and wait for it to initialize. Next, we will open the installation txt file, which is located inside the battery stats folder. Copy the Docker command, which is stated at step four, and paste it into the Docker console. You know that Battery Historian is running on your computer if you see the following text in the Docker console, "Listening on port 9999. " In addition, we will open a new terminal window and find out the Docker Machine IP by inputting docker-machine, space, IP, space, default. Then we will copy the IP address, and paste it inside our browser, following the port number. Finally, click Enter. This is it, Battery Historian is installed on your computer. If you are using a Windows computer, I encourage you to open the Battery Historian documentation on its GitHub page and follow a similar process.

Analyzing Battery Historian Report
Now, where do we get the data to input our Battery Historian? It's actually pretty simple. It involves only one adb command, which can be found in our installation txt file at step six. Copy and paste it into our terminal. Make sure your real device is connected and click Enter. This can take a couple of minutes. Please note that if your device has an Android version older than 7. 0, we will see the following error message, "Failed to get bugreports version. " But that's fine. We can ignore that as it doesn't have any effect on the produced report. The bugreport will be downloaded to our terminal's location. After the battery information from our device has been downloaded to our computer, we can input it into Battery Historian and press submit. You would see a beautiful report similar to this one. For simplicity, you can load the data from our battery stats folder as well. Here we can see a very rich report. It's a lot of information to process, I know. We will not cover all the entries but the basic ones. If we take a look at our chart, we can see a timeline on the x-axis, and the battery level on the y-axis. The black line indicates the battery level at that time. On the left side, we can see a lot of different metrics. For example, we can see when doze mode was activated or not, which is represented by the blue bar. It's interesting to look at the slope of the black line. When doze is not activated, we can see that the slope is steeper. And when doze is activated, we can see that the slope is more moderate. The difference between the slopes represents the contribution of doze mode for saving battery. Another indication for that is the distance between the battery level bars. When doze is activated, it is longer. In addition, after doze was activated, we can recognize the maintenance windows. During these windows, we can see that our JobScheduler and the GPS were activated. If we click on the first JobScheduler entry, we can see some more information about our job. Here we can see that my job service ran 92 times, and its duration was a little bit more than three seconds. We can examine more metrics related to the Wi-Fi, the screen, and the phone state. I encourage you to get familiar with this chart in order to know all the different matrices. By the way, I forgot to mention that the device was in the same place all the time, which means that we spend here a lot of unnecessary energy on turning the GPS frequently, although it's not necessary, because the location hasn't changed. If we want to get some more specific statistics, we can scroll down and filter our app. Here we can see information about our app's wakelocks, the running services, and information about our scheduled job. For example, we can see that our scheduled job ran 1, 382 times, and it's total execution time was around 30 seconds. in addition, we can see the sensor usage of our app. We can see that we use the GPS frequently. Imagine that the battery consumption of your application for users which use Android 5 and below, and don't have the support of those mode. We look like the first part of our chart. In a time period of three hours, our app consumed around 15% of the battery. It's way too high, especially for our use case. How can we optimize our app's battery consumption? We will perform multiple optimizations. Instead of using the normal location manager, we will use the fused location provider, which uses low energy. In addition, we will ask location updates less frequent, and send the location only when there is a new one. Let's open Android Studio and open our after module. Here we have an additional service for our location requests. In our main activity, we will ask for the required permissions, and start the service. If we open MyLocationService class, we can see the following. After the service has started, we initialize our location request, and our Google API client. We set the update interval of our location request to be 60 seconds, and a faster interval to be 20 seconds. which means that we are going to get a location update in intervals between 60 and 20 seconds, and only if the device location was updated. Next, we initialize our Google API client with the location services API, and try to connect to it. After the onConnected call back is called and the API client is connected, we will start our location updates. Every time there is a new location, the onLocationChanged call back will be called. Then we will input our location into the scheduledJob method. Finally, we will schedule our job and set its extras to be a PersistableBundle which contains the latitude and longitude of the current location. When a new location is available and our task is executed, we will rid the location parameters and send it via get HTTP request like before. If we run our optimized code on a device for a couple of hours, dump the battery stats and load it in Battery Historian again, it will look like that. Here we can see that as long as the device was moving, and a new location was frequently changed, the GPS was on, and our JobScheduler task has been executed on every location change. Interesting to see here is that when the device didn't get any location updates, AKA, it was in the same place, the GPS was off. At around 12 o'clock, we can see there was a location update, and this is why we can see the GPS bar and the JobScheduler bar. After our optimizations, during a time period of three hours, our battery level went down only by 6%. It's a huge improvement. If we take a look at our app-specific stats, we can see that our MyJobService ran only 92 times. In addition, we use the GPS only 49 times, which is much less than before. If we compare the battery drain of our application, before and after, we can see that before our optimizations in the time period of three hours, the battery level went down by 16%, and after only by six. That's a huge improvement.

Summary
So, what have we learned? First, we learned what is those mode and how it works. Second, we revealed another battery saving mode called app stand-by mode, and learned how to use the JobScheduler APIs for scheduling a background task. Last but not the least, we learned how to use Battery Historian in order to analyze our app's battery consumption and detect inefficient usage. Remember, your users are really sensitive when it comes to their battery lifetime. We have to use the network and other sensors with consideration in order to make sure our users are happy and won't uninstall our application. That's it. You have reached the end. I want to personally congratulate you on completing this course. By now, you should have the skills and knowledge to analyze and detect memory hazards in your application using the memory monitor, hunt memory leaks, and exterminate them, optimize the UI rendering performance of your application using Systrace, Hierarchy viewer, and the GPU monitor, and examine your application's network and battery consumption using tools like the network monitor and Battery Historian. And all of that in order to reach one important goal, to provide a superior user experience for your users. Please make sure to rate this course here in Pluralsight, and feel free to write on the discussion board if you have any questions. I wish you a lot of fun and excitement in inspecting, detecting, and optimizing your Android app's performance. Thanks for watching, and I hope to see you in the next courses.

Course author
Author: Omri Erez	
Omri Erez
Omri is a software engineer and award-winning Android developer. His first introduction to programming was learning C at the age of 12. While starting his bachelorâ€™s degree in Information...

Course info
Level
Beginner
Rating
4.9 stars with 38 raters(38)
My rating
null stars

Duration
2h 4m
Released
13 Jan 2017
Share course
