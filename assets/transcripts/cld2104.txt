Creating and Administering Google Cloud SQL Instances
by Janani Ravi

Cloud SQL is the Google Cloud Platform's managed SQL service which offers powerful and simple RDBMS functionality. Cloud SQL supports both MySQL as well as PostgreSQL on the cloud.

An important component of an organization's on-premises solution is the relational database. Cloud SQL is an RDBMS offering on the GCP which makes the operational and administrative aspects of databases very easy to handle. In this course, Creating and Administering Google Cloud SQL Instances, you will learn how to create, work with and manage Cloud SQL instances on the GCP. First, you will assess the range of data storage services on the GCP and understand when you would choose to use Cloud SQL over other technologies. Then, you will create Cloud SQL instances, connect to them using a simple MySQL client, and configure and administer these instances using the web console as well as the gcloud command line utility. Next, you will focus on how Cloud SQL can work in high-availability mode. After that, you will configure failover replicas for high-availability and simulate an outage event to see how the failover replica kicks in. FInally, you will see how to use read replicas for increased read throughput and how data can be migrated into Cloud SQL instances using a SQL dump or from CSV files. At the end of this course, you will be comfortable creating, connecting to, and administering Cloud SQL instances to manage relational databases on the Google Cloud Platform.

Course author
Author: Janani Ravi	
Janani Ravi
Janani has a Masters degree from Stanford and worked for 7+ years at Google. She was one of the original engineers on Google Docs and holds 4 patents for its real-time collaborative editing...

Course info
Level
Beginner
Rating
0 stars with 3 raters
My rating
null stars

Duration
2h 29m
Released
24 Sep 2018
Share course

Course Overview
Course Overview
Hi, my name is Janani Ravi, and welcome to this course on Creating and Administering Google Cloud SQL Instances. A little about myself. I have a masters degree in electrical engineering from Stanford, and I've worked at companies such as Microsoft, Google, and Flipkart. At Google, I was one of the first engineers working on real-time collaborative editing in Google Docs, and I hold four patents for its underlying technologies. I currently work on my own startup, Loonycorn, a studio for high-quality video content. In this course, you will learn how to create, work with, and manage Cloud SQL instances on the GCP. We start off by assessing the range of data storage services on the GCP and understand when you would choose to use Cloud SQL over other technologies. We'll also understand the pricing structure for Cloud SQL. We'll then create Cloud SQL instances and connect to it using a simple MySQL client. We'll understand how we can configure and administer these instances using the web console, as well as the gcloud command-line utility. We'll also see how we can work with PostgreSQL instances. We'll then focus on how Cloud SQL can work in high-availability mode. We'll configure failover replicas for high-availably and simulate an outage event to see how the failover replica kicks in. We'll also see how we can use read replicas for increased read throughput and see how data can be migrated into Cloud SQL instances using either a SQL dump file or from CSV files. At the end of this course, you should be comfortable creating, connecting to, and administering Google Cloud SQL instances to manage relational databases on the GCP.

Understanding Cloud SQL in the GCP Service Taxonomy
Module Overview
Hi, and welcome to this course on Creating and Administering Google Cloud SQL Instances. We'll start this module off by understanding Cloud SQL in the GCP service taxonomy. Cloud SQL is a fully-managed database service that Google offers that makes it very easy to set up, maintain, manage, and administer your relational database on the Google Cloud Platform. Cloud SQL has support for both MySQL databases, as well as PostgreSQL databases. When you need a vertically scaled relational database management system and you do not want to set one up on-premise, that's when you'll use Cloud SQL. Cloud SQL works very well when your data storage requirements do not exceed 10 TB and your input/output QPS does not exceed 40, 000. Setting up and managing databases on your on-premise data center can be time consuming. Cloud SQL offers managed install, backup, and replication, making it extremely easy for administrators and developers to get up and running with the relational database on the cloud. Cloud SQL offers vertical scaling. That means you can move your database to a more powerful machine in order to get lower latencies with greater storage requirements. It's also possible to configure your Cloud SQL instances for high availability using failover replicas. You can add in read replicas for greater read throughput.

Prerequisites and Course Outline
There are a few prereqs that you are to have completed in order to make the most of this course. You need to make sure that your MySQL fundamentals are strong, and if they are not, the MySQL Fundamentals course on Pluralsight can help you out here. Another good course for you to complete before you start on this course is the Google Cloud Platform Fundamentals. This will give you a basic understanding of the GCP. This course assumes that you have a basic understanding of how cloud computing works. We'll work directly with the Google Cloud Platform without walking through the basics of creating VM instances and other resources on the cloud. You need to be very comfortable with RDBMS concepts because this course focuses on administering your MySQL database on the cloud. You also need to be comfortable with MySQL or PostgreSQL administration. The demos in this course primarily focus on MySQL databases on the GCP; however, they can be extended very easily to apply to PostgreSQL as well. We start this course off by first understanding how Cloud SQL works on the GCP. We'll see the various Cloud SQL features available and limitations. We'll then see how pricing works for your SQL instances, and we'll compare Cloud SQL to other GCP storage technologies. This will allow you to choose the right storage system for your needs. We'll then move on to creating Cloud SQL instances. We'll start off at the very basic level. We'll see how we can create, start, and stop new instances. We'll then see how we can connect to these instances with and without SSL. We'll then move on to using the Cloud SQL Proxy to manage external connections. The next module will cover advanced features on Google's Cloud SQL, specifically focused on replication and data management. We'll see how we can set up high-availability configurations using the failover replica. We'll also see how we can create and manage read replicas. We'll then cover how we can import data into Cloud SQL and export data from Cloud SQL. These will help with migration scenarios. All the demos in this course assume that we are working in an organization called SpikySales. com. This is a hypothetical online retailor and e-commerce site that sets up flash sales. Any trending product is available for sale on SpikySales. com. Flash sales require high-computing capacity on demand, whereas at other times the resources may not be used very much, which makes it a perfect candidate to use the GCP. SpikySales primarily runs on the Google Cloud Platform, which offers you cloud computing pay as you go. SpikySales uses a setup where there is no idle capacity during the off-sale period, and when a sale is on, it uses huge computing resources on the GCP. All the demos in this course focus on the relational database management system used by the reviews and ratings team in SpikySales. com. This e-commerce site has reviews and ratings for all of its products, and these are set up in one relational database table. This relational database table contains the ratings_id, the product_id, the customer_id, the display_name for every customer, the date at which that rating or review was written, the actual rating, and the text of the review. This RDBMS needs to have vertical scaling so that storage grows as data grows, it needs to be highly available with failover replicas, and it needs to be accessible to various teams across the organization. Teams such as the search team, the recommendations team, all of them run applications which access review and ratings data.

Introducing Cloud SQL
Cloud SQL is the Google Cloud Platform's fully-managed MySQL and PostgreSQL database service. Initially, Cloud SQL was only available for MySQL databases, but it has been recently extended to include PostgreSQL as well, which means Cloud SQL supports all of the standard connection drivers and migration tools that are supported by MySQL, as well as PostgreSQL. Cloud SQL works on a platform as a service model where it offers database features as a service. That means there is platform support for creating databases, deleting databases, vertically scaling databases, automated backups, replication, high-availability configuration. All of these are very easy to set up. All cloud platforms offer databases as a service with direct AWS equivalent. For Cloud SQL on the GCP is the Amazon RDS, or Relational Database Service. At this point in time, Cloud SQL just supports MySQL and PostgreSQL services. AWS supports many more database engines such as Oracle, MS SQL Server, and so on. If you want support for Oracle and MS SQL Server on the GCP, you need to explicitly create a virtual machine and install the software yourself on that VM. Only MySQL databases and PostgreSQL databases are available on Cloud SQL at this point in time. For more advanced cases where you want your database to be globally distributed and strongly consistent, you will use Cloud Spanner instead of Cloud SQL on the GCP. Cloud Spanner is an enterprise-grade relational database that allows horizontal scaling on the cloud along with transactional consistency. The Cloud SQL equivalent on the Azure platform from Microsoft is the SQL Database, the SQL Server Database Engine. The relational database service on AWS, SQL Database on Azure and Cloud SQL on the Google Cloud Platform, are all competitors. When you use Cloud SQL, you can choose the kind of machine you want hosting your database. Your machine type can range from 500 GB to 10 TB of storage, and it can have up to 416 GB of RAM. With just a few checkboxes, you can configure automatic backups which will occur at the same time every day. You can also set up your instance for high availability. GCP offers built-in security for your Cloud SQL instances by setting up firewalls and allowing only authorized networks and IP addresses to access your database.

MySQL on Cloud SQL
Let's quickly take a look at what features are available for MySQL databases on Cloud SQL. Cloud SQL runs MySQL's Community Edition, and you will find when you use Cloud SQL that your MySQL databases are divided into First and Second Generation instances. Cloud SQL Second Generation is the latest version for Cloud SQL for MySQL. The Second Generation instances support most of the features of First Generation instances, but they offer higher performance and storage capacity at a lower cost. Cloud SQL instances are secure. Data stored here will be encrypted when at rest and in transit. You can use SSL to connect to your Cloud SQL instances. Logging and monitoring on the GCP is available via Stackdriver tools. Stackdriver also allows you to monitor your Cloud SQL instances and set up alerts and other policies. Cloud SQL allows a variety of features which you can access very easily to keep your data backed up and secure. You can import data from external instances into Cloud SQL. This allows for easy migration. You can export data from Cloud SQL elsewhere as well. Imports and exports can use. sql, as well as. csv files. Cloud SQL offers two kinds of backups, automated, as well as on-demand backups. The automated backups for your Second Generation MySQL instances are incremental, which means you also save on storage costs for your backups. Cloud SQL instances which run the MySQL Database, also allow point-in-time recovery, which means you can roll back your database to a very specific operation or a point in time. You can also clone your instance before you perform any major change or migration of data. The first thing you'll see when you use MySQL databases on the GCP is the First and Second Generation instances. The Second Generation instances are basically the latest MySQL versions. As you can see, version 5. 7 is available in the Second Generation. First Generation instances tend to be smaller. They allow 16 GB of RAM and 500 GB of storage. Second Generation instances can hold much more data. You can have up to 416 GB of RAM and 10 TB of storage, so they are far larger. Second Generation MySQL databases have built-in support for read replicas. This is not available in First Generation instances. In the First Generation MySQL instances, the high-availability configuration is enabled by default. In Second Generation instances, the high-availability configuration is available, but is not the default. This allows you to save on costs up front. Both First Generation and Second Generation instances have automated backup and maintenance windows; however, these windows are system-defined for the First Generation, but can be configured based on your needs for the Second Generation instances. When you use First Generation MySQL instances on the Google Cloud, you have no Cloud Proxy support, which makes these instances less secure when you connect to them. In the Second Generation, you can use the Cloud SQL Proxy to manage external connections, which makes connecting to these instances simpler and more secure. The First Generation MySQL instance supports multiple database engines, the InnoDB, as well as MyISAM. The Second Generation instances support only InnoDB. Overall, the Google Cloud Platform recommends that you use Second Generation Cloud SQL instances rather than First Generation. You get up to 7 times the throughput and 20 times the storage capacity in Second Generation instances as compared with the First, they are much faster and more efficient, and Second Generation instances usually work out to be cheaper than First Gen. So when you're using MySQL databases on Cloud SQL, Google's recommendation is that you stick to using Second Generation instances. Google offers a wide variety of ways in which you can connect to MySQL instances on Cloud SQL. You can use any mysql client, you can use the SQL Workbench, you can use App Engine, Compute Engine, Kubernetes Engine, Cloud Functions, as well as Apps Script. There are some MySQL features which are not yet supported on Cloud SQL. So if you have use for UDFs, the FEDERATED engine, InnoDB memcached plugin, or the SUPER privilege, you may not be able to use Cloud SQL directly. There are also certain SQL statements that are not yet supported on Cloud SQL. These are examples of unsupported statements. If these SQL queries are important to you, you may find that you can't use MySQL on the GCP as of now. Cloud SQL is constantly evolving, and at this point in time, there are certain client features that remain unsupported. There are restrictions on the mysqlimport and the mysqldump commands, and there are also some MySQL flags that are unsupported. When you use the web console to configure your MySQL database, you'll see a list of flags that are currently available. You also cannot import or export triggers, stored procedures, and views from your database. If you're migrating from an on-premise MySQL database that uses triggers, stored procedures, or views, you have to set these up manually. All of the remaining data can be imported in an automated manner.

PostgreSQL on Cloud SQL
Cloud SQL recently added support for PostgreSQL as well. So if you want to run a PostgreSQL database, you can do so on a Cloud SQL instance. The PostgreSQL version is 9. 6, and it's based on Second Generation MySQL instances, and it has up to 10 TB of storage, which means your relational database can be pretty huge. You can connect to the PostgreSQL database on the cloud in all of the usual ways. All of the ways that are available for a MySQL database are available for PostgreSQL as well. You can use any psql client, any tools supporting standard client-server protocol. You can connect from App Engine, Compute Engine, Kubernetes Engine, and Cloud Functions. PostgreSQL connections from Apps Script are not yet supported. As we saw in the case of MySQL databases on Cloud SQL, there are certain features on PostgreSQL that are not yet supported by the Cloud SQL instance such as point-in-time recovery, CSV import and export using gcloud or the console. Both of these are features which are available for MySQL databases. PostgreSQL on Cloud SQL does not support superuser privileges or operations that require reconnecting to the database, and there are also several flags that are unavailable for this database on the cloud. All of these are things which you need to explore before you move to PostgreSQL on the GCP.

MySQL 2nd Generation Pricing
In this clip, we'll understand how pricing works for Cloud SQL instances on the GCP. We'll consider three different kinds of instances. The first is the MySQL Second Generation, that's the one that's most commonly used; we'll look at PostgreSQL instances; and then we'll look at MySQL First Generation instances. If you're planning to use MySQL on the Google Cloud Platform, Google recommends that you use MySQL Second Generation instances. There are three basic components to pricing your MySQL instance on the cloud. The total price that you'll pay is the instance price, the storage price, and the network price, and there are subtleties to each of these. Instance price refers to the price that you pay for the virtual machine where you're running your MySQL database. This is per-minute billing for the time period for which your instance is up and running. The instance price that you pay depends on the tier of your machine. It depends on how powerful your machine is. The more powerful your machine, the higher the instance price. The instance price also varies by region. So if you are in a region which has a lot of machines of this particular kind, the instance price will be lower. If you're in a region such as Asia or Australia, it's possible that your instance price will be higher. If you've worked on the GCP before, you know that Google offers some pretty nifty sustained use discounts. These sustained use discounts are automatically applied to your instance price. The longer you use your instance, the greater your discount, lower the rate that you pay per minute for the instance. If you're using your Cloud SQL instance the high-availability mode, it's possible that you have additional read replicas and failover replicas. These are separate instances for MySQL Second Generation, and they are charged additionally as stand-alone instances. The storage price is the price you pay for the persistent disk that is associated with your instance. Your storage price depends on whether you choose to use SSDs, solid-state drives, to store your data or HDDs, hard disk drives, to store your data. HDDs are cheaper than SSDs, but SSDs have lower latencies and faster reads, so they are preferred choice. Remember, when you're working with storage, persistent disks are pay as you allocate, not pay as you use. Your instance price depends on how long you use a particular VM. Persistent disks are based on allocation. Once you've allocated a 20 GB SSD, you pay for the entire 20 GB, even if you use up just 1 GB of storage. SSDs typically cost around 22 cents per GB per month. HDDs cost 12 cents per GB per month. HDDs are cheaper, but as you might already know, they are also slower. If you back up the data that you stored in your Cloud SQL instance, that takes up additional storage space. They are charged separately on a pay-as-you-use basis at 10 cents per GB per month. Second generation MySQL instances allow for incremental backup of your data, which means the storage used for backup will be lower than in the case of First Generation MySQL instances. The third component of the price that you pay for a Cloud SQL instance is the network price. All ingress traffic into your Cloud SQL instance is free; however, if the source of the traffic is within the GCP, the egress price from the source still applies. For data that is retrieved from your Cloud SQL instance and sent back to the user, egress pricing applies. This is the usual network pricing for egress. If you configured a static IP address, these IP addresses cost 1. 5 cents an hour when they are idle, when they are not being used. At this point in time, we understand what components affect the price of a Cloud SQL instance; however, we have no feel for how much Cloud SQL actually costs. Let's consider a few scenarios here so that we understand how much we might need to pay. If you set up a MySQL Second Generation test instance on an f1-micro machine with a shared CPU and just 10 GB of storage, you pay just $9 per month, assuming that you never stop this test instance, it's always up and running. If you stop this test instance when it's not being used, assuming that you just run 4 hours a day for 5 days a week, the amount that you pay falls to $3 a month. Notice that these are for Cloud SQL instances set up in the us-central1 region. Test instances are cheap. If you're running a production instance of the MySQL Second Generation, and if you're using a high-memory machine with 32 dedicated CPUs with 208 GB of memory and a storage capacity of 10 TB, this will cost you $7, 670 per month. This is a production instance that serves traffic 24 hours a day, 7 days a week, and is configured for high availability.

PostgreSQL Pricing
Let's now see how pricing works for a PostgreSQL instance running on Cloud SQL. We'll consider the same three components of pricing. These are the same components which apply to a MySQL Second Generation instance. The total price is calculated by taking the total of the instance price, the storage price, and the network price. There are two important differences between MySQL Second Generation pricing and PostgreSQL pricing. If you use dedicated core instances, the memory and the CPUs for your virtual machine are charged separately when you use PostgreSQL. If you're using shared core instances and not dedicated cores, the pricing is very similar to MySQL Second Generation. PostgreSQL instances can also be configured for high availability, but the charges for high-availability storage are higher than just regular storage. That's not the case with MySQL Second Generation where the high-availability failover replica is just considered another Cloud SQL instance. These are the only differences from how the GCP prices MySQL Second Generation; however, you'll see that these differences are minor, and they don't change the pricing scenarios that we saw earlier. If you're running a test instance that never stops, you'll roughly be charged $9 a month. If your test instance runs 4 hours a day, 5 days a week, you'll be charged roughly $3 a month, exactly the same as the MySQL Second Generation. If you're running a production instance as powerful as the production instance that we saw for the MySQL scenario earlier, you'll find that the costs are very similar. A production instance will cost you $7, 613 a month roughly. If you see the parameters of the PostgreSQL machine here, they are very similar to the MySQL Second Generation production instance that we saw earlier.

MySQL 1st Generation Pricing
MySQL First Generation instances are no longer recommended by Google and are not often used, especially if you're starting up. Their pricing is also far more complicated. This is what I got from the GCP's documentation. Google offers two billing plans, packages and pay per use. If you're planning to use your First Generation instance for more than 450 hours a month, you should use the package plan. That will work out to be cheaper. Now if it was TL;DR for you, you should know that pricing is complicated, and First Generation instances tend to be somewhat more expensive than Second Generation MySQL instances.

Cloud SQL vs. Other GCP Storage Technologies
GCP offers a number of different storage services. Cloud SQL is just one of amongst these. How do you know when you have to choose Cloud SQL over other storage services on the Google Cloud. If you're making a choice to move to the Google Cloud Platform, it's possible that you want to migrate already existing data. Let's say you already use an RDBMS on your on-premise data center or in a competing cloud platform. One use case is when you want to move your data to the Google Cloud. This is a migration scenario. Or it might be the case that your organization is starting afresh. This is a cloud-first scenario. You're starting with a clean slate on the Google Cloud Platform. You don't have any existing data yet. Let's see when you would choose Cloud SQL in the migration scenario. Let's look at the storage service that you might want to use on the Google Cloud Platform based on your use case. On the left here are a variety of use cases that might be your current scenario. Let's see the choice of technology that you'll make in each of these cases. Let's say you currently have an on-premise data center where you already have the MySQL database or a PostgreSQL database running. You will choose Cloud SQL. Cloud SQL offers MySQL and PostgreSQL out of the box. If you're using a relational database management system on a competing cloud platform, you can choose to use Cloud SQL provided your data is less than 10 TB. Cloud SQL works well for up to 10 TB of storage. If your database of choice is Microsoft SQL Server and you want to migrate to the Google Cloud, you'll have to instantiate a GCE virtual machine with the MS-SQL image. Cloud SQL is not for you in this case. The same thing holds true if you want to use the Oracle database on the Google Cloud. You have to set up a special VM with a custom image for Oracle. If your site or organization has a number of users, most of whom are concentrated in one region such as the US, Europe, Australia, or Asia, you will use Cloud SQL, which is a regional database. It's confined to a particular region. Cloud SQL instances cannot be distributed across regions. Cloud SQL is not global. If you have a relational database and you have your users located all around the world, you'll want to use Cloud Spanner, which can be horizontally scaled and is available globally. Cloud Spanner instances can span multiple regions. If your requirements are such that vertical scaling works just fine, that is you just use a bigger machine for lower latency, greater storage, and greater performance, then Cloud SQL is for you. If you need horizontal scaling with a relational database management system, you'll need to use Cloud Spanner. These were migration scenarios. Let's look at cloud-first scenarios and see when you would choose to use Cloud SQL. In a cloud-first scenario, let's first ask ourselves whether we need a relational database management system at all. Is relational data the kind of data that we want to store, or is our data of a different kind? Let's see how Cloud SQL works versus other GCP storage technologies, such as Cloud Spanner. Cloud Spanner is a relational database with horizontal scaling. Adding more nodes to Cloud Spanner will improve your performance, throughput, and storage. Or you might really want an OLAP system, an Online Analytic Processing system, not an Online Transaction Processing system, in which case, you'll choose to use BigQuery. BigQuery is Google's very popular data warehousing system. If you want a NoSQL database with very fast lookup for keys and very fast scans, you'll choose to use Bigtable, which is very similar to HBase. If you want a NoSQL document database with order of one lookups and very low storage requirements, you'll choose to use Google's Datastore. If you're looking for globally distributed blob storage for media, video, and other kinds of files, you'll choose to use Cloud Storage buckets. Of all of these storage technologies, Cloud Storage buckets is the only one for blobs or unstructured data. All of the other technologies work with structured or semi-structured data. In all of the other cases, let's see a quick comparison of when we would choose to use Cloud SQL versus any of the other technologies. For example, we'll start off with Cloud SQL versus Cloud Spanner here. We'll choose Cloud SQL when we want a relational database with ACID support, atomicity, consistency, isolation, and durability. Cloud Spanner is also a relational database, and it provides ACID++ support, ACID support on a global scale. If your users are restricted to a specific region, you can choose Cloud SQL. Cloud SQL is regional only. Cloud Spanner can be regional or global, distributed across multiple regions. Cloud SQL scales vertically. That means you can increase your performance and storage by taking a bigger instance. Cloud Spanner scales horizontally. You simply add nodes for higher performance and more storage. If your storage and QPS requirements are not very high, Cloud SQL is a good option. Cloud Spanner scales to any size and any QPS. Cloud SQL works with open technologies that are widely available such as MySQL and PostgreSQL. Cloud Spanner is Google proprietary. Cloud SQL is relatively cheap, of the order of $7500 a month for a production instance. Cloud Spanner is very expensive because of its distributed nature, global asset properties, and Google proprietary technology. Between Cloud SQL and BigQuery, you'll choose to go with BigQuery if you have analytical processing or data warehousing requirements. Cloud SQL is a SQL RDBMS. BigQuery is a SQL data warehouse. If your data is less than 10 TB and you have strict ACID requirements, Cloud SQL is a good choice. BigQuery scales to petabytes of data. If you want to instantiate a single instance as a server, then you'll use Cloud SQL. BigQuery is a serverless technology offered by Google. BigQuery, once again, is a proprietary technology offered by Google, and the cool thing is BigQuery is relatively cheaper than using Cloud SQL instances. So if you have petabytes of data at global scale with no ACID requirements, you'll choose to go with BigQuery. You'll choose to use the Bigtable storage technology as opposed to Cloud SQL if you have analytical processing requirements with a NoSQL database. Bigtable is often described as very similar to HBase, a columnar database, with very wide columns. With scales to petabytes of data, it provides horizontal scaling by adding nodes. It's a Google proprietary technology, which allows very high QPS. Bigtable is not a serverless technology. It requires you to set up instances, and it can get expensive very quickly. If you're looking for a NoSQL document database, then Datastore is the choice for you, and you might choose to go with it over Cloud SQL. Datastore works in two modes, one of which has transaction support. It scales well, but the data that it can store is limited. It's best for data in terabytes. It has serverless autoscaling and works on a Google proprietary technology.

Cloud SQL Internals
Let's get a quick sense of how Cloud SQL instances work based on GCP internals. Now all of Google Cloud Platform's resources are divided into zonal, regional, and global resources. A zone is an availability zone, similar to a data center in one location. A number of zones or data centers make up a single region, and a region is one area within the United States which holds multiple data centers. All of these zones are connected using high-speed network links. You can use resources on the Google Cloud Platform in multiple regions around the world. Regions are available in the US, Europe, Australia, and Asia. A network on the GCP is a virtual private cloud. Multiple IP addresses reside on a network which are logically grouped into subnets. Firewalls can be configured to allow or deny traffic from and to resources which live on a particular network. An example of a zone on the GCP is us-central1-a. A zone is typically suffixed by an alphabet. The region is us-central1, that is one area of data centers in the US, and the default network that is configured whenever we create a Google Cloud Platform account is called default. Even if you don't explicitly set up a VPC, when you log into the Google Cloud, any resources you create belong to this default network. Cloud SQL essentially refers to a Compute Engine virtual machine, which is an instance running either the MySQL database or the PostgreSQL database. This is the database as a service option that the Google Cloud Platform provides. When you configure a single Cloud SQL instance, this is a virtual machine that is associated with a specific zone. One Cloud SQL instance lies in one zone. It's not distributed across multiple zones. So if there is a failure in that particular data center, your database will be unavailable, which means a single instance does not have high availability. If you're interested in making your Cloud SQL instance highly available, there is something called the high-availability configuration. The high-availability configuration replicates your Cloud SQL single instance, so we have an original instance which serves as the master, and the replicated instance is called the failover replica. The addition of a failover replica, which mimics all of the data that's present in the master, is your high-availability configuration. The high-availability configuration ensures that even if a single zone fails, your data will still be available because your failover replica will be present in a different zone from the original master instance. The master and the failover replica will be in the same region. Cloud SQL is a regional resource, but they'll be in different zones in that region. Just like other virtual machines which you can connect to, Cloud SQL instances are also vulnerable to security threats, which means GCP configures firewalls in order to restrict access to your Cloud SQL instances. Because of this restricted access to your Cloud SQL instance, there are three explicit things that you can do in order to gain access to your database. You can whitelist the IP of the client machine which you're using to connect to your database, you can configure SSL so that the client and the server trust each other, or you can choose to use a Cloud SQL Proxy. We'll see how to work with all of these three ways of connecting in the next module. Cloud SQL instances scale vertically, which means in order to get better performance and more storage capacity, you need to get a bigger and better machine. If the number of queries per second you make to your Cloud SQL instance goes beyond 10, 000, you'll find that your performance will suffer, and your instance will become a bottleneck. In order to mitigate this, you can use something called read replicas. Read replicas are a complete copy of your master database instance which allow just reads. All writes to your database will be performed to the master instance. All of the changes to your master will be replicated to your read replica with a little lag. This is the replication lag. The replication to read replicas are close to real time on Cloud SQL, which means you can quickly improve read performance by adding read replicas.

The Web Console and Cloud Shell
In this demo, we'll get hands on. We'll log into the Google Cloud Platform console, and we'll learn how to use the Cloud Shell terminal window. You can access the Google Cloud Platform at console. cloud. google. com. Just type this in on any window, and you can use your Gmail account to log in. I'm using the Gmail account that belongs to my organization, cloud. user@loonycorn. com. Specify the password, and you're logged in. Now Google gives you some free trial credits that you can use. You automatically have those available if you're logging in for the first time. However, setting up Cloud SQL instances and some of the other operations that we'll performed in demos in this course might require you to enable billing on the GCP, so I recommend that you set up a billing account and associate your credit card. As long as you turn down your Cloud SQL instances after you create and use them within these demos, your bill should not exceed a few dollars. All GCP resources lie within a specific project. A project is a billing unit for your organization. So let's say you have different teams within your organization. Each team will have its own project to work on. For the purposes of the demos in this course, I'm going to create two separate projects. One project that's associated with my reviews and ratings team. This project is going to be called spikey-reviews. I've clicked on the CREATE button and created the spikey-reviews project here. The spikey-reviews project is where I'm going to create my Cloud SQL instances that will hold my reviews and ratings data. I'm now going to use the drop-down at the top in order to create a second project. This project will be called spikey-developers. Click on the NEW PROJECT button here, and set up the spikey-developers project. This is a general project that all developers within the SpikySales organization use to test out their prototypes. Go ahead and click on the CREATE button here, and get this project set up. So we have two projects, and make sure you look at the blue bar at the very top to know which project you're working on at any point in time. We are currently in the spikey-reviews project. You can click on the drop-down there in order to change the project that you're working on. So you can switch over to the spikey-developers project using this pop-up, and now we are in spikey-developers as you can see from the top bar. I'm going to pop up this dialog and switch back to the spikey-reviews project. And within spikey-reviews, I'm going to activate the Cloud Shell. This I can do by clicking on the Cloud Shell icon on the top right. When you use the Cloud Shell on the GCP, it creates an ephemeral virtual machine and allows you to log into that virtual machine to use as your terminal prompt. At the time this course was recorded, the Cloud Shell was exclusively on an ephemeral VM. That means any files that you download, any settings that you set up here will not be permanently saved, so beware. Don't use it like your laptop or your local machine. It's an ephemeral VM. Notice from the prompt on this Cloud Shell VM that we are in the spikey-reviews project. If you're confused at any point in time during the demos that we perform which project we are working on, you can always look at the prompt of your Cloud Shell terminal. And on this note, we come to the very end of this introductory module on Cloud SQL on the GCP. Cloud SQL is the MySQL or PostgreSQL database running on the Google Cloud Platform. It's capable of storing up to 10 TB of data and supporting 40, 000 queries per second. Cloud SQL is Google's relational database on the cloud, comparable to Amazon's Relational Database Service or Azure's SQL Database. Cloud SQL allows managed install, backup, and replication and is very easy to set up and use. Cloud SQL instances are regional resources on the GCP, which means Cloud SQL instances cannot span multiple regions. Cloud SQL is easy to scale vertically. You simply use a bigger and better machine for better performance and higher storage requirements. You can also configure Cloud SQL in a high-availability mode using failover replicas. In this module, we got started on the Google Cloud Platform console. The next module will be very hands on. You'll see how you can create, edit, start, and stop Cloud SQL instances and connect to them by whitelisting your IP address using SSL or configuring the Google Cloud SQL proxy.

Creating Cloud SQL Instances
Module Overview
Hi, and welcome to this module where we'll get very hands on. In this module, we'll study how you can create, edit, administer, and work with Cloud SQL instances. We'll start off by seeing how we can create and edit instances using the web console, as well as the gcloud command-line utility. We'll see how you can clone, start, and stop instances and connect to instances with and without SSL. Cloud SQL also allows you to configure the database flags that you set when you start up your MySQL or PostgreSQL instance. The Google Cloud Platform offers a very useful facility that allows you to label or tag your resources. This is useful for billing or any other purpose. We'll see how we can use instance labels with Cloud SQL, and we'll see how we can connect to these Cloud SQL instances using the Cloud Proxy, which mitigates the need to whitelist your IP address.

Creating a Cloud SQL Instance
In this demo, you'll see how you can use the web console in order to create a Cloud SQL instance. We start off at the Google Cloud Platform dashboard. The one thing for you to notice here is that I'm on the spikey-reviews project, as you can see from the top blue bar. The spikey-reviews project is where we'll be creating our Cloud SQL instances. Click on the hamburger icon on the top left, and this will give you access to the Navigation menu that brings up all of the features and services that the GCP provides. Here, under STORAGE, you will choose SQL. I'm going to click on the pin right next to the SQL option in order to pin it to the very top. We'll be navigating to this often. The option to get to Cloud SQL is now available at the very top of the menu just under Home, and that's where I'll navigate to to create my first instance. This button takes you to a menu where you have two options. You can create a MySQL instance on the cloud or a PostgreSQL instance on the cloud. I'm going to work with MySQL Second Generation. You can see that the option to create First Generation instances is also present here. But almost all of the demos in this course will be using the MySQL option. So click on MySQL, and you'll see that you have three options here, a Development environment, a Staging environment, and a Production environment. Now all three of these options lead to the same configuration page, but the default configuration that will be checked will be based on the kind of environment that you want. Notice that the MySQL Development environment does not have the Automatically increase storage and High availability options checked. The Staging environment, on the other hand, comes with the Automatically increase storage configuration enabled, but High availability is not enabled by default. And finally, the recommended Production environment is to have Automatic patches & maintenance, Automatic backups, Automatically increase storage, as well as High availability. Remember that these are the default settings. Once you get onto the Instance Configuration page, all of these are configurable. Click on MySQL Development, and we can navigate to the configuration page and see that these options remain unchecked. Notice that the Create failover replica check button there is unchecked. That is what sets up high availability on Cloud SQL. This is the option available under Enable auto backups and high availability. However, had you chosen the MySQL Production environment from the previous page, you would have found that the Create failover replica checkbox would come prechecked. I'm going to set up an instance here using this configuration page, and I'm going to configure the settings as I want it be. My Instance ID is reviews-prod. I want it to be a production instance, but I'll be enabling high-availability configuration later on in this course, not right away. The Cloud SQL Instance ID should be unique, and you should not have created an instance with the same name within the last week. It's always good practice to specify a root password for your instance. You can choose to leave it blank if you want to, but generally, for good security, you need to have a root password. Now you need to specify a region and maybe a zone where you want this instance located. Your choice of region is permanent. Once you've configured the region, you can't edit the region for a particular instance. The zone can be changed at any point in time. My region is us-central1, and I'm going to allow the GCP to locate my instance in any zone within this region. I want my MySQL database to use the latest version available at the time of this recording, that is version 5. 7, and I want to configure the machine type and storage requirements for my instance. You can see that for a development instance the machine type chosen is one of the smaller ones, db-f1-micro, with just 1 vCPU and just 614. 4 MB of memory. You can, of course, change this by clicking on the Change button that you see on screen, but however, because this is for demo purposes, I'm going to leave it as is. I'm also going to go with the default SSD recommendation because I want lower latency with higher QPS. You can choose how much storage capacity your Cloud SQL instance has. I'm going to increase it to 20 GB. Notice that when you make a configuration change to your instance on the right you have the exact configuration of your instance as you're creating it. It'll immediately update to show you that your SSD storage is 20 GB. I'm going to set it back to 10 GB here. I don't really need this much, even for my demo, and I'm going to enable automatic storage increases. One thing to watch out for here is that let's say the data that your Cloud SQL instance stores exceeds 10 GB, the storage capacity will automatically be increased, but in case you delete data, the storage capacity will not be automatically shrunk. You can see the fine print just below that checkbox, All increases are permanent. In the next option to Enable auto backups and high availability, I'm going to leave Automate backups and binary logging checked; however, I'm not going to create a failover replica at this point in time. We'll be talking a lot more about high-availability configuration settings and failover replicas in the module after this one. At this point, I'm pretty happy with the current settings of my Cloud SQL instance. Click on the Create button. Your instance will be created. Now this might take a while depending on how long it takes for the VM to spin up in that particular region and zone. You can click on the SQL button at the top left, and that will take you to the page that you currently see on screen. You might have to wait anywhere between 2 to 7 minutes for your instance to spin up. Once the instance has been created, as you can see by the green checkbox here, you can click on reviews-prod in order to view the details of your instance. This is the OVERVIEW page of the instance, and there are a number of other details available here in the form of tabs such as USERS, DATABASES that have been configured in this instance, and so on. If you scroll down this page, you'll find a lot of useful details available right here on the screen. You can take a brief look at the configuration of your instance. You can immediately see here that this instance is not highly available. We haven't configured it as such. There are ways to connect to this instance, suggestion actions, and logs at the very bottom. This is a quick way to get to the logs for your instance. The logs just have one bit of information at this point in time. Our instance creation has been done. Now it might take a while for the graphs associated with your instance to be updated. You can see that there is a variety of information that you can view graphically here. This will give you a quick idea of the utilization for this particular MySQL instance on the cloud.

Editing an Instance Using the Web Console
In this clip, you'll see how you can edit an already existing Cloud SQL instance using the web console. Click on the reviews-prod instance. That's the one that we want to edit. This will immediately take us to the OVERVIEW page, which you can think of as the dashboard for the instance as a whole. Click on the EDIT button at the very top here, and this will allow you to edit the configuration settings of the instance. Now when you go to the Edit instance page, you'll immediately see that certain portions are grayed out. This is the Instance ID and the Region. These are permanent settings and cannot be changed once the instance has been created. However, within the us-central1 region, I can change the zone where my instance is located. I'll leave it the same for now. And then I'm going to change the storage for my machine type. I'm going to change it from 10 GB to 20 GB. This is what I had set up earlier, but I'd switched it back to 10. Go ahead and click on the Close button here, and then scroll to the bottom and click on Save. This will cause the instance to be updated and will trigger a restart if it's needed. The only user that you're prompted to create when you're setting up your Cloud SQL instance is the root user. If you want to add additional users, you can do so using the USERS tab. This will show you all of the existing users for your database, and you can create a new user account by clicking on the button there. This will bring up a dialog where you can specify the username and password. Our user is the developer, and the password is any password that you assign. I want developers to be able to connect from any machine, which is why I allow any host that is the %. Go ahead and click on CREATE to create a new user for this instance. Notice that for every action that you perform the screen is updated, and you get a message at the bottom left. You have all of the databases available within your Cloud SQL instance when you click on the DATABASES tab. You can also use this tab to create a new database here. The reviews team in my SpikySales organization uses a database called spikey_db, and that's what I want to create here. There are a number of options that I can specify when I set up the database. You can explore the options available here to configure the database how you want it to look. Go ahead and click on CREATE, and that will create a new database here, right here, using the web console UI. Notice that creating a new database within Cloud SQL does not restart the instance, which makes sense. Go back to the OVERVIEW page, and scroll to the very bottom of this page. Notice the Operations and logs section. You can see that logs have been updated to reflect all the actions that you've performed. Click on View all operations, and that'll take you to the OPERATIONS tab. Now it's not necessary for you to create and edit your Cloud SQL instance only using the web console. There are equivalent operations available using the gcloud command-line utility. A systems administrator would ideally like to script or automate all of these mechanical tasks, and he or she can do this using gcloud. Let's activate the Cloud Shell using the icon on the top right of your screen, and let's see how we can use the command line to run gcloud commands. The Cloud Shell opens up a terminal window to an ephemeral VM on your GCP, and running gcloud sql instances patch allows you to edit a Cloud SQL instance that has already been created. The --help option will show you all the options that are available with your patch command. The demos that we perform in this course will give you a mix of using the web console and the gcloud command-line utility so that you're familiar with both. Remember that almost all options on the web console can be scripted using gcloud.

Cloning, Starting, Stopping, and Deleting Instances
In this clip, we'll see how we can perform a number of simple operations on your Cloud SQL instance such cloning, starting, stopping, and restarting your instance. We'll start off in the OVERVIEW page of our reviews-prod instance within our spikey-reviews project. Click on the CLONE button that's available on the top right of your screen. This allows you to create a clone of your instance. Now why might you need to create a clone? Cloning an instance is one way for you to create an exact copy of the current state of your instance, which can assist in point-in-time recovery. The GCP gives you other ways to back up your Cloud SQL data as well. Cloning an instance is one of options available. I'm going to simply click on Create clone and create an instance with the ID reviews-prod-clone. It'll take some time for the new instance VM to spin up. You can watch the progress on your SQL Instances page. My clone has been created, and the clone is present in us-central1. If you click on reviews-prod-clone and explore the instance, you'll find that its configuration and setup is exactly the same as the reviews-prod instance that you had created earlier. You can see that the same users are present here. The developer user that you had added to your reviews-prod is present in the clone as well. The databases that are present in your clone are the same. The spikey_db that you'd created for your team is present here. Click on the icon on the top right to activate your Cloud Shell terminal window. We are going to perform the rest of the actions for this particular clip using the gcloud command-line utility. If you want to stop an instance using the command line, you can use the gcloud sql instances patch command. Stopping a Cloud SQL instance requires that you set the activation-policy of your instance to NEVER. The instance will then be stopped. Our reviews-prod-clone is the instance that I want to stop here. Stopping an instance requires that all connections to that instance be closed and the instance be shut down, so it might take a couple of minutes. You can go to the list of instances in your SQL Instances page, and you can see that the reviews-prod-clone instance has now been stopped. You can see by the stop icon to its left. If you click on the instance here, the OVERVIEW page will give you a message on top indicating that the instance is stopped. You can start it once again, and you can do so either by using the START button on the top of your screen or by using the gcloud command-line utility. When you want to start a Cloud SQL instance using the command line, you'll use the patch command as before, and you'll set the activation-policy of the Cloud SQL instance to ALWAYS. The activation-policy for other resources on the GCP may have a slightly different meaning. In the case of Cloud SQL, they are only used to start and stop the instance. Starting the instance might take a couple of minutes here. You can switch over to the Instances page and see the status of your operation. One thing I've noticed with the GCP is that the web UI doesn't update immediately, so your instance might be started, but it might take a minute or so for the web UI to update. If the connections to your Cloud SQL instances are hung and you can't tell why, you might need to restart your instance. You can do this with a single command as well. The gcloud sql instances restart command allows you to restart your Cloud SQL instance. Now you might see a message on screen which talks about the activation policy on demand. Remember that for Cloud SQL Second Generation MySQL instances, the activation policy on demand is no longer applicable. The activation policy can only be NEVER or ALWAYS. I'll enter Y at this prompt because, yes, indeed, I do want to restart my clone, and I'm going to go ahead and wait patiently until my instance is restarted. Now once your restart is complete, you can hide the Cloud Shell, and then you can go to the OVERVIEW page of your clone and go to Operations and logs. Notice there that your logs reflect all the actions that you've performed on this instance. And finally, let's go back to our Cloud Shell terminal window and delete this clone instance that we've created. I can do this using gcloud sql instances delete. We'll get a warning which will rightly tell you that once you delete this instance all the data associated with this instance will be lost. I'll say Yes and then wait for my instance to be deleted. You'll get a visual confirmation that your instance has been deleted when you go to the SQL Instances page, and you see here that reviews-prod clone has disappeared from my instances list.

Connecting Using the Cloud Shell
Well, you have your instance set up at this point in time, but database administrators and even your developers will need to connect to your Cloud SQL instance in order to run their SQL commands. Let's see how you can connect using the Cloud Shell. We start off in the OVERVIEW page of the reviews-prod Cloud SQL instance, which we created in the spikey-reviews project. The spikey-reviews project is for the reviews and ratings team, which maintain the reviews and ratings for our SpikySales organization. The easiest way to connect using the Cloud Shell is to scroll down a little bit on this page and use the link available here, Connect using Cloud Shell. Just above that link, you see the primary IP address of your Cloud SQL instance and also your instance connection name. We'll use those later. For now, we want to connect using the Cloud Shell. This will activate your Cloud Shell terminal window and automatically populate the command that you need to connect to this particular instance, the gcloud sql connect command followed by your instance name, and by default, we use the root user. When you hit Enter, the first message that you'll see is that your IP for this Cloud Shell instance is being whitelisted by Cloud SQL. Now when you use the Cloud Shell, the VM that has been set up is an ephemeral VM, so it doesn't have a static IP address. But in order for this VM to be able to connect to your Cloud SQL instance, the GCP will automatically whitelist your Cloud Shell IP for a short period of time. The fact that your Cloud Shell IP address has been whitelisted is available in your AUTHORIZATION tab. So if you click on the AUTHORIZATION tab in your web console, under the Authorized networks section, you'll see an IP address, which has been automatically added. This is the GCP's doing. You haven't explicitly whitelisted any address yet. If you remember, the default security provided by the GCP does not allow you to connect to your Cloud SQL instance unless you whitelist your ID, use an SSL certificate, or use the Cloud Proxy. Here our Cloud Shell IP has been automatically whitelisted to enable you to connect. In the Cloud Shell terminal window, we've connected as the root user. Go ahead and enter your root user password, and then you'll be allowed to connect. You've successfully connected at this point in time, and you are at the MySQL prompt. You can now see all the databases that are present here. Show databases should show you all the databases that you see in the DATABASES tab on the web console. I want to use the spikey_db database, and you can see that the MySQL prompt changes to indicate that I'm working in the spikey_db database. If I run the show tables command within this database, that'll show me that there are no tables yet because I haven't created any. Now my reviews and ratings team needs a table in order to populate information about reviews and ratings that are available on the SpikySales site. Here is a simple CREATE TABLE command to create the reviews table. This Cloud SQL instance that I have running is simply a MySQL database running on the cloud, so all of the commands that I use within MySQL will work here as well. Once I've created this table, I'm going to run the show tables command once again, and this will show me that my reviews table has been successfully created. I'll run the describe reviews command to ensure that the column names within my MySQL table has been set up correctly. This table is now ready for my developers to use. You can run some more test SQL statements, such as select * from reviews in order to test this prompt out. And if you type in exit, you'll exit from this MySQL client shell. We had connected earlier as the root user. If you want to connect as the developer user, this is the user that you had set up using the web console, you simply change the --user flag to developer. Each time you run the gcloud sql connect command on your Cloud Shell, your Cloud Shell IP will be whitelisted. Because its IP is ephemeral, it needs to be whitelisted each time. So go ahead and type in the password for your developer account, and you'll find that you'll be able to successfully connect. Everything else from here on in remains the same.

Connecting Using a MySQL Client
Using the Cloud Shell to connect to our SQL instance is a quick and dirty way to get up and running, but what your developers and administrators really want is a permanent way to connect, which you can do using the IP address and the MySQL client. This is something that they are probably familiar with already. When you use IP addresses along with the MySQL client, you can connect to your Cloud SQL instance from any machine whether it's a desktop, your laptop, or any VM on any cloud platform somewhere. For this demo, I'm going to create a new VM in the spikey-developers project. This is the project that I'd set up earlier. Confirm that you're indeed in the spikey-developers project, as you can see from the blue bar at the very top. Click on the hamburger icon on the top left. That is your navigation menu. Go to the Compute Engine option on this navigation menu where we'll create a new VM instance. Click on the VM instances option on this menu. This will take you to a page where you can create a virtual machine in your spikey-developers project. The idea of setting up a VM in a completely different project is that it mimics a completely different machine which has no links or connection with the Cloud SQL instance that we had set up earlier. Go ahead and click on the Create button here, and this will take you to the VM instance creation page. I'm going to call this VM the mysql-client-vm. Because all I really need here is a simple virtual machine, I'm going to accept all of the defaults that GCP proposes. Notice that this is a Debian Linux machine. If you're working with a MySQL client on a Windows machine, I assume you know how to connect to a particular SQL instance. You have to do the exact same thing here when you connect to Cloud SQL. Go ahead and click on the Create button here, and wait for your virtual machine to be created. This might take a couple of minutes, so you need to be a little patient. The next step is for me to SSH into this particular virtual machine so that I can set up the MySQL client. Click on the SSH drop-down, and open up an SSH terminal window on a new browser. If you have pop-ups blocked like I do, you need to enable pop-ups from console. cloud. google. com. Once you've enabled pop-ups, you can click on the button to SSH into your virtual machine once again. And once I've logged into my Linux terminal, I'm going to run sudo apt-get update in order to get the latest version of the MySQL client package. Once this update command has run through, I'm now ready to install the MySQL client on this virtual machine, which I can do using sudo apt-get install mysql-client. If you're working on your personal laptop, you might already have the MySQL client installed. You can go ahead and use that. Make sure that the versions match the version of your MySQL database on the Cloud SQL instance. The rest of the install should flow through as expected. I now have MySQL client on my virtual machine in the spikey-developers project. I'll now open up a terminal window and go to console. cloud. google. com. This will bring up my Google Cloud Platform dashboard, and I'm going to switch over to the spikey-reviews project. The spikey-reviews project is used by my reviews and ratings team, and that is where I have my Cloud SQL instance. In order to be able to connect to my reviews-prod Cloud SQL instance in the spikey-reviews project from the virtual machine that I have set up in my spikey-developers project, I need the static external IP address of my virtual machine in order to whitelist it on my Cloud SQL instance. I'm going to switch back to the browser tab on my spikey-developers project, notice the project name on top here, and copy over the external IP address of my mysql-client-vm. You can see the external IP address highlighted here on screen. Switch back to the reviews-prod instance in the spikey-reviews project and go to the AUTHORIZATION tab. This AUTHORIZATION tab is where you can explicitly add the IP addresses that you want to whitelist. Click on the Add network button that you can see here. This will pop up a form that you can fill in. You can specify a name for your connection. This is optional, but it's recommended because it's easy to identify a particular ID using this name. Specify the IP address that you want to whitelist, and click on Done. Once you go ahead and hit Save on this screen, you've effectively whitelisted the IP address of the VM that you set up in the spikey-developers project. Once your reviews-prod Cloud SQL instance has been updated with this new whitelisted IP address, you copy over the public IP address of this Cloud SQL instance. This is the IP address that we'll use to connect using our MySQL client. If you're connecting from a MySQL client that you've installed on your laptop at home, you can get your public IP address by going to this website, whatsmyip. com. Switch over to the terminal window connected to our virtual machine in the spikey-developers project, and here we'll run the mysql command in order to connect to our Cloud SQL instance. The host parameter is where we specify the IP address, the public IP address of our Cloud SQL instance, and we want to connect as the root user. And that's it. Once you enter the root user's password, you'll find that you've been connected. Using the MySQL client with a whitelisted IP address of your virtual machine is a simpler and more permanent way to connect to your Cloud SQL instance.

Connecting Using SSL
What we've viewed so far are insecure connections, but the best practice, especially with production instances, is for you to use a secure connection with SSL, and that's what we'll see how to do right now. We'll start off in the OVERVIEW page of our reviews-prod instance within our spikey-reviews project. We'll also be using the spikey-developers project. At any point in time, I'll talk about which project we are working in, so pay close attention to that. Now within here, notice the SSL tab. This is what will allow you to set up the SSL certificates that you need to connect securely to your Cloud SQL instance. You can see here on this page that you have an option to allow only SSL connections. You can click on this button, or you can set this configuration using the Cloud Shell, and that's what we'll do here. The gcloud command-line utility allows us to patch our reviews-prod instance so that it requires SSL, which means non-encrypted connections will not be allowed to connect to our Cloud SQL instance. Once your Cloud SQL instance has been patched with the require-ssl property set to true, you can refresh your web console, and you'll see a message indicating that your instance requires all incoming connections to be encrypted to be secure. However, in order to allow SSL connections, you need to configure client certificates, and we haven't set up any yet. That's what we'll do right now. But let's first confirm that non-encrypted connections are indeed prevented or prohibited by our Cloud SQL instance. Switch over to the mysql-client-vm in the spikey-developers project, and connect as we did before. You'll see that we have Access denied here. Only secure connections are allowed. Switch back to our reviews-production instance in our spikey-reviews project, and let's set up a client certificate which will allow us to connect securely to this Cloud SQL instance. Click on the Create a client certificate button that you see in the middle of the screen here. This will throw up a dialog prompt asking you to create your client certificate and give it a name. Go ahead and click on CREATE, and this will pop up another dialog with three important pieces of information. The first of these is the client-key. pem, which is the file which contains the private key that you'll use to authenticate your virtual machine to the Cloud SQL instance that is the server. The client-key will only be available in this dialog here. Once this dialog is closed, you'll no longer have access to your client-key, so make sure you download it and store it safely. Once you download it, then you'll be allowed to close this dialog. So I'm going to download this client-key onto my local machine here, and I'm going to save it to a folder somewhere. Go ahead and download the client certificate, as well as the server certificate. Once these three files are saved to your local machine, you can click on CLOSE and close this dialog. At this point, you have your private key, the client certificate, and the server certificate, everything that you need to enable you to log in using SSL. These are the three files that you need to enable your SSH handshake. Your MySQL client virtual machine, which is in the spikey-developers project, needs access to these client certificates in order to be able to connect securely to your Cloud SQL instance. Now we'll give it access using Cloud Storage buckets. Cloud Storage buckets, if you haven't used them before, are GCP's equivalent of Amazon's Simple Storage Service, and this is a general purpose Blobstore where you can upload any kind of files. I'm going to create a new bucket here. I'll make it a regional bucket, meaning it'll only be located within a single region. This bucket is called spikey-dev, and it's located in us-west2. Go ahead and click on the Create button. A new bucket will be created, and you can upload the three certificate files that you have on your local machine to this Cloud Storage bucket. Uploading to this Cloud Storage bucket will allow us to transfer them to the virtual machine which contains our MySQL client. There are several ways to get files onto your virtual machine in any project. This is the simplest possible way. I have uploaded the certificate files to a bucket in the spikey-developers project, and I have made the links of each of these files public temporarily. For every. pem file, I'm going to copy the link address and use wget on my Linux virtual machine, that is which has MySQL client, in order to download the contents of this file onto this VM. Switch over to the mysql-client virtual machine in the spikey-developers project. I've already SSHed into this terminal. I'm going to create a directory called SSL and cd into that directory. This is the directory where I'm going to store my client certificates in order to connect via SSL, and I get the client certificates using the wget command as you see on screen. Wget can be used with any public URL, and this gave me an easy way to circumvent any permission-related issues. The ls command shows me that my client certificate has been downloaded to this machine successfully. I'll do the same thing for all of the other certificates. I'll copy the link address and run wget on this public URL. Do this for all three certificates. Once you've completed executing all three wget commands for the three client certificates, the client-key, the client-cert, and the serve-ca. pem, confirm that it's present on your local machine. You're now ready to connect securely to your Cloud SQL instance. Run this command within the SSL directory where your client certificates are located. Specify the server-ca, the client-cert. pem, and the client-key. pem to your MySQL command. Enter your root password, and you're connected. Once you're connected using SSL, your connection is secure and cannot be sniffed. The remaining operations within your MySQL prompt remain exactly the same as before. Just a reminder here for you to clean up those client certificates which are available in the spikey-dev bucket which are now publicly accessible. Make sure you unshare them publicly, or you simply delete them. For the remaining demos in this course, I want to be able to connect insecurely without using the SSL certificates, so I'm going to switch back to the reviews-prod page, this is in the spikey-reviews project, and allow unsecured connections under my SSL tab. I'm going to make this change so that it's easier for me in the demos that follow. So I have reset my Cloud SQL instance to allow unencrypted connections. If you want to confirm that you can indeed connect without SSL, you can switch to your mysql-client-vm, run a simple MySQL connect command, and see that it works.

Configuring Database Flags
You can configure the database flags on your MySQL instance running on Cloud SQL to adjust MySQL parameters and options and to configure and tune our instance. Let's start with the reviews-prod instance that we had set up earlier, and you can edit the database flags that apply to your instance by going to the edit configuration page. This is the same page that you'd access if you click on the EDIT link at the very top. Scroll down on the configuration settings page, and there is a section for Add database flags. This is a section you can use to access the key-value pairs to configure your database flags. You can also configure your database flags using your terminal window using the gcloud utility, and that's what we'll do here. Let's connect to our reviews-prod instance using Cloud Shell as the root user first. In this demo, we are going to set up a database flag that allows us to configure our table names only in lowercase. Before that, let's confirm that we can indeed create a table name in the uppercase before that flag has been set. So within the spikey_db, when we run the show tables command here, you can only see the reviews table. Let's use the create table command in order to create a table called TEST_CAPS. Notice that the name of the table is in all capital letters. Now if you see the tables, you'll see that the TEST_CAPS table has been successfully created. Let's exit from this MySQL connection and switch over to the configuration details page of our reviews-prod instance. In this page, we are going to add a database flag that ensures that all tables that we create from here on in have lowercase table names. All the database flags that can be configured in our MySQL instance is available here in this drop-down, and you can explore and use the configuration parameter that you're interested in. Set lower_case_table_names to 1, and then hit Close, and then go ahead and save these new settings. Wait for your instance to be updated. This requires a restart, and it might take a while. Once the new database flag has been applied to your Cloud SQL instance, let's connect using the Cloud Shell. Connect us to root user, and specify the password. Switch over to the spikey_db database by calling the use command as you see on screen. The show tables command will show you that the TEST_CAPS table that you created earlier where the table name is all in capital letters still exists. That has not been affected. However, if you try to create a new table now called TEST_LOWER, notice that the table name we have still specified using capital letters. If you run the show tables command after creating this table, you will see that the table name has been converted to lowercase. The database flag that you applied to this Cloud SQL instance has forced your table names to be represented in the lowercase. This is how you can use the web console to configure the database flags that apply to your MySQL or PostgreSQL instance.

Labeling Cloud SQL Instances
Labels on the GCP are key-value pairs that help you organize your Cloud Platform resources. This can be for billing purposes to identify which are production instances, which are developer instances, to identify the team to which a particular resource belongs to, and so on. This demo, we are working in the spikey-reviews project. We start off in the OVERVIEW page of our reviews-prod Cloud SQL instance, and we are going to configure labels on this instance. These labels will have some meaning in order to indicate what exactly we are going to use this instance for. Click on the EDIT button on the top in order to go to your configuration settings page. You can add labels to this instance using the web console by going to the Add labels section at the bottom and clicking on the Add label button. Labels are key-value pairs that you can use to identify your resource in some ways. You can have labels for team or call centers. Labels can be used to determine the state of a resource, whether the resource is in an active state, a ready-to-delete state, archival state, and so on. Using the web console to add labels is very straightforward, as you can see. In order to get some more practice using the command line, we'll switch over to our Cloud Shell and use the gcloud utility in order to add labels to our instances. You can associate labels with instances at the time of instance creation. Here I'm using the gcloud beta sql instances create command in order to create a new instance called reviews-stage. At the time this course was recorded, associating labels during instance creation requires the beta sql instances create command. You can see from the labels that this is a staging instance for our spikey-reviews team, and the billing team associated with this instance is spikey_reviews. This command will spin up a new Cloud SQL instance using the MySQL database. This is the reviews-stage instance, and you might have to be a little patient, wait for a couple of minutes before this instance is fully created. On the SQL Instances page where all of our Cloud SQL instances are listed, you can see at the very right column we have the Labels column, and you can see the billing-team and environment labels associated with our reviews-stage instance. You can also associate labels with instances that have already been created. For example, our reviews-prod instance can have labels associated with it using the gcloud beta sql instances patch command. Here we are going to update labels. We want the environment for this instance to be production, env=prod, and billing-team is spikey_reviews. Wait for a couple of minutes. This will cause our existing reviews-prod instance to be updated, and the labels will now be associated with this instance as you can see at the top-right column, the Labels column, on this Cloud SQL Instances page. Go to the OVERVIEW page of your reviews-prod instance, and click on the EDIT configuration. And if you scroll down and see the labels that are associated with this instance, you can see the billing-team is equal to spikey_reviews, and the environment is prod. Your spikey-reviews team might want to have the same labels associated with all of the resources they use on the Google Cloud Platform. This will allow them to quickly look up what their billing is for every month. This will allow them to allocate costs for each of their resources. If you label your Cloud SQL instances correctly, you can use the gcloud command-line utility to quickly filter all of the resources with a particular label. Gcloud sql instances list labels. env=prod will show you all your production instances. We have just one production instance here, but you can imagine that your reviews team might have multiple instances running on the cloud. Let's filter on a different label here. Let's filter on the billing-team label. We want to see all of the resources that the spikey-reviews team uses, and you can see that there are two Cloud SQL instances with this label.

Introducing the Cloud SQL Proxy
A common use case for the SpikySales company that uses the Google Cloud Platform is for external applications to connect to our MySQL database running on our Cloud SQL instance. An external application could be an app running on GCP's App Engine, running on a Kubernetes cluster, or on your on-premise data center. One easy way for all your external applications to connect without having to explicitly whitelist their IP address is to use the Cloud SQL Proxy. We studied earlier that a zone on the Google Cloud Platform is similar to a data center, that is the availability zone for your resources; zones are grouped together into regions, regions are a set of zones with high-speed network links between them; and a network is the user-controlled IP addresses that belong to the same VPC, or the virtual private cloud. An example of a zone present in the US on the GCP is the us-central1-a. It belongs to the us-central1 region. And whenever you create a resource on the GCP, such as a VM instance or a Cloud SQL instance, it belongs to the default network. As is the case for any virtual machine running on your on-premise data center or on the cloud, Cloud SQL instances are vulnerable to security threats, and it is the GCP firewall that restricts access to your Cloud SQL instances. We know that we can't explicitly access any Cloud SQL instance without either whitelisting the IP of the client that is about to connect or getting the client and the server to trust one another by configuring SSL. There is a third option here. We can use the Cloud SQL Proxy. We've already seen how to do the first two. Let's now look at the Cloud SQL Proxy and how it works. The way the Google Cloud Platform enforces security is by having all resources belong to some network. If you don't explicitly configure the network, it belongs to the default network that is set up by default. Each network enforces firewall rules, and by default, only those resources whose IP address has been explicitly whitelisted on your Cloud SQL instance can access that Cloud SQL instance. You can't have any arbitrary VM instance A access your Cloud SQL instance X. Which basically means that if you have multiple developers working on your spikey-reviews project, each of their machines need to be explicitly whitelisted on your Cloud SQL instance X. Now you might have multiple applications running not just in the spikey-reviews project, but on other projects as well. Maybe the search team, maybe the recommendations team of your e-commerce platform needs to access review data. All of those IPs will need to be explicitly whitelisted. This is not just a configuration and management problem. The IP address of all of the machines you want to be able to access your Cloud SQL instance must be static. All of your machines must have static IP addresses for them to be explicitly whitelisted, and on the cloud, this is a problem. If you have applications running on the cloud, they might have ephemeral IPs. Now another option for you to have your VM talk to your Cloud SQL instance is by using SSL. SSL allows the client and the server to trust each other. We've already seen how we can set up client certificates and then use the MySQL client on a trusted client to connect to your Cloud SQL Server. This solution enables your traffic to be encrypted and enables trust between the client and the server. We still need static IP addresses though. In order to overcome the static IP address problem, you use the Cloud SQL Proxy. When you use the Cloud SQL Proxy that GCP provides, the proxy handles authentication with Cloud SQL, it removes the need for static IP addresses, and there is no requirement to whitelist the IPs explicitly either. There is no need to use SSL as well. The proxy automatically encrypts traffic to and from the database. The Cloud SQL Proxy works by having a local client called the proxy client which runs in the local environment, and the application communicates with the proxy with the standard database protocol used by your database. A secure TCP tunnel is set up between your proxy client on your local environment and the proxy server that's running on the Cloud SQL instance. Once you have your Cloud SQL Proxy up and running, all of your external applications can simply connect to this proxy and not directly to your Cloud SQL instance.

Using the Cloud SQL Proxy
In this demo, we'll see how we can configure and use the Cloud SQL Proxy to manage external connections to our Cloud SQL instance. We'll be working across two projects, the spikey-reviews project, which contains our Cloud SQL instance, and the spikey-developers project, which contains our mysql-client-vm. We'll first go to the spikey-reviews project and enable the Cloud SQL API. This will allow the Cloud SQL Proxy to connect to our Cloud SQL instance. In order to administer it, this we'll do in the spikey-reviews project. Within the spikey-reviews project, we'll then create a service account with Cloud SQL access. A service account on the GCP is a special Google account that can belong to an application or a virtual machine rather than to an end user. Your application uses the service account to call the Google API of a particular service so that users are not directly involved. The Cloud SQL Proxy will use the service account to administer your Cloud SQL instance. This service account has to be created in your spikey-reviews project, which is where your Cloud SQL instance lives. We'll download the JSON key for this service account. This JSON key is available from the spikey-reviews project, but will be used by the spikey-developers project, which is where we have the VM that we'll connect using the Cloud Proxy to our Cloud SQL instance running in the spikey-reviews project. The Cloud SQL Proxy executable has to be downloaded to the VM instance that is running in our spikey-developers project. The spikey-developers project also needs access to the service account JSON key which was generated in the spikey-reviews project. It will then use this key in order to connect using the Cloud SQL Proxy. We start this demo off in the spikey-reviews project. We are in the SQL Instances page. Use the Navigation menu to go to API & Services. This is a dashboard where we'll enable the Google Cloud SQL API. Go to ENABLE APIS AND SERVICES, and search for Cloud SQL API. This is the first box that you see on screen. Go ahead and click on the Cloud SQL API, and click on the ENABLE button that you see here on screen. Wait for a couple of minutes for this API to be enabled. Once the API has been enabled, it's time for step two. We now need to create a service account with Cloud SQL access. Our Cloud SQL instance is in the spikey-reviews project. Our service account also has to be created here. Go to Identity and Access Management & administration, and click on the Service accounts option from your Navigation menu. This will take you to a page where you can click on CREATE SERVICE ACCOUNT. Go ahead and give the service account a name. I'm going to call it CloudSQLProxyConnection. Give it a meaningful name so that you can identify the service account if you see it. Notice that the service account ID has been automatically generated. We need to assign a project role to this service account so that we know what its privileges are. Go to select a role, click on Cloud SQL, and click on Cloud SQL Client. You can see here that Cloud SQL Client allows you connectivity access to your Cloud SQL instance. The next step is for us to download the JSON key associated with this service account. You can do this using the same menu where we create the service account. Click on Furnish a new private key, and the Key type is JSON. Go ahead. Clicking on the SAVE button here will open up a dialog on your local machine that will allow you to download the service account key in the JSON format. Click on Save. You now have the key saved on your local machine. Now let's switch over to our VM instance running in the spikey-developers project. This is the VM instance where we'll run our Cloud SQL Proxy. We first need to download the proxy client here. The cloud_sql_proxy is available from dl. google. com. You can download it using the wget command that you see here on screen. Make sure you choose the Cloud SQL Proxy that corresponds to your VM instance. Mine is a Linux instance running on amd64. The ls -l command will show me that the cloud_sql_proxy is now available on my machine, but it's not executable yet. I need to run the chmod command +x in order to make the cloud_sql_proxy executable. List the contents of this directory again. You can see that the cloud_sql_proxy is specified in green, indicating that it's now executable. This VM running in the spikey-developers project needs access to the service account JSON key that we downloaded to our local machine. The easiest way to give it access is to upload it to the bucket that we had created, the spikey-dev bucket in the spikey-developers project. Upload the JSON key from your local machine, and make the key link public. To make this JSON file publicly accessible via a link, we'll use wget to download it to our VM instance. This is the step where in our spikey-developers project we download the service account key. So copy the link address, use wget, and get the JSON service account key onto your VM. Run the ls -l command, and confirm that the JSON key is now available on this VM instance. So we have the service account key from our spikey-reviews project available now on this VM, which is in the spikey-developers project. Make sure that you don't leave your service account JSON key publicly accessible. I've gone back to the bucket, and I have unshared it, so it's no longer publicly available. We are now ready for our very last step, which we'll perform using the VM in our spikey-developers project. We will connect to our SQL instance using the cloud_sql_proxy. So switch over to your VM instance, and run the cloud_sql_proxy as you see on screen. The -instances flag specifies the Cloud SQL instance that you want to connect to. We specified the full name of the instance, spikey-reviews in us-central1:reviews-prod, and we want to connect to it using a TCP tunnel that's running on TCP 3306. The credential_file that we use is the spikey-reviews service account that we had created earlier, and the key for that service account is available in JSON format on this machine. The ampersand that you see at the very end will run this process in the background. Hit Enter. Our cloud_sql_proxy is now running in the background, and it's ready for new connections. We can use the mysql command to connect to the cloud_sql_proxy. The cloud_sql_proxy is running on this host, which is why the host parameter is 127. 0. 0. 1. I'm connecting here as the root user. I specify the password for my Cloud SQL instance, and that's it. I am connected via the Cloud SQL Proxy, and I didn't need to whitelist my IP address. You can try it on a VM where the IP address is not whitelisted, and you'll see that you'll connect just fine.

Working with PostgreSQL
All the Cloud SQL demos that we've worked on so far have involved MySQL. Now using PostgreSQL on the Google Cloud is very similar to using MySQL, except for the subtle database differences. Here we'll see a quick demo of setting up a PostgreSQL instance, and you'll see how similar it is to working with MySQL on the cloud. So go to CREATE AN INSTANCE, and choose PostgreSQL. Notice that we once again have three options here on the screen. You have PostgreSQL for Development, Staging, and Production, and the default configuration settings for each of these options will be different. You are, however, free to choose the configuration settings that you want. You can go into PostgreSQL Development and configure it to be a production instance if you want to. I'll click through through the Development configuration setting page. I'll call it reviews-prod-psql and use the spikey_rr as my root password, and go ahead and explore the other configuration settings. You can see that the settings are very similar to the settings that we saw when we set up a MySQL instance. The features offered by PostgreSQL is very similar to the Second Generation MySQL instances. They are also available with the high-availability configuration. Go ahead, click on the Create button to create a new instance. You'll need to wait for a couple of minutes for the instance to spin up. We can then click through to that instance from your SQL Instances page. This will take you to the OVERVIEW page. You have all the other tabs. Very similar to your MySQL instance. You can connect to this PostgreSQL instance using Cloud Shell as before. The actual connect command is a little different. That's because the default user for your PostgreSQL instance is the postgres user. When you use the Cloud Shell as before, your ephemeral IP will be whitelisted, and GCP will allow your connection to go through. Specify the password, and there you are at the PostgreSQL prompt. You can run \l in order to list out all the databases that are available by default. You can exit out of this window by hitting :q. This will take you back to the PostgreSQL prompt. You can use SELECT datname FROM pg_database in order to see all the tables that you have available by default, and you can hit \q in order to quit this prompt. PostgreSQL has very few conceptual differences from MySQL running on the Google Cloud Platform. You'll find that once you understand how to manage Cloud SQL instances which are running MySQL, using Cloud SQL instances with PostgreSQL is very straightforward. And here are docs that can get you up to speed quickly. And on this note, we come to the very end of this module where we got plenty of hands-on experience working with Cloud SQL instances. We started off by creating and editing Cloud SQL instances before we moved on to more complex tasks. We saw how we could clone, start, and stop instances. We then saw how we could connect to instances when we use SSL and without using SSL. We saw that we could configure database flags using the web console UI, as well as the gcloud command-line utility. We then saw how we could add labels to instances in order to group our resources. When your database is on the cloud, you'll have applications running either on your on-premise data center, other cloud platforms, or even on different projects, all of which need to access your Cloud SQL instance. Making this access easy is the Cloud SQL Proxy, and we saw how we could configure and use the Cloud SQL Proxy on the GCP. In the next module, we'll focus on how we can configure Cloud SQL for high availability using a failover replica. We'll also see how we can create and manage read replicas and how we can import and export data from Cloud SQL.

Replication and Data Management
Module Overview
Hi, and welcome to this module where we'll study replication and data management on Cloud SQL instances. Now Cloud SQL instances, typically when used in production, are required to be highly available. If your zone or a particular data center goes down, you do not want the data in your database to be inaccessible. For this, you need to configure your instances for high availability. High-availability configuration involves setting up a failover replica which replicates all of the data that is present in our master, which means there might be a replication lag. In this module, we'll see how you can set up alerts to monitor this replication lag. We'll also see how we can configure on-demand and automatic backups on Cloud SQL and restore our instance using these backups. We'll then move on to configuring read replicas in order to improve read throughput from our database. We'll then see how we can import and export data using SQL dump files, as well as CSV files.

High Availability Configuration
Let's first understand how the high-availability configuration works on Cloud SQL. In this discussion of high availability, we'll talk primarily about MySQL Second Generation instances. A Second Generation instance is in a high-availability configuration when it is configured with a failover replica. The original Cloud SQL instance to which all of the writes are transmitted is the master. This master is configured along with a failover replica. This failover replica should be in the same region as the original master, but in a different zone. All changes to the data in the master database are replicated to the failover replica. Now there might be a slight replication lag based on how long it takes to write out the data from the master to the failover replica. The replication is set to be semisynchronous. Semisynchronous replication is a trade off between completely asynchronous and completely synchronous replication. The master waits after the transaction has been committed until at least one worker or one replica has received and logged the event. This is the basic definition of semisynchronous replication, and that's what Cloud SQL follows. Remember that Cloud SQL is a regional resource. That means the failover replica that you configure for any master has to be in the same region as the master and also in the same GCP project as the master. Failover replicas are separately charged. They are charged as standalone Cloud SQL instances. When you use the MySQL First Generation instances on Cloud SQL, high availability is configured by default. That is the default configuration. Second Generation instances need to be explicitly configured for high availability. The default is non-high availability in order to save on costs. High availability is typically only required for production instances. Your test, dev, and staging instances need not be highly available. Let's visually understand how failover works on a Cloud SQL instance. Let's say you have a client application which typically communicates with your Cloud SQL master instance. This master is present in us-central1-a. It has been configured in the high-availability mode. It has a failover replica present in us-central1-b. The master and the failover replica are kept in sync using the high-speed links that connect different zones. Notice here that the master and the failover are in different zones, though in the same region, us-central1-a and b. This is an important point to remember. Cloud SQL is a regional resource, not a global resource. Its instances cannot span multiple regions. All the writes that you make to your relational database are directed to the master instance. Changes are then replicated to the failover instance using these high-speed links. All your client applications which need to connect to the database connect to the master. This includes you Cloud SQL Proxy; your MySQL client, which might connect directly; your App Engine scripts; anything. Now it's quite possible that the zone in which your master instance is available suffers an outage. Maybe network connectivity is lost, and you can't access the master. In such a situation, the failover replica is promoted to be the new master. During the failover, there is a short period of time for which your database will be unavailable. This period of time depends on the replication lag between the master and the failover replica. Once the failover is promoted to be the new master, a new failover replica is instantiated and backed up with all of the data from the new master. Notice that the original failover replica, which is now the new master, continues to remain in the new zone, us-central1-b; however, the IP address of the master is the same as the IP address of the original master. The IP address has been preserved so that all of the existing connections can continue to connect at the same IP address. Existing connections will be closed and will have to be reestablished, but the IP address remains the same. Remember, the brief outage depends on the length of the replication lag, the time it takes for the failover replica to get all the information that was available in the original master. You can simulate a failover on the GCP to test out your failover replicas. Failovers are typically triggered when the zone of the master instance experiences an outage, when the master is unresponsive for some reason or the other for about 60 seconds, when the master is in the normal state, when it's not stopped or in the maintenance state. So if you're simulating failover in your test scenarios, you can't explicitly stop your master instance. The gcloud command-line utility offers commands to simulate failover.

Configuring Failover Replicas for High Availability
In this demo, you'll see how you can configure your Second Generation MySQL instance to a high-availability configuration. We start off in the OVERVIEW page of our reviews-prod instance. We'll configure high availability on our reviews-prod instance using the terminal window on our Cloud Shell. Go ahead and activate the Cloud Shell, and let's see what instances we have available at this point in time. The gcloud sql instances list will list out all of our instances within this project. We have three instances configured. In order to configure an instance for high availability, you need to have two prerequisite configuration settings set up correctly. Firstly, you need to have automated backups. I'm going to use the gcloud sql instances patch command to set up my backup-start-time to 23:00 hours. My instance has been updated. I have automated backups enables at 23:00 hours every day. The second condition to enable high-availability configuration is to enable binary logs on your Cloud SQL instance. Reviews-prod already has binaryLogEnabled. I'm going to go ahead and enable it using the command line anyway. Enable-bin-log reviews-prod will enable binary logging on my Cloud SQL instance. We can now use the command line to create a failover replica for our reviews-prod instance. Use gcloud sql instances create. The instance will be called reviews-failover, the master-instance is the reviews-prod instance, the replica-type is a FAILOVER replica, and I'm going to create it in the us-central1 region. This will spin up a brand-new Cloud SQL instance and replicate all of the data that's available in the reviews-prod. Your instance might take some time to set up. You need to be a little patient here. Once the failover replica has been created, it's available on the SQL Instances page. Notice that the reviews-failover replica is indented to show that it's associated with the reviews-prod instance. Also notice that reviews-prod has now been enabled for high availability. The High availability column shows Enabled. Another thing to note here is that our reviews-prod instance, the master instance, is in us-central1-f, and our failover replica is in us-central1-b. Let's click through on the reviews-failover and explore our failover replica. When you go to the OVERVIEW page, it clearly shows you that this is a failover replica, as you can see from the left navigation pane. If you click on the USERS tab here, you'll see all of the users that are available, and our master instance is also available on the failover instance. If you click on the DATABASES tab, you'll see that the spikey_db database is present here as well. The AUTHORIZATION tab will show you that the MySQLClient-vm that we had configured in our spikey-developers project is allowed to access the failover replica as well. If you click on the reviews-prod link at the very top, that will take you to the reviews-prod instance. And if you scroll down here within the Configuration settings, you'll see that it's now highly available. Let's confirm that this failover replica works and it replicates any data that we add to our master instance. For this, I'm going to connect to the reviews-prod master instance using the Cloud Shell. And once I'm connected, I'm going to see all of the databases that exist here. On my reviews-prod master instance, I'm going to create a new database called replicated_db. I'll run the show databases command once again to confirm that this new database has been created. Now I'm going to use the Google Cloud Platform's web console to switch over to the failover replica. I'll go to the DATABASES tab here, and on this tab, you'll see that the replicated_db is present. The database has been replicated to our failover replica. Let's now simulate a failover scenario. Open up your Cloud Shell once again where you're connected to the reviews-prod MySQL database, switch over to the spikey_db database within here, use spikey_db, and run the show tables command. You can see that these are the tables currently available under spikey_db. Click on the plus tab just above your Cloud Shell terminal window. This will allow you to add a new Cloud Shell session. Open up a new Cloud Shell terminal window. This is where we are going simulate the failover of our master instance. You can simulate failover for your reviews-prod instance by calling gcloud sql instances failover reviews-prod. I am on the SQL Instances page on my web console so you can see what's going on. Once you hit Enter, failover will be triggered. It'll confirm that you want to continue with failover first. Go ahead and click on Yes. Your failover will start. And once you're failing over, your Cloud SQL instances will be unavailable. So if you switch over to spikey-reviews where you're connected to your Cloud SQL reviews-prod instance and you run the show tables command, you'll see Server shutdown in progress. Your reviews-prod is currently failing over. Go ahead and exit from this connection, and let's see what's going on in our SQL Instances console page. You can see here that reviews-prod has a warning icon there indicating that it's failing over. You'll need to wait for a couple of minutes before failover is complete. When you refresh the page, you'll find that first reviews-prod is up and running. That's because your failover replica has been promoted to be the new reviews-prod master on the same IP address. If you notice the IP address of your reviews-prod instance, you'll see it's the same IP address that we've been using so far. Your failover replica has been promoted. Notice that the location of your reviews-prod is us-central1-b. If you remember, our original master was in us-central1-f, and our failover replica was in central1-b. The new master is our original failover replica, which is in us-central1-b. The GCP is in the process of creating a new failover replica. That's still in progress. It hasn't completed yet. That'll take a little longer. Meanwhile, let's use our Cloud Shell terminal window in order to connect to our reviews-prod master instance. You'll see that our connection has been successfully reestablished for our reviews-prod instance, which is the original failover replica promoted to be the new master, and you can run the database commands on it as you did before. If you wait for a few more minutes, the new reviews-failover instance will be up and running. It's now green. Notice that it's located in us-central1-f. The new failover replica is in a different zone than the original failover replica.

Configuring Replication Lag Alerts Using Stackdriver
In the Cloud SQL high-availability configuration, an important metric that you need to track is the replication lag because this is what determines how up to date your failover replica is. Let's see how we can use Stackdriver monitoring to monitor this metric. Click on the hamburger icon on the top left, that will take you to your navigation menu, and go to STACKDRIVER Monitoring. This is where we'll configure our replication flag alert. If you're using Stackdriver for the very first time on this project, you might need to create a new Stackdriver Account for this spikey-reviews project. So go ahead and click on Continue and enable a new Stackdriver Account. Once the Stackdriver Account has been created, you'll need to scroll down on this page here and click on Continue in order to add the spikey-reviews project to this account. Stackdriver is a company that made monitoring and logging tools, and it was acquired by Google in 2014. Stackdriver can also be used to monitor your AWS resources. We don't really want to do that at this point in time, so I'm going to say Skip AWS Setup and hit Continue on the next page as well. I only want to set up a simple alert. I don't want any Stackdriver reports as well. Click on Continue, and wait for a little bit while Stackdriver gathers information about the resources that you have on the GCP. Once Stackdriver has completed the initial collection of information, it will enable the Launch monitoring button, and that's the button I'm going to click on at this point in time. This will take you to the main Monitoring dashboard. Click on the CREATE POLICY button, which will allow us to monitor our replication lag. This will take you to a page where you can create a new alerting policy. There are a number of steps that you need to fill in for this alert. The first is the condition under which you want to be alerted. Click on the Add Condition button, and select the Metric Threshold. That is the first option that you see here on screen. You can also get alerted on a Metric Absence, but Metric Threshold is what we are looking for here. The RESOURCE TYPE drop-down here shows all of the resources that can be monitored using Stackdriver. Choose the Cloud SQL option because that's what we want to monitor. You can see that automatically reviews-failover is the instance that has been selected. The metric that we want to monitor is the Seconds Behind Master. If our replication lag goes beyond a certain threshold and your replica is several seconds behind the master, that's when we want to be alerted. The CONDITION here is above. If Seconds Behind Master is above the threshold 180 seconds for a period of about 3 minutes, that's when the alert will be set. Once your metric threshold has been configured, you can scroll down and click on the Save Condition button. This will set up a new condition for your alert. The next step is for you to configure your notifications. Click on the Add Notification button here, and ask to be notified by email at reviews@spikey-sales. com. You can add multiple notifications if you want to. Scroll down a little, and click on Add Documentation. This will contain the text message that you want sent along with your notification. I'm going to set up a very simple text message here. The replication lag threshold is exceeded for your spikey reviews database. Scroll down to step 4, and give this policy a name. I'm going to call it SpikeyReviewsCloudSQLReplicationLag. Click on the Save Policy button, and you've successfully configured an alert for your replication lag. If your replica is more than 180 seconds behind the master instance for a period of 3 minutes, that's when you'll be alerted.

Automated and On-demand Backups
In this demo, we'll see how we can set up automatic, as well as on-demand backups for our Cloud SQL instance. We start this demo off in the OVERVIEW page of our reviews-prod master instance. Click on the BACKUPS tab, and you'll see that because this instance has been up and running for a while there are a number of backups that have been created. Notice that all of these backups are on-demand backups, and you can see that these backups have been taken on demand when we created the failover replica. Notice the description for the first backup is Taking a master backup for failover. Automatic backups are typically taken every 24 hours. This instance has not been up and running for 24 hours, which is why there are no automatic backups. Typically, Cloud SQL will retain the seven most recent automated backups and all of the on-demand backups. Automatic backups have already been configured for this instance. We did so at creation time. In any case when you want to set up a failover replica, we have to configure automatic backups for our instance. You can manage your automatic backup settings by clicking on the Manage automated backups button here. This will pop up a dialog box that will allow you to configure your automated backup settings. You can back up your instance at this point in time by clicking on the Create backup button. This is an on-demand backup. This will take you to a screen where you can choose to fill out a description for why you want this particular backup. I'm going to say Complete backup of the Spikey reviews data. Go ahead and click on the Create button in order to create this backup. My backup has been successfully created, and you can see it's at the top of the list here, along with the description that I filled in. If you want to save on storage space costs, you can choose to delete old backups. Simply click on the three-dot menu at the extreme right here, and you can choose the Delete option to delete your backups. Remember, with MySQL Second Generation instances though that backups are incremental, and they may not take up very much space. This will pop up a dialog box that'll ask you to confirm your delete. Go ahead and click on DELETE, and your backup will be deleted.

Restoring from a Backup
We've created an on-demand backup. Let's see how we can restore our Cloud SQL instance from backups and also see how point-in-time recovery works. In this demo, we'll use the on-demand backup that we created in the last clip. We start off in the OVERVIEW page of our reviews-prod instance, and we'll connect to this instance using the Cloud Shell. So click on Connect using Cloud Shell, specify your password for the root user, and go ahead and get connected. Let's switch over to the spikey_db and create some new tables in our spikey database. We'll use the create table command for this, create table Blah, Blah1, and Blah2. We've created only the table Blah so far. Run the show tables command. You can see that the blah table is present there in lowercase. Remember, we have the lower_case_table_names database flag set. Create the table Blah2 and the table Blah3. Once these three tables have been created in our spikey_db, run the show tables command and confirm that they are present. Now these are temporary test tables that we've created. Let's say your reviews team has been trying something out. We want to go ahead and back up from an earlier version of this Cloud SQL instance which does not have these temporary test tables. Go to the OVERVIEW page of our reviews-prod instance, and switch over to the BACKUPS tab. Here is the on-demand backup that we created in the last clip. Click on the three-dot menu, and hit Restore. This pops up a dialog, and you'll immediately see that you cannot perform restores on a replicated setup. You cannot restore the master from a previous backup if you have a replicated setup. Go ahead and click CLOSE. What we are going to is switch over from our high-availability configuration to a regular single master instance. This we'll do by deleting the reviews-failover instance. Scroll over to the very right in your SQL Instances page, and click on the three-dot menu for reviews failover, and hit Delete. In the dialog that pops up, confirm that you indeed want to delete the reviews-failover instance. Go ahead and click on DELETE. Once the failover replica has been deleted, our reviews-prod Cloud SQL instance is no longer in the high-availability mode, which means it can now be restored from an on-demand backup that we set up earlier. Click on reviews-prod, and go to the BACKUPS tab. This is where you can see a list of all the backups that have been made for this particular instance. Click on the three-dot menu for the latest backup, and click Restore. Now we have another dialog box, and here you see a warning. Notice that the Target Instance that we want to back up is reviews-prod, and the warning says Restoring an instance from a backup will overwrite the data currently in the instance. Yes, we do want to restore this database to a previous state. Click on OK, and your reviews-prod instance will be updated. It will be restored to the backed-up version. Let's check the databases and the tables available in this backed-up version of our instance by connecting using the Cloud Shell. Enter the root password, and you are connected. Let's switch over to our spikey_db. That's the database where we had created the Blah tables. If you run the show tables command within spikey_db, you'll find that our tables have been restored to a previous state. The Blah tables that we created after the backup was created are no longer present when we restored from our backup. Now that we've understood restoring a Cloud SQL instance from a backup, let's see how point-in-time recovery works. We've enabled binary logging on our reviews-prod Cloud SQL instance, which means we can run the command show binary logs and see all the binary logs that have been generated. Now every binary log is made up of a sequence of events. These are granular events for every update that we've made to our database. You can see binlog events using the command that you see on screen. There might be many events in a single log file. Make sure you limit the number of rows that you retrieve. This particular binary log that we've chosen, mysql-bin. 000005, contains exactly 3 events. Notice that for every event we have a corresponding Position column. Positions of 4, 123, and 194. Now if you know exactly what these binary logs represent, you can restore your database to a particular event by specifying your binary log position within a binary log. Let's see how you can create a clone of this reviews-prod Cloud SQL instance by using a specific position within your binary logs. This is point-in-time recovery. Click on the CLONE link at the top right that you see here. When you clone an instance, you have an advanced option, Clone from an earlier position. Fill out the remaining details in this form. The new instance ID for the clone is reviews-prod-clonefromposition, and the binary log file name where we have our point from which we want to recover is mysql-bin. 000005, and the recovery position is 123. Click on the Create clone button in order to create a clone from this point in time. Now, in my particular situation, it does not allow me to create a clone at this point in time because a daily backup of my data has not been completed yet. Reviews-prod is an instance that I recently created. It'll ask you to wait for one cycle when automated backup is available, and then you can do point-in-time recovery from a particular position, just like how we specified here. In order to clone from a particular position, your current instance needs to be up for at least 24 hours.

Creating and Managing Read Replicas
Let's say your SpikySales organization is due for a sale, which means you'll require a huge number of resources, and there'll be lots of read operations on your ratings and reviews database. This is a situation where you might consider using read replicas in order to improve the read throughput on your data. In this demo, we'll see how you can create and manage read replicas on Cloud SQL. We'll start off in the Cloud SQL Instances page. Here are the instances that we currently have running. Now if you scroll over to the extreme right and click on the three-dot menu, that's where you can configure read replicas for each instance. In order to create read replicas, you need to have automated backups and binary logging enabled for your current instance. When we created our reviews-stage instance in an earlier demo, we did not enable automated backup and binary logging, which means you have to enable these first before you can create your read replica. The create read replica option helps you out here. It shows you where you can enable your automated backups and binary logging and then allows you to create your read replica. Let's go back to an instance which has this enabled, that is our reviews-prod instance, and click on Create read replica. This will take me to the instance details page of my replica. I'm going to specify an instance ID. This is the reviews-prod-replica. I'm going to accept the remaining configuration settings to be the default. Go ahead and click on the Create button at the bottom and create your read replica. The read replica is present in the same region as the master. That is us-central1. It's located in a different zone. It's located in us-central1-c, whereas our master is in us-central1-b. If you click on the DATABASES tab of your read replica, you'll see all of the databases that were available in your master instance are available here. The one difference here is that Create new database, that button, is not present. That's because your read replica is read only. Let's test the read-only nature of our read replica by connecting to it using the Cloud Shell. I'll bring up the Cloud Shell instance and connect using the root password, which is the same as the root password for our master instance. We'll switch to the spikey database, and we'll try to create our table here. The tables that are currently available are the reviews table, the TEST_CAPS, and the test_lower table. I'm going to create a new table called the replica_table, and you'll find that this table cannot be created because the MySQL server is running with the read-only option. Let's switch over to our reviews-prod master instance, and in the OVERVIEW page we'll click on the button which allows us to connect using the Cloud Shell. Enter the root user and password, and switch over to the spikey_db database. We still have just three tables in the spikey_db database, the TEST_CAPS, reviews, and test_lower. I'm going to create a new table called the replicated_table. The table creation succeeds because we are now writing to the master instance. If we exit from here and switch over to our read replica in us-central1-c, you'll see that this new table is now available in our read replica. We'll connect to the read replica using Cloud Shell. Once the connection is successful, switch over to the spikey database, and then run the show tables command. This will show you that we have the replicated_table, which was created in the master, has been replicated in near real time to our read replica. Then in the Instance details page of your read replica, you have the option to temporarily disable replication. Click on the DISABLE REPLICATION link at the very top, and this will dissociate your read replica from the master instance temporarily. You can see from the warning here that data from the reviews-prod master will not be replicated to this read replica until you enable replication once again. Click on OK and DISABLE REPLICATION. Wait for a little bit for this to update. Now let's switch over to our reviews-prod master instance, and within the DATABASES tab here, let's create a new database. This is a new database called some_db. Click on the CREATE button. This database will not be replicated to our read replica when replication has been disabled. Replication has been disabled here. Click on DATABASES, and you'll find that some_db is not present here in this list. If you want the read replica to start replicating from the reviews-prod master once again, click on ENABLE REPLICATION at the very top. Wait for the changes to propagate after we click on OK in this dialog box. Once the replica is up and running, you'll find that the some_db will be present in your read replica. We saw earlier that we were unable to create a table within our read replica because the replica was read only. The database was running with the read-only flag. Now, at the very top right, you can see a link there. That allows you to promote your replica to be a fully-fledged standalone instance. If you click on Promote replica, this will become a standalone Cloud SQL instance which allows read operations on its MySQL database. If you navigate to the SQL Instances page, you'll see that the reviews-prod-replica is no longer indented, indicating it's no longer a replica of reviews-prod. Click through to see the instance details of this replica that has been newly promoted to be an independent Cloud SQL instance. If you click on the DATABASES tab on the screen, you'll see that in the list of databases we now have the Create database button. You can click on this button and create a new database. This database will only be present in this promoted replica. It's an independent Cloud SQL instance. This database will not be present in our reviews-prod instance. You can confirm this by switching over to the reviews-prod instance, look at DATABASES, and you'll find that only_in_promoted_replica_db is not present here. The replica has been promoted to a standalone instance. It has no connection with the reviews-prod instance at this point in time.

Creating a SQL Dump File
It's often the case that you want to migrate the data that you have within your SQL database on a Cloud SQL instance to another instance or to another database. It's possible to do this using a MySQL dump. Let's see how to do that within Cloud SQL. Before we get a SQL dump of the data present in our reviews-prod Cloud SQL instance, I'm going to edit the configuration of this instance to turn off the database flag that says that lowercase table names are not allowed. I need to do this first because we have a table within our spikey_db database which has its name in capital letters. We created this table before we enabled this flag, and this creates problems when I create a SQL dump file from my spikey_db table. Go ahead, turn off the database flag, and click on Save to update your reviews-prod instance. If you switch over to the AUTHORIZATION tab, you'll see that our VM, the MySQLClient-vm, is still authorized to connect to this instance. And that's what we are going to do here. We are going to switch over to the mysql-client-vm, SSH into it. This is in our spikey_developers project, if you remember. Use the mysql-client to connect to our database using its IP address. Specify the root password. Once you're connected, switch over to the spikey_db database. The show tables command will show you the tables that we have currently available. We have the TEST_CAPS, the replicated_table, reviews, and test_lower. We'll use the insert SQL command in order to insert some data into our reviews table. These are reviews for the various products that we have in our SpikySales catalog. I'm going to run three separate insert commands in order to get three rows of test data into our reviews database. You can now run a select * from reviews to ensure that our inserts have been successful. Once the rows have been inserted, you can exit from the SQL connection. Now let's get a dump of our spikey_db database using the mysqldump command. The mysqldump command is available with your mysql-client. The databases that we want to dump for is spikey_db. The -h flag specifies the IP address of the Cloud SQL instance which has this database. Importing data into Cloud SQL instances on the GCP does not support triggers, which is why the skip-triggers option is enabled here. We'll write out the contents of the SQL dump file to spikey_db. sql. This file will contain a series of SQL commands to replicate the spikey_db database in any other instance. Once this command has run through, you can run an ls -l command on your terminal window to confirm that a spikey_db. sql file is present. Let's use the nano editor in order to view the contents of this file, and you can see that it contains a series of SQL commands. Hit Ctrl+X to exit from this nano editor window, and now let's copy over the spikey_db. sql command to a Cloud Storage bucket. We'll use another utility that the GCP provides in order to copy over to a Cloud Storage bucket from the command line. This is the gsutil utility. Run gsutil config -b to enable permissions to write to Cloud Storage. Clicking on the URL that appears on screen here will take you to a page that will authorize you to connect to Cloud Storage and give you an authorization token to copy into this particular terminal window. So choose the Cloud User account. Allow GSUtil to view and manage your data across the GCP. Clicking the ALLOW button will take you to a page where you'll find an auth token. Copy the contents of this page here, and paste the authorization code in your terminal window. You're now authorized to use gsutil to copy to Cloud Storage buckets within this project. This is the spikey-developers project, if you remember. I'm going to create a new bucket here. Click on the CREATE BUCKET link at the very top, and I'm going to create a multi-regional bucket. A multi-regional bucket spans multiple regions across the globe. Click on the Create button so that the spikey_test bucket is created. You can then switch over to the terminal window for your VM and use gsutil to copy the spikey_db. sql command to the spikey_test bucket. Once the file has been copied over successfully, you can switch to your browser window, which holds the Cloud Storage bucket, hit REFRESH, and you'll see that spikey_db. sql is present within the spikey_test bucket. We've successfully created a SQL dump file of our spikey_db database and uploaded it to Cloud Storage. We'll now use this dump file in the next clip.

Importing Data from a SQL Dump File
In this demo, we'll see how you can import data from a SQL dump file into Cloud SQL. We'll start off with the spikey_developers project, and we're going to create a brand-new Cloud SQL instance in this project. You can think of the spikey_developers project as where the developers of our SpikySales organization run their test projects. You can imagine a scenario in the SpikySales organization that the search and recommendations team want to use review data in order to rank their search results. They prototype this functionality by first creating a test instance within the spikey-developers project. Activate the Cloud Shell, and create a new Cloud SQL instance using the command line, gcloud sql instances create reviews-test. Your instance is created in an f1-micro machine, this is a pretty small machine, in the us-central1 region. Click on the SQL Instances page, and wait for your reviews-test instance to be up and running. This might take a couple of minutes. This is a brand-new instance. Go to the Instance details page, and click on DATABASES, and you'll find that only the default databases have been set up. Now I'm going to import some data into this instance by clicking on the IMPORT link at the very top. This brings us to a page which allows us to browse our Cloud Storage buckets for either SQL or CSV files which we can use to import data. I'm going to click on the spikey_test bucket. That's the bucket where the spikey_db. sql file is stored. Hit Select, and then hit Import. This Import button will cause all of the SQL commands in our. sql file to be executed on this reviews-test instance. And after a while, provided all the data has been imported successfully, you'll get a message which says Import from the spikey_db. sql file succeeded, as you can see on the bottom left here. Clicking on the DATABASES tab for the reviews-test instance should show you that the spikey_db database has been set up. You can then use the Cloud Shell to connect to this database and view the tables and the contents of the tables within your spikey_db. In order to avoid repeating myself, I'm going to connect to this instance using the Cloud Shell, switch over to the spikey_db instance, and view the tables that are created and view the contents of the reviews table. You can see that all of the records from our reviews-prod instance are now available in reviews-test. Creating a SQL dump file from your existing MySQL database and importing it to your Cloud SQL instance is one way by which you can migrate data from your on-premise database to the cloud.

Exporting and Importing CSV Files
In the last demo, we saw how we could import data from a SQL dump file into a Cloud SQL instance. In this demo, we'll work with exporting and importing data using CSV files. We start off in the OVERVIEW page of the reviews-test instance within our spikey-developers project. Remember, this is the project where the developers in our organization run and test their prototypes. Click on the EXPORT link at the very top, and this will bring up the options that you have to export the data that is present in this instance. Using the web console, you can export the data in the form of a. sql file or a CSV file. The. sql file will allow you to configure databases and the tables within a database as well, whereas when you export as CSV, this will only contain the records within a particular table in a database. We want to export our data as a CSV file, and we want the CSV file to be stored in a Cloud Storage bucket. Within the spikey_test bucket, we'll call the reviews. csv. That's the name of the file. Click on Select. The data that we export will be stored in this file. If you want to run your query on a particular table within a particular database, you can use the Database option to select this database. I'm going to choose spikey_db, and I'm going to run select * from reviews. The records returned from this query will populate the CSV file, the reviews. csv. Click on the Export button, and confirm that yes, indeed, you do want to perform the export, and go ahead and click on EXPORT. Now depending on how much data was present in the reviews database, this export could potentially take a long time. I just had three records. This export took just a minute or so to complete. Now you can switch over to the Cloud Storage bucket using the Navigation menu, choose the Browser option under Storage, that will take you to your Cloud Storage buckets, and the spikey_test bucket will contain the reviews. csv file. You can click and download the CSV file and confirm that it has the records from your reviews table. Use the Navigation menu which you get by clicking on the hamburger icon, and go to the SQL Instances page. Click on the reviews-test instance, and go ahead and connect to this instance using the Cloud Shell. Switch over to the spikey_db database once you've connected, and create a new table called reviews_backup. You want the configuration of this table to be exactly like the reviews table, so you'll say create table reviews_backup like reviews. Once the table has been created, run the describe command on reviews_backup, and you'll see that this table looks exactly like the reviews table. This table is completely empty though. The table schema has been copied over, but the table data has not. Stay connected to this database, but hide the Cloud Shell, and then click on the IMPORT link at the very top. This will allow you to import data either in the form of a. sql file or a. csv file. Both of the options are available here. I'm going to choose. csv, and I'm going to browse for the bucket where the reviews. csv file is available. This is under the spikey_test bucket, and the name of the file is reviews. csv. Select this file from the Cloud Storage bucket. You go back to the Import screen. We want to import this into the spikey_db database, specifically into the reviews_backup table. This table has to exist when you try to import data into it; otherwise, this will be an error. Click on the Import button, and wait for the import to complete. The import will complete successfully only if the columns within the reviews. csv file match the columns of our reviews_backup data. They do, which is why when you run select * from reviews_backup in our Cloud SQL connection you'll see that all of the records, all three of them, are available in reviews_backup. Creating the databases manually on your Cloud SQL instance and then importing data using a CSV file is also a viable migration scenario from your on-premise database.

Summary and Further Study
And on this note, we come to the very end of this module and to the end of this course on Cloud SQL on the GCP. In this module, we primarily focused on replication and data management on Cloud SQL instances, specifically the high-availability configuration which we can set up using a failover replica. We then saw how we could use Stackdriver monitoring to configure replication lag alerts. Cloud SQL allows you to configure on-demand, as well as automatic backups of your data. You can configure these automatic backups to occur once every 24 hours at a time where traffic is slow. You also saw how you could configure and manage read replicas in order to improve the read throughput of your SQL database. We then covered possible migration scenarios to the Cloud SQL where we created a SQL dump file and imported data into a Cloud SQL instance using either a. sql or. csv file as the source. And on this note, it's time for us to say goodbye. But before I leave, let's look at what you can study further on Pluralsight in order to further your understanding of relational databases on the cloud. If you're interested in AWS, Amazon's cloud platform, Implementing SQL Server on Amazon RDS is a good course for you to follow up with. Or if you're interested in Azure SQL databases, Azure SQL Database for SQL Server DBA is a good course. Rather than focus on a specific cloud platform, if you want to implement a hybrid cloud scenario, SQL Server: Implementing a Hybrid Cloud Solution in Azure is a great course. Any of these make interesting follow-ups to this course on Cloud SQL on the GCP. It's time for me to say goodbye now. Thank you for listening.

Course author
Author: Janani Ravi	
Janani Ravi
Janani has a Masters degree from Stanford and worked for 7+ years at Google. She was one of the original engineers on Google Docs and holds 4 patents for its real-time collaborative editing...

Course info
Level
Beginner
Rating
0 stars with 3 raters
My rating
null stars

Duration
2h 29m
Released
24 Sep 2018
Share course