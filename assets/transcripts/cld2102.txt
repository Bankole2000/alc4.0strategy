Google Cloud Functions: Getting Started
by James Wilson

Serverless technologies are taking the developer world by storm because they allow you to push out code quickly and know that it will scale. This course teaches you how to get started writing serverless code on the Google Cloud Platform.

Developers want to build serverless microservices so they can create new content, reduce maintenance, scale easily, and deliver new features to users faster. In this course, Google Cloud Functions: Getting Started, you will learn the skills you need to create high-quality microservices that will enhance the experience of your app or website. First, you will set up a Google Cloud project, install the gcloud command line tools, and set up Google Cloud Functions on your local machine. Next, you will explore all of the different events your functions can respond to, including storage, PubSub, and HTTP triggers. Finally, you will dive into deploying the functions and see how everything integrates seamlessly into the Google Cloud Platform. When youâ€™re finished with this course, you will have a solid understanding of Google Cloud Functions that will allow you to create and deploy your own microservices that will automatically scale with your users.

Course author
Author: James Wilson	
James Wilson
As a mobile developer, James always had a passion for building exciting apps and always striving to make user interfaces that were easy and intuitive to use. Today, he now works at Pendo where he...

Course info
Level
Beginner
Rating
4.4 stars with 22 raters(22)
My rating
null stars

Duration
2h 27m
Released
20 Dec 2017
Share course

Course Overview
Course Overview
Hi everyone, my name is James Wilson, and welcome to my course, Google Cloud Functions: Getting Started. Serverless technologies are taking the developer world by storm. As a developer, you can push out your code quickly and know that it will scale. This course is going to teach you how to get started writing serverless code on the Google Cloud Platform. Some of the major topics that we will cover include working with all the cloud function types on the Google Cloud Platform, exploring the cloud functions dashboard on Google Cloud, building local functions with no JS, and managing and deploying functions with the G Cloud Command Line Interface Tool. By the end of this course, you'll know how to write, deploy, test, and maintain serverless functions on the Google Cloud Platform. We start with the basics, so no prior knowledge is necessary for this course. I hope you'll join me on this journey to learn serverless cloud functions with the Google Cloud Functions: Getting Started course at Pluralsight.

Introducing Event-driven Microservices
Introducing Serverless Functions
When you're developing software, there is a lot of work involved in making sure that you have the proper servers set up, it has the correct versions running of whatever infrastructure you need, and that your app or services on that server will be able to run. And if you get a lot of users, that the server won't buckle and break. Now this is one of the key reasons why the cloud has become so popular is having that available infrastructure to you. However, there's still a lot of set up involved. You can't just pick up code and deploy your app and have it up and running immediately. Until now. Google Cloud Functions allows you to have a serverless environment where you can write node. js code and deploy it directly up to the Google Cloud service. And what they will do is manage it for you. They'll spin up an instance and execute your code and the more users that hit it, the more instances they'll ramp up, so you can deliver quality services and features to your customers. My name is James Wilson, and I'm super excited to present this course Google Cloud Functions: Getting Started. You're going to learn a lot of great things in this course. First, we're going to introduce serverless functions and why they are so great. And then once we've laid out the framework, we're then going to look in depth at the functions themselves. We're just going to look at their basic components, what happens when you call one or call it a million times, and looking at some of the advantages and disadvantages to using serverless functions from that perspective. Then, we're going to dive into the Google Cloud platforms Cloud Functions' Dashboard. This is in the browser and it has so many great tools to get you up and running quickly with Cloud Functions. Then, we're going to jump to leveraging the Google Cloud SDK. We're going to install it on our local machine and we're going to leverage the G Cloud Command Line interface tool to manage our functions. It's always important to script everything so you can replicate the Cloud environments from project to project and the G Cloud tool is instrumental in being able to accomplish that. Once we're done with that module, you will not even nee to use the Google Cloud dashboard on a regular basis, at least from managing deployments, deleting functions, and viewing logs even. And finally, we're going to take a look at building functions on our local machine. While you'll see earlier in the course we can do this from the Google Cloud Platforms website, it's so much better to do it on your local machine, being able to interact with node. js and node package manager and pulling in your dependencies, running unit tests. So we'll just take a quick look at getting started and building some basic functions and then deploying them with the Google Cloud SDK, otherwise known as the G Cloud Commandline interface. There's so many cool things. So let's look more in depth at what we're going to learn in this first module.

Module Overview
So in this module we're just going to establish the basics. Why use Cloud Functions in the first place? It's an important question to answer and you may not always need to. However, there are so many advantages, especially if you're a small development shop, so it's definitely worth considering why they are useful and why it's important for you to adopt them. Then we're going to take a closer look at the Google Cloud Platform's Compute Spectrum product line. There are so many options to choose from when you start looking at any cloud provider. Whether it be Amazon, Microsoft, or Google, it can seem overwhelming. So we're just going to take a quick look at their Compute Spectrum to get a better understanding of the other offerings and why Cloud Functions are a great place to start if it fits your needs. We're going to review the coding environments offered by Cloud Functions. Now right now as of this recording, it's only node. js, but we'll take a quick look and introduce it just in case you're not familiar. And then finally, we're going to set up our first Google Cloud project. So we're going to go to cloud. google. com, sign up, get up and running, so we are ready to go for the rest of the course.

Why Use Cloud Functions?
There are so many great reasons why you should use Cloud Functions. And let's just start with one of the best ones, reducing maintenance. There are no servers to take care of since everything is just going to be handled for you by Google. Now there's a little set up involved, but it's ultimately very simple, right? You're not thinking, oh how much disk space do I need? Do I need a small CPU or do I need a larger CPU on this machine? All of that's abstracted away, you don't see it. Now the only preference you do have to pick is the amount of memory that you need and we'll discuss that later in the course. But when you think about all the questions that you need to to typically ask as you're setting up a server and the only one left is really just selecting the memory? That's a pretty sweet deal. That is a lot of work that's being taken out of your hands. It reduces maintenance in another way because you're writing small microservices, which are easier to write and have less maintenance over time compared to a monolithic code base. You can make small changes and just deploy those single functions. And it's going to have a much smaller impact on your entire system. There's nothing worse than a small bug in a crucial file that can buckle an entire ecosystem and bring down multiple sites. When you're dealing with Cloud Functions, not that you don't always have to be worried about it, but the bugs can be fewer and far between, they can have less of an impact as long as you architect your software properly. Another great reason is you're securing your code. You can put a lot of logic on the server and keep it safe. Code that takes a long time to process can be taken off the client's device, ensuring it doesn't get interrupted. And again, with this auto-scaling servers that are just scaling for you on your behalf, you are not clogging up any of your own servers which could potentially bring down other sites. Let's say you're making a video game, the logic could be stored in Cloud Functions so you know no users can find a way to cheat. This gives you the ability to ensure your users have a consistent experience, and a great one for that matter. You can easily deploy secure updates without waiting for a third party company to approve your app for release into their store. I can tell you from firsthand experience as an IOS developer, that there could be a crucial bug in our app and it could take two or three days before we can get that to our users. But when you're using these serverless functions, you are no longer beholden to a particular company and their rules. You can deploy the changes immediately and ensure your users have a great experience. And finally, Google is working hard on making sure that your code is secure. You'll notice every http trigger function that we create is already https compatible. The pub/sub functions are running on their internal message queue system, Cloud Pub/Sub. And with cloud storage, files are protected by default. And it's up to you as the developer to make them publicly accessible and you have control over those rules. While it's still important to use your own security practices, it's nice to have Google on your side in helping to make everything as secure as possible on the Google Cloud platform. And finally, it's really easy to integrate serverless functions with the rest of the Google Cloud platform and third party services. Since Google is managing your cloud function instances, you have access to a lot of the other GCP services directly from those functions since they are managing the security on your behalf. This means you have access to the Cloud Vision API, or the Natural Language Processing API. You have access to databases like the cloud data store. And so this is really cool and adds so much flexibility because with those services already open, you can code faster, add new features quicker, and give your users a great experience. And just to be clear, we aren't limited to Google's services. Node. js is great for making http calls, and we can use this to leverage all different kinds of services. Maybe you want to send messages to your company's slack channels when a certain event happens, or you want to use a emailing service to email a set of users based on an event that's happening within the Google Cloud Platforms' system. You have the ability to do this. And this is going to be one of the best ways to utilize these serverless functions. So now that we've taken a look at why you should use serverless functions, and I think there are some very good reasons, let's take a look at Google Cloud's Compute Spectrum, just to get a better understanding of the different products that you have available to you and where Cloud Functions lies within those range of products.

Google Cloud Platform's Compute Spectrum
The Google Cloud platform has so many products, and just their Compute instances alone can be a little mind boggling. So we're just going to quickly go over the different Compute products that are offered to you as a customer and where Cloud Functions falls within those line of products. Now they offer products ranging from simple virtual machines, bare bones, nothing on them, to fully managed services, like App Engine and Cloud Functions. So let's start with the ones that involve the most work and the most set up. The first product is Compute Engine. And now this is your basic VMs I was talking about. You would go in, select the type of machine you want, how much memory you need, how much disk space, the type of OS you want to use, and the Compute Engine will go in and set that up. And at that point it's up to you to, let's say, install whatever version of. net you're deciding to run or node. js or setting up container system on there. It's really up to you how you want to use it. So you can see if you're a small development shop how this might not be the best product choice for you unless you have someone on your team that's very familiar with setting up infrastructure like this. The next level up is Kubernetes Engine. And this runs a special type of software called Kubernetes, which is really an orchestration management tool. If you have not seen Nigel Poulton's course on getting started with Kibernetes, I definitely urge you to check out his course because he gives an incredible overview of this amazing technology. Now, you might find this to be overkill though. While orchestration management delivers a lot of power, again, there is a lot of work involved because you have to learn Kubernetes, you have to figure out, well what type of environment it is that I want to run? How many instances of this node. js container do I need? Or how many instances of this. net core container do I need to be running? And how should they communicate with each other? So it offers an incredible amount of flexibility, but that flexibility does come at a price and it could take some time to really get this set up the way you want it. And so the next level up is App Engine. Now App Engine is one of Google Cloud Platforms' oldest products and it has come a long way. And it really is setting up a server-side app for you. Perhaps you wanted to build some sort of CMS web app and you could use App Engine and select various languages, you can built it with Python, Ruby,. Net, Javascript, with Node. js, you could use Go, and this provides you with a service that scales very well. You don't need to provision any servers, you can dictate kind of the minimum number of instances you want running and if you happen to get a huge spike in traffic, App Engine's going to take care of scaling everything up for you so users can continue to use your web app or your service. And it comes with a bunch of other features as well, with kind of built in caching and some other stuff. And so again, this might actually fit your needs. You know one of the challenges is though, it's not very good for making tiny services because you always need an instance of those services running and then you're going to be charged constantly for all those services. So it definitely has some advantages and some disadvantages. And then finally, there are the Google Cloud Functions. And we've already introduced them a little bit and one of the nice things is, these are kind of a fully managed environment. There's not even a language to choose. You're forced to use node. js. One of the nice things about it, though, is you're only charged for the time that you're using it whereas the other three products, Compute, Container Engine, and App Engine, you need it up and running, right? You always need your VMs up and running. Container Engine, while you might have some services that spin down to one, you ultimately have to keep that VM running so Kubernetes can run on top of that. And with App Engine as well, you pretty much always want to have one instance running so your website's available. If somebody, you know, went to your webpage and you had zero instances and you're waiting for App Engine to spin up, that's going to be a horrible experience. Quickly, one of the bigger advantages of Cloud Functions, you're only paying for when you're using it and then Google goes and spins it back down to zero, but we'll get into that in a later module. So there you have it. There's a brief introduction into all the different Compute Services. I urge you, if you want to learn more, to go to the Google Cloud website and read up a bit more on all of them. They're really great and when it comes down to it, it really does cover all your needs to some extent, right? You have your basic VM to a fully managed service with Cloud Functions. Now that we've introduced the Compute Spectrum, let's take a look at the developer experience for Cloud Functions.

Developer Environment
So with Google Cloud Functions, the developer experience is pretty simple. And some might see this as a huge advantage, others might see it as a disadvantage. But right now, the only way to build Cloud Functions is by using node. js and as of this recording, they're using the longterm support version of 6. 11. 1, although I'm sure they will soon be upgrading to version 8 since that just recently for longterm support. Now some people might see this as a disadvantage because maybe you're a. net shop, Ruby, or Go, and hopefully one day they do add support for more languages. But for those that are already familiar for node. js this should be exciting news because you are already familiar with setting up an environment and running and building nod. js apps. While we don't go in depth into node. js in this course, it's very basic stuff that we do and so any developer should be able to follow along with all of the examples. And now that we know the coding environment we're going to be working in, let's go sign up for Google Cloud and get our first Google Cloud project set up.

Setting up the Project
Okay, so what we're going to do here is sign up for Google Cloud and get our first project set up. It's super easy to do. So as long as you have a gmail account, you can go to console. cloud. google. com and one of the nicest features about Google Cloud is that they offer you 12 months free or up to $300 worth of cloud services during the free trial period, and what this means is, you can either have 12 months free access, but if you use up that $300 then the trial ends. But this is a lot to work with and it's more than enough for this course. So go ahead and select your Terms of Service, and then continue. And now we're at the Google Cloud dashboard. So I'm going to go up and select Start Your Free Trial. I don't want any emails, and I'm going to go and agree to the Terms of Service and then hit Agree and Continue. Now here you need to fill in some extra information and you need to get out a credit card. However, this is the most important part, you will not be charged once the free trial ends. So what that means is, you could have a bunch of containers running on Container Engine, or you could have a mysql instance running on Cloud SQL, and once that free trial ends, your card will not be charged. What will happen by default is your services will be shut down and they will stop. Now if you want to continue paying for these services, you can do that and accept being charged before the trial ends, however, for our purposes, once you're done with this course, you want to make sure your credit card isn't charged and that is what will happen by default. So go ahead and fill out this information. I'm going to do the same and we'll be right back. Okay, we got our free trial started. And you can see here it's saying, Creating projects, this may take a few moments. So I'm going to use the magic of video editing and fast forward through this. Brilliant, so here we are now. We have our first project set up here. And we're at the dashboard. You have a little welcome screen and if you want to take a tour go ahead and do that, but for this moment I'm going to skip this. At the top here you can see that our first project has been set up for us. And this is the project that we're going to be using throughout the course. Now there are a couple of common services that we're going to be using throughout the entire course and so what I'm going to quick is just pin them to the top. So I'm going to go to the Compute section and pin Cloud Functions and then I'm going to go down to Big Data and pin Pub/Sub. And then finally I'm going to go to the Storage section and pin Cloud Storage. So as you can see now, we have the various Google Cloud Platform products that we're going to be using throughout the course. And that's it, we are all set up now.

Summary
So we've already gone over a lot of great stuff in this first module. First, we looked at why you should use Cloud Functions in the first place. What benefit do they bring to you as a developer. We then went over Google Cloud Platforms' Compute Spectrum. We discussed briefly about the different type of products from Compute Engine all the way to Cloud Functions and just to give you a better sense of why you might choose Cloud Functions, and does it really fit your needs? Or do one of the other product offerings fit your needs better? We briefly went over the coding environment. Again, it's node. js, I'm a little biased, I love node. js. It's a lot of fun to program in and work in. That is what we're going to be using throughout this course. And then we went ahead and signed up for the Google Cloud Platform and we created our first Google Cloud Project. And now we are ready to start digging in to Cloud Functions.

Breaking Down a Function
Introduction
Hi, welcome to the module breaking down a function. These functions are really powerful to use and what we want to explore in this module is what are they exactly? What are they doing? And how are they created, and when we answer these questions we're kind of able to see the pros and the cons that are associated with these cloud functions. So to get started, we're first going to look at the different trigger types that are available to us. And as of this recording, there are three different types. If you're dealing with firebase there are more. However that's outside the scope of this course. But I would keep an eye out. I'm sure that Google is going to be adding more trigger types as they connect many more aspects of the Google cloud platform to cloud functions. We're then going to take a look at the events and data associated with those trigger types. What type of information are you getting? Or what type of information should you intend to send so you can receive that data and process it in the proper way. And this changes depending on each trigger type. So we're going to go over this in detail. Finally, we're going to take a look at the full function lifecycle. So what happens from the trigger point all the way to the end? What happens when 1000 people call it? Or a million people? And then what if nobody's calling it at all? So we'll go over that to give you a good understanding of how the whole process works overall. I mean how is it able to even scale to the size that it does? Finally, we're going to go into the Google cloud platform's website and create a function for each trigger type. By the end of this module, you're just going to have a great understanding of well, what type of triggers do I have accessible to me? What data should I expect to receive from them? And how are these things even able to scale to a million users? Then finally, you're just going to see how easy it is to get started with all the different trigger types. You'll be building micro services using Google cloud functions before you know it. So let's get started.

The Triggers
So in this video, we're going to take a look at the different function types and their associated triggers. Now there are three different types of function types in the Google cloud platform. And the first one is HTTP trigger, which is just a standard web hook that you can call and send data into. Then we have a Pub Sub trigger which works along Google cloud's pub sub message queue system. With this technology, you are able to create a topic and you can have multiple systems subscribe to this topic and pub sub will go and push that information out to all the different subscribers. And now a subscriber could be a Google cloud function, it could be a end point running in app engine or you could use Google cloud dataflow if you have some kind map reduce processing you want to do on a bunch of data. And so all these different systems can hook up to the same topic. Then we also have a storage trigger which is leveraged by the Google cloud storage system. So if a file gets uploaded to a specific bucket this is going to trigger that type of event. So let's take a look at the bigger picture here. We have three different trigger types. We have HTTP, pub sub, and cloud storage. Now this has a many to one relationship with functions. So if you see here, we can connect function one to an HTTP trigger. We can connect function two and function three to the same pub sub trigger and both functions will trigger when that pub sub topic is published to. And then finally, we have function for going to the cloud storage trigger. One thing to understand what this is that you can't have the same function go to a HTTP trigger and a cloud storage trigger. That's just not allowed. And if you think about it, would you really want that anyway? Any processing that's done because a cloud storage bucket is updated can just simply be done and you could call a separate HTTP end point afterwords. So with that basic understanding, let's ahead and take a look at how exactly do you trigger these anyway? When the function is finally going to be called, how does that happen? What is needed for that to occur? For the HTTP, it's just simply an HTTP request. This is a standard web request that you can make anywhere on the Internet. And what will happen is, when you call that URL it's going to trigger and invoke the function and that small function is going to run and process the request. And with cloud pub sub in the pub sub trigger, the whole system of pub sub uses messages. Basically a message is published to a topic and then that topic pushes that message or it can be pulled to the different subscribers of that topic. And so that's when a pub sub trigger occurs. It's when a topic is written to and there's a cloud function that subscribes to that topic. Now while that might sound a little esoteric right now, you'll see when we go ahead and build these functions throughout the course, that as we create a pub sub trigger we'll see that we have a subscriber to our topic and it'll start to make a little more sense at that point. And then with cloud storage, what happens is at any time a file is created, is any time it's updated or deleted. Well pretty much it happens in all of these cases. And so it's an object change notification that occurs within the storage bucket. You go in and you upload a new file that will trigger the function. You go and edit a file, that will trigger the function. You go and delete a file, that will trigger a function. And you have the ability to see through the data what type of event is occurring so you can act appropriately. You're not going to always want to respond to deletions maybe all you care about is files being created, and you have the ability to do that. Let's take a closer look at these functions events, when these functions are triggered.

The Event Object
So we have a function that's connected to a particular trigger. And that trigger occurs either somebody sends an HTTP request or pub some message is pushed through the topic you're subscribed to, or somebody update something on a cloud storage bucket that you're connected to. With the HTTP request, you actually get an express JS request which we're going to get into an a little bit. However with the pub sub and cloud storage functions, if you're going to receive an event object. Now while event object might not be as important as let's say the data that's actually contained within that event, it is still good to know because this allows you to be able to track down the invocations of each function that's occurring. And this is pretty crazy if you think about it, right? If a message is being pushed through pub sub a hundred thousand times, that's going to be 100, 000 invocations on your function. So what does this event object look like? Well first we have the event ID, a unique identifier for the specific event that's occurring. And then we have the timestamp as well, the date and time that this event was created. And that might be useful in some instances because maybe you just want to say, well we really only need to be processing this data for the first week and anything after that we never want to process it again, perhaps somebody would forget to take down the function or anything like that. Well you could check this date timestamp to see whether you should be processing the data. It's a bit of a contrived example but hey, it's there, so use it. We also have the event type and this is going to change depending on the type of functions that you have. For instance, if you have pub sub the event type would be the fact that a topic was published to. With cloud storage it would be the fact that somebody uploaded a file to the bucket. And then the resource property will also change depending on what you're dealing with. So again with pub sub, it's going to be the specific topic that was published to. So if we had a pub sub topic hello world, we connected a function to that trigger type and then published to that. That would be our resource. That's the one that's driving the invocation of this function. Now you might find the need to use these properties, I'll be honest, I really don't use them that often. But the most important property that you will be using on the event object is the data property. So let's take a closer look at the different types of data you're going to receive from each trigger type.

Digging Through the Data
The data is the most important part that you're going to receive from the event object. So let's take a look in this video at the different data objects that you're going to receive because it's based on the trigger type which makes a lot of sense right? Why would you receive the same data object from a pub sub trigger as you would from an HTTP request? So unlike the event object really only applying to the pub sub and cloud storage triggers, HTTP trigger does have a datatype. And that is the request object from express JS which this is awesome. That means you can do request stop method and easily see whether the call was a get or a post or a delete and you can act accordingly. Now we're not going to go super in depth on the request objects because one you can see the documentation at expressJS. com, and two Pluralsight has a lot of great courses that cover express JS already. And if you go and look at Jonathan Mills courses especially he's done a lot of great stuff with note and in particular express. So, once you get that request object though not only do you have the method but you also have query parameters, the body if you put any data in the body of the call you can retrieve that. And you can retrieve the headers. So it's really cool, and it's really powerful. Then, we have pub sub and the datatype for that is the pub sub message. This is a pretty vanilla object. You're going to get this data object and you have a few different properties on here that you can take a look at. One you have the actual data property, just a string. And you're going to see in later demo how we base 64 and code a message, send it through pub sub, and then we simply retrieve that message and print it to the consul. So it'd be super easy. You're going to see this in action. You'll also have the attributes which is a keyvalue pair type object. And again depending on what your pub sub topic needs are you may use this or you may not. You might just put everything on the data property. And then we have the message ID. Which is a unique identifier that you can use especially if you don't want to process data multiple times and you want say whoa, I've already gotten this message once I don't want it again. And then there's also the published time. The time the actual topic was published to. Just to be clear, the event had a timestamp as well and these could differ because while message queue system is really fast that pub sub topic for some reason could've taken a little while to get there. So these two times will differ and you should leverage that if you ever need to. And then finally we have cloud storage and that has the cloud storage object. And this is a pretty big object with a lot of different properties and so you can get things from it like the ID of the file, the name and what bucket it's from. You have the time the file was created, updated and deleted which lets you know what types of actions to take. The content type, is it a json file or a JPEG or PNG type image? Along with other applicable metadata. And it also comes with the access control list. So what users should even have access to this file? As well as encryption. So there is a lot to this object. But again, when you're processing files there's a lot that you might want to do or need to know to process it and send it through properly. So it's awesome that they did this and gave you all this information to work with. While we really don't go in depth on a lot of these properties throughout the demos, it's just good for you to know that there is a lot of information that you're getting especially in terms of the HTTP trigger and the cloud storage trigger. And so keep that in mind as you're writing and building these functions out. Now that we've taken a look at the different triggers and how you can tie multiple functions to a specific trigger we also looked at the event object that's going be given to you, along with the attached data from that event object. There's really one last piece we need to look at in the full lifecycle of a function, and that's how do you end it? How do we finish it properly? So let's take a look.

Ending the Function
So, we have a function that is tied to an HTTP trigger. Someone makes an HTTP request, which triggers the function and some code runs. Now once that code's done running, how do we end it? And this is really important. And this is not going to be the last time I say this throughout the course. It is very important to end your functions because you're going to see one we create them here in a little bit, that there is a timeout time that you set on those functions which defaults to 60 seconds. And if you don't properly end the function, they will run until that timeout time. And you will be charged for those full 60 seconds. Now if you properly end the function and make the appropriate call, you're only going to be charged for however long that function ran. Let's say 300 milliseconds. So it really adds up especially if you've got thousands and thousands of calls going into this. And it's really easy to end any of these functions. And you can really break it down into two different categories. We simply add the HTTP trigger in one category and with that, you just use an express JS response. You do response. send or response. end. There are a few different function calls you can make. That make sense right? You're just saying, all right I've processed everything and now I can send a 204 success or 200 with the data that people requested. And then on the other side you have pub sub and cloud storage, and you'll see when we create those functions that there's a callback parameter that's passed in. You have the event and the call back. And so the event is kind of the one that contains all the information you need to process, and then you just call the call back. It doesn't even take any parameters. And that will tell the cloud function system that you are done using that particular instance or that particular invocation of the cloud function. This should make a lot of sense right? Why have the call back? Well, say you had to go and call another service a third-party service and so you're going and making an HTTP call to them. And you have to wait on that response. Well Google cloud can't assume you're not doing that right? And so that's why they do provide this callback because let's say you make the HTTP request and you're waiting on a promise to resolve and then once that promise resolves, you can go ahead and call your call back within that resolution. Or if it fails, you can go ahead and do something else. So it's really easy to do and the nice part about it is two, Google cloud provides a lot of information. So if you do happen to forget it, as you're developing and going through QA, you're going to say, oh wow my function is literally taking 60 seconds to run every time it's invoked. And that will tell you that you are not calling that call back or the express JS response object somewhere in your function. So that's the high level view of one invocation and now let's take a higher level view of thousands and thousands of users hitting these functions and see what Google cloud is doing under the hood to make all of this work.

The Full Lifecycle
So we have seen the full cycle of a function between connecting a function to a trigger, a trigger occurring, you being passed an event, grabbing the data from that event and then either sending a response or calling a call back to end that function. So now we're going to take a look at from the outside and seeing once your function's deployed, what's going to happen? And so here you can see we have an HTTP trigger and it gets called. When that occurs, it's going to spin up an instance of that cloud function, that code that's going to run. And now one challenge about this is is you have this cold start time. Now you're not going to have this every time, it's only going to be when you're going from zero to one typically. And it's going to take maybe about 500 milliseconds for that function to spin up and get ready to go. But if you make invocations again and again and again right after that, those are going to go a lot quicker because they're going to be using that same instance that's already been brought up. Yes, it's a negative because you have that small time gap in the beginning every once in a while especially after a long downtime period. However, the bigger positive is that you aren't being charged for this, right. Because it's bring it down to zero instances, you're only charged for what you use unlike a standard VM, which you may need to have running 24 seven, even though nobody's using it. So now, once this is triggered all of a sudden we can have a lot more users start calling this function, even 100, 000 times more. And this function is just going keep getting invoked every single time, and it's going to run each individual event. And under the hood, if it's just not enough for that one instance, it's probably going to spin up more and just set up some sort of load balancer and pass it to multiple instances of these functions. But this is all abstracted away. You don't see this, and you don't get charged for it. They're not going to charge you for having 10 instances of your cloud function spun up like an app engine. What they're going to charge you for is purely those single invocations on a function. It doesn't care how much it has to spin up to get this thing running, and to get it to work properly. And so this is really cool, and you have to think too that this has quite a bit of fallback safety too. Let's say you had a pretty rare bug in your function and it gets triggered every once in a while. Well this isn't going to bring down the whole system. Yes that instance of that function will crash but Google cloud under the hood is going to bring back up another instance. So the next user that comes along will be able to have this called and get back whatever they need. And so this is really great. This takes away all the server management out of your hands abstracts it away. You don't care about this. Then finally when your function is no longer being triggered constantly and there is a small gap in time without it being used, Google will go and shut down that last instance of the function. You'll go from one down to zero. And that way again, you're not being charged. And so this is it at a really high level. It's really cool. Google is managing everything for you basically. You just need to write some code and make sure you send a response or call the call back to end that code and then when you deploy it, you connect it to a specific trigger. Whether it's HTTP, pub sub or cloud storage. And you have a simple micro service running on the Google cloud platform. Doesn't get much better than that. So enough theory though. And now that we've looked at all of this, let's get into the weeds and let's start creating our functions. We're first going to do this through the Google cloud platform dashboard. So I'll see you there.

Creating an HTTP Function
So let's get started and create our first function. We're already at the dashboard on the Google cloud platform and if you come up here and click the menu, we've already pinned our cloud functions in an earlier module and now the first screen you see is that the API is not enabled and this is really easy to do. All you need to do is just come to the top and enable API. And this is a common thing in the Google cloud platform. As you're using let's say cloud vision or cloud machine learning or cloud storage, typically what you have to do is enable the API so you're not billed for them unnecessarily. Once the API is enabled, you can go ahead and create your first function. There's not much to choose from. We just got one button. So create function. And we're taken to the create function screen. Now this is the same screen you'll see every time you go to create a new function. So we're just going to go ahead and name it. So what we'll do is we'll call it PS hello HTTP. Since we're setting up in an HTTP function. And now we have a bunch of options set up for us. They're going to change depending on the type of trigger that were going to use, but these ones up above here are always going to be the same for every type of function. One of the awesome things about Google cloud is too is the help. So if you come over and look at region, you might say well I really don't understand what's going on here. So you can come over and click this link and it takes you to the documentation about regions. I'm not going to go into regions and zones right now because that could be a course on its own but I just want you to know that it's there in case you're ever confused about some functionality, use those help links and it'll take you to the appropriate place in the documentation. I'm going to use the default region here, US central one and now the memory allocated field. Right now it's 256 megabytes. I pretty much always pick this by default. While you have options to use more, you're only going to need those if you're doing something that's very memory intensive. Maybe you have a function where we're going to be processing images or video. In that instance, you might want to raise the memory allocation significantly so your function can handle that type of processing. As long as you're leveraging note JS properly and using those callbacks or promises, your memory profile should remain pretty low in most circumstances. For the timer, we're going to select 60 seconds. This is another consideration you need to make. Again, if you think that it's going to take a long time to run a particular process, you might want to raise this. However, you do need to use caution here because as I explained earlier to end the function, you need to either for HTTP trigger send the express JS response. Or you need to call call back on a pub sub or storage trigger. And if you don't, the function is going to run until this timeout time. And so if you just go ahead and set the timeout to five minutes for every function just to be safe, well you could be shooting yourself in the foot if a bug ever arose and you're not properly closing that function out. Suddenly you'll be charged five minutes for every invocation instead of the hundred to 400 milliseconds it should've taken. So for demo purposes, we'll keep this at 60 seconds but you may find a need especially if you're calling a lot of third-party services to extend this timeout time. And you'll figure it out as you go along and build your own micro services. Now we're going to go ahead and select HTTP trigger. And this is the really cool part right here. So what happened is it automatically populated the endpoint URL. That will be generated for us once we create this HTTP function. And the URLs always have the same format. We start with HTTPS. Again, another great feature about Google cloud. It's securing everything for you. Then we have the region we're working in, US central one. And then our project ID, Zeta sky 181023, in my case. Yours in going to be different depending on your project ID. Cloud functions. net slash, the name of our function PS hello HTTP. So what you have here is a unique URL based on your project ID. And when you call this URL, it's going to trigger the function, amazing, it's awesome. Now if we look at the next step down, we have options for how we can access the source code that we're going to use for the function. So you could see here that we have an in-line editor option and right below you're able to just code the function right then and there. You also have a zip upload. So you can go and upload a zip file with your code. Or you can choose a zip file that's already been uploaded to cloud storage. Which you'll see later that what happens when you create a function is Google cloud actually goes and bundles up your code and uploads it to cloud storage as a zip file anyway. And then the final one is Google's cloud source repository. So you if you have that set up in your Google cloud project and you're managing your code that way, you can just go ahead and grab the code from there as well. So you got several different options which is really nice. And right now we're just going to use the in-line editor and later in the course, we're actually going to be building them on our local machine and then using the Google cloud SDK and G cloud commandline tool to upload them to Google cloud. One of the nice things about the in-line editor is you could see here that we have the two standard files that are typically generated with the node project. We have index. JS and package. json ready to go. You can see in the code too they start off with a pretty standard template. So you can see her we just have an if statement. It's checking that express JS request object, the body property and seeing if there's a message. And if there isn't, it goes ahead and returns a 400 saying no message defined. But it does find one, it goes and prints it out to the console and then also returns a 200 status with the success and the very message that we sent in with the URL. So if we look below the code here, you see this next section has a stage bucket. And one of the nice things that Google cloud does is when you deploy your function, it goes ahead and wraps up the code and stores it on Google cloud storage for you. And so here you're basically selecting the bucket in your project where you want that code stored. And so what you want to go ahead and do is select your staging bucket. So if we click the button. And now you'll see here that we actually don't have any buckets created yet in our project. So I'm just going to go ahead and type in here staging-functions, we'll keep multi-regional selected and then go ahead and create. Well that's a fun error. This bucket name is already in use. Bucket names must be globally unique, try another name. I think that error is pretty self-explanatory. So your name has to be unique compared to every other bucket name on the Google cloud platform. And so it is crucial to come up with custom names. I suggest using your project ID within the name to really try to make sure it's unique. And so that's the thing we're going to do here. I'm going to use the project ID, zeta-sky-181023-staging-function and hit create. Perfect. And now, the last piece is the function to execute. Now here we have hello world. And you're going to see this in a later module but what you'll find is we're able to actually have multiple functions within that single index. JS file. And then what happens is we then pick a function to execute and it has to match the exports. function name. So in this instance, in our code snippet you can see that we have exports. hello world and our function to execute is hello world. If these don't match, then the trigger will fail when it tries to get created. Now I'm going to go ahead and create the function and this is going to take a couple minutes. So I will see you when it's fully uploaded. Now that our function is set up, let's go ahead and click into it. Now in an upcoming module, we're going to be looking at all this information in depth. But for the moment, let's go over to the trigger tab and go ahead and click on the URL. You could see a new tab opened up in chrome and we're getting the error message, no message defined which if you remember from that template code if it didn't find a message property within the body you've returned a 400 and the text no message defined. So looks like our function is working. If I jump back to the first tab, going back to the dashboard and I refresh the function, you can see here that we now have one invocation on our function. So congratulations, you have now set up your first cloud function. Now let's take a look at setting up pub sub and a cloud storage functions as well.

Making the PubSub and Storage Function
We've created the HTTP triggered function but let's go ahead now and set up the other two. Now before I jump into create function, I want to go over two things that I've already set up. One, I've gone into pub sub and I've gone ahead and created a new topic. Now if you haven't created one yet, this is going to be super simple, you're just not going to see anything here. And you'll get the the ability to type in whatever topic you want. In this instance, I just typed in hello world. The projects zeta sky slash topics was already pre-populated for me. Of course zeta sky-181023 is my project ID. Yours is going to be different. And I also went into the storage area and created a new bucket. And so all I went up to was create bucket and went ahead and typed in my name, selected multi-regional and hit create. And the reason for and the reason for this is we're storing our functions here in the zeta sky staging functions area. But when we trigger our storage functions, we do not want to use that same area. In this instance, to me staging functions bucket is kind of like source control for our code. But now the functions production this one we can use and tie our function to that. So whenever we upload files to it, it's going to trigger our function. So now that we've gone over the pub sub topic and our new storage bucket, let's jump in and create our two functions. Again, it's super simple right 'cause it's just going to come up here, hit create function and we'll do our pub sub one first. So we're going to do PS hello pub sub, got US central one, 256 megabytes, 60 second timeout. And the trigger topics already selected by default and going to do hello world. Now we do have a new option here that we didn't have with the HTTP trigger. And that's retry on failure. And you have this with both pub sub and storage. And now what this allows you to do is if this is checked it's going to automatically retry it when it fails. You might not always want to check this, you may want to. It's one of those things you just need to think about. So think about it and you're going to set this on a case-by-case basis. For our purposes, I'm just going to leave it unchecked. And now once again we're in the source code area and we're going to use the in-line editor. You'll see here, here's our function that it looks a little different from the HTTP trigger. We have this new exports. subscribe function. And instead of getting that request response parameters that we had earlier, we just have event and call back. Now event, we've gone over this and it stores all the important information about the trigger and the data that is being transferred to this function. And the callback is equally as important because when you call this callback function this is going to tell cloud functions hey I'm done, stop charging me time. And that's important right? Because say your function only takes 500 milliseconds, and if you don't call that call back it's just going to keep running until the 60 second timeout. So you always want to make sure that you call the callback. And that is true for both pub sub and cloud storage. So again, our function is very simple here right. We're just going ahead and grabbing the message that's transferred through the data property and it is base 64 encoded, and all we're doing is printing to the console. And then the callback is called. And that's it. Our staging bucket, once again we're just going to set up and use the same one that we did for the HTTP trigger and the function to execute, you see they populated subscribe here. And that matches the one up here in the in-line editor. That's crucial. So everything looks great, let's go ahead and create it. Now, while that's getting set up. Let's just go ahead and create function again. We could just keep creating these all day. But what we're going to do here is we're just going to do PS hello storage. We're going to select the storage bucket trigger. We are already set up our new bucket. So again, I didn't want to select this one because that's where our functions are being kept. So we got this functions production bucket. Go ahead and select that. Retry on failure, just going to leave that blank. In-line editor, and once again we have this event and callback parameters. And all we're doing is it's just looking at the event and the data property and describing the name of the file and printing it out. And then it calls the callback. The most crucial part. Select our staging function. And once again, function to execute process file. Matches the in-line editor, beautiful. Let's create it. And that's it. We just created three functions and all of them do three different things. One is a web hook that we can call whenever we need. The other is a subscriber to a pub sub topic and the other responds to any files being uploaded to a cloud storage bucket. It's amazing and it was so simple to do.

Summary
We learned a lot of great stuff in this module. First, we went over the different trigger types that you can connect functions to. And now remember, you can connect multiple functions to the same trigger type. However, you're not able to connect functions to multiple triggers. It's a many to one relationship. And then once we understood what triggers were and how they get invoked, we then took a look at the event and data objects that will receive on each invocation. And the event that it does have some useful information in there, however the real important data is coming through on that data property of the event object. And we went over the different type of data objects you get depending on the trigger. HTTP, pub sub, and cloud storage each have their own unique data object. And this is really helpful when you're trying to determine which trigger you want to use. As we were explaining triggers and events in the data, we also went over the lifecycle of an invocation. We took a look at how you can connect a function to a trigger, and one that trigger fires your passed an event and then you can go ahead and process that data. Then finally, we took a look at how you can end the instance of that function. Again this differs depending on the functions you're working with and the trigger type you're working with. HTTP is going to take and express JS response object while the other two, pub sub and cloud storage you have to call the call back. And remember it is crucial to call that call back or send a response on the HTTP trigger so you don't max out your execution time that you set up. Once we were comfortable with one invocation, we then took a higher level view and looked at what happens when we call the function a 100, 000 times. Google cloud is going to bring up an instance of that function and depending on the load it gets, it my have to generate more instances. And then if there's a good enough down time, after our last call, your function could go down to zero instances. Which means you're going to have a cold start time, the next time that function gets called. It's a small disadvantage. However, I think the advantages to Google cloud functions far outweigh the disadvantages. And then we went into Google cloud and built each type of trigger. We did a HTTP function, a pub sub function and a cloud storage function and went over the various options that we have while creating them. However, there is still so much more to cover. We still need to learn about monitoring our functions and debugging them and testing them. And while this can be done in different ways from your local machine to Google cloud, we'll first going to take a look at it through Google cloud and through the chrome browser because the visual feedback will just give you a better understanding of the tools that you have at your disposal when dealing with cloud functions. So there's a lot of great stuff in the next module, I'll see you there.

Exploring the Cloud Functions Dashboard
Introduction
One of the things I like most about the Google cloud platform is their dashboard in the web browser. It's really clean, they divide their products up in a way that makes a lot of sense and doesn't feel overwhelming. You get a great sense about what the product offers to you and what it's capable of doing. In this module, exploring the cloud functions dashboard we're going to go through and see what cloud functions has to offer from the perspective of the dashboard. Now while this isn't the only way to maintain your functions it's a great way to just get started. So in this module, we're going to take a deep dive on the dashboard itself and just go through everything. So there are no surprises for you. And then we're going to look at how we can actually test our functions through the Google cloud platform website. We're also going to take a look at how we can find the source code that's saved in cloud storage. And this is true whether you upload a function using the Google cloud dashboard or in a later module you'll see we can deploy it using the G cloud command line interface tool. We'll see where that code is stored and how you can retrieve it yourself in case you need to pull it down and take a look at it. We're also going to look at viewing and filtering our logs. The logging system for Google cloud functions is incredible. And you have the ability to filter by the function you can break it down per process, every execution of the particular function. It's really powerful and really cool and we're just going to take a quick look at this. And in a later module, we'll go in even more depth. Then finally, we're just going to see how to delete functions. It's super easy and super quick. And with that, let's get started and take a tour of the Google cloud functions dashboard.

The Cloud Functions Dashboard
So we're back in our GCP project in the cloud functions overview screen. And we're going to be going through this dashboard thoroughly. So you get a good understanding of the information that you have accessible to you on here. And this includes the ability to edit functions, test functions, check the metrics on functions such as invocations and execution time. There's a lot of great stuff. And while you can do a lot of this from the Google cloud SDK which we'll be covering later in the course, it's always nice to jump in here just to get a good overview and a good feeling about how your functions are running on a day-to-day basis. So on the overview screen here you can see the three functions that we had created in the prior module. And it's just very straight forward the information right? We have the name that we gave the functions, the region that they're running in, and the type of trigger that they are. And the nice thing is, for the pub sub and this cloud storage triggers is we're given extra contextual information right? We see here that this function is subscribed to the pub sub topic hello world. And that our cloud storage function is tied to events that are triggered by the bucket functions production. So it always gives you a good sense of where that trigger is occurring. We have the memory allocated and the executed function name from our source code. Remember this was the exports. hello world, exports. subscribe, export. processfile and it was important that those names had matched up before we had saved our function. And finally the date of the last time that they were deployed. And if you go ahead and click this triple dot here we get some quick things that we can do. We can copy the functions, test them, view the logs or even delete them. And the top nav bar here is no different. We have the ability to create functions, refresh them, delete them or even copy them. As well as extra filters too right? Perhaps we only want to see trigger for pub sub topics, boom done. Or we want to see the name for an HTTP trigger and right now these two conditions don't match up so we don't see any functions. But as we remove the pub sub condition, we have our HTTP trigger and if we remove that all the triggers are shown. So it's really powerful. And you can always add extra columns, which in fact, I personally like to add the timeout function as well. Especially if my project contains a lot of video or image processing. So that's the overview screen let's go ahead and jump into one of these functions. So we'll do the PS hello HTTP. And now you're brought to this initial function detail screen, and we're given a lot of information here. Just on the general screen alone, we have a nice overview of when our function's been running. And you can break this down into an hour, six hours, all the way back to 30 days. And here you are told well how many times did it run okay? We're there errors? Was it fatal? And if it does even crash, an extra property will show up here with a pink line. So it's very informative. We pretty much always just want to see blue. And on top of that, you have different metrics that you can view on this graph, right? So we have the invocations, we have execution time, and we have memory usage. And now why are these significant? These are the parameters that you're charged on. This is the pricing. So it is important that you know how many times it's being invoked, you want it as fast as it can be, and to utilize as little memory as possible. And that's how you're getting the biggest bang for your buck and right here Google provides all three metrics for you so you can view every function and find out if they're performing as well as you like. And if you happen to notice your bill is really high you can go ahead and say, alright, what's going on here? Which function or functions are out of control? And maybe you didn't call that callback like you should and some of them are just running for 60 seconds all the time and you would just see a flat line right here and that will give you a good indication that something went wrong. And then below here, we do have a lot of extra contextual information. And you've seen this before already right? We've seen in the timeout, we've seen the memory allocated the region and the last time it was deployed. We had done all of this already. If there were any errors, you would get a nice breakdown of the errors that have been reported in the last seven days. So there's just a lot right on the front general detail screen. And don't forgot as well that we have the ability to edit the function. So if I come in here, again perhaps we need more memory or we would like to upload the source code in a different way or just simply edit the source code right here. And we have the ability to do that and go ahead and save. Right now I'm just going to hit cancel. We can delete the function, copy the function, copying is just very vanilla. We are pretty much given the same parameters. And it's going to use the original source code that's already saved for that function. So you notice here it didn't even default to the in-line editor like we normally would. But I could just go ahead and select it and It'll populate it for me anyway. I'm not going to save that one either. And then you can also view the logs as well and this is going to take you into the stack driver logging system that is used and you can see here from the couple times that I've run the function earlier. So in the prior module, when I first set up the function I went ahead and clicked on the URL for the browser and we got the 400 code, you can see it when it ran right then. And then another time I tested the code which we're about to do in a little bit. And you can see I got a 200 response and that it just printed to the console hello world. So there is a lot of useful information that you can always use and you should. Now let's take a look at some of these other tabs up here. We've gone through the general tab in detail but there are plenty more, We have the trigger. So here you get the trigger type and we see that's a HTTP trigger. And once again, the URL. And if we go ahead and click on that, we'll get another 400. But we're not going to do that right now. We have the source, and so here we been using the in-line editor. Once we use the cloud SDK later, we're going to see how that differs here. But you can see here that we have our code preview and we can go ahead and look through the code. If I go ahead and try to type, nothing's going to happen. I have to actually go and click edit to be able to edit that source code. So this is just very static, and a clearer view. We have the source location. And again, we remember doing this when we saved our function during the initial creation. And we know we went and saved it to our staging functions bucket in cloud storage. A little later in the module, we're going to dig through this zip file but for right now you know it's there. And something that's always important is that function to execute hello world. We have the package. json, which we didn't populate but you can view it from there as well. So let's quickly jump into the pub sub and the storage triggers. So I'm going to quickly run through. You can see we have the same type of information invocations the trigger is a little different with the topic information and retry on failure if we had selected it. Source code looks the same, source location. The function to execute, again it matches our exports. subscribe and then we have testing, which we're going to get into in a little bit. And storage is no different. Go to the trigger, here we see the bucket that our trigger event is tied to. We have the source, and testing. So the dashboard is super simple to use and again it provides a lot of information. Now let's take a look here at testing our functions.

Testing the Functions
So now, what we're going to do is we want to test each of our functions. How can we do this? I mean of course we could go to the HTTP URL. We could use the Google cloud SDK and actually publish something to our pub sub topic. And we could upload a file to cloud storage and it will trigger the event. However, sometimes we just need something quick and dirty so we can just see how the function is running, see if we're printing information out properly and just get a general good feeling of it. And the GCP cloud functions dashboard provides that capability. And we're going to use it really quick here. So let's just jump into our HTTP function first. Here we have the ability to enter json and we can go ahead and just tap test the function and we're going to get an output and logs for that specific function invocation. If I jump to source, just so we can reacquaint ourselves with the template code, essentially we have a request response object that's passed. It's in express JS fashion and we're going to look for message property within the body and if we don't have one, we're just going to get no message defined. And if we do have one, it's going to log the message out and also return it to us. And so that's all we need to do right? As long as we just pass a message property, we're good to go. And now when we're remember doing this json, this is the body object for an HTTP trigger. So all we need to do is simply come here and say hello HTTP. And if I test the function, boom. We got the response that we were expecting and we can see here that the logs are starting to calculate and process from the stack driver side. And they do take a little while to load. And you can see here, we have our console. log printout hello HTTP. And if I just go ahead and delete this completely, and hit test a function, you can't even do it right? Because it's like, well you're not providing me anything. It has to be valid json. And so let's just give it an empty json object and we get that 400 return. So it's just very very quick. Now what about the pub sub and storage though? They're very similar. And if I go to pub sub, once again let's just review the source code quickly. We're going ahead and trying to grab this data object. Remember we don't have that same request object that we have with express JS. So we have this data object, and we're just going ahead and pulling the information from there, and it's base 64 encoded and we just print it out to the console after encoding it and we call the call back, simple. So let's jump up to testing. And we can come up here and do data which is our property. And now we need to do some base 64 encoding. And as you can see here I'd already done a little snippet print that hello pub sub, piped into base 64. And here's my encoded string right here. So I'll put that in, hit test function and you didn't see anything in the output which makes sense, right? Because all we did was write to the console. So this output had showed what we had sent as a response with the HTTP trigger, but not pub sub. But what we do here in the logs is our decoded message, hello pub sub. So it worked perfectly. And finally we can do the same with storage. Now ideally, when you're working with files and text files this is going to work really well. But if you're going to be working with images or video you know, something that's not text, then really testing this with the console isn't going to work very well. In some instances it will, but you're going to run into a lot of errors because you're just not can I have that image information that the function needs to complete its processing. However, that's not the case here we're just going to upload a simple json file or a simple text file, and all we're passing is just the file name. And so again we have this event. data. name. And for testing, this is our data object, right? So I can just come in here, name hello world. txt, test the function. And again, we didn't have any output that really made sense. So nothing's showing up there but the logs are processing right now and we got our logs, and here you go. We got processing file, hello world txt. So this is really useful right? And again it's kind of quick and dirty because you can input kind of the data that you need, it might not always be the data you expect. You know, once you upload a file, perhaps your function, you're just using your test data with TXT files and then suddenly a PDF gets uploaded and boom, you know, it crashes your function. So you need to be prepared for those types of things. This is just very quick and dirty testing. But it's good to know about, and it's really easy to use. So you should definitely use it.

Viewing the Source Code
Let's now talk about source code. You saw as we created each of these three functions that we were required to select a staging bucket for them. And this kind of makes sense if you think about it right? GCP, your uploading the code to it and GCP is taking it and processing it and creating what he needs to generate this function. So let's go to storage and you can see here if I go to staging functions, that I have these three subfiles here with my source code right? And so you know we have the name of the function and when it was deployed and saved. And so this is really useful right? The fact that it's going to use this naming convention and have the deployed date means you can go back in time. If a function broke you could just say, what was the prior one? Of course you should be using source control and unit testing and continuous integration and have all these other processes set up to be able to do that anyway. But Google went ahead and did it for you as well. And I'll just go ahead and grab our HTTP function and see it downloaded. So I'll go ahead and open it up. And when I open it, we just at the very basics of what we need. Go ahead and we can open this in TextEdit and you can see here we have our templated hello world function. And we also have our package. json. And it's just as simple as that. But you'll see once we start working with the Google cloud SDK and uploading these from the terminal, these zip files become far more important because it has all of those different files, all that different information that we need. And you can find out, well my function isn't working properly. You could pull the zip file down say, well oh, I'm totally missing one of my template HTML files and that's why the web UI is crashing as I'm trying to generate this information. So it's just important to know where it's saved, that way you can quickly debug any silly situations like that if they crop up.

Reading Stackdriver Logs
One area that I glossed over earlier that I would like to take another deep look at is the ability to do logs. If you come down to the stack driver and you look at logging, you can go ahead and view the logs for different functions. But another quick way to do this is also just by going to cloud functions and viewing the logs for a particular function. And here you have the ability to simply select any functions you want or you could show information for all of them. I could go ahead and uncheck any of these or make sure that they're all checked. I can go ahead and select any of these and see all detailed information for them. And not only that, but you can view all the logs you can check the log level. And while we're not going to get into it into this course, you have very powerful ability to control the logs and monitoring system for stack driver even to cloud functions. However it is beyond the scope of this course. But you should know that they are there. And you have the ability to look at a specific date as well. And so all of these are incredibly powerful and if you go ahead and click into them, you can see that you just are provided an incredible amount of information. And you have a play button up here that if you have somebody uploading a bunch of files, running a function, you could just sit here and watch the logs coming through left and right. So it's incredible and it's powerful.

Deleting the Functions
Now one of the last things I want to touch on is deleting functions. And it's super super easy to do. You can go ahead and you can select one and hit delete. Another process, or you can go ahead and select multiple. Come up here and hit delete. Are you sure you want to delete two functions? Yes I am, and that's it right? And so this is really really just basic. And we're provided once all those functions are deleted, it even gives us the simple QuickStart screen to select create function again.

Summary
We went over a lot of great stuff in this module. We took an in-depth tour of the Google cloud functions dashboard. You saw how you can view metrics for function. And the most important metrics right? The ones that you're going to be charged on. The timing of the function, the memory usage and the number of times that it's invoked. And you saw how you can view the functions in depth. You can see the source code, you can see the trigger type and the functioning that's going to be called. And all this information really helps you to troubleshoot any small minor issues you might have. Then we looked at testing the functions and we went through all three of them. We tried the HTTP trigger, pub sub and cloud storage And you saw how you could just send these little json payloads to your function and while it doesn't provide super in-depth testing and it's not going to cover every scenario, it's great to just quickly be able to make sure that they're functioning properly or perhaps you want to print something out to the log quickly to see a particular set of values. So to really nice. Then we took a look at grabbing the source code. It's always saved in cloud storage. It's a nice feature. Google cloud has your back. It's uploading that code, your always able to pull it out and download it and view the source code files. So if you have a more advanced deployment mechanism, perhaps you're building the functions on your computer, you're stripping test files out. You can always pull this code down and make sure those files are truly being stripped out. Then we took a look at being able to view and filter our logs. Stack driver logs an incredible amount of information and there's a lot to it and again you can break it down per function, per process. It's really informative. And while there are even more powerful mechanisms, we just wanted to take a look at at least being able to view the basics. Then finally, we deleted functions. Just a standard operation that we can perform within the Google cloud platform. And so we went and deleted all three functions. So we have a clean slate for the next module. While the dashboard is a very powerful tool, especially for logging, it's also crucial that you're able to manage your functions in a way that are replicable, that can be repeated. You want the ability to be able to create them, delete them, manage them through code. And so that's why in the next module, we are going to take a look at the Google cloud SDK and being able to develop our cloud functions locally and write scripts and be able to deploy them with commands.

Managing Functions with gcloud
Leveraging gcloud
When you're dealing with Google Cloud, there are so many different products that you can choose from, and there's a lot to configure and set up. You always want to make sure that, as you're doing this in a dev environment or QA environment, and then, finally, a production environment, that you are getting it right, and so one important tool that Google provides is the Google Cloud SDK, and with that SDK comes the GCloud command line tool. In this module, Managing Functions with GCloud, we are going to look at leveraging that tool so we can manage our functions, and this will ultimately allow you to be able to set up your project, create functions, deploy them, call them, view their logs, and even delete them, and it's important to get a firm grasp of this tool. So, in this module, we're going to start by installing the Google Cloud SDK, and then we're going to go ahead and review basic commands. We'll just go ahead and log in and set up our project so it's ready to go. Then, at that point, we're really going to just concentrate on the functions within GCloud. While you have access to every Google Cloud product, you'll find that the documentation, and just the way the tool works, is really nice and easy to learn. And once we've reviewed the basics, we're going to go ahead and concentrate on learning how to call our functions, and once we've called them, we want to be able to verify that they've run properly, and so we're going to use the logs call within GCloud to read those logs in, and you'll see that you can do this based on all the functions, a specific function, a specific instance of that function running, and finally, we'll look at deleting functions. By the end of this module, you're just going to be comfortable using Gcloud, and it's going to be crucial for the following module, when we finally build our functions locally and deploy them. There's a lot of great stuff to go over, so let's get to it.

Installing gcloud
So, I'm at the Google home page here, and if I just go ahead and search for Google Cloud SDK, you'll see it's the first search result that pops up, and it's at cloud. google. com/sdk. You can go ahead and select for the platform that works for you, and their directions are really detailed and great. So you see here, we have Mac OS, Windows, Linux, Ubuntu, Red Hat, and so I'm going to ahead and install OSX, and what you want to do is, for your respective platform, just run through the directions, and it's really simple to set up. So here, it says to make sure that we have the appropriate Python version installed on our system, so if I go ahead and open terminal, Python -V, you can see that we're running 2. 7. 10, so we're good there. And now you can go ahead and select one of the packages here and extract the archive to any location, and then, finally, run the Bash script, and then it says to restart your terminal for the changes to take effect. If you don't want to go through all those steps, one thing you can do is jump to the installing the SDK guide here, and they have a bit of a quicker version that you can use. Right here, you could see that they have this interactive installer. This really speeds up everything, really fast, so what you can go ahead is grab this URL, and again, they have directions for Linux, Mac OSX, and Windows, and once again, open terminal. I'm just going to paste this in and hit enter. You want to hit enter, and then you can accept to provide statistics to them, and now this is going to take a few minutes to run, so I'll see you back once it's fully installed. Okay, so great. Now, it's asking do you want us to modify the profile to update your path and enable the shell command completion, and yes, you do. At least, I do. This way, you can just call GCloud directly from the terminal. Go ahead and use the default. Brilliant. So, now that it's installed, I just go ahead. I'm going Command Q to quit out this terminal session. Command space bar, and type in terminal, relaunch. Now, if I type gcloud, you can see that it's running. We have GCloud installed, and we're going to go ahead and set up our project now.

Setting up the Project
So, first thing we need to do is we need to login, so I'm going to go ahead and do gcloud auth login, and this is going to open the browser window, and go ahead and sign into your Google account. It's asking you to accept various criteria that the Google Cloud SDK wants to do with your account. Hit Allow, and great. It now says we're authenticated, so if we go ahead and jump back to our terminal, you could see here that it's giving us some extra directions now, saying gcloud config set project PROJECT_ID. Now, we could go to the Google Cloud dashboard and get that project ID, but again, GCloud is awesome, and they already have our backs on this, so if you just go ahead and do gcloud projects list, it's going to go ahead and list all of the projects for that particular account you just logged into, and you can see here that it provides the project ID, so I can go ahead and just copy that, and then do gcloud config set project, and paste that in. It says updated property, core/project. But how are you sure that the project was set right? I mean, it's really not giving any good feedback, and so, to check that, you can just go ahead and list out all the configuration properties, and so here, we can clearly see that we set up the project, and that it's the right account, and so this is something you want to do often, and I personally never try to set the production SDK. Again, I script everything out for those deployments, but while I'm working QA and dev, I at least can toggle between those projects very easily and keep moving. So, one other thing that we want to do is, because the Google Cloud functions are in beta, let's go ahead and make sure that we have all of our beta components installed, and so you can just do this by doing gcloud components install beta, and there you go. For the latest full release, let's hit yes, and these are going to install. Now, by the time of this recording, Google Cloud functions are in beta, but if you're watching this at a later date, it may finally be ready for production, and so you don't need to do this step. However, there might be some cool new beta features that you would like to try out, and if that's the case, then you do need to install these. Awesome, so now we have everything set up, so let's go ahead and check out our functions quick.

Getting to Know gcloud and Cloud Functions
So, we've got the GCloud set up, and now we're going to go ahead and check out our functions and our project, so just type in gcloud beta functions list. We're provided our function list, but wait a minute. Didn't I delete these at the end of the last module? I think I did, so how do we have two here? Well, I went ahead and reinstalled them, and you can see here that there are only two, and what I want to do is just jump into our Google Cloud project quick, and just show you how I was able to recreate them quickly, without even using the template code. I'm just going to go to create function, HTTP trigger, and now I'm going to select ZIP from Cloud Storage. Now, we didn't use this one in our earlier modules, but if you remember, we were always forced to pick a location to have that code uploaded in Cloud Storage, and so it's still sitting there, and if I go ahead and select the proper bucket, you can see that the code is still in here, and so I'm going to go ahead and select this HTTP trigger, and the function to execute. We never changed this name, so it's still helloWorld. I'm going to go ahead and create the HTTP function as well. Oh, you can see how I didn't update the name to psHelloHTTP, but that's alright. So, if we clear this out and we run this again, you can see how we have our two functions are ready, and that the http one is in a deploying status, and so this is awesome, right? You're just getting that instant feedback from GCloud right away, and in a couple minutes, that function-1 is going to show ready as well. Now, let's take a look at some other commands while we're waiting for that to get ready. Let's take a look at the gcloud beta functions describe, and we'll do helloPubSub. Whoop, it's the wrong name. Command W, delete that last word, and psHelloPubSub. Awesome. So, we got all the information that we would typically get from the dashboard on GCloud. Remember, we have the available memory is 256 megabytes. The entry point for this trigger is subscribe. The status, the source code is being saved within Cloud Storage. The project it's under, and so this is really cool and really powerful. Finally, you might be saying well, that's great. You're able to just, you know, pull out all these different commands on a whim, but how do I figure these out? And that is one of the best parts about GCloud. The help is awesome, and pretty much all you need to do is you type in several commands down and do dash dash help, and it's going to give you the help and documentation for that specific scope, so let's take a look at that quick. So, we could do gcloud beta functions dash dash help, and here, it's giving us all the different commands we can use. So, you see here, we have our logs. We can look at event types. Here, we can call, delete, deploy, describe, and list our functions, and so, if we wanted to look at another one of these in depth, let's pick call, I just do call dash dash help, and now this is giving me the information for the call command specifically, and so this is really cool, right? I can now see, ah, here's some flags. I can pass some data to it. I can specify the region if I want to, and it even provides some examples down below. Command helloWorld dash dash data and the JSON message. So, now that we've checked this out and we've gotten a good overview of the functions commands, let's go ahead and call our functions and view the logs for them as well.

Calling Functions and Viewing Logs
Alright, so we've reviewed these function commands within GCloud, and now let's call our existing functions. So, I'm going to go ahead and start with the PubSub one first, but you know what? Why don't we go ahead and just make sure that all three functions are ready. Whoop, forgot the beta. Alright, cool. All three functions are ready now. So, let's go ahead and we'll do gcloud beta functions call and we want to do psHelloPubSub, and now do data equals, and notice the equals sign is actually optional, but I'm going to include it. I just like to. And we're going to send the message. Ah, yes, we have our base 64 encoded string from earlier, so I'm going to go ahead and grab that, and it was tied to the data property, and so, now, as we've run this, oh, no, what happened? Unrecognized argument. Well, once again. Why don't we just go to gcloud beta functions call, and find out what we did wrong? Ah, I see. I forgot the double dash in front of the data. That was a silly mistake but, you know, again, we had the help here, so all is well in the world. So, I'm just going to jump back here, get my two dashes in, hit enter. Awesome. So, executionId, what does this mean? Well, let's go ahead and take a look at the logs so we can get a better idea. So, I'm going to go ahead, call beta, gcloud beta functions logs. Ah, see? GCloud is still giving me some help. It said you're almost there, but the command you might be looking for is read. It's so helpful, so useful, so why don't we try that again and include read? Ah, perfect. So now you can see that we have a bunch of logs listed here. You could see the log level, the name of the function, the execution ID, and now this execution ID is that instance of that function running, and the reason that's useful is if you want, narrow it down based on just that execution, you can do that, and we have the time the function ran at and there's our log message, so this is really cluttered with the psHelloHTTP, so why don't I go ahead and do logs read psHelloPubSub? Ah, perfect. So now, the logs that we're getting back, you could see a couple errors from earlier, but you can see now that we're getting all psHelloPubSub down the whole line. So, this is really, really informative, right? And we can quickly go ahead and grab everything that we need to, but again, what other help can we get from this to keep it cleaner? 'Cause it is very verbose, and it's easy to get lost in all this text, so again, help is on the way with the dash dash help under the read command, and you can see here that we have a bunch of different ways that we can filter our logs, and one of them is that execution ID. So, let's take a look at another function call quick, and we've already done PubSub, so let's go ahead and do the storage one. So, do gcloud beta functions call psHelloStorage, and then dash dash data. Don't want to mess that up again, and we'll do just a name property and we'll call it helloStorage. txt, and get that end quote on there, too, which I missed earlier. So now, I went ahead and ran it, and that's great, so we have this execution ID now, so why don't we go ahead and use it this time? So, I'm calling the functions logs read command, and then I'm going ahead and grabbing the ID, and there you have it. Now it's only reporting the one execution that we had just ran, and here, we can see that it printed out the helloStorage. txt, and so, this is really cool, and now we just have one last function to run, so why don't we check that out to see what it looks like? So, again, just going to do the gcloud beta functions call, and now the name is function-1 instead of psHelloHTTP, data, and we want to send our message, so message Hello HTTP. End result, success. Hello HTTP, and now you're probably saying well, wait a minute. Why did we get a response with this one and not the other ones? Well, if you remember, we had talked about how the HTTP functions are basically synchronous functions, right? When you call it, you're expecting to get some sort of a response back, where you can really treat those PubSub and storage ones as asynchronous. I mean, maybe you might be looking for a response or observing a response somewhere, but ultimately, you're not going to be notified directly like you are with the HTTP event trigger, and so this is really, really cool. So, now that we've seen how to call and view our logs, let's look at how we can delete the functions from GCloud.

Deleting Functions
Deleting functions is super easy in GCloud, and again, all we need to do is just go ahead, call the delete function with the function name, and it's going to go ahead and delete the function for you, so I'm going to hit yes here, and it's going to go ahead and complete the operation. And it's just that simple. Go ahead and just do it for psHelloPubSub, and that's all it takes. You're just deleting all the functions now. I'm going to go ahead and delete all of them. I suggest you do the same, because in the next module, we are going to be building them locally and deploying them.

Summary
We've learned a lot of great stuff in this module. First, we installed the Google Cloud SDK. We've reviewed a lot of the basic commands, and we saw how to use the help. Always use the help, whenever you're in doubt. It's great, and it's easy to use. We then called all of our functions using the call command, and you were able to see that we were able to call all three types, PubSub, storage, and HTTP, but again, I will say this. You do need to take some care, right? Because the data is only so good when you're passing it from the command line like that. You know, again, test thoroughly, you know? With Cloud Storage, with those storage triggers, if you're going to be doing image processing, make sure you go ahead and upload images to Cloud Storage and see the function run for real. After calling the functions, we were able to review the logs, and you could see that you have a ton of different ways to filter those logs. We specifically looked at just grabbing the execution ID from a specific function, run, and we were able to view the logs just from that one instance call. We were also able to just look at them based purely on the name, and so there's a lot there, and you could limit them. You know, I don't want 50. I just want the last 10. And then, finally, we looked at deleting functions. It was super simple, right? GCloud beta functions delete function name. It pretty much just took care of the rest for you. Now, I do want to add one last thing here, and again, this is just so important. Script your deployment. Use GCloud. I use it personally from Travis CI to deploy my functions out, and you know why? It's great because I am going to be able to create a consistent environment with it. If I'm going into Google Cloud and checking these check boxes, and saving to this storage bucket and, you know, maybe tweaking something here and there, that's going to be very hard to replicate from production, and that's why this module was so important. It's because you need to be comfortable with GCloud if you're truly going to succeed. We're not done yet. There is one major thing we have not done in this course, and we're about to do it in the next one. Let's go ahead and create some functions locally using node. js, and then deploy them to Google Cloud using the GCloud command line tool. See you there.

Constructing Functions with Node.js and NPM
Introduction
With the Google Cloud Platform, Google gives you this incredible dashboard to work from so you can easily manage your functions, check logs and even write code. However, the more complex your function is going to get, the more challenging it's going to be to just use their console within the Google Cloud Platform. Now it's probably going to improve over time, however there are a lot of benefits to just coding the functions on your local machine and then using G Cloud to deploy them. And so that's what we're going to do in this module: constructing functions with Node. js and npm, Node Package Manager. We'll first going to look at installing Node. js. We'll see what the latest version is running on the Google Cloud Functions and we'll install that on our local machine. Then at that point, we'll go ahead and create a new Node. js project. This is really simple to do and it's not necessary to have any prior knowledge of Node itself because we're really just going to be doing basic stuff and this is going to be fairly easy to follow along with. Once we have this project created, we can then go ahead and write our functions. This is going to be a super simple process and we'll be able to write all three of them very quickly. And then once we have them written, we're going to go ahead and deploy those functions with G Cloud and the Google Cloud SDK. Now in the prior module, you've seen how to check logs, test and call your functions, but the one piece we did skip over was deployment and we're going to do it in this module and then we'll check to make sure everything is all right. And that'll be it. We're going to be doing a lot of cool stuff so let's start and get coding.

Installing Node.js
Now one of the challenges when you're first dealing with Cloud Functions is well, what version of Node. js are they using anyway? It's not very apparent when you start digging through and trying to find out what version they're using. And they're going to constantly be upgrading it. This is a good thing, this is one of the key benefits of using Google Cloud Functions because they're managing those cloud function instances for you. And as new security concerns come up with all the versions of Node. js, they're going to continue to upgrade and push forward. So here we are the Cloud Functions product page and if we go to view documentation, you scroll down to concepts, all concepts and do overview of Cloud Functions, and it's on this page. If you look at this paragraph right here, Cloud Functions are written in JavaScript and execute in a Node. js version 6. 11. 5 environment. Now this is an older version of Node, however it's only recently that Node 8 became the latest LTS version. Up to a couple months ago of this recording, Node 6 was still the latest LTS. So they're not too far behind. So now that we know the correct version, let's go ahead and go to nodejs. org. And at this point you can see here that the latest LTS version is 8. 9. 1, but that's not the one we want. So we're going to go to the other downloads link and we'll scroll down to the bottom and we'll select the link, previous releases. And at this point you can see we have a whole list of versions to choose from. And right here, you can see 6. 11. 5. You can go ahead and grab the correct package, whether you're on Windows, Mac or Linux. Go ahead and grab this package file here. And so now it's going ahead and downloading it. So what you can do is we open it up, you could just go ahead and see this package will install Node. js 6. 11. 5 and npm version 3. 10. 10. And just go ahead and run through the steps here. Agree. Install. And it's going to go ahead and install this for you. So now that this is installed, you can see that it went ahead and updated our paths for Node and npm. So if I go ahead and close this, move to trash, now I have a terminal window open here but because we just installed this, I'm just going to go ahead and close this out so we have a fresh terminal to work with. Hit Command + spacebar to bring up the Spotlight search and type in terminal. So now that I have a clean terminal open, I can go ahead and type node -v, and we see that I'm running 6. 11. 5. So good to go here. There's something I'd like to note here though. While it was easy to go ahead and download 6. 11. 5, there is a great tool that I personally like to use called Node Version Manager. And if you go to Github at creationix/nvm, you can go ahead and see what Node Version Manager is all about. And the reason I like this is it's a really nice tool to be able to install different versions of Node. js and just set to working with that version. And so you can have Node version 6. 11 installed, as well as the latest LTS version, Node 8. 9. 1, and you can jump between the two. Now I will note that NVM only works on Mac and Linux. However, you can see here, if you're a Windows user, that there are a couple NVM Windows projects that you can install and use as well ahead. But I'm getting ahead of myself. Again, if you're getting more into advanced Node and you're really writing a lot of Cloud Functions, this is just an additional tool that will make your life a lot easier. Now that we have Node installed, let's go ahead and get our projects set up.

Setting up Your Node Project
So again, I'm back in the terminal here and I'm going to go and make a new directory. And we'll just call this Cloud Functions. I'm going to go into that directory and from here I'm going to set up our Node project. And all I need to do is npm init and hit the Enter key. What npm init does is it's a nice little tool that just goes ahead and initializes a basic project for you with Node. js. And so it's going to go through here and pretty much just hit Enter for all of these. If you ever want more detail, there are, again, so many Node. js courses on Pluralsight that go through all of these options in more detail. So again, I'm just going to hit Enter, our entryPoint can be index. js. And we'll just go ahead and say is this okay? Yes. All right, now our project is set up. So let's go ahead. And now you can see that we have our package. json file. Now where does the benefit come in for using this over the Cloud Function dashboard? Well, you're about to see right now. First, we're going to go ahead and create our own JavaScript file. So we'll create that index. js, but then we're also going to go ahead and install a popular dependency that I like to use when I'm dealing with dates and times and that's called moment. js. And so I can just come here and say npm install --save moment. And now this is going to go ahead and install Moment for us. But what I'd like to show you is, I'm just going to open Visual Studio Code here, just go ahead and open the folder we're working with. So going to go to desktop, jump up, and we had our Cloud Functions folder. Some I'm going to go ahead and open that. And now something you could see here is in the package. json, when we did that --save, it added the dependency moment and it grabbed the latest version for us and went ahead and added it here. While you can do that on your own, it's just so much nicer to work with node and npm versus the cloud console where you might have a typo, you might not grab the latest version because you're manually updating this package. json. So this is just one instance where it's just so much better to build these on your local machine. So there we go, we have our project set up now. So let's go ahead and start creating our functions.

Writing the Functions
So we've set up our project and we're now going to start coding. First, to change things up a bit, we are going to use moment. js for these examples just to add a little variety from Google's standard templates in Google Cloud. So here I'm going to go ahead and just write my first function. So I'm go exports. helloHTTP. We'll function, we'll just keep it the same, why not? And do request response. Now this is really easy to do, right? We just (mumbles) wrote our HTTP trigger and now we have to be able to respond to this object, right? So we're just going to go ahead and send a 200 response and we'll just send the date and time using Moment. And we'll format it so I want the month, day, year, hour, minute, seconds and a. m., pm And there you go, we wrote a function just right then and there. It was that quick and easy. And we can go ahead and do the same as for the others. So we'll do psHelloPubSub. And now if you remember these functions had different signatures, right? We had the event and we had callback. And the event triggers and we have to make sure that we call that callback so we don't get charged beyond that timeout time, right? If we don't call callback, it's going to be 60 seconds, which is going to be our default timeout time. Now just the same with the HTTP, we're not going to go ahead and use anything crazy and complex from the event. We'll just go ahead and log this out. And we even have the latest ES6 features so I can go ahead and use the latest formatting here to call this function. And then I'm going to call my callback, right? Because again, I don't want that 60 seconds timeout. This is going to run it in milliseconds, not seconds. Perfect, we have two here. And now you'll note really quick that here I got the psHelloPubSub which is the naming format we have been using throughout the course, and here I'm just using helloHTTP. And I promise, there is a reason for it, which we'll get to when we deploy these functions. And then finally we have the storage. And in fact, I can just go ahead, copy and paste this one. We can do HelloStorage, HelloStorage, event callback and we will update the console. log here. And there you have it, we just wrote three functions. Now really quick, this is the most important part. It's the exports in the index. js file that are important. When we are using G Cloud to deploy our functions, it is looking for these export statements. And so it's crucial that you have those. If I were to go here and write another random function and then try to deploy this using the Google Cloud tools, it's not going to see it and it's going to throw an error. So that is the significance of this. And another great part about this too is we're not limited to just this index. js file, right? In the Google Cloud dashboard, we could've made extra files as well but this project is going to grow. And so here I can make a new date format or file and we can have exports date equals function. We can even use an anonymous function here. We'll call this formatted date. We'll bring in const moment equals require, moment. And now I can come here and just go ahead and grab this function, right? And return that as a string. And now instead of here, I can have dateFormatter. And if we're using our local files instead of node_module, we need to do the folder structure to that file, so that's why I have the. / here. Again, if you watch one of the other Node. js courses on Pluralsight, you'll learn more about that. And then what was that? Formatted date. Let's just call that format, and here we'll just call format, formatted date just go ahead and call that function here. So that's all there is to it, right? We've got our normal IDE, we've written our code and now let's go ahead and deploy these functions. So first we'll start with the HTTP function.

Deploying the HTTP Trigger
So I'm in terminal now and we're going to go ahead and deploy our functions. And now this is really easy to do. Again we have G Cloud beta functions and here we've called call, we've used logs but now we're going to use the keyword deploy. And again, if you need help you can always do the --help and there's a lot of incredible information here. And you can go through it and read it in detail. But for now, we're just going to concentrate on some key highlights here. So let's go ahead, clear this out. I'm going to do gcloud beta functions deploy and now I'm going to do psHelloHTTP. And you'll remember that this is what we've been naming the function throughout the course. However, that's not the name that's in index. JS. In index. JS, we have exports. helloHTTP. So here were able to go ahead and add a flag called entryPoint, --entry, -point. And here we can write our functions, name. And so what it's going to do is it's going to go to index. js and look for the exports. helloHTTP and it's going to map it to HTTPs on Google Cloud. It's that simple and that easy. And then there's one other important flag we want to set here and that's our staging bucket for the source code. And we're going to upload this to the same spot that we've been using throughout the whole course, our staging functions bucket, which is really our project name staging stash functions. And then finally, we have one last flag, and that's our trigger flag. And here we're doing --trigger-http. And you'll see what the PubSub and storage triggers are in a bit. So I'm going to go ahead and hit Enter, and now what it's going to do is it uploading it to Google Cloud. Okay, so it's copying the files, it's getting ready to deploy the function and now it's saying it might take a while, it could take up to two minutes. So we're going to let it do it's thing and I'll see you once it's deployed and ready to go. Great, so we now have our function deployed and this is really nice. So here we're getting a lot of the information that we already had. We have entryPoint helloHTTP. We have the HTTPS trigger that was generated as we deployed it, and you can see that it matches that psHelloHTTP. We have the name that was used, the status that it is active, the timeout, 60 seconds, version ID is one. So we're good to go. And now let's check it out quick in Google Cloud. So here I'm in the Google Cloud Functions and I'm going to go ahead and refresh this page. And you now see our HTTP trigger here. So congratulations, you've deployed your first function. Now just to check us out, why don't we call it quick and see everything is working okay? So I'm going to go and come back here, I'll clear this out and again, we'll use the gcloud beta functions call. We did this in the prior module. I'm going to do psHelloHTTP and we have our data flag here although I really don't need to send any data since we're just basically using Moment and just grabbing the current time and passing that as a response. So going to go ahead and hit Enter, let it run. Okay, great. So the function ran and here, we got our response back. It's December 4th, 2017 around 4:40 pm So that's perfect, right? Everything ran fine and the functions deployed. So now let's take a look at deploying PubSub in storage triggers.

Subscribing to the PubSub Trigger
So we're back at the terminal and we're going to go ahead and deploy PubSub and our storage triggers. Now if you remember from the prior modules that our PubSub topic is hello world. And right now we have zero subscriptions because we deleted the prior function, and now in terminal, what I'm going to do is going to go ahead and do gcloud beta functions deploy and this time we're going to call it psHelloPubSub. And now I can leave that entryPoint flag out because we're using the same name that's in the index. js file, it's the same mapping. And once again, I'm going to the staging bucket, which is the same for all of them. eight, one, zero, two, three, staging functions. And now I'm going to a new flag that we haven't used, the --trigger-topic, and I'm going to list our PubSub topic that we want to connect this function to and then hit Enter. And so now again this is going to go ahead, it's uploading our function, it's connecting it as a subscriber to our hello world PubSub topic. And once it fully uploads, we're going to be good to go. So I'm going to let this run and we'll see you back in a little bit. All right, we're back and as you can see, we have our new PubSub trigger. So the entryPoint psHelloPubSub. We can see that it's currently active, that this is version one and the time it was updated at. So this is cool. And if I go ahead back to our Google Cloud and I refresh this, we can see that we now have one subscriber for our PubSub topic. And if we go ahead to Cloud Functions, we should see two functions listed. And we do, perfect. And we see our new one, psHelloPubSub. So let's go ahead and call our PubSub trigger. So I'm going to go ahead and do gcloud beta functions call psHelloPubSub, and I'm going to pass the data flag and just basically pass an empty JSON object here like we did with the HTTP trigger since we're just accessing Moment directly. Hit Enter. Ooh, and the function crashed. Moment is not defined. So let's take a look. Ah, so there you go. In my rush to create this small little date formatter, I deleted the requirement for Moment and I'm still trying to call it here. So let's go ahead and we can replace that on both of these. So again, we have properly formatted code. And I can go ahead and redeploy these. So I'm going to go ahead and do that and I'll meet you right back here. So we're back and we redeployed the PubSub function. Now you see here that was the same command, nothing changed there but this time we have an updated version two. So if I go ahead and try to call it again, we've got the execution ID. Perfect. And now let's take a quick look in Google Cloud. We could see that it executed. We see our error from before. Our crash, I should say. Let's view the logs. So here we could see the error from the first one that came through. And now if we scroll up, I should say scroll down. There we go, hello PubSub December 4th. So again, everything is working great here. And we have our PubSub trigger that's working as well. And now all that's left is deploying our storage trigger. So let's take care of that.

Connecting to Storage with the Bucket Trigger
We're now going to deploy our storage trigger, and this is very similar to the HTTP and PubSub but it is slightly different again because we have to hook everything up, right? So let's take a look at Google Cloud really quick and jump to storage. One of the things that we had to do when creating our function was connecting it to a bucket and we were using this function's production storage bucket here. And so we can just go ahead and reuse this in our command line. So it starts out the same way, the first four words, gcloud beta functions deploy, and now we have psHelloStorage. And with that, we do our staging bucket one more time, and this hasn't changed from the last two videos. 181023 staging functions. And then we have another trigger type here, and that's going to be the trigger-bucket. And now we're going to list our other bucket here, not our staging functions bucket but the one we want to tie this event to. It'd be functions production. And hit Enter. So now this is going to go ahead and upload it for us and it's going to create that storage trigger and it's going to make sure that the events occur when we upload to that function's production storage bucket. So we're going to let this upload and I'll see you once it's done. All right, we are back. And now we've reviewed this before with the HTTP and the PubSub so I'll go through this really fast. But again, we have our entryPoint psHelloStorage. We have the project that it's tied to, we have that it's active, we have the timeout and the version ID so we are good to go. So this is really cool. And if I jump back to Google Cloud and we look at the Functions page, we can see our psHelloStorage trigger here. And now what I'm going to do here is instead of using gcloud to call it, let's go ahead and just create a small file to pass in. So I created a small text file that just does writing some text, and let's go to storage, let's go to our bucket here, upload file, go up to my home folder, cloudfunctions, and here's our text file. Open. So it uploaded it and now this should trigger the function. So if I go back to Cloud Functions, psHelloStorage and it hasn't invoked yet but it should here in a minute so let's go over to the logs, hit the Play button and here we go. Hello Storage December 4th, 2017 5:03 pm So our storage function is working as well. And if we were to upload this to some other bucket, it would not have triggered this function. So congratulations, this is awesome. You programmed all three trigger types locally with three different functions and you deployed them using the Google Cloud SDK and the gcloud command line interface tool. Congratulations.

Summary
We went over a lot of great stuff in this module. First, we installed Node. js and we made sure we got the right version, we went to the Google Cloud Function's product page and made sure we were able to see what current version of Node. js that they were running so we could download that appropriate version. We then used Node Package Manager, npm, and created our first Node. js project so we could build our functions locally. We then installed Moment. js, a really popular library for working with date and time in JavaScript. And we were able to leverage this and write functions that formatted the current date and time for us and either printed it out or sent it as a response in our HTTP trigger. And then finally, we deployed all three functions with GCloud and you were able to see how easy it is. And you got to think about one of the nicest aspects with that, is the fact that we were able to just go ahead and script each function and send it out separately. And the nice thing about that is, is you can move a lot faster that way, you can write a function and push it out the door and you don't have to worry about some small bug taking down every instance of your app, right, because these are small individual micro services and that way you are able to isolate potential disasters within your code base. Of course there are upsides and downsides to micro services in general, but if you look at from the perspective of that, it's very forgiving. Well, this is the end of the course, congratulations, you have learned so much. First, you learned about the different trigger types associated with Google Cloud functions and then you got a deep understanding of the Functions lifecycle, how they're created, how do you trigger them, the event data that comes in with them and finally, how do you end them properly. Plus you learned how really Google is managing the scaling for you, which is awesome. This lets you concentrate on the code and the business objectives that you're trying to achieve for your customers. You then went and created all the different function types, and you did this two different ways. We did it through the Google Cloud dashboard where we set up our PubSub triggers. We created Google Cloud storage buckets and then went and created our functions using the standard templates that Google Cloud provides. But then later on in that course, you went ahead and did it on your local machine and deployed them directly to the Google Cloud Platform. You also get a deep understanding of the information that's available to you when dealing with cloud functions. And we did this by navigating the Google Cloud Dashboard and looking at the logs and the charts that are provided so you can see how many invocations have occurred, if there were any errors or crashes. And that's just skimming the surface of the information that we covered. And then finally, one of the most important aspects of the course in my opinion is you leveraged the GCloud CLI tool. With this, you were able to pull in logs, you were able to deploy your functions from the local machine. And while we only skimmed the surface of this powerful tool, don't forget that help option and the incredible amount of detail that is provided to you by Google. So now that you have the basic understanding of building the small cloud functions, I encourage you to go out and explore what they're truly capable of. There are so many things that we didn't do in this course. You can bring in promises and then call third-party services outside the Google Cloud Platform. Google has some really cool AI services like vision and speech APIs that you can leverage in your own projects within these cloud functions. And just have fun. One of the things I love the most about Cloud Functions was the ability to just connect to so many different systems. It allows you to experiment. You can create these quickly and they're cheap to operate so you can find out if it works for you and it's not going to cost you much. So again, congratulations. I'll see you soon.

Course author
Author: James Wilson	
James Wilson
As a mobile developer, James always had a passion for building exciting apps and always striving to make user interfaces that were easy and intuitive to use. Today, he now works at Pendo where he...

Course info
Level
Beginner
Rating
4.4 stars with 22 raters(22)
My rating
null stars

Duration
2h 27m
Released
20 Dec 2017
Share course

