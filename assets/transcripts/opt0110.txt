Leveraging Fully Managed Redis Datastores Using Google Cloud Memorystore
by Vitthal Srinivasan

Cloud Memorystore is a new addition to the Google Cloud Platform. It provides a fully managed, cloud-hosted Redis service, allowing you to cache common responses on GCP-hosted web applications, providing your users low latency and high performance.

Due to its in-memory nature, Memorystore features some of the lowest latencies on the platform, down to sub-millisecond levels. This managed-Redis service is hosted on Google’s highly scalable infrastructure, which means that it can support instances up to 300 GB and network throughput of 12 Gbps. Memorystore offers an easy migration path for users of Redis, a technology that is fast gaining popularity, especially for use from within Docker containers running on Kubernetes. In this course, Leveraging Fully Managed Redis Datastores Using Google Cloud Memorystore, you'll examine all of these aspects of working with Memorystore, and learn how to get the best out of this powerful managed database service. First, you will explore the suite of storage products that are available on the GCP and where exactly Memorystore fits in. You will be introduced to the capabilities of using Redis to cache data for transactions, and as a publisher-subscriber message delivery system, and you will learn about the LRU eviction policies that Memorystore follows. Next, you will implement Memorystore integrations with applications that you host on Compute Engine VMs, App Engine, and on Google Kubernetes Engine clusters. These are the current options that the GCP supports for working with managed Redis. Finally, you will dive into how you can configure Memorystore for high-availability configurations. Memorystore offers two Redis tiers: basic tier and standard tier instances. Basic tier instances do not support cross-zone replication and failover, while standard tier applications are equipped with both features. In addition, the standard tier offers far lower downtime during scaling. You’ll also see how you can monitor Redis instances using Stackdriver. When you’re done with this course, you will have a good understanding of how you can use Memorystore to cache your data on the cloud and know how you can integrate managed Redis with your applications running on various compute options on the GCP.

Course author
Author: Vitthal Srinivasan	
Vitthal Srinivasan
Vitthal has spent a lot of his life studying - he holds Masters Degrees in Math and Electrical Engineering from Stanford, an MBA from INSEAD, and a Bachelors Degree in Computer Engineering from...

Course info
Level
Beginner
Rating
0 stars with 4 raters
My rating
null stars

Duration
1h 31m
Released
11 Jan 2019
Share course

Course Overview
Course Overview
Hi! My name is Vitthal Srinivasan, and I'd like to welcome you to this course on Leveraging Fully Managed Redis Datastores Using Google Cloud Memorystore. A little bit about myself. I have master's degrees in Financial Math and Electrical Engineering from Stanford University and have previously worked in companies such as Google in Singapore and Credit Suisse in New York. I am now co-founder at Loonycorn, a studio for high-quality video content based in Bangalore, India. Due to its in-memory nature, Memorystore features some of the lowest latencies on the platform down to the level of sub-milliseconds. This managed Redis service is hosted on Google's highly scalable infrastructure, which means that it can support instances of up to 300 GB and network throughput of 12 gbps. Memorystore offers an easy migration path for Redis users. That's a technology that is fast gaining popularity, especially for use from within Docker containers running on Kubernetes clusters. This course covers all of these aspects of working with Memorystore and explains how to get the best out of this powerful managed database service. First, you will understand the suite of storage products that are available on the GCP and know exactly where Memorystore fits in. You will be introduced to the capabilities of Redis for caching data, for transactions, as a publisher/subscriber message delivery system, and you will also understand the LRU eviction policies that Memorystore follows. Next, you will implement Memorystore integrations with applications hosted on Compute Engine VM instances, App Engine, and the Google Kubernetes Engine clusters. These are the current compute options on the GCP which support working with managed Redis. Finally, you'll understand how you can configure Memorystore for high-availability configurations. Memorystore offers two Redis tiers, basic and standard. Instances belong to one of these two tiers. Basic tier instances do not support cross-zone replication and failover, while standard tier applications are equipped with both these features. In addition, the standard tier offers far lower downtime during scaling. You'll also see how you can monitor Redis instances using Stackdriver. When you're done with this course, you'll have a good understanding of how you can use Memorystore to cache your data on the cloud and know how you can integrate managed Redis with your applications running on various compute options on the GCP.

Creating and Managing Redis Instances
Module Overview
Hello, and welcome to this course on Leveraging Fully Managed Redis Datastores Using Google Cloud Memorystore. This module starts with an introduction to Redis. You may or may not be aware that Redis is an in-memory key-value store. Well, technically Redis is just a little bit more than a key-value store, but that's really its most popular use case. And Redis is indeed extremely fast and extremely popular. It often comes in at the top of rankings of key-value stores and NoSQL database technologies, and Memorystore is now the Google Cloud Platform's managed Redis service. Memorystore offers two tiers, basic and standard, and using the standard tier, it's possible to get a combination of high availability and on-the-fly scaling. Now remember that Redis is an open-source technology. Memorystore is fully compliant to the Redis protocol, but its implementation details differ quite a bit. For instance, it does not offer persistence, and it does not rely on Sentinel for high availability.

Prerequisites and Course Outline
Before we jump into the concepts and practice of Memorystore, let's spend a moment discussing the prerequisites and an outline of this course. Redis is a simple technology, and this is a simple course, so you should not require very heavy prior exposure to cloud computing. However, it would certainly help if you have had some exposure to the Google Cloud Platform. Just in case you don't, here are a couple of courses from the Pluralsight catalog which give you that exposure, Architecting Scalable Web Applications Using Google App Engine and Choosing and Implementing Google Cloud Compute Solutions. Let's quickly outline the material coming up ahead. We start with a look at the concepts behind both Redis and Memorystore and the nitty-gritty involved in creating and managing instances. Then we move on to the interplay between Memorystore and other GCP compute solutions such as Kubernetes, App Engine, and, of course, just Compute Engine. And we round out the course with a detailed look at the exact mechanics of scaling and failover in Redis. A quick word about all of the demos in this course. They are based on scenarios located in a hypothetical e-commerce retailer called SpikeySales. SpikeySales specializes in flash sales of trending products, deals-of-the-day types of promotions, and these are given to spikes in user traffic. Cloud computing is a natural fit for this kind of business model because it allows SpikeySales to pay as it goes. It does not have to grow provision capacity, which stays idle during off-sale periods. In this context, Redis and Memorystore both have a really important role to play. It also makes sense for SpikeySales because during their peak periods, they require really fast web serving, and Redis provides exactly that. Redis offers millisecond-level latencies on lookups, and Memorystore provides an easy migration path into the GCP, as well as high availability and ease of scaling.

Introducing Redis
In this video, we'll spend some time talking about and understanding Redis. Redis is a very popular, in-memory, key-value NoSQL database. We'll come back to every part of this definition in a moment, but before that, let's move on to Cloud Memorystore. This is the Google managed service for Redis, and it offers scaling, high availability, and a convenient migration path if you're using Redis on-premises. Let's now come back to Redis and understand the different aspects of its definition. The first thing we're paying attention to is the adjective popular. There's a good reason why the GCP has gone ahead and launched a managed Redis service and not, say, a managed MongoDB or CouchDB service. Redis is consistently ranked as one of the most popular NoSQL technologies out there. For instance, in the db-engines rankings, it's the most popular key-value store two years running. And in case you're wondering what's at number two, the answer is Amazon's DynamoDB. And a big driver for the increase in Redis's popularity recently has been how easy it is to use Redis from within Docker containers. Redis ranks only after Nginx as the most commonly used technology accessed from within Docker, so as Docker and Kubernetes explode in popularity, Redis is increasing along with them. Next, let's turn to the in-memory nature of Redis. So typically Redis is going to hold the entire dataset in memory. It is also possible to turn on Redis persistence. Open-source Redis offers two different mechanisms in which the contents of the dataset can be written to file at periodic intervals. It's worth mentioning here that an important limitation of Google Cloud Memorystore is that it currently does not support persistence at all. Now this is much less of a dealbreaker than you might imagine, and that's because we are running Redis on the cloud in Memorystore, and Memorystore offers high availability and replication, which make persistence somewhat unnecessary. Another defining characteristic of Redis is its use as a key-value store. So unlike relational databases or document databases or any other more complex kind of database, in Redis we always associate keys with values. Now technically Redis is more than just a plain old key-value store because the term key-value store is often used to refer to values which can only be strings. So Redis is actually a data structure server, and the values could be data structures such as lists, sets, and so on. However for all intents and purposes, Redis is considered a key-value store. This is a fine technical difference which not a lot of folks care about. Redis is most definitely a NoSQL technology. It has its own simple commands. There are about 55 right now. These include commands such as GET, SET, DEL, and so on, and you can find an exhaustive list on the Redis documentation URL. Redis definitely has additional functionality beyond being a key-value store including transactions, support for a publisher/subscriber model. Note that this has nothing to do with Google Pub/Sub. It also offers scripting in a language known as Lua. This has an interpreter built in to Redis. We can specify keys which have a limited time to live, and Redis will eliminate them once they have expired. It's also possible to use Redis as an LRU or least recently used cache. And, finally, even in open-source mode, Redis has automatic failover. As we shall see, the open-source version of automatic failover is quite different from the Cloud Memorystore version, and the Cloud Memorystore version is in many ways much more robust. These features of Redis are actually quite important to getting the most out of it, so let's quickly go ahead and discuss them. Let's start by talking about transactions in Redis. The mechanics of these transactions are pretty simple. You use the MULTI command to signal the start of a transaction. All commands which follow are then queued. They are not actually executed until we issue an EXEC command. That EXEC command will trigger the execution of all of the queued commands. In the interim, if we wish to abort the transaction, we'll type in a DISCARD command, which will simply abandon all of the contents of the queue. Now these mechanics are simple enough, but you will be right to wonder whether transactions can really guarantee the ACID properties even in a purely in-memory database. And you are right that your ability is not strictly guaranteed. Redis transactions are safe against connection drops, so let's say a client connection drops before the EXEC command is issued. All of the queue contents are lost, and so nothing is going to be executed. However, let's say that the Redis server crashes or the administrator does a hard restart. In such a situation, atomicity is actually not guaranteed. It is possible for some but not all of the queued commands to be executed. If atomicity is violated, Redis is smart enough to detect this on restart, and it will exit with an error. So at the very least, you will be notified that the transaction was only partially executed and that your system's integrity might be compromised. Redis also has a feature called Pub/Sub. This is a useful feature, but it should not be confused with the Google Pub/Sub technology. Pub/Sub on Redis also refers to the publisher/subscriber model. Redis has commands SUBSCRIBE, UNSUBSCRIBE, and PUBLISH. So messages are organized into channels. If you are familiar with Google Cloud Pub/Sub, channels are roughly analogous to topics. Publishers then go ahead and publish messages onto these channels. Subscribers subscribe to channels and, therefore, receive the messages. As with any Pub/Sub implementation, this decouples the senders from the receivers. One important note is that the subscribers do not know which publisher is sending a particular message. Redis's implementation of Pub/Sub is not reliable. This means that, let's say, a subscriber client goes down, all of the messages sent while the client was down are going to be lost. Again, I'd like to emphasize that Redis Pub/Sub is completely unrelated to Google Cloud Pub/Sub. If you'd like to implement reliable messaging on Redis, there's no great way to do so on Cloud Memorystore. However, in very recent versions of Redis, version 5.2 for instance, there is something known as Redis streams. These are a way in which to get reliable messaging. Redis streams are not supported on version 3.2, which is what's currently run by Cloud Memorystore.

Features of Redis
Let's keep going with our run-through of some of the cool features present in Redis. After transactions and Pub/Sub comes scripting. Redis offers a simple way to execute commands in a language called Lua. Lua is a lightweight interpreted language, and an interpreter is built in to Redis. So we can pass in arbitrary Lua syntax. This has to be specified using the EVAL or the EVALSHA commands, and then the Lua programs if we pass in as arguments are going to be executed in the context of the Redis server. The EVAL and EVALSHA commands also take in arguments. These arguments are going to be key names in the Redis context, and the key names will be translated into values, which will be made available to our Lua program. Redis also allows us to specify keys with a limited lifespan. These are known as limited time-to-live keys. TTL is an acronym for time to live. Effectively, we set a timeout on a key. Redis will monitor that timeout and expire the key-value pair when the timeout expires. Such keys are said to be volatile. Many operations performed on volatile keys are going to result in the key timeout being transferred. This includes rename operations. It is possible for us to convert a volatile key into a regular key using the PERSIST command. It is also possible for us to clear the timeout when we use commands such as DEL, SET, and other commands which actually replace the value associated with the key. Another handy feature is the ability to turn on LRU eviction. LRU is an acronym for least recently used. LRU eviction can be turned on in order to automatically evict old data once our instances run out of memory. So you've got to use this with the maxmemory directive. And as we shall see, this is one of just two Redis configuration properties which we can set in Cloud Memorystore. In general, Memorystore sets default values for Redis configuration properties, and we are not allowed to change those. There are two exceptions to this, and maxmemory is one of them. LRU is just one possible eviction algorithm. Let's understand very quickly how eviction works in Redis. Let's say a client runs a new command, and this results in more data being added to our instance. Redis will check the memory usage on the instance, and if it's greater than the maxmemory limit, Redis is going to have to decide which keys have to be evicted. This decision is going to happen according to the eviction policy. When we discuss Cloud Memorystore, we will see what the possible values for the eviction policy are and when they make sense to use. Let's round out this conversation with a quick look at automatic failover. Open-source Redis has an automatic failover feature which relies on a program known as Redis Sentinel, so we need to explicitly run our Redis server in Sentinel mode and also run an additional Sentinel program. Sentinel will monitor our server and initiate failover if things go wrong. Now one really important note, automatic failover is also present on Cloud Memorystore, but Memorystore does not use Sentinel. Instead, it makes use of its own cross-zone replication mechanism in order to provide more robust failover. This is a pretty cool feature of Cloud Memorystore relative to open-source Redis.

Cloud Memorystore for Redis
We've now understood a lot of the features of Redis. Let's now turn our attention to Cloud Memorystore. As we've already discussed, Cloud Memorystore is a Google managed service for Redis, which offers high availability, a smooth migration path, and ease of scaling. Memorystore brings with it many of the common advantages of cloud-based managed services. The first is ease of deployment. It's trivial for us to deploy a single instance. The second is high-availability because managed Redis has replication across zones, that's available in the standard tier, and we also have fast automatic failover. Finally, security is an important consideration for Redis users particularly because it offers telnet as a way to connect. The Google Memorystore service makes sure through a combination of firewall rules and other Google hardening that Redis is actually pretty secure. Memorystore also makes it really easy to scale on an as-needed basis. We can scale our Redis instances to a maximum of 300 GB, and GCP will also provide commensurate network throughput up to 12 Gbps. There are important differences in the way in which standard and basic tier instances scale. We'll discuss those in more detail. And, finally, when we use Memorystore, we get all of the benefits of the Stackdriver suite such as monitoring, logging, and so on. A couple of important notes. Billing, which happens for Redis instances, is on the basis of provisioned capacity. This is billing by the hour. Also the version of Redis that is provided by Memorystore is currently 3.2 .11. This is maintained by the GCP. This is another advantage of using a cloud managed service. Applying critical patches is not our responsibility. As we've already mentioned on a couple of occasions, Redis instances in Memorystore come in two service tiers, basic and standard. Standard tier instances offer high-availability using cross-zone replication. What's more, they can be scaled up and down without significant downtime. On the other hand, basic tier instances will experience downtime if they are scaled, and their cache is going to be flushed. We will come back to these differences in much more detail. But for now, let's just summarize them. The basic tier does not have any kind of replication, so the data resides in a single zone. Standard tier instances have masters and replicas, so there's a pair. The pair exists in different zones. Basic instances do not have any failover, so if there is a zone or an instance outage, our application is going to go down. Standard tier instances do, indeed, have failover replicas, and in this way they implement high-availability. Both basic and standard tier instances can be scaled. However, basic tier instances do not perform very well on scaling because there is a blocking period when read and write calls do not go through. Also, when the new scaled up version comes back online, it's cache has been completely flushed. Standard tier instances, on the other hand, bring up the larger instance before switching over. As a result, they have far smaller downtime. Because of these differences, basic tier instances tend to be used for pure caching use cases. If you'd like to use Redis as a database and not merely as a cache, then it makes sense for you to go with a standard tier instance because of the persistence. Note that the persistence in a standard tier instance is not obtained using the open-source Redis mechanisms. Rather, we simply get it because of the cross-zone replication.

Memorystore vs. Other GCP Storage Services
Redis is a very unusual technology. It's not quite a storage technology. It's not quite a pure caching key-value service. It's some combination of the two. It's also pretty new to the GCP, so it's worth our while understanding where exactly it fits in relative to other GCP storage services. In order to orient ourselves, let's first draw out a taxonomy of the different storage options. The first decision parameter is whether our data is structured or unstructured. If it's unstructured, then we might either place it in cloud storage buckets, which are globally available and infinitely scaling in size, or we could place them on persistent disks for ease of access from within compute. If, on the other hand, our data is structured and it adheres to clear schemas and specifications, then we've got to decide on how the data is going to be accessed. If it's mostly going to be accessed by human users, it probably makes sense for us to go with a SQL-based technology. If, on the other hand, our data is mostly meant for programmatic access, such as from within other web applications, NoSQL is an excellent choice. Within the world of SQL technologies, there are both transaction processing and analytical processing use cases. For transaction processing, we need ACID support, and the GCP has Cloud SQL and Cloud Spanner, both of which provide all of the ACID properties. Just as an aside, ACID is an acronym for atomicity, consistency, isolation, and durability. The ACID guarantees provided by full-scale relational databases like Cloud SQL or Cloud Spanner are ironclad. They are much stronger than what we get using, say, Redis transactions. If, on the other hand, we are most interested in analytical queries, such as data warehousing applications might provide, BigQuery is the way to go on the GCP. Let's now turn our attention to the NoSQL fork in the tree. The current GCP offerings there are BigTable and Datastore. Datastore is in the process of being rebranded and merged into Firestore, but basically the idea is the same. BigTable for very fast, low latency reads and writes on truly huge datasets, Datastore for hierarchical document-oriented data. As we've already discussed, Redis and Memorystore do not conveniently fit into this classification. However, if we did have to pick, the most likely spot here for Memorystore would be somewhere close to Datastore. And here we are mostly referring to Memorystore used as a database and not merely as a cache. Let's quickly go ahead and compare Memorystore and Datastore. Memorystore offers a maximum of 300 GB of data. Datastore, on the other hand, can easily scale into the terabyte range. Memorystore makes use of a server that's a Redis instance. Datastore is serverless. Scaling in Memorystore is manual, and it has some very real limitations. Datastore's scaling is automatic and quite seamless. Redis is for all intents and purposes a key-value server. More generally it belongs to the category known as data structure servers. Datastore, on the other hand, supports document object data and fairly complex data models. Memorystore is ideal for key-based lookup. Datastore supports far more complex hierarchical queries. The latency in Memorystore is really, really low because of its in-memory nature. This is on the order of sub-milliseconds. Datastore has latency on the order of milliseconds, so that's pretty fast as well. Perhaps the most important of all of these differences is the last one. Memorystore runs Redis. Redis is free of platform lock-in, and, what's more, it's extremely popular. It's a technology on the rise. Datastore, on the other hand, is very GCP-specific, and frankly Datastore's adoption has probably not quite lived up to GCP's expectations. So Memorystore is a technology with a very bright future. Datastore is probably in the process of being folded into Firestore.

Memorystore Limitations
Let's spend some time understanding Memorystore's limitations. Perhaps the most real of these has to do with Redis parameters. On Memorystore, most Redis parameters are preconfigured, and the user cannot change them. There are just two exceptions to this. These are the maxmemory-policy directive and the notify-keyspace-events directive. Let's go ahead and understand each one of these in turn. The maxmemory-policy directive basically serves to define the eviction policy. This is something we had discussed in the context of LRU eviction. When a client runs a new command which results in adding memory into our instance, Redis is going to check the memory usage, and if it is greater than the maxmemory limit, then the maxmemory-policy comes into play. There are many possible eviction policies which we could go with starting with a noeviction policy. Here once the memory limit has been reached, our instance is going to throw an error if we try to add any additional data. If our queries follow a power law distribution, in other words, if we have some keys which are widely popular and referenced all the time, then we probably ought to go with something called allkeys-lru. This is going to remove the less recently used or LRU keys first in order to make space. If we have a habit of marking the less commonly used keys as volatile, we could go with an eviction policy of volatile-lru. This would restrict the LRU eviction only to the ones which have an associated timeout. Then there is the allkeys-random policy. This is going to take a key at random for eviction, and it makes sense if we have a uniform usage of keys within a dataset. We could restrict the random eviction merely to volatile keys. To do so, we go with the volatile-random eviction policy. Finally, there is also a really interesting one called volatile-ttl. This is going to evict keys which have the shortest time to live or TTL first. Let's now turn our attention to the other configuration policy which users can actually control with Memorystore. This one has to do with keyspace notifications. This is a configuration property which is by default set to blank. However, we need to enable it if we would like to make use of the Redis Pub/Sub feature. To enable Pub/Sub on Redis, we've got to set the notify keyspace events property to a string, which combines different characters from this table onscreen now. In particular, the first character of the string needs to be either K or E. K stands for keyspace events, E stands for keyevent events. If we use K, our notifications are going to contain the name of the event. If we you E, our notifications are going to contain the name of the key. Memorystore also has some other limitations, notably that it does not currently support persistence. As we have discussed, this is not that much of a dealbreaker because we can get high availability in Memorystore using cross-zone replication. In any case, the open-source Redis persistence mechanisms, which are RDB and AOF, are not supported by Memorystore. And, finally, there are several Redis commands which are blocked. We will not talk about these in detail, but you can get a full list of the blocked commands which are not available in Memorystore from the Google docs.

Create a Memorystore Instance
Let's go ahead and get started with our first demo in which we will create our first Memorystore instance. We begin from the Google Cloud Platform. The project that we are in is called spikey-redis. We can see the project name over up top. It is important that billing be enabled for this project. We can tell this from the Billing card in the middle right. Other important information about the project such as the project ID is in the top left card. Down below, we also have the various resources currently in use. Let's click on the three horizontal lines over on the top left. These are called the Navigation menu toolbar, or the hamburger, and they are the most convenient way to navigate around the different services on the Google Cloud Platform. The first thing we need to do is to enable the required APIs, so we find the APIs and Services section and click on the Library tab. We are taken to a search page. There we type Google Cloud Memorystore or any variant of that, and that in turn selects just the one result, the Google Cloud Memorystore for Redis API. Let's click on that and then go ahead and enable using the big blue button. This takes us into the Overview section where we can confirm that the activation status of the Memorystore API is enabled. We are now ready to go ahead and create our first instance. So we go back into the Navigation menu, search for Memorystore, and once we click there, we are prompted to create our first instance. We click through and enter an instance ID. Let's call this spikey-test-cache. Note that the version of Redis currently supported by Google Cloud Memorystore is 3.2. Next, perhaps the most important choice related to creating a Redis instance, this is the instance tier. There are two tiers, basic and standard. The basic tier is obviously cheaper but does not provide high availability and is a strictly zonal resource. The standard tier offers instances which are far more robust because they have cross-zonal replication, as well as high availability. They are also much more expensive. Moving on, we can see that this basic tier configuration with 1 GB of memory is going to set us back by around $36 a month. That works out to around $ 0.049 /GB/hr, which is really quite expensive when you consider that a cross-regional bucket costs $ 0.026 /GB/month. But then, of course, this is talking about in-memory storage and not persistent disk. Moving on, we need to specify a region and a zone. So let's go ahead and pick the region as being us-central1. We could optionally pick a zone within this region, or we could just go ahead and leave it with the default value of Any. Remember that basic tier instances are zonal resources. They do not have cross-zone replication, so if we are going to be using Memorystore from, say, a GCE compute engine instance, we would like to keep the data as close as possible ideally in the same zone. Moving on, we have a slider there for the instance capacity. This refers to the memory provisioned for Redis usage. The default value is 1 GB, and this is what costs $ 35.77 up above. Notice that the network throughput supported is also a function of the memory, so at 1 GB for a basic tier instance, the GCP will support network throughput of up to 375 Mbps. Let's modify the value on that slider. So let's say we increase it to 126 GB. That now leads to an estimated cost of $1471 a month. Do notice, however, that the price per GB has dropped significantly from $ 0.049 /GB/hr to just $ 0.016 /GB/hr. Increasing the instance capacity also increases the supported network throughput. The maximum for basic tier instances is currently 1.5 Gbps. If we take the slider all the way to the maximum of 300 GB, that is going to set us back by around $3500 a month. Note that the per-gigabyte rate stays constant at around $ 0.016 /GB/hr. Please note also that these are specific to each region, so if we change the region from us-central1 to, let's say, asia-east1, the price of the instance is going to go up, and that's because memory in that region costs $ 0.019 /GB/hr. And we can see that by choosing different values, the cost of our instance can vary quite a bit. That's quite a bit more than we can afford, so let's just go back to the default value, that was us-central1, and just the 1 GB of instance capacity. This also gets the supported network throughput back down to 375 Mbps. The next item on the agenda is picking the authorized network on which our Redis instance is going to reside. Now this is a much more important choice than we might imagine at first. The whole point of using an in-memory store like Redis is to get latencies down to the sub-millisecond range. And once we are at those low latencies, it matters whether we are on the same VPC network or not. So we should be sure to keep our compute very close to our storage if we are going with Cloud Memorystore.

Create Redis Instances from the Command Line
We are still in the process of configuring our first instance. In our example, it's fine to just go with the default VPC as the authorized network. Next, we get to the Redis configuration section. An unmanaged Redis instance would have a redis.conf configuration file. This would contain a number of directives. It turns out that because we are using the managed Memorystore Redis version here, there are only two of those many Redis parameters which we can tweak. Those are the maxmemory-policy and the notify-keyspace-events. There are a large number of additional Redis parameters which we cannot tweak. Those will always simply take default values provided by the GCP Memorystore service. We go ahead and set a value for the maxmemory-policy. This is going to determine how our instance behaves when the maxmemory limit or the instance capacity is reached. We can see that there are several possible policies that we could go with. Here we will just go with the noeviction policy. This will result in an error being thrown when the instance capacity is reached. To be precise, this policy comes into play when we have reached the instance capacity, and a new command is run which seeks to go ahead and add more data. The next configuration parameter we can specify is the notify-keyspace-events. This is something which we need to enable in order to make use of the Pub/Sub functionality within Redis. Just to be clear, Pub/Sub in this context simply refers to publisher and subscriber and not to the GCP Pub/Sub service. We'll have a lot more to say on this. For now, let's simply set the value to be Ks. This is going to enable one of the two types of Pub/Sub notifications on Redis. K is going to allow keyspace events, and s is for set commands. If we had omitted a value, this would have disabled Pub/Sub notifications. We now are finally ready to go ahead and create our first Redis instance. It comes into existence. It's called spikey-test-cache. Notice that it has an IP address, a port number, and it has the instance capacity of 1 GB. We should be aware that this IP address is an internal IP address, that is, it is an RFC1918 IP address. This will only be accessible from within this project and this VPC network. The link for the instance ID is a clickable one. So if we click through into it, there's a dashboard up top to keep tabs on one of many important properties. By default this is measuring memory usage versus the maximum of 1 GB. If we scroll down, we can see the instance properties such as the Redis version, the authorized network, and the maxmemory policy, which is noeviction. Also we can find the connection properties, which include the IP address and the port number. Let's scroll back up and customize the dashboard. Notice as an aside that we are currently using 3.58 MB out of the maximum allowable instance capacity of 1 GB. In the drop-down, we can measure a different metric. We can have the cache hit ratio, the number of calls, the number of keys in the database, information about the evicted and expired keys, instance uptime, the number of connected and blocked clients, and the CPU utilization. Most of these graphs will currently be empty because we don't have any data inside our instance, so the cache hit ratio or the number of calls, these simply say that there is no data for the chart. We are now ready to connect to our instance, so let's go ahead and activate Cloud Shell. Cloud Shell is a browser-based terminal window, which is hosted on a GCP VM instance. And to activate it, we click on the little icon over on the top right. Cloud Shell opens up, and we launch it into a separate browser window. We then change our prompt. You can see that the default Cloud Shell prompt includes both the signed-in user, as well as the name of the project. Let's just change it so it occupies a lot less screen real estate. To do this, we export the PS1 environment variable, and we are now ready to run our first Redis-related command. Here we are going to go ahead and create yet another Redis instance using the gcloud utility. The command we run is gcloud redis instances create followed by the name of the instance and the various properties. Here size refers to the instance capacity in gigabytes. We also need to specify the tier, which here we have chosen to be standard. We run this, and it brings our instance into existence. If we now switch back into the web console, we will see that we now have two instances there. Notice how the spikey-prod-cache is of standard tier. Let's switch back to the Cloud Shell window and run a describe command. So this is gcloud redis instances describe followed by the name of our instance. All of the configuration information is onscreen now. We can see that because this is a regional instance and not a zonal one, there is also an alternative data location. This is going to be used for the cross-zone redundancy. We can also list all of the Memorystore Redis instances using this command. Because we are filtering by the region, there is only the one instance, the spikey-test-cache in us-central1. If we rerun the command with the other region, that's us-east1, we get our spikey-prod-cache as well. Let's also demonstrate the use of gcloud to delete an instance. So we run gcloud redis instances delete. When we run this command, it takes quite a while even after we've given our confirmation. So we wait for a considerable length of time, and eventually the deletion is successful. If we now switch back into the web console and refresh the list of instances, we can see that we have just the one instance in existence, that's the spikey-prod-cache.

Connect to Redis Using Telnet
Let us now go ahead and connect to our Memorystore Redis instance using telnet. Let's pick up the action from the web console. We are going to create a VM instance because we have to be on the same VPC network in order to be able to use the internal IP address of our Redis instance. So we click on the Create button in order to add a new VM instance. We specify a VM name. Let's call this spikey-vm. And then we've got to specify the region and the zone. It's very important for us to select the exact same region, so this is us-east1. Let's also go ahead and pick a zone. Ideally this should be the same zone as our primary zone for our instance, and that's why we pick us-east1-b. We then go ahead and specify the machine type, as well as the boot disk image. Neither of these matters too much for our particular use case. Let's just go with a small machine, 1 vCPU, and Debian 9/Linux. The next important bit is this firewall configuration. We've got to allow both HTTP and HTTPS traffic for our connection to work successfully. At this point, we can go ahead and create this VM instance. Once it comes into existence, we can SSH into it. And the first command that we are going to run there is sudo apt-get install telnet. Telnet is a protocol which is used for interactive text-based communication, and when we run this command, we are installing the telnet client. Now this is a slightly controversial operation. Telnet is not used a lot these days. It's actually been out-of-favor for several years, even decades, because it's not very secure. Redis offers a telnet connectivity option. This is okay to use in this context because our Redis instance, as well as all of our VM hosts, lie within the same GCP network. In any case, this method of accessing Redis is also present in the Google docs, so let's go ahead and study it. We need to copy the IP address of our telnet instance, as well as the port on which it's listening. We can then switch back to Cloud Shell and run the telnet client with both of these parameters. This opens up a telnet connection with our Memorystore Redis instance. And now we can interact with Redis just as usual. Let's start with the customary PING, and the response that we get back is PONG. If we try to retrieve a key which we have not yet set, we get back a -1. Let's set a key-value pair that's PS and Pluralsight. And if we now go ahead and get this key, we find that Redis works exactly as we would expect it to. We have successfully created a Redis instance and then connected to it using telnet.

Connect to Redis Using CLI
There are two standard ways of connecting directly to a Redis instance. One of these is using telnet, and the other, and the recommended one, is using the Redis CLI. We've already seen how to use Redis. Let's now try out the other method. Let's pick up the action from an SSH connection into our VM instance. The first thing that we need to do is to run sudo apt-get update. This is to get the latest version of apt-get, which we are going to need in a moment to install Redis server. So the next command we run is sudo apt-get install redis-server. You might now be wondering why we are installing Redis server when, after all, we actually need a Redis client. And the answer to that is that the Redis server package comes along with a Redis CLI. The CLI, a command line utility, can then we used to connect to our Memorystore instance on its internal IP address. So once we've got Redis server installed, we switch back to the web console and copy the IP address, and then we connect to it using redis-cli -h followed by that private IP address. Again, please remember that this is an RFC1918 IP address, and that's why we have to be in the same VPC network. This opens up a terminal connection. Notice how it has both the IP address and the port, which here is the standard 6379. Let's start with the customary PING greeting, and we get back the message PONG. Next, let's go ahead and use the SET command to set the key visitor to have the value 1. SET is pretty much the simplest Redis command. If a key already holds a value, it's going to be overwritten. This command also has variants in which we can specify an expiry time and whether or not to override an existing key. Next, let's go ahead and use the MULTI command. This is going to start a transaction. All of the commands which follow this keyword MULTI until the next EXEC command are together going to point to a transaction and be executed in an all-or-nothing fashion. So having initiated the transaction using the MULTI command, we then increment the value of the physical variable twice and, finally, invoke EXEC so that at the end of this, our visitor has the value 3. Note that when we next run GET visitor, we get the string value 3 because Redis does not have a dedicated integer type. Also note how the commands to increment the value of visitor were both queued, and they were only executed when we finally invoked EXEC after them. Let's also demonstrate the publisher/subscriber functionality that's offered by Redis. For this let's switch back into the web console and open up yet another SSH window into this VM instance. The most convenient way to view this is to have the two browser windows placed side by side so we have two Redis CLIs. So we invoke the same command in both of these SSH connections. Then in the window up top, we make use of the SUBSCRIBE command, SUBSCRIBE followed by the channel name, which here is just offers. When we run this command, the output is three elements in length. The first is subscribe, which is just telling us what command we ran. The second is a list of the channels that we subscribe to. Here there is just one, that's offers. And, finally, the number of channels that we subscribe to, which is 1. Next, let's go ahead and switch to the lower window. And here we will invoke the PUBLISH command followed by the channel on which we wish to publish the message, that's called offers, and, finally, the message body, which is "The winter sale is here. " And when we run this, Redis is going to send out this message to everyone who has subscribed on this channel. So over up top, we get a message, this is on the channel offers, and the message body. In this way, we can see that Redis has a publisher/subscriber mechanism. Now this is not reliable, but it does serve to decouple senders from receivers. Notice that the receiver, that's the subscriber up top, did not have any way of identifying which publisher the message came from. We've now come to the end of this first module in which we spent most of our time understanding the conceptual basics behind Redis. We saw how Redis is an in-memory, key-value store. Well, technically it's a little more than that. It's a data structure serving store, but for all intents and purposes, it's used for key-value serving applications. Redis is extremely fast because of its in-memory nature, and it's also very popular. It has emerged as one of the big winners from the recent increasing popularity of containers. Because of Redis's great popularity and ease of use, the Google Cloud Platform has now launched its own managed Redis service. That is Cloud Memorystore. Cloud Memorystore is fully Redis compliant. However, it relies on its own mechanism for various important internal aspects such as high availability. Memorystore offers two tiers of instances, basic and standard. Using the standard tier, it is possible for the user to get many of the benefits of working on the cloud, high availability and on-demand scaling. We will now turn our attention from concepts to implementation. And in the module coming up ahead, we will see how Redis and Memorystore can be accessed from App Engine, Compute Engine, and Kubernetes Engine.

Configuring and Using Cloud Memorystore
Module Overview
Hello, and welcome to this module on Configuring and Using Cloud Memorystore. This module is a fairly simple one. It's devoted to the use of Memorystore and Redis instances from the compute options on the GCP. These include Memorystore from Compute Engine VMs, as well as from the App Engine flexible environment. You should be aware that it's not possible to access Memorystore from App Engine standard or from Cloud Functions currently. We then move on to accessing Memorystore from within a Kubernetes cluster. This is an extremely important use case because Redis is really one of the most popular container-based technologies. Surveys have shown that Docker containers reference Redis more often than any other technology except Nginx. So clearly just like Nginx, Redis has been a big winner from the rise in Docker containers and Kubernetes usage.

Memorystore from GCP Compute
Next, let's turn to a discussion of Memorystore and the GCP compute options. Recall that on the GCP, we have four distinct flavors. These are cloud virtual machine instances using Compute Engine, Docker container clusters on Kubernetes Engine, App Engine as a platform for web-hosted applications, and, finally, Cloud Functions for lightweight trigger-driven compute. The first important point to be aware of is that it is not currently possible to use Memorystore from Cloud Functions or from the App Engine standard environment. It is, however, possible to use Memorystore from the other three compute options, namely the App Engine flexible environment, the Compute Engine VM instances, and clusters of Docker containers running on the Kubernetes Engine. The restrictions around App Engine standard and Cloud Functions have to do with the tightly sandboxed environments which those compute options rely on. Redis does not fit nicely into those tight sandboxes, and that's why we've got to go more towards the Infra a Service offering if we'd like to use Memorystore. There are also some additional restrictions that we need to be aware of whichever one of these options we wish to go with. Whether it's App Engine flexible or Compute Engine or the Kubernetes Engine, our client needs to be on the same network, that's the same VPC network, and within the same region as our Memorystore instance. It is, however, completely fine for the clients to be within the same region and within different subnets on the same VPC network. Even so, this is quite significant. It's really worth paying attention to. This is a pretty strong restriction, and if you are going to use Memorystore, you'll have to plan your GCP projects and networks carefully around this. In general, you need to use Memorystore from within the same project in which you instantiate it. You can, however, use Shared VPC provided the Redis instance is in the host project. You cannot use a Redis instance if it is in a service project.

Using Memorystore from a Web Application Hosted on GCE
In this example, we will build a simple HTTP web server. This is going to make use of a Redis instance. The server is going to be hosted on a Compute Engine VM instance. Remember that our VM instance has to be in the same region and the same VPC network as our Redis instance. We start from a Cloud Shell terminal where we make use of the gcloud command line utility to create a new VM instance. Notice that we are running the gcloud compute instances create command followed by the hostname and the various configuration properties. A couple of points worth noting. Our Redis instance has IP address 10.0 .0 .12 and has Redis port 6379. And that's why these are two values which we have passed in as metadata in our gcloud instantiation command. We run this command, and our VM instance comes into existence. It's called spikey-app-host, and it has both an internal and an external IP. Notice that while instantiating this IP, we had specified a tag of http-server. We're going to make use of that in the very next step where we create a firewall rule. This firewall rule is going to allow port 8080 access to all VM instances which are tagged as http-server. Notice how the source-ranges consists of all 0s. This allows incoming traffic from any IP address. This might actually be not very safe to do in a production environment. In any case, the command runs through to completion, and we now have a VPC network with the correctly configured firewall rule. We can verify this by using the Navigation menu, and there we find that our firewall rule called allow-http-server-8080 is, indeed, set up exactly as we wished it to. Let's now switch into the Compute Engine, VM instances tab. There we will find our spikey-app-host. We click on the SSH button over on the extreme right and use it to open an SSH session in our browser. Inside this SSH session, we are now going to go ahead and create our simple http-server app. Let's start by creating a directory for it and cd-ing into it. It's called spikey_counter_app. Next, let's go ahead and use nano, which is a free text editor available on all VM instances in Cloud Shell sessions, in order to create a requirements.txt file. This file is going to contain all of the Python packages, which are dependencies for our application. We have just the two dependencies, Flask and redis. Flask is a simple web server program, and redis is, of course, required for our Redis client. We can save the contents of this file by hitting Ctrl+X. That takes us back to our SSH terminal prompt, and we will now use nano to open up the main Python file of this application. Now we are going to go through this code in a lot of detail, and I feel Sublime Text is a more intuitive editor to go with. So let's switch over to the same code in a Sublime Text window. We start out the file with various imports. These include the logging, OS, and Redis packages, as well as the Flask module from the eponymous package. We then instantiate an instance of our Flask class. Notice how we've got to pass in the argument which corresponds to the modular package name. Here we are making use of the special built-in attribute called __name__, which always has the module name. We also have variables for the redis_host and the redis_port, and we finally instantiate the redis_client. In case you're wondering what that redis.StrictRedis is all about, that's because there are two client classes. One of them is called StrictRedis, which adheres to the standard. The other is just Redis. That's what you use if you need backward compatibility. Next, we have a couple of functions which respectively increment and decrement the value of a counter in our Redis server. Notice how each one of these functions is decorated using the @ app.route decoration. This is a way of telling Flask what URL is going to trigger the execution of the corresponding function. So, for instance, if someone hits our web server with the URL /decr, that is going to call this decrement function. The return values from these functions are going to go into the HTTP response. Next, we have a server error function. This is going to serve to log any exceptions, as well as to return a message saying that an internal error has occurred. Notice that this function also has a decorator, that's the @ app.errorhandler decorator, as well as the error code of 500. And, finally, we have the main section of this Python file. Here we simply invoke the run method on our app class. There's one important bit here. The hostname has got to be set to all 0s. This is so that we can hit this particular web server on the external IP address. If instead of the host of all 0s, we had gone with localhost, that's 127.0 .0 .1, we would not have been able to hit this particular web server using its external IP address in a browser. We are done with the code, so let's copy/paste all of this code into our main.py. As always, we can hit Ctrl+X to save the code. And now we can go ahead and run it. Now remember that we are doing all of this from an SSH window on our VM instance. We are not making use of an IDE like Jupyter or Datalab, so let's also go ahead and install pip. So we run sudo apt install python-pip. This runs through. It prompts us for the amount of disk space, which we are fine with. And once it's all done, we can go ahead and install our requirements. We simply run sudo pip install followed by the -r switch and our requirements.txt. The command does its thing, and we now have both redis and Flask successfully installed on this VM instance. We'll now go ahead and run our Python code. This is simply sudo python main.py. Our server is now up and running in debug mode. We can connect to it using the external IP address of our VM instance. We switch back to the web console. There under the VM instances list, we get the external IP address, copy it, paste it into our browser window. We also include the port, which is 8080, that's optional, followed by the actual URL endpoint, which is /incr. That bit is not optional. And when we hit this URL, we can see that our counter has been incremented. This value is currently 1 because the key counter did not previously exist in our Redis instance. That's why when we call the increment command, it's initialized to 1. If we now go ahead and refresh a bunch of times, it's incremented further. Note that each of these is making a call to the Redis server in the back end. Next, let's also hit the decrement URL, and this time, we can see that the counter value gets decremented and goes back down. We have successfully connected to our Memorystore Redis instance from a GCE VM instance.

Using Memorystore from an App Engine Application
In this example, we will build a simple App Engine flex web server, and from that web server, we will connect to a Redis instance. Recall that we cannot connect Redis from App Engine standard, and we've got to have our app in the same region, the same network, and the same project as our Redis instance. Let's get started by activating Cloud Shell as usual. This time we will do something a little different. We will launch the code editor. This requires us to click on the little pencil icon. That in turn brings up a really handy way to navigate our file system. This is our home directory on the Google Cloud Platform. We'll go ahead and use the File menu to create a new folder, which is going to house our App Engine flex application. Let's call this redis_appengine. And within this folder, we'll set about creating all of the standard files. The first one is the requirements.txt. This time we have Flask and redis just as we did in the previous example, but there's also a new one in there. This is gunicorn. Gunicorn is a package which is used as a WSGI server. That's an acronym for Web Server Gateway Interface. WSGI is a standard interface used to implement Python web apps. More on that in a moment. Let's go ahead and create a new file. This is the app.yaml file. Because we are running App Engine, we need to define all of the configuration parameters for our App Engine application in this app.yaml. We need to specify the IP address and the port number of our Memorystore Redis instance in that app.yaml, so let's go ahead and copy those. And we switch back and reference them in our app.yaml file. Notice that we pass in the two environment variables, REDISHOST and REDISPORT. Let's also go ahead and understand all of the other line items in this app.yaml. The runtime is the software stack, which is going to install and run our app. The environment is flexible, which means that this runtime is going to be built using Docker. The default version of Python is actually 2.7 .9, but here we have explicitly specified that we'd like to use Python version 3. The entrypoint section specified in this YAML is going to be used by that runtime to start up our application. This entrypoint is to start a process, which will respond to HTTP requests. It's recommended that will make use of gunicorn or a similar WSGI server, and this line here is the standard syntax which we would use for gunicorn as the entrypoint into a Flask application. So we specify our entrypoint as gunicorn. We also include the port environment variable, which is where the HTTP requests are going to come in, and then the main module inside the application package. We are now ready to go ahead and code up the actual main.py. That's the file which we referenced in that app.yaml. Main.py looks very much like the little code we wrote a moment ago. The only difference is that this time we have just the one route. That is the default route. And here we increment the counter by a value of 10. So unlike the previous example where we had separate increment and decrement functions, here we have just the one index function. That does it for all of the code in our little App Engine app. We can now switch back into Cloud Shell, cd into the correct directory, and deploy the app using gcloud app deploy. This is the standard syntax. It's going to pick up on the app.yaml and the requirements.txt files within the current directory and push our app out onto the target URL, https://spikey- redis.appspot .com. We are prompted for a confirmation. We provide this confirmation, and very quickly our app is up and running. As an aside, notice how it's Docker that's being used to package up the app. Once it's done, let's copy the target URL into our web browser. Once we do this, we are greeted with the initial value of 10. If we now hit Refresh, this is going to increment the counter value by 10 more, and that in turn quickly causes the counter count to increase to 50.

Packaging a Web Application into a Container
Let's go ahead and put together an example of working with a Redis instance from within a GKE cluster. Here we are going to package up the same web server application which we previously deployed from App Engine flexible except that we will now host it in a Kubernetes cluster. We start from a Cloud Shell terminal in which we begin by setting a property. This is the compute/zone property, which we set to be us-east1-b. We are going to use it in a moment when we instantiate our GKE cluster. Next up, we use the gcloud container clusters create command followed by the name of our cluster, which here is called spikey-counter-cluster, then the number of nodes, which is 3, and, finally, the --enable-ip-alias flag. The advantage of setting the --enable-IP-alias flag is that our cluster and ports will now be mapped to internal IP addresses on this VPC network. Notice that we did not have to specify the zone because we had already set the compute/zone property. Our Kubernetes cluster comes into existence. We can confirm that this is the case by switching to the web console, navigating into Kubernetes Engine, and clicking on Clusters. We can see that our cluster, the spikey-counter-cluster, has come into existence there. Let's now switch back to our Cloud Shell code, open a window, and create a new folder this time for our Redis GKE application. A quick word about the steps up ahead. A lot of what we are about to do is going to seem quite similar to the App Engine flexible example, and there's a reason for that. App Engine flexible relies on Docker containers to package up our application, and so does Kubernetes. But this example is a lot more involved because App Engine flex abstracts us from a lot of the actual Kubernetes interfacing. In any case, we start by putting together a requirements.txt. This has the exact same dependencies as the previous example. Next, let's go ahead and create our main.py. Once again, this code looks virtually identical to the previous example. So it's got a Flask web server. It reads in the redis_host and the redis_port from the environment and goes ahead and instantiates a redis_client. We've only tweaked it ever so slightly from the previous example because now in the index method, we only increment our counter by 1. In the App Engine flex example, we were incrementing it by 10 each time around. So far this is virtually identical to App Engine flex, but now it gets very different. We've got to create a Docker file, and in here, we have all of the exact steps which were being performed for us by App Engine flex under the hood. There we simply specified that the runtime was Python and the version was 3. Here we actually pull the Python container from the Google Container Registry and then run the VIRTUAL_ENV with Python 3.4. Next, we defined several environment variables such as the VIRTUAL_ENV, the PATH, the REDISHOST and REDISPORT. And then we go ahead and explicitly add our requirements.txt. Finally, we run pip install in order to ensure that all of those packages are actually installed. Finally, we add the current directory into our app, and then run gunicorn so that it accepts HTTP requests on the external IP address and redirects them into our main app. In the case of App Engine flex, the port 8080 was not explicitly mentioned in the app.yaml because it was set by the environment. Here we actually have to invoke it in the gunicorn command. Let's close out of the code editor and cd into our redis_gke directory. Once we are there, we will need to actually run the Docker build command. This is going to package up the contents of the current directory into an image and apply a tag. We see that the image is successfully built, and we then go ahead and push it into the Google Container Registry using the gcloud docker push command. Note that we push it using the same format, that is gcr.io /(the project ID)/(the container image name and the version), which here is just v1. We can now verify that this is available in the Google Container Registry by switching to the web console. In the Navigation menu, we find the Images section of the Container Registry. There we have a repository for our spikey-counter app, and within it is the actual image which we just pushed.

Using Memorystore from a Kubernetes Cluster
Now that we've packaged up our app into a Docker container and pushed it to the Google Container Registry, the next step is for us to actually use this container in a Kubernetes cluster. In order to do this, we will make use of the kubectl command line utility. We are going to use this to configure our cluster. And the first command that we run is to create a configmap. This is a Kubernetes abstraction, which is used to pass information into our apps. Here we use it to avoid hardcoding the redishost IP address. Note how this command does not actually include the cluster name. Now in this example, because we have a clean project and just one cluster in it, kubectl automatically picks up that all of our commands need to apply to that cluster. If you are performing these steps from a project which has several clusters, you should configure kubectl to point explicitly to your cluster using the command onscreen now, gcloud container clusters get-credentials followed by the name of your cluster and the zone. In any case, we have now created a configmap which contains the redishost IP address. Let's verify that this has happened successfully using the kubectl get configmaps command. We follow this by the name of the configmap, which is redishost. We've asked for the output in YAML, and that's what we get. The data is a key-value pair, which has key REDISHOST, and the value is the IP address of our Memorystore instance. Now we need to configure a YAML specification for our pod. This is a Kubernetes abstraction, so we switch back into the code editor window. And inside the redis_gke directory, we create a new file. This is our spikey- counter.yaml, and it's going to contain the specification of our app. We define two Kubernetes abstractions. The first of these up top is a Deployment. And then down below is a Service. The Deployment is named spikey-counter, and it defines the number of replicas, which is equal to 1, as well as the container which defines our app. Here we have just the one container. It's called spikey-counter, and the image is pulled from the Google Container Registry. This refers to the exact same container that we just pushed there. This Deployment specification also has an explicit configMapKeyRef. The name that it's referring to is redishost. Down below is the Service object. This is effectively a load balancer, which is going to accept incoming requests on port 80 and direct them into our Deployment up above. Having defined the specification for these Kubernetes objects, we can switch back into the Cloud Shell and simply apply these declaratively. This is done using the kubectl apply -f command followed by the YAML file that we've just defined. Kubernetes will parse that YAML file, make sense of its contents, and create the required objects. And, indeed, it comes back and tells us that it has created one deployment and one service object. Now remember that our service object was of type load-balancer, which means that it's going to provide a stable front end for us to access the app. Let's look this up. So we switch back to the web console, and in the Navigation menu under Kubernetes Engine, we click on the Services tab. There, sure enough, we find our spikey-counter load balancer service object defined. Notice that there are also endpoints listed, specifically an external IP address and a port. Like any load balancer, this front end requires a back end, and we can examine that in the Workloads tab over on the left. If we click there, we see the spikey-counter deployment object that we had defined in our YAML file. And remember that the deployment object encapsulates within it a port specification and the number of replicas, and the port specification in turn pointed to the Docker container, which we had pushed into the Google Container Registry. Let's switch back to Cloud Shell and run some commands there. We can also list the service object information using kubectl get the service followed by the name of our service object. This displays a bunch of information including both the cluster IP, the external IP, and the port. The cluster IP can be thought of as the internal IP address. This service will be accessible within our VPC network on the cluster IP, and it will be accessible from outside our VPC network on the external IP address. Because we'd like to access our app from a browser, we need the external IP address. The cluster IP will not work from the browser. We paste it into our browser bar, and we do indeed see our application. Each time we hit Refresh, we are successfully able to increment the value of the counter in the Redis instance. That gets us to the end of this module, which was devoted to the configuration of a Redis Memorystore instance. We saw how there are only a couple of Redis properties which we are allowed to tweak from within Memorystore, but these are sufficient for many interesting use cases. We then moved into an exploration of Redis as accessed from Compute Engine VMs, from an App Engine flexible environment, as well as from container applications which have been packaged up using Docker and are now running on a Kubernetes cluster. Do also remember that as of right now, that is December 2018, it's not possible to access Memorystore for Redis from within App Engine standard or Cloud Functions.

Scaling and Monitoring Cloud Memorystore
Module Overview
Hello, and welcome to this module on Scaling and Monitoring Cloud Memorystore instances. As the name of the module would suggest, this one is all about scaling behavior on Memorystore. As we've seen, Memorystore offers instances in two tiers. These instances have pretty different behaviors during scaling. It's also possible to achieve high availability using the standard tier instances. This high availability is not achieved using Sentinel, which is the open-source Redis route to high availability. Rather, standard peer instances just make use of cross-zone replication along with a replication mechanism between the master and the replica.

Two Tiers of Redis Instances
In this video, we will understand in detail the scaling, replication, and failover characteristics of the two classes of Redis instances. Recall that basic instances have no replication at all. On the other hand, standard instances have built-in cross-zone replication, so there is a master-replica pair. The master and the replica are in different zones. From this, it follows logically that there is no possibility of failover for basic instances. Standard instances can, indeed, failover from the master to the replica. This is only possible for high availability, not as a read replica. This also implies that the scaling of basic instances requires blocking, and the cache needs to be flushed. Standard tier instances, on the other hand, can scale without taking down the other instance. And, as a result, their downtime on scaling is far smaller. And the implication of all of these differences is that the basic tier is best used for pure caching use cases. If you'd like to use Redis as a database or as a storage layer, then the standard tier is much better. Let's now go ahead and understand how basic tier instances scale. As we've already discussed, there's no replica, and so there's no alternative but to block reads and writes during the instance resizing. Because the instances before and after scaling have no link, all data from the cache is going to be lost. So it obviously makes sense to scale a basic tier instance during a period of really low activity. Because standard instances have a master-replica pair, scaling also becomes a lot cleaner. The replica is first resized. After that replica has been resized, it is synced with the master. And only after the replica and the master have both been brought up to speed with each other, only then is the master going to fail. The master then simply fails over to the replica, and the replica and the master now switch roles. At this point in the process, the old master is going to be resized as well. So once the entire scaling process is done, both the master and the replica are going to have the new resized capacity. There is going to be a very brief period during which all connections to the Redis instance are going to be terminated. But this is only the failover period, so it's really a very brief downtime. In contrast with basic instances, the connection termination period happens all through the resizing of the instance, which is much longer. It's also important to note that no applications need to change their connection strings or IP addresses, and so the applications simply need to incorporate retry logic. There's nothing else that they need to do. We definitely don't need to recode our applications to deal with scaling. Some more factors to keep in mind, while scaling, even if we are scaling a standard tier instance, we should try and keep the instance write load to a minimum. Otherwise, the time taken for scaling can increase significantly, and it can even cause the scaling operation to fail. To see why this is the case, just remember that there is a brief interval when the replica is being scaled, so there's only one copy of the data. And what's more, after being scaled, the replica has to catch up with the master. That's the re-sinking process. During this re-sinking process, expired keys are not going to be transferred over. As a result of this, if we had a large number of expired keys, after the instance has been scaled, the size of the memory consumed can dip significantly. Remember that Memorystore instances are charged for what you allocate, so it's important for us to be able to reduce the size of the instance, as well as to increase it. And, therefore, it's good that scaling is possible in both directions. While scaling down the capacity of our instance, we must choose a size greater than the amount of data currently stored in the instance, else the downscaling operation will fail.

Scaling Basic Tier Instances
Now that we've understood the scaling behavior of both types of instances in detail, let's turn our attention to another aspect of Memorystore, which is automatic failover. Automatic failover clearly is going to be present only with standard tier instances, and that's because it relies on the master-replica pair. Remember that standard tier instances by default have a master-replica pair. So effectively there are two copies of the same data, and these are in different zones. An important note here, automatic failover is also present in the open-source version of Redis. That makes use of a program called Redis Sentinel. Redis Sentinel is not used by Memorystore. Rather, it relies entirely on the cross-zone replication we just alluded to. This automatic failover is going to occur whenever the zone for the master goes down or when the master becomes unresponsive. In other words, the platform takes ownership of automatic failover and ensures that it happens on either zone or master outages. This replication mechanism which all standard instances come with is based on something known as the Redis asynchronous replication protocol. It's important to note that these are purely failover replicas, so the replica is never going to be available as a read replica to reduce latency. It only exists for failovers. The actual mechanics of failover are made pretty smooth by the platform. All existing connections to the master are dropped. That doesn't matter because failover is only going to happen if the master is down, not unresponsive. On reconnect, clients are going to be redirected to the new master. They'll need to reconnect to the new master, and that's why it's a good idea for them to have retry logic. But what's crucial is that they can do so using the same connection string and IP. So there's absolutely no need to update the application. If the failover is entirely caused by an outage of the master, it ought to take about 30 seconds to complete. Of course, if the outage is happening because an entire zone is down, then the failover can take longer. During the failover period, there is no replica of the data, so clearly even the standard tier instances are regional. If a regional outage occurs, well, then our service is simply going to be unavailable.

Stackdriver Monitoring for Memorystore
In this example, we will demonstrate the use of Stackdriver Monitoring in order to keep tabs on our Memorystore Redis instance. We start from the Navigation menu and scroll down until we find the Stackdriver section, and there we click on Monitoring. This in turn opens up Stackdriver Monitoring where we can set up of alerting policies. So let's go ahead and click on the Create Policy button in the middle there. The alerting policy requires us to first pick a metric which we are interested in monitoring. Metrics are organized by resource type. So in the drop-down, we've got to scroll down until we find our Cloud Memorystore Redis instance. Let's select this, and we then have a set of different metrics which we can pick. For now, let's pick the number of calls. This is going to be calculated on a per-second basis. Next, we can configure the condition. Here we would like to be notified every time the number of calls is above a certain threshold, and that threshold is 0. This is going to be measured over a certain period, and that period we set to just the most recent value. So each time calls come into our Redis instance, we are going to be notified. Let's go ahead and save this alerting configuration. We now have to specify the notification channel. There are several different choices here of which email is the only one which is available to us for free. So we select that and then configure the recipient of this email. There is also a button there which allows us to add additional notification channels. We can also use Stackdriver to configure the exact form of the email that we wish to send, as well as a policy name. We are fine with the default for this, so let's go ahead and save this policy. Next, let's switch into App Engine where we still have our App Engine application published. So we click on the Services tab. This brings up the default application, and we can hit that in our browser by clicking on the little expand icon. The first time we hit the URL, the visitor number is incremented to 67. We then hit Refresh several times. This causes a bunch of hits to our Redis instance culminating in the visitor count being 137. If we now switch back into Stackdriver Monitoring and hit Refresh, we can see that there is an open incident there. Stackdriver Monitoring reports that there has been a violation of our condition because the number of calls is now greater than 0. Notice that it has normalized this on a per-second basis for a value of 0.02. If we now switch into our email inbox, we will see that there is a Stackdriver alert there, and this has the same message. We have successfully demonstrated the use of Stackdriver Monitoring with Redis. Let's now switch back into Stackdriver Monitoring and acknowledge the incident.

Scaling a Memorystore Instance
In this example, we will demonstrate scaling Memorystore instances on the fly. Let's start out in the Navigation menu and check out the logs related to our Memorystore instance. So we click on the Logs, and that takes us into Stackdriver Logging. There we can use the drop-down to pick the kind of resource we would like to audit. This is organized according to the RESTful API endpoints of the Google Cloud services, so we've got to pick the service first, which here is redis.googleapis .com, and then the method, which here is CreateInstance. And in this way, we can drill down into specific kinds of operations. We can also pick a time and date range. Let's pick the last six hours, and we can see that there are four CreateInstance operations corresponding to different experiments as I was playing around with the service. We can tell from the instance IDs that two of these had to do with the spikey-test-cache in us-central1, and the last two had to do with the spikey-prod-cache in us-east1. Recall that along the way, we had also deleted the spikey-test-cache instance, but here because we filtered on only Create operations, those log entries don't show up. Now let's switch back into the Memorystore portion, and let's scale up our spikey-prod-cache instance. This is a standard tier instance. We click through and click on the little pencil icon to edit this instance. That now brings up all of the editable properties of the instance. Let's go ahead and change the instance capacity from 1 to 2 GB. Doing this has a significant impact on the cost. Before the increase, the estimated cost is $ 46.72 per month. After we confirm that we would like to, indeed, scale this instance, the new estimated cost is $ 93.44. We can also see from this warning message that when we scale a standard tier instance, unreplicated data is going to be lost, and the instance will briefly be unavailable. We are fine with this, so we go ahead and scale the instance, and we can immediately see that the maxmemory in our graph has gone up from 1 GB to 2 GB. There's also a small fall in the memory usage quite possibly attributable to that loss of unreplicated data we were just referring to. In any case, the spikey-prod-cache instance has now been updated. Let's see if this is reflected in Stackdriver Monitoring. So we switch back over into the tab that we have open with Stackdriver Monitoring. This time we are not looking to create an alerting policy. Rather, we are looking to build a dashboard. So we give it a name. Let's call it to the redis-instance-monitoring-dashboard, and then go ahead and add a few widgets into it. So we click on the Add Chart button in the top right, and that in turn allows us to go ahead and add different charts. We are going to add the resource type, which is Cloud Memorystore Redis Instance, and then the metrics. Let's go with the calls once again. A difference between the alerting and the charting is that we now can specify an aggregation operator. So this time, let's go ahead and sum up the number of calls per minute. This gives us a fairly nice widget in which we can see that there was a period of some activity when we hit the server multiple times. Let's now go ahead and add yet another chart, this one with the maximum memory of our instance. Let's go ahead and add this in there. And immediately we can see that the scaling operation that we just performed shows up. The maximum memory increased from 1 GB two 2 GB. We've successfully scaled our Redis instance and captured that scaling in the Stackdriver Monitoring.

Summary and Further Study
That gets us to the end of this module and to the end of this course. This module was devoted to the behavior of Memorystore instances during scaling. We studied both the concepts, as well as the theory behind scaling, the high-availability configuration made available on standard tier instances, and the mechanics of replication and failover. The whole appeal of Redis lies in its ease of use and simplicity, and that clearly came through in the simple nature of the demos here. Replication and scaling are simple and easy to use because they just work. We've almost come to the end of this course. Before that, I'd like to leave you with a reminder to delete any resources that you provisioned during the course examples. This includes all of our Memorystore for Redis instances, the VM instances, and Google Kubernetes Engine cluster, as well as the App Engine deployment. And, finally, here are a couple of excellent courses for further study on Pluralsight. The first of these is called Architecting Scalable Web Applications Using Google App Engine. And the second is AWS Developer: Getting Started with DynamoDB. DynamoDB often and consistently ranks as the second most technology in the key-value store space with Redis being the first.