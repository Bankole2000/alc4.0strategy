Google Cloud Platform Fundamentals: Core Infrastructure
by Google Cloud

In this course, Google Cloud Platform Fundamentals: Core Infrastructure, you will learn about virtual machines and networks in the cloud, storage in the cloud. Youâ€™ll also learn about developing, deploying, and monitoring in the cloud.

This class provides an overview of Google Cloud Platform services. Through a combination of presentations, demos, and hands-on labs, participants learn the value of Google Cloud Platform and how its services work together. Participants also get foundational skills in how to use GCP.

Course author
Author: Google Cloud	
Google Cloud
Google Cloud can help solve your toughest problems and grow your business. With Google Cloud, their infrastructure is your infrastructure. Their tools are your tools. And their innovations are your...

Course info
Level
Intermediate
Rating
4.8 stars with 20 raters(20)
My rating
null stars

Duration
2h 55m
Updated
17 Jan 2019
Share course

Introduction to Google Cloud Platform
Module Introduction
Hi, I'm Brian Rice from the Google Cloud training team. In this course my colleagues and I are going to introduce you to Google Cloud Platform. GCP offers four main kinds of services. Compute, Storage, Big Data, and Machine Learning. This course focuses mostly on the first two together with the topic of networking. After all, you can't use resources on the Cloud without Cloud networking. The Cloud is a great home for your applications and your data, because using it frees you from a lot of overhead chores. And the Google Cloud gives you reasonably priced access to the same planet scale infrastructure that Google runs on. What exactly is Google Cloud Platform? How's it organized? And what makes it unique? In this module we'll orient you to the basics.

What Is Cloud Computing?
The Cloud is on everybody's lips these days. What exactly does that mean? Turns out there's a simple, clear definition. The US National Institute of Standards and Technology created it, although there's nothing US specific about it. Here it is. Cloud computing is a way of using IT that has these five equally important traits. First, you get computing resources on-demand and self-service. All you have to do is use a simple interface and you get the processing power, storage and network you need with no need for human intervention. Second, you access these resources over the net, from anywhere you want. Third, the provider of those resources has a big pool of them and allocates them to customers out of that pool. That allows the provider to get economies of scale by buying in bulk, and pass the savings on to the customers. Customers don't have to know or care about the exact physical location of those resources. Fourth, the resources are elastic. If you need more resources, you can get more, rapidly. If you need less, you can scale back. And last, the customers pay only for what they use or reserve as they go. If they stop using resources they stop paying. That's it. That's the definition of Cloud.

How Did We Get Here?
But why is this model so compelling nowadays? To understand why, we need to look at some history. The first wave of the trend that brought us towards Cloud computing was colocation which IT shops had been doing for decades. Instead of building costly, capital intensive data centers they can rent space and shared facilities. That frees up capital for more flexible uses than real estate. In the first decade of the 2000s, IT departments need for efficiency drove them to use virtualization. The components of a virtualized data center matched the parts of a physical data center. Servers, disks, and so on, but now they're virtual devices separately manageable from the underlying hardware. Virtualization lets us all use resources more efficiently, and just like colocation it lets us be more flexible too. With virtualization you still buy, house and maintain the infrastructure so you're still in the business of guessing how much hardware you'll need and when, setting it up, and keeping it running. About 10 years ago Google realized that its business couldn't move fast enough within the confines of the virtualization model, so Google switched to a container based architecture on automated, elastic, third wave Cloud built from automated services. We'll explain exactly what containers are later in this course. In Google's internal Cloud, services automatically provision and configure the infrastructure that's used to run familiar Google applications. Google has spent billions of dollars building this platform and making it resilient and efficient. Today Google Cloud Platform makes it available to Google customers.

Every Company Is a Data Company
Google believes that in the future, every company, regardless of size or industry will differentiate itself from its competitors through technology largely in the form of software. Great software is centered on data thus every company will become a data company if it isn't already one now. Google Cloud provides a wide variety of services for managing and getting value from data and doing that at scale.

GCP Computing Architectures
Virtualized data centers brought you infrastructure as a service IaaS and platform as a service PaaS offerings. IaaS offerings provide raw compute storage and network organized in ways that are familiar from data centers. PaaS offerings on the other hand bind application code you write to libraries that give access to the infrastructure your application needs. That way you can just focus on your application logic. In the IaaS model you pay for what you allocate. In the PaaS model you pay for what you use. Both sure beat the old way where you bought everything in advance based on lots of risky forecasting. As Cloud computing has evolved, the momentum has shifted towards managed infrastructure and managed services. GCP offers many services in which you need not worry about any resource provisioning at all. We'll discuss many in this course. They're easy to build into your applications and you pay per use. By the way, now that I've mentioned PaaS and IaaS, you might be asking yourself what about SaaS? Of course Google's popular applications like Search, Gmail, Docs, and Drive are software as a service applications in that they're consumed directly over the internet by end users. G Suite is outside the scope of this course although I hope you like it.

The Google Network
According to some estimates out there publicly, Google's network carries as much as 40% of the worlds internet traffic everyday. Google's network is the largest of its kind on Earth and the company has invested billions of dollars over the years to build it. It's designed to give its users the highest possible throughput and the lowest possible latencies for their applications. The network interconnects at more than 90 internet exchanges and more than 100 points of presence worldwide. When an internet user sends traffic to a Google resource, Google responds to the users request from an edge network location that will provide the lowest latency. Google's edge caching network sites content close to end users to minimize latency.

GCP Regions and Zones
Here's how GCP is organized. Let's start at the finest grained level, the Zone shown here on the right. A zone is a deployment area for Google Cloud Platform resources. For example, when you launch a virtual machine in GCP using compute engine which we'll discuss later, it runs in a zone you specify. Although people think of a zone as being like a GCP data center that's not strictly accurate because a zone doesn't always correspond to a single physical building. You can still visualize a zone that way though. Zones are grouped into regions. Independent geographic areas, and you can choose what regions your GCP resources are in. All of the zones within a region have fast network connectivity among them. Locations within regions usually have round trip network latencies of under five milliseconds. Think of a zone as a single failure domain with a region. As part of building a fault tolerant application you can spread their resources across multiple zones in a region. That helps protect against unexpected failures. You can run resources in different regions too. Lots of GCP customers do that both to bring their applications closer to users around the world and also to protect against the loss of an entire region say due to a natural disaster. A few Google Cloud Platform services support placing resources in what we call a multi-region. For example, Google Cloud storage which we'll discuss later let's you place data within the Europe multi-region. That means it's stored redundantly in at least two geographic locations separated by at least 160 kilometers within Europe. As of the time of this videos production, GCP had 15 regions. Visit cloud.google .com to see what the total is up to today.

Environmental Responsibility
The virtual world is built on physical infrastructure and all those racks of humming servers use vast amounts of energy. Together, all existing data centers use roughly 2% of the worlds electricity. So Google works to make data centers run as efficiently as possible. Google's data centers were the first to achieve ISO 14001 certification which is a standard that maps out a framework for improving resource efficiency and reducing waste. This is Google's data center in Hamina, Finland, one of the most advanced and efficient data centers in the Google fleet. It's cooling system uses sea water from the Bay of Finland to reduce energy use. It's the first of its kind anywhere in the world. Google is one of the worlds largest corporate purchasers of wind and solar energy. Google has been 100% carbon neutral since 2007 and will shortly reach 100% renewable energy sources for its data centers. Just like its customers, Google is trying to do the right things for the planet. GCP customers have environmental goals of their own and running their workloads in GCP can be a part of meeting them.

Pricing Innovations
GCP offers a lot of ways to save money. Google was the first major Cloud provider to build by the second, rather than rounding up to bigger units of time for its virtual machines as a service offering. This may not sound like a big deal but charges for rounding can really add up for customers who are creating and running lots of virtual machines. Per second billing is offered for virtual machine use through compute engine and for several other services too which we'll also look at in this course. Kubernetes engine which is container infrastructure as a service, Cloud Dataproc which is the open-source big data system Hadoop as a service and App Engine flexible environment which is a platform as a service. Compute engine offers automatically applied sustained use discounts which are automatic discounts that you get for running a virtual machine for a significant portion of the billing month. When you run an instance for more than 25% of a month, compute engine automatically gives you a discount for every incremental minute you use it. Here's one more way compute engine saves money. Later in this course you'll learn about how virtual machines are configured. Among other things you specify how much memory and how many virtual CPUs they should have. Normally you pick a virtual machine type from a standard set of these values, but compute engine also offers custom virtual machine types so that you can fine tune the sizes of the virtual machines you use. That way you can tailor your pricing for your workloads.

Open APIs
Some people are afraid to bring their workloads to the Cloud, because they're afraid they'll get locked into a particular vendor, but in lots of ways, Google gives customers the ability to run their applications elsewhere, if Google becomes no longer the best provider for their needs. Here are some examples of how Google helps its customers avoid feeling locked in. GCP services are compatible with opensource products for example take Cloud Bigtable, a database we'll discuss later. Bigtable uses the interface of the open-source database Apache HBase which gives customers the benefit of code portability. Another example, Cloud Dataproc offers the open-source big data environment Hadoop as a managed service. Google publishes key elements of technology using open-source licenses to create ecosystems that provide customers with option other than Google. For example, TensorFlow, an open-source software library for machine learning, developed inside Google, is at the heart of a strong open-source ecosystem. Many GCP technologies provide interoperability. Kubernetes gives customers the ability to mix and match microservices running across different Clouds and Google Stackdriver lets customers monitor workload across multiple Cloud providers.

Why Choose Google Cloud Platform?
Google Cloud Platform lets you choose from computing, storage, big data, machine learning and application services. For your web, mobile, analytic and backend solutions. It's global, it's cost effective, it's open-source friendly and it's designed for security. Let's sum up. Google Cloud Platforms products and services can be broadly categorized as compute, storage, big data, machine learning, networking and operations and tools. This course considers each of the compute services and discusses why customers might choose each. The course will examine each of Google Cloud Platforms storage services, how it works and when customers use it. To learn more about these services you can participate in the training courses in Google Clouds data analyst learning track. This course also examines the function and purpose of Google Cloud Platforms big data and machine learning services. More details about these services are also available in the training courses in Google Clouds data analyst learning track.

Multi-layered Security Approach
Because Google has 7 services with more than a billion users, you can bet security is always on the minds of Google's employees. Design for security is pervasive throughout the infrastructure, the GCP, and Google services run on. Let's talk about a few ways Google works to keep customers data safe starting at the bottom and working up. Both the server boards and the networking equipment in Google data centers are custom designed by Google. Google also designs custom chips including a hardware security chip called Titan, that's currently being deployed on both servers and peripherals. Google server machines use cryptographic signatures to make sure they're booting the correct software. Google designs and builds its own data centers which incorporate multiple layers of physical security protections. Access to these data centers is limited to only a very small fraction of Google employees not including me. Googles infrastructure provides cryptographic privacy and integrity for remote procedure call data on the network which is how Google services communicate with each other. The infrastructure automatically encrypts our PC traffic in transit between data centers. Google's central identity service which usually manifest in users as the Google login page goes beyond asking for a simple username and password. It also intelligently challenges users for additional information based on risk factors such as whether they have logged in from the same device or a similar location in the past. Users can also use second factors when signing in including devices based on the universal second factor, UTF open standard. Here's mine. Most applications at Google access physical storage indirectly via storage services and encryption is built into those services. Google also enables hardware encryption support in hard drives and SSDs. That's how Google achieves encryption at rest of customer data. Google services that want to make themselves available on the internet register themselves with an infrastructure service called the Google Front End which checks incoming network connections for correct certificates and best practices. The GFE also additionally applies protections against denial of service attacks. The shear scale of its infrastructure enables Google to simply absorb many denial of service attacks. Even behind the GFEs, Google also has multi-tier, multi-layer, denial of service protections that further reduce the risk of any denial of service impact. Inside Google's infrastructure machine intelligence and rules warn of possible incidence. Google conducts red team exercises, simulated attacks to improve the effectiveness of its responses. Google aggressively limits and actively monitors the activities of the employees who've been granted administrative access to the infrastructure. To guard against phishing attacks against Google employees, employee accounts including mine require use of UTF compatible security keys. I don't forget my keys as much as I used to. To help ensure that code is as secure as possible, Google stores its source code centrally and requires two party review of new code. Google also gives its developers libraries that keep them from introducing certain classes of security bugs. Externally Google also runs a vulnerability rewards program where we pay anyone who is able to discover and inform us of bugs in our infrastructure or applications.

Getting Started with Google Cloud Platform
Module Introduction
When you run your workloads in GCP, you use projects to organize them. You use Google Cloud Identity and Access Management, also called IAM, or I-A-M, to control who can do what, and you use your choice of several interfaces to connect. In this module, you'll use these basics to get started. Projects are the main way you organize the resources you use in GCP. Use them to group together related resources, usually because they have a common business objective. The principle of least privilege is very important in managing any kind of compute infrastructure, whether it's in the cloud or on-premises. This principle says that each user should have only those privileges needed to do their jobs. In a least privilege environment, people are protected from an entire class of errors. A coworker of mine once accidentally deleted a running production database. Why? Because he was working as the root user on the system when he shouldn't have been. His name is Brian. And he's still very, very sorry. GCP customers use IAM to implement least privilege, and it makes everybody happier. There are four ways to interact with GCP's management layer. Through the web-based console, through the SDK and its command line tools, through the APIs, and through a mobile app. In this class, you'll mostly use the console and the command line tools. When you build an application on your on-premises infrastructure, you're responsible for the entire stack security, from the physical security of the hardware and the premises in which they're housed, through the encryption of the data on disk, the integrity of your network, all the way up to securing the content stored in those applications. When you move an application to Google Cloud Platform, Google handles many of the lower layers of security. Because of its scale, Google can deliver a higher level of security at these layers than most of its customers could afford to do on their own. The upper layers of the security stack remain the customers' responsibility. Google provides tools such as IAM to help customers implement the policies they choose at these layers.

Google Cloud Platform Resource Hierarchy
You may find it easiest to understand the GCP resource hierarchy from the bottom up. All the resources you use, whether they're virtual machines, cloud storage buckets, tables in BigQuery, or anything else in GCP are organized into projects. Optionally, these projects may be organized into folders. Folders can contain other folders. All the folders and projects used by your organization can be brought together under an organization node. Projects, folders, and organization nodes are all places where the policies can be defined. Some GCP resources let you put policies on individual resources too, like those cloud storage buckets I mentioned. We'll talk more about cloud storage later in the course. In the meantime, remember that policies are inherited downwards in the hierarchy. All Google Cloud Platform resources belong to a project. Projects are the basis for enabling and using GCP services like managing APIs, enabling billing, and adding and removing collaborators and enabling other Google services. Each project is a separate compartment, and each resource belongs to exactly one. Projects can have different owners and users. They're billed separately and they're managed separately. Each GCP project has a name and a project ID that you assign. The project ID is a permanent, unchangeable identifier, and it has to be unique across GCP. You use project IDs in several contexts to tell GCP which project you want to work with. On the other hand, project names are for your convenience and you can assign them. GCP also assigns each of your projects a unique project number, and you'll see it displayed to you in various contexts, but using it is mostly outside the scope of this course. In general, project IDs are made to be human readable strings, and you'll use them frequently to refer to projects. You can organize projects into folders, although you don't have to. They're a tool at your disposal to make your life easier. For example, you can use folders to represent different departments, teams, applications, or environments in your organization. Folders let teams have the ability to delegate administrative rights so they can work independently. The resources in a folder inherit IAM policies from the folder, so if project_3 and 4 are administered by the same team by design, you can put IAM policies into Folder B instead. Doing it the other way, putting duplicate copies of those policies on project_3 and project_4 would be tedious and error prone. One word of caution. To use folders, you need an organization node at the top of the hierarchy. So what's that? Let's talk about it now. You probably want to organize all the projects in your company into a single structure. Most companies want the ability to have centralized visibility on how resources are being used and to apply policies centrally. That's what the organization node is for. It's the top of the hierarchy. There are some special roles associated with it. For example, you can designate an organization policy administrator so that only people with privilege can change policies. You can also assign a Project Creator role, which is a great way to control who can spend money. So, how do you get an organization node? In part, the answer depends on whether your company is also a G Suite customer. If you have a G Suite domain, GCP projects will automatically belong to your organization node. Otherwise, you can use Google Cloud Identity to create one. Here's a tip. When you get a new organization node, it lets anyone in the domain create projects and billing accounts just as they could before. That's to avoid surprises and disruption. But it'd be a great first step with a new organization node to decide who on your team should really be able to do those things. Once you have an organization node, you can create folders underneath it and put it in projects. Here's an example of how you might organize your resources. There are three projects, each of which uses resources from several GCP services. In this example, we haven't used any folders, although we could always move projects into folders. Resources inherit the policies of their parent resource. For instance, if you set a policy at the organization level, it is automatically inherited by all its children projects, and this inheritance is transitive, which means that all the resources in those projects inherit the policy too. There's one important rule to keep in mind. The policies implemented at a higher level in this hierarchy can't take away access that's granted at a lower level. For example, suppose that a policy applied on the bookshelf project gives user Pat the right to modify a Cloud Storage bucket, but a policy at the organization level says that Pat can only view Cloud Storage buckets, not change them. The more generous policy is the one that takes effect. Keep this in mind as you design your policies.

Identity and Access Management (IAM)
IAM lets administrators authorize who can take action on specific resources. An IAM policy has a who part, a can-do-what part, and an on-which-resource part. The who part names the user or users you're talking about. The who part of an IAM policy can be defined either by a Google account, a Google group, a service account, an entire G Suite, or a Cloud Identity Domain. The can-do-what part is defined by an IAM role. An IAM role is a collection of permissions. Most of the time to do any meaningful operations, you need more than one permission. For example, to manage instances in a project, you need to create, delete, start, stop, and change an instance, so the permissions are grouped together into a role that makes them easier to manage. The who part of an IAM policy can be a Google account, a Google group, a service account, or an entire G Suite or Cloud Identity Domain. There are three kinds of roles in Cloud IAM. Let's talk about each in turn. Primitive roles are broad. You apply them to a GCP project and they affect all resources in that project. These are the Owner, Editor, and Viewer roles. If you're a viewer on a given resource, you can examine it, but not change its state. If you're an editor, you can do everything a viewer can do plus change its state. And if you're an owner, you can do everything an editor can do plus manage roles and permissions on the resource. The Owner role on a project also lets you do one more thing, set up billing. Often, companies want someone to be able to control the billing for a project without the right to change the resources in the project, and that's why you can grant someone the Billing Administrator role. Be careful, if you have several people working together on a project that contains sensitive data, primitive roles are probably too coarse. Fortunately, GCP IAM provides finer-grained types of roles. GCP services offer their own sets of predefined roles, and they define where those roles can be applied. For example, later in this course, we'll talk about Compute Engine, which offers virtual machines as a service. Compute Engine offers a set of predefined roles, and you can apply them to Compute Engine resources in a given project, a given folder, or in an entire organization. Another example, consider Cloud Bigtable, which is a managed database service. Cloud Bigtable offers roles that can apply across an entire organization to a particular project or even to individual Bigtable database instances.

IAM Roles
Compute Engine's Instance Admin role lets whoever has that role perform a certain set of actions on virtual machines. The actions are listing them, reading and changing their configurations, and starting and stopping them. And which virtual machines? Well that depends on where the roles apply. In this example, all the users of a certain Google group have the role, and they have it on all the virtual machines in project A. If you need something even finer grained, there are custom roles. A lot of companies have a least privileged model in which each person in your organization has the minimum amount of privilege needed to do his or job. So, for example, maybe I want to define an Instance Operator role to allow some users to start and stop Compute Engine in virtual machines, but not reconfigure them. Custom roles allow me to do that. A couple cautions about custom roles. First, you have to decide to use custom roles. You'll need to manage their permissions. Some companies decide they'd rather stick with the predefined roles. Second, custom roles can only be used at the project or organization levels. They can't be used at the folder level. What if you want to give permissions to a Compute Engine virtual machine rather than to a person? Then you would use a service account. For instance, maybe you have an application running in a virtual machine that needs to store data in Google Cloud Storage, but you don't want to let just anyone on the internet have access to that data, only that virtual machine. So, you'd create a service account to authenticate your VM to cloud storage. Service accounts are named with an email address. But instead of passwords, they use cryptographic keys to access resources. In this simple example, a service account has been granted Compute Engine's Instance Admin role. This would allow an application running in a VM with that service account to create, modify, and delete other VMs. Incidentally, service accounts need to be managed too. For example, maybe Alice needs to manage what can act as a given service account, while Bob just needs to be able to view them. Fortunately, in addition to being an identity, a service account is also a resource, so it can have IAM policies on its own attached to it. For instance, Alice can have an Editor role in the service account and Bob can have the Viewer role. This is just like granting roles for any other GCP resource. You can grant different groups of VMs in your project different identities. This makes it easier to manage different permissions for each group. You can also change the permissions of the service accounts without having to recreate the VMs. Here's a more complex example. Say you have an application that's implemented across a group of Compute Engine virtual machines. One component of your application needs to have an Editor role on another project, but another component doesn't. So you would create two different service accounts, one for each subgroup of virtual machines. Only the first service account has privilege on the other project. That reduces the potential impact of a miscoded application or a compromised virtual machine.

Interacting with Google Cloud Platform
There are four ways you can interact with Google Cloud Platform, and we'll talk about each in turn, the console, the SDK and Cloud Shell, the mobile app, and the APIs. The GCP console is a web-based administrative interface. If you build an application in GCP, you'll use it, although the end users of your application won't. It lets you view and manage all your projects and all the resources they use. It also lets you enable, disable, and explore the APIs of GCP services, and it gives you access to Cloud Shell. That's a command line interface to GCP that's easily accessed from your browser. From Cloud Shell, you can use the tools provided by the Google Cloud Software Development Kit, SDK, without having to first install them somewhere. What's the software development kit? We'll talk about that next. The Google Cloud SDK is a set of tools that you can use to manage your resources and your applications on GCP. These include the gcloud tool, which provides the main command line interface for Google Cloud Platform products and services. There's also gsutil, which is for Google Cloud Storage, and bq, which is for BigQuery. The easiest way to get to the SDK commands is to click the Cloud Shell button on the GCP console. You get a command line in your web browser on a virtual machine with all these commands already installed. You can also install the SDK on your own computers, your laptop, your on-premise servers, or virtual machines in other clouds. The SDK is also available as a Docker image, which is a really easy and clean way to work with it. The services that make up GCP offer application programming interfaces so that the code you write can control them. These APIs are what's called RESTful. In other words, they follow the Representational State Transfer paradigm. We don't need to go into much detail of what that means here. Basically, it means that your code can use Google services in much the same way that web browsers talk to web servers, the APIs name resources and GCP with URLs. Your code can pass information to the APIs using JSON, which is a very popular way of passing textual information over the web. And there's an open system for user login and access control. The GCP console lets you turn on and off APIs. Many APIs are off by default and many are associated with quotas and limits. These restrictions help protect you from using resources inadvertently. You can enable only those APIs you need, and you can request increases in quotas when you need more resources. For example, if you're writing an application that needs to control GCP resources, you'll need to get your use of the APIs just right. And to do that, you'll use APIs Explorer. The GCP console includes a tool called the APIs Explorer that helps you learn about the APIs interactively. It lets you see what APIs are available and in what versions. These APIs expect parameters and documentation on them is built in. You can try the APIs interactively, even with user authentication. Suppose you explored an API and you're ready to build an application that uses it. Do you have to start coding from scratch? No, Google provides client libraries that take a lot of the drudgery out of the task of calling GCP from your code. There are two kinds of libraries. Cloud Client Libraries are Google Cloud's latest and recommended libraries for its APIs. They adopt the native styles and idioms of each language. On the other hand, sometimes a Cloud Client Library doesn't support the newest services and features. In that case, you can use the Google API Client Library for your desired languages. These libraries are designed for generality and completeness. Finally, one more tool that's of interest to everyone, not just developers, there's a mobile app for Android and iOS that lets you examine and manage the resources you're using in GCP. It lets you build dashboards so that you can get the information you need at a glance.

Cloud Launcher
Say you want a quick way to get started with GCP with minimal effort. That's what Google Cloud Launcher provides. It's a tool for quickly deploying functional software packages on Google Cloud platform. There's no need to manually configure the software, virtual machine instances, storage or network settings, although you can modify many of them before you launch if you like. Most software packages in Cloud Launcher are at no additional charge beyond the normal usage fees for GCP resources. Some Cloud Launcher images charge usage fees, particularly those published by third parties with commercially licensed software, but they all show you estimates of their monthly charges before you launch them. Be aware that these estimates are just that, estimates. And in particular, they don't attempt to estimate networking costs since those will vary based on how you use the applications. A second note of caution. GCP updates the base images for these software packages to fix critical issues and vulnerabilities, but it doesn't update the software after it's been deployed. Fortunately, you'll have access to the deployed systems so you can maintain them.

Lab: Getting Started With Cloud Launcher
In this demonstration, I'll use Cloud Launcher to deploy a solution on Google Cloud Platform. The solution I've chosen is a LAMP stack. LAMP stands for Linux, Apache, MySQL, PHP. It's an easy environment for developing web applications. I'll use Cloud Launcher to deploy that stack into a Compute Engine instance. In the GCP console's Products and services menu, I click Cloud Launcher. In the search bar, I type lamp. LAMP stacks are environments for web development. Notice that estimated costs are provided. Now I click Launch on Compute Engine. I'll leave the deployment name at lampstack-1 and I'll accept the default GCP zone. I'll accept the other defaults and click Deploy. It takes a few minutes to create the deployment. When the deployment is finished, the console displays a summary of information about what has been deployed. Let's visit our website's temporary home page. It works. Let's perform some configuration. We'll log in using SSH. We'll change to the directory where the software is installed, and we'll copy in a test page for PHP. (Typing) Let's end our SSH session and confirm that our PHP test page is visible. Now I could continue to configure my PHP website. In this demonstration, I used Cloud Launcher to deploy a LAMP stack into a Compute Engine instance.

Lab Link
(no audio)

Virtual Machines and Networks in the Cloud
Module Introduction
Of all the ways you can run workloads in the cloud, virtual machines may be the most familiar. Compute Engine lets you run virtual machines on Google's global infrastructure. In this module, we'll learn how Google Compute Engine works, with a focus on Google virtual networking. One of the nice things about virtual machines is that they have the power and generality of a full-fledged operating system in each. You configure a virtual machine much like you build out a physical server, by specifying its amounts of CPU power and memory, its amounts and types of storage, and its operating system. You can flexibly reconfigure them. And, a VM running on Google's cloud has unmatched worldwide network connectivity.

Virtual Private Cloud (VPC) Network
The way a lot of people get started with GCP is to define their own virtual private cloud inside their first GCP project, or they can simply choose the default VPC and get started with that. Regardless, your VPC networks connect your Google Cloud Platform resources to each other and to the internet. You can segment your networks, use firewall rules to restrict access to instances, and create static routes to forward traffic to specific destinations. Here's something that surprises a lot of people who are new to GCP. The virtual private cloud networks that you define have global scope. They can have subnets in any GCP region worldwide, and subnets can span the zones that make up a region. This architecture makes it easy for you to define your own network layout with global scope. You can also have resources in different zones on the same subnet. You can dynamically increase the size of a subnet in a custom network by expanding the range of IP addresses allocated to it. Doing that doesn't affect already configured VMs. In this example, your VPC has one network. So far, it has one subnet defined, in GCP's us-east1 region. Notice that it has two Compute Engine VMs attached to it. They're neighbors on the same subnet even though they are in different zones. You can use this capability to build solutions that are resilient, but still have simple network layouts.

Compute Engine
Compute Engine lets you create and run virtual machines on Google infrastructure. There are no upfront investments, and you can run thousands of virtual CPUs on a system that is designed to be fast and to offer consistent performance. You can create a virtual machine instance by using a Google Cloud Platform console or the gcloud command line tool. Your VM can run Linux and Windows Server images provided by Google or customized versions of these images. And you can even import images from many of your physical servers. When you create a VM, pick a machine type which determines how much memory and how many virtual CPUs it has. These types range from very small to very large indeed. And, if you can't find a predefined type that meets your needs perfectly, you can make a custom VM. Speaking of processing power, if you have workloads like machine learning and data processing that can take advantage of GPUs, many GCP zones have GPUs available for you. Just like physical computers need disks, so do VMs. You can choose two kinds of persistent storage, standard or SSD. If your application needs high performance scratch space, you can attach a local SSD, but be sure to store data of permanent value somewhere else because local SSD's content doesn't last past when the VM terminates. That's why the other kinds are called persistent disks. Anyway, most people start off with standard persistent disks, and that's the default. You'll also choose a boot image. GCP offers lots of versions of Linux and Windows ready to go, and you can import your own images too. Lots of GCP customers want their VMs to always come up with certain configurations, like installing software packages on first boot. It's very common to pass GCP VM startup scripts that do just that. You can also pass in other kinds of metadata too. Once your VMs are running, it's easy to take a durable snapshot of their disks. You can keep these as backups, or use them when you need to migrate a VM to another region. Suppose you have a workload that no human being is sitting around waiting to finish, say a batch job analyzing a large dataset. You can save money by choosing preemptible VMs to run the job. A preemptible VM is different from an ordinary Compute Engine VM in only one respect, you've given Compute Engine permission to terminate it if its resources are needed elsewhere. You can save a lot of money with preemptible VMs, although be sure to make your job able to be stopped and restarted. You can choose the machine properties of your instances, such as the number of virtual CPUs and the amount of memory, by using a set of predefined machine types or by creating your own custom machine types. I mentioned a bit ago that you can make very large VMs in Compute Engine. At the time this video was produced, the maximum number of virtual CPUs in a VM was 96, and the maximum memory size was in beta at 624 GB. Check the GCP website to see where these maximums are today. These huge VMs are great for workloads like in-memory databases and CPU-intensive analytics, but most GCP customers start off with scaling out, not scaling up. Compute Engine has a feature called autoscaling that lets you add and take away VMs from your application based on load metrics. The other part of making that work is balancing the incoming traffic across the VMs, and Google VPC supports several different kinds of load balancing. We'll consider those in the next section.

Important VPC Capabilities
Much like physical networks, VPCs have routing tables. These are used to forward traffic from one instance to another instance within the same network, even across sub-networks, and even between GCP zones without requiring an external IP address. VPC's routing tables are built in. You don't have to provision or manage a router. Another thing you don't have to provision or manage for GCP, a firewall instance. VPCs give you a global distributed firewall you can control to restrict access to instances, both incoming and outgoing traffic. You can define firewall rules in terms of metadata tags on Compute Engine instances, which is really convenient. For example, you can tag all your web servers with, say, web, and write a firewall rule saying that traffic on ports 80 or 443 is allowed into all VMs with the web tag, no matter what their IP address happens to be. Remember I mentioned that VPCs belong to GCP projects? But what if your company has several GCP projects and the VPCs need to talk to each other? Don't worry, that's totally doable and manageable. If you simply want to establish a peering relationship between two VPCs so that they can exchange traffic, that's what VPC peering does. On the other hand, if you want to use the full power of IAM to control who and what in one project can interact with a VPC in another, that's what shared VPC is for. A few slides back we talked about how virtual machines can autoscale to respond to changing load, but how do your customers get to your application when it might be provided by 4 VMs one moment and 40 VMs at another? Cloud load balancing is the answer. Cloud load balancing is a fully distributed, software-defined managed service for all your traffic, and because the load balancers don't run in VMs you have to manage, you don't have to worry about scaling or managing them. You can put cloud load balancing in front of all your traffic, HTTP and HTTPS, other TCP and SSL traffic, and UDP traffic too. With cloud load balancing, a single anycast IP front ends all your back-end instances in regions around the world. It provides cross-region load balancing, including automatic multi-region failover, which gently moves traffic in fractions if back ends become unhealthy. Cloud load balancing reacts quickly to changes in users, traffic, back-end health, network conditions, and other related conditions. And what if you anticipate a huge spike in demand? Say your online game is going to be a hit, do you need to file a support ticket to warn Google of the incoming load? No. No so-called pre-warming is required. If you need cross-regional load balancing for a web application, use HTTPS load balancing. For secure sockets layer traffic that is not HTTP, use the global SSL proxy load balancer. If its other TCP traffic that does not use secure sockets layer, use the global TCP proxy load balancer. Those two proxy services only work for specific port numbers, and they only work for TCP. If you want to load balance UDP traffic or traffic on any port number, you can still load balance across a GCP region with the regional load balancer. Finally, what all those services have in common is that they're intended for traffic coming in to the Google network from the internet. But what if you want to load balance traffic inside your project, say between the presentation layer and the business logic layer of your application? For that, use the internal load balancer. It accepts traffic on a GCP internal IP address and load balances it across Compute Engine VMs. One of the most famous Google services that people don't pay for is 8.8 .8 .8, which provides a public domain name service to the world. DNS is what translates internet host names to addresses, and as you would imagine, Google has a highly developed DNS infrastructure. It makes 8.8 .8 .8 available so that everybody can take advantage of it. But what about the internet host names and addresses of applications you build in GCP? GCP offers Cloud DNS to help the world find them. It's a managed DNS service running on the same infrastructure as Google. It has low latency and high availability and it's a cost-effective way to make your applications and services available to your users. The DNS information you publish is served from redundant locations around the world. Cloud DNS is also programmable. You can publish and manage millions of DNS zones and records using the GCP console, the command-line interface or the API. Google has a global system of edge caches. You can use this system to accelerate content delivery in your application using Google Cloud CDN. Your customers will experience lower network latency, the origins of your content will experience reduced load, and you can save money too. Once you've set up HTTPS load balancing, simply enable Cloud CDN with a single checkbox. There are lots of other CDNs out there, of course. What if you're already using one? Chances are your CDN is a part of GCP's CDN Interconnect Partner Program, and you can continue to use it. Lots of GCP customers want to interconnect their other networks to their Google VCPs, such as on-premises networks or their networks in other clouds. There are many good choices. Many customers start with a virtual private network connection over the internet using the IPsec protocol. To make that dynamic, they use a GCP feature called Cloud Router. Cloud Router lets your other networks and your Google VPC exchange route information over the VPN using the Border Gateway Protocol. For instance, if you add a new subnet to your Google VPC, your on-premises network will automatically get routes to it. But, some customers don't want to use the internet, either because of security concerns or because they need more reliable bandwidth. They can consider peering with Google using direct peering. Peering means putting a router in the same public datacenter as a Google point of presence and exchanging traffic. Google has more than 100 points of presence around the world. Customers who aren't already in a point of presence can contract with a partner in the Carrier Peering program to get connected. One downside of peering, though, is that it isn't covered by a Google service level agreement. Customers who want the highest uptimes for their interconnection with Google should use Dedicated Interconnect in which customers get one or more direct private connections to Google. If these connections have topologies that meet Google specifications, they can be covered by up to a 99.99 % SLA. These connections can be backed up by a VPN for even greater reliability.

Lab: Getting Started with Compute Engine
In this demonstration, I will create two virtual machines in Compute Engine and connect to them, and I'll also connect between them using several different protocols. First, I'll create a virtual machine using the GCP console. In the Products and services menu, I scroll down to Compute Engine and choose VM instances. I click Create. I'm going to name my VM instance my-vm-1. I'll accept the zone that's offered to me. I'll accept the default machine type. I'll accept Debian GNU/Linux 9 for its operating system. I'll leave its identity and API access the same. And I'm going to modify its firewall to allow inbound HTTPS traffic. Now I click Create. Now I'll demonstrate building a virtual machine using the command line. To do this, I'll launch Cloud Shell. Let's put a Cloud Shell in its own window. I'd like to put this virtual machine in the same region, but a different zone as the previous one. Our first VM is in the us-central1 region. Let's display a list of all of the zones in that region. There are four zones and they're all up. I'm going to set my default zone for new virtual machines. I'm going to set my default zone for new virtual machines to zone c. Now I'll launch a new virtual machine using the gcloud command. This command creates a new virtual machine called my-vm-2. Its machine type will be n1-standard-1, it'll be a Debian Linux 9 version machine, and it'll be connected to my default subnet. Now it's been created. Now I'll close my Cloud Shell window. Let's refresh our VM instances display. Notice that both virtual machines are now listed. First, I'll SSH into my-vm-2. I'll try to ping my-vm-1. Success. Now I'm going to log in to my-vm-1 using ssh. Because I've never done so before, I'm asked to confirm the key fingerprint. Now I'm logged in to my-vm-1. Now I'm going to install a simple web server, and I will edit its default home page. In this demonstration, I'll use the nano text editor. I'm going to edit the home page simply to include a custom message. (Typing) Now I'll write out my file and exit. (Working) Now let's confirm that the web server is serving my new page. I'll use the Curl command-line web browser. (Typing) Yes, I see the message I included. Now let's exit my SSH session on my-vm-1 and return to my-vm-2. Can my-vm-2 see the message I put on the web server home page? Yes, again. Now I'm going to exit from this SSH session and return to the VM instances list. Notice that the external IP address is shown for my-vm-1. Let's attempt to connect to it. And here, once again, is my custom message. HTTP traffic is allowed into this virtual machine. In this demonstration I created two virtual machines in Compute Engine, one using the command line and another using the GCP console. I connected between them using SSH, ping, and HTTP.

Lab Link
(No audio)

Storage in the Cloud
Module Introduction
(Introduction) Every application needs to store data, maybe media to be streamed or sensor data from devices or customer account balances, or maybe the fact that my Dragonite has more than 2600 CP. Different applications and workloads require different storage database solutions. You already know that you can store data on your VM's persistent disk. Google Cloud Platform has other storage options to meet your needs for structured, unstructured, transactional, and relational data. In this module, I'll tell you about the core storage options, Cloud Storage, Cloud SQL, Cloud Spanner, Cloud Datastore, and Google Bigtable. Depending on your application, you might want to use one or several of these services to get the job done.

Cloud Storage
Let's start with Google Cloud Storage. What's object storage? It's not the same as file storage in which you manage your data as a hierarchy of folders. It's not the same as block storage in which your operating system manages your data as chunks of disk. Instead, object storage means you say to your storage, here, you keep this arbitrary bunch of bytes I give you, and the storage lets you address it with a unique key. That's it. Often, these unique keys are in the form of URLs, which means object storage interacts nicely with web technologies. Cloud Storage works just like that, except better. It's a fully managed, scalable service. That means that you don't need to provision capacity ahead of time. Just make objects and the service stores them with high durability and high availability. You can use Cloud Storage for lots of things. Serving website content, storing data for archival and disaster recovery, or distributing large data objects to your end users via direct download. Cloud Storage is not a file system. Because each of your objects in Cloud Storage has a URL, each feels like a file in a lot of ways. And that's okay to use the word file informally to describe your objects. But still, it's not a file system. You would not use Cloud Storage as the route file system of your Linux box. Instead, Cloud Storage is comprised of buckets you create and configure and use to hold your storage objects. These storage objects are immutable, which means that you do not edit them in place, but instead you create new versions. Cloud Storage always encrypts your data on the server side before it is written to disk, and you don't pay extra for that. Also, by default, data in transit is encrypted using HTTPS. Speaking of transferring data, there are services you can use to get large amounts of data in Cloud Storage conveniently. We'll discuss them later in this module. Once they're in Cloud Storage, you can move them onwards to other GCP storage services. Just as I discussed, your Cloud Storage files are organized into buckets. When you create a bucket, you give it a globally unique name, you specify a geographic location where the bucket and its contents are stored, and you choose a default storage class. Pick a location that minimizes latency for your users. In other words, if most of your users are in Europe, you probably want to pick a European location. Speaking of your users, there are several ways to control access to your objects and buckets. For most purposes, Cloud IAM is sufficient. Roles are inherited from project to bucket to object. If you need finer control, you can create access control lists, ACLs, that offer finer control. ACLs define who has access to your buckets and objects, as well as what level of access they have. Each ACL consists of two pieces of information. A scope, which defines who can perform the specified actions, for example a specific user or group of users, and a permission, which defines what actions can be performed, for example, read or write. Remember, I mentioned that Cloud Storage objects are immutable, you can turn on object versioning on your buckets if you want. If you do, Cloud Storage keeps a history of modifications. That is, it overrides or deletes all of the objects in the bucket. You can list the archive versions of an object, restore an object to an older state or permanently delete a version as needed. If you don't turn on object versioning, new always overrides old. What if versioning sounds good to you, but you're worried about junk accumulating? Cloud Storage also offers lifecycle management policies. For example, you could tell Cloud Storage to delete objects older than 365 days, or you could tell it to delete objects created before January 1, 2013, or keep only the three most recent versions of each object in a bucket that has versioning enabled.

Cloud Storage Interactions
Cloud Storage lets you choose among four different types of storage classes. Regional, Multi-Regional, Nearline, and Coldline. Here's how to think about them. Multi-Regional and Regional are high performance object storage, whereas Nearline and Coldline are backup and archival storage. That's why I place that heavy dividing line between these two groups. All of the storage classes are accessed in comparable ways using the Cloud Storage API, and they all offer millisecond access times. Now let's talk about how they differ. Regional storage lets you store your data in a specific GCP region, us-central1, europe-west1 or asia-east1. It's cheaper than Multi-Regional storage, but it offers less redundancy. Multi-Regional storage, on the other hand, costs a bit more, but its georedundant. That means you pick a broad geographical location, like the United States, the European Union, or Asia, and Cloud Storage stores your data in at least two geographic locations separated by at least 160 kilometers. Multi-Regional storage is appropriate for storing frequently accessed data. For example, website content, interactive workloads or data that's part of mobile and gaming applications. People use Regional storage, on the other hand, to store data close to their Compute Engine virtual machines or their Kubernetes Engine clusters. That gives better performance for data intensive computations. Nearline storage is a low cost, highly durable service for storing infrequently accessed data. The storage class is a better choice than Multi-Regional storage or Regional storage in scenarios where you plan to read or modify your data once a month or less, on average. For example, if you want to continuously add files to Cloud Storage and plan to access those files once a month for analysis, Nearline storage is a great choice. Coldline storage is a very low cost, highly durable service for data archiving, online backup, and disaster recovery. Coldline storage is the best choice for data that you plan to access, at most, once a year. This is due to its slightly lower availability, 90-day minimum storage duration, cost for data access, and higher per operation costs. For example, if you want to archive data or have access to it in case of a disaster recovery event. Availability of these storage classes varies, with Multi-Regional having the highest availability of 99.95 % followed by Regional with 99.9 % and Nearline and Coldline with 99%. As for pricing, all storage classes incur a cost per gigabyte of data stored per month, with Multi-Regional having the highest storage price and Coldline the lowest storage price. Egress and data transfer charges may also apply. In addition to those charges, Nearline storage also incurs an access fee per gigabyte of data read. And Coldline storage incurs a higher fee per gigabyte of data read. Regardless of which storage class you choose, there are several ways to bring data into Cloud Storage. Many customers simply use gsutil, which is the Cloud Storage command from the Cloud SDK. You can also move data in with a drag and drop in the GCP console if you use the Google Chrome browser. But what if you have to upload terabytes or even petabytes of data? Google Cloud Platform offers the online Storage Transfer Service and the offline Transfer Appliance to help. The Storage Transfer Service lets you schedule and manage batch transfers to Cloud Storage from another Cloud provider, from a different Cloud Storage region or from an HTTPS endpoint. The Transfer Appliance is a rackable, high-capacity storage server that you lease from Google Cloud. You simply connect it to your network, load it with data, and then ship it to an upload facility where the data is uploaded to Cloud Storage. This service enables you to securely transfer up to a petabyte of data on a single appliance. As of this recording it's still beta and it's not available everywhere, so check the website for details. There are other ways of getting your data into Cloud Storage, as this storage option is tightly integrated with many of the Google Cloud Platform products and services. For example, you can import and export tables from and to BigQuery, as well as Cloud SQL. You can also store App Engine logs, Cloud Data Store backups, and objects used by App Engine applications like images. Cloud Storage can also store instant startup scripts, Compute Engine images, and objects used by Compute Engine applications. In short, Cloud Storage is often the ingestion point for data being moved into the Cloud and is frequently the long-term storage location for data.

Cloud Bigtable
Cloud Bigtable is Google's NoSQL, big data database service. What does NoSQL mean? Well, this isn't a database course, so I'll give you a very informal picture. Think first of a relational database as offering you tables in which every row has the same set of columns and the database engine enforces that rule and other rules you specify for each table. That's called the database schema. An enforced schema is a big help for some applications and a huge pain for others. Some applications call for a much more flexible approach, for example, a NoSQL schema. In other words, for these applications not all the rows might need to have the same columns, and in fact, the database might be designed to take advantage of that by sparsely populating the rows. That's part of what makes a NoSQL database what it is. Which brings us to Bigtable. Your databases in Bigtable are sparsely populated tables that can scale to billions of rows and thousands of columns, allowing you to store petabytes of data. GCP fully manages the service so you don't have to configure and tune it. It's ideal for data that has a single lookup key. Some application developers think of Bigtable as a persistent hatch table. Cloud Bigtable is ideal for storing large amounts of data with very low latency. It supports high throughput, both read and write, so it's a great choice for both operational and analytical applications, including Internet of Things, user analytics, and financial data analysis. Cloud Bigtable is offered through the same open source API as HBase, which is the native database for the Apache Hadoop project. I'll talk more about Hadoop later in the course. Anyway, having the same API enables portability of applications between HBase and Bigtable. Given that you could manage your own Apache HBase installation, you might ask yourself why should I choose Bigtable? Here are a few reasons why you might. First, scalability. If you manage your own HBase installation, scaling past a certain rate of queries per second is going to be tough. But with Bigtable, you can just increase your machine count, which doesn't even require downtime. Also, Cloud Bigtable handles administration tasks like upgrades and restarts transparently. All data in Cloud Bigtable is encrypted in both inflight and at rest. You can even use IAM permissions to control who has access to Bigtable data. One last reference point, Bigtable is actually the same database that powers many of Google's core services, including Search, Analytics, Maps, and Gmail. As Cloud Bigtable is part of the GCP ecosystem, it can interact with other GCP services and third-party clients. From an application API perspective, data can be read from and written to Cloud Bigtable through a data service layer like managed VMs, the HBase REST server or a Java sever using the HBase client. Typically, this will be to serve data to applications, dashboards, and data services. Data can also be streamed in through a variety of popular stream processing frameworks like Cloud Dataflow streaming, Spark Streaming, and Storm. If streaming is not an option, data can also be read from and written to Cloud Bigtable through batch processes like Hadoop MapReduce, Dataflow or Spark. Often summarized or newly calculated data is written back to Cloud Bigtable or to a downstream database.

Cloud SQL and Cloud Spanner
A moment ago I discussed NoSQL databases. Now, let's turn our attention to relational database services. Remember, these services use a database schema to help your application keep your data consistent and correct. A moment ago I discussed NoSQL databases. Now, let's turn our attention to relational database services. Remember, these services use a database schema to help your application keep your data consistent and correct. Another feature of relational database services that helps with the same goal, transactions. Your application can designate a group of database changes as all or nothing. Either they all get made or none do. Without database transactions, your online bank wouldn't be able to offer you the ability to move money from one account to another. What if after subtracting $10, 000 from one your accounts, some glitch prevented it from adding that $10, 000 to the destination account? Your bank would have just misplaced $10, 000. Classically, relational databases are a lot of work to set up, maintain, manage, and administer. If that doesn't sound like a good use of your time, but you still want the protections of a relational database, consider Cloud SQL. It offers your choice of the MySQL or PostgreSQL database engines as a fully managed service. Cloud SQL offers both MySQL and PostgreSQL databases that are capable of handling terabytes of storage. As of this recording, Cloud SQL for PostgreSQL is in beta, so check the website for details of its status. Of course you could always run your own database server inside a Compute Engine virtual machine, which a lot of GCP customers do, but there are some benefits of using the Cloud SQL managed service instead. First, Cloud SQL provides several replica services like read, failover, and external replicas. This means that if an outage occurs, Cloud SQL can replicate data between multiple zones with automatic failover. Cloud SQL also helps you back up your data with either on demand or scheduled backups. It can also scale both vertically by changing the machine type and horizontally via read replicas. From a security perspective, Cloud SQL instances include network firewalls and customer data is encrypted when on Google's internal networks and when stored in database tables, temporary files, and backups. Another benefit of Cloud SQL instances is that they are accessible by other GCP services and even external services. You can authorize Compute Engine instances for access Cloud SQL instances and can configure the Cloud SQL instance to be in the same zone as your virtual machine. Cloud SQL also supports other applications and tools that you might be used, to like SQL Workbench, Toad, and other external applications using standard MySQL drivers. If Cloud SQL does not fit your requirements because you need horizontal scalability, consider using Cloud Spanner. It offers transactional consistency at a global scale, schemas, SQL, and automatic synchronous replication for high availability, and it can provide petabytes of capacity. Consider using Cloud Spanner if you have outgrown any relational database, are sharding your databases for throughput high performance, need transactional consistency, global data and strong consistency, or just want to consolidate your database. Natural use cases include financial applications and inventory applications.

Cloud Datastore
We already discussed one GCP NoSQL database service, Cloud Bigtable. Another highly scalable, NoSQL database choice for your applications is Cloud Datastore. One of its main use cases is to store structured data from App Engine apps. You can also build solutions that span App Engine and Compute Engine with Cloud Datastore as the integration point. As you'd expect from a fully managed service, Cloud Datastore automatically handles sharding and replication, providing you with a highly available and durable database that scales automatically to handle load. Unlike Cloud Bigtable, it also offers transactions that affect multiple database rows and it lets you do SQL-like queries. To get you started, Cloud Datastore has a free, daily quota that provides storage, reads, writes, deletes, and small operations at no charge.

Comparing Storage Options
Now that we've covered GCP's core storage options, let's compare them to help you choose the right service for your application or workflow. This table focuses on the technical differentiators of the storage services. Each row has a technical specification and each column is a service. Let me cover each service from left to right. Consider using Cloud Datastore if you need to store unstructured objects or if you require support for transactions and SQL-like queries. This storage service provides terabytes of capacity with a maximum unit size of 1 MB per entity. Consider using Cloud Bigtable if you need to store a large amount of structured objects. Cloud Bigtable does not support SQL's queries, nor does it support multi-row transactions. This storage service provides petabytes of capacity with a maximum unit size of 10 MB per cell and 100 MB per row. Consider using Cloud Storage if you need to store immutable blobs large than 10 MB, such as large images or movies. This storage service provides petabytes of capacity with a maximum unit size of 5 TB per object. Consider using Cloud SQL or Cloud Spanner if you need full SQL support for an online transaction processing system. Cloud SQL provides terabytes of capacity while Cloud Spanner provides petabytes. If Cloud SQL does not fit your requirements because you need horizontal scalability, not just through read replicas, consider using Cloud Spanner. We didn't cover BigQuery in this module as it sits on the edge between data storage and data processing, but you will learn more about it in the Big Data and Machine Learning in the Cloud Module. The usual reason to store data in BigQuery is to use its big data analysis and interactive querying capabilities. You would not want to use BigQuery, for example, as the backing store for an online application. Considering the technical differentiators of the different storage services helps some people decide which storage service to choose. Others like to consider use cases. Let me go through each service one more time. Cloud Datastore is the best for semi-structured application data that is used in App Engine's applications. Bigtable is best for analytical data with heavy read/write events like ad tech, financial or IoT data. Cloud Storage is best for structured and unstructured binary or object data like images, large media files, and backups. SQL is best for web frameworks and existing applications like storing user credentials and customer orders. Cloud Spanner is best for large scale database applications that are larger than 2 TB. For example, for financial trading and eCommerce use cases. As I mentioned at the beginning of the module, depending on your application, you might use one or several of these services to get the job done.

Lab: Getting Started with Cloud Storage and Cloud SQL
[Autogenerated] First we create a Web server and the G C B consoles, products and service is menu. I scroll down to compute engine and choose VM instances. I'll create an instance. I'll name the instance blawg host and a leave it in the offered zoon us Central one, eh? I'll take the other defaults and I'll configure the firewall to allow http traffic in. I also want to add a startup script. This start up script will install a Web server. I click create and the virtual Machine Instances created for me Notice its external I P address. We'll need that leader now I'm going to make a cloud storage bucket using cloud shell, I enter the command gs uto m b dash l that I named the location in which I want the bucket to reside. In this case, the U. S. Multi region. The name of my cloud storage bucket must be globally unique. The easiest way to make sure of this is to name the bucket after my gcb project I D, which is also globally unique in the cloud shell The environment Variable Dev Shell Project I d always contains my project I d now I've created my bucket. I'm going to copy a graphical image from another cloud storage bucket. This one called cloud Training. Now I have the graphical image here in my directory. My excellent blawg dot png Now I used the g Sutil cp Command again toe upload it to my own cloud storage bucket. I can see the resulting file both from the command line using GS you told L s and also from the G c P Consuls storage browser. There's my bucket and there's the object I created in the bucket. Recall that my VM instances in Zone U. S Central 18 Now we'll create a Cloud sequel instance in the same zone in the products and service is menu we scroll down to sequel. We choose my sequel for our database engine Second generation. We name our instance blawg D B and we define a root password will place this instance in the same zone as our compute instance When the data's base instance has been made, we click on its name to configure it with my database Instance finished provisioning I can click on its name to configure it. I want to create a my sequel user name called blogged Eby User. Well, define a password. Now I want to configure this database incident so that can be only contacted from my virtual machine. Instance. So I need to go back to its entry in the VM instances listing and capture its public I p address. There it is, will copy it. We returned to our sequel instance. Click on our Instance name and click authorization. We wish to authorize a network consisting only of the desired Veum. Instance. We'll name the network Web front end and insert the I P address of that instance, followed by a slash 32 now our database instances protected from broad Internet access. Now I'm going to return to my virtual machine and configure it to use. The resource is we've set up. I'll log into it using SS h. I'm going to edit It's PHP home page. I've prepared a page that I can paste in. I'll fill in my databases. I p address and password noticed the comment in a real blawg. We would never store the mice equal password anywhere in the document root. Instead, we restored in a separate configuration file somewhere else in the Web server virtual machine. Now let's try it. We'll restart the Web server _____. Now we'll return to the G C P Consuls Veum Instances List and attempt to view the home page. Our database connection succeeded. If this were a real blawg, we would now begin to load Blawg content into our sequel database. Now let's enhance our home page by adding our graphical image to it. In the G. C B console will navigate to the storage browser and create a public link to our graphical image. There's the image we check the box share publicly that gives us, ah, hyperlink that we conclude. Now we return to our Ph Be home page and add in an HTML, a reference to that image. Now let's return to our PHP home page and refresh it. Our page now contains an image hosted in Google Cloud storage

Lab Link
Containers in the Cloud
Module Introduction
(music) Welcome to this module on Containers and Google Kubernetes Engine. We've already discussed Compute Engine, which is GCP's Infrastructure as a Service offering, with access to servers, file systems, and networking, and App Engine, which is GCP's paths offering. Now I'm going to introduce you to Containers and Kubernetes Engine, which is a hybrid that conceptually sits between the two with benefits from both. I'll describe why you want to use containers and how to manage them in Kubernetes Engine.

Introduction to Containers
Let's begin by remembering that Infrastructure as a Service allows you to share compute resources with other developers by virtualizing the hardware using virtual machines. Each developer can deploy their own operating system, access the hardware, and build their applications in a self-contained environment with access to RAM, file systems, networking interfaces, and so on. But flexibility comes with a cost. The smallest unit of Compute is an app with it VM. The guest OS may be large, even gigabytes in size, and takes minutes to boot. But you have your tools of choice on a configurable system so you can install your favorite runtime, web server, database, middleware, configure the underlying system resources, such as disk space, disk IO, or networking, and build as you like. However, as demand for your application increases, you have to copy an entire VM and boot the guest OS for each instance of your app, which can be slow and costly. With App Engine, you get access to programming services, so all you do is write your code in self-contained workloads that use these services and include any dependent libraries. As demand for your app increases, the platform scales your app seamlessly and independently by workload and infrastructure. This scales rapidly, but you won't be able to fine tune the underlying architecture to save cost. That's where containers come in. The idea of a container is to give you the independent scalability of workloads and an abstraction layer of the OS and hardware. What you get is an invisible box around your code and its dependencies with limited access to your own partition of the file system and hardware. It only requires a few system calls to create and starts as quickly as a process. All you need on each host is an OS kernel that supports containers and a container runtime. In essence, you're virtualizing the OS, it scales like paths, but gives you nearly the same flexibility as IaaS. With this abstraction, your code is ultra-portable and you can treat the OS and hardware as a black box. So you can go from development to staging to production or from your laptop to the cloud without changing or rebuilding anything. If you want to scale, for example, a web server, you can do so in seconds and deploy dozens or hundreds of them, depending on the size of your workload, on a single host. Now, that's a simple example of scaling one container, running a whole application on a single host. You'll likely want to build your application using lots of containers, each performing their own function like microservices. If you build them like this and connect them with network connections, you can make them modular, deploy easily, and scale independently across a group of hosts. And the host can scale up or down and start and stop the containers on demand, as demand for your application changes or as hosts fail. A tool that helps you do this well is Kubernetes. Kubernetes makes it easy to orchestrate many containers on many hosts, scale them as microservices, and deploy rollouts and rollbacks. First, I'll show you how to build and run containers. I'll use an open source tool called Docker that defines a format for bundling your application, its dependencies, and machine-specific settings into a container. You could use a different tool, like Google Container Builder, it's up to you. Here's an example of some code you may have written. It's a Python app. It says, "Hello World! " or, if you hit this endpoint, it gives you the version. So how do you get this app into Kubernetes? You may have to think about your version of Python, what dependency you have on Flask, how to use your requirements.txt file, or how to install Python, and so on. So you use a Dockerfile to specify how your code gets packaged into a container. For example, if you're used to using Ubunto with all your tools, you start there. You can install Python the same way you would on your dev environment. You can take your requirements file that you know from Python and you can use tools inside Docker or Container Builder to install your dependencies the way you want. Eventually, it produces an app. And here's how you run it. Then you use the docker build command to build the container. This builds the container and stores it locally as a runnable image. You can save and upload the image into a container registry service and share or download it from there. Then you use the docker run command to run the image. As it turns out, packaging applications is only about 5% of the issue. The rest has to do with application configuration, service discovery, managing updates, and monitoring. These are the components of a reliable, scalable distributed system.

Kubernetes
Now I'll show you where Kubernetes comes in. Kubernetes is an open source orchestrator that abstracts containers at a higher level so you can better manage and scale your applications. At the highest level, Kubernetes is a set of APIs that you can use to deploy containers on a set of nodes called a cluster. The system is divided into a set of master components that run as a control plane and a set of nodes that run containers. In Kubernetes, a node represents a computing instance, like a machine. In Google Cloud, nodes are virtual machines running in Compute Engine. You can describe a set of applications and how they should interact with each other and Kubernetes figures how to make that happen. Now that you've built a container, you'll want to deploy it into a cluster. Kubernetes can be configured with many options and add-ons, but can be time consuming to bootstrap from the ground up. Instead, you can bootstrap Kubernetes using Kubernetes Engine, or GKE. GKE is hosted Kubernetes by Google. GKE clusters can be customized, they can support different machine types, numbers of nodes, and network settings. To start up Kubernetes on a cluster in GKE, all you do is run this command. At this point, you should have a cluster called k1 configured and ready to go. You can check its status in admin console. Then you deploy containers on nodes using a wrapper around one or more containers called a pod. A pod is the smallest unit in Kubernetes that you create or deploy. A pod represents a running process on your cluster as either a component of your application or an entire app. Generally, you only have one container per pod, but if you have multiple containers with a hard dependency, you can package them into a single pod and share networking and storage. The pod provides a unique network IP and set of ports for your containers and options that govern how a container should run. Containers inside a pod can communicate with one another using a local host and ports that remain fixed as they're started and stopped on different nodes. One way to run a container in a pod in Kubernetes is to use the kubectl run command. We'll learn a better way later in this module, but this gets you started quickly. This starts a deployment of a container running in a pod. And the container is an image of the nginx server. A deployment represents a group of replicas of the same pod and keeps your pod running, even when nodes they run on fail. It could represent a component of an application or an entire app. In this case, it's the nginx web server. To see the running nginx pods, run the command kubectl get pods. By default, pods in a deployment are only accessible inside your GKE cluster. To make them publicly available, you can connect a load balancer to your deployment by running kubectl expose command.

Kubernetes Engine
Kubernetes creates a service with a fixed IP for your pods. And a controller says, I need to attach an external load balancer with a public IP address to that service so others outside the cluster can access it. In GKE, the load balancer is created as a Network Load Balancer. Any client that hits that IP address will be routed to a pod behind the service. In this case, there's only one, your simple nginx pod. The service is an abstraction, which defines a logical set of pods and a policy by which to access them. As deployments create and destroy pods, pods get their own IP address, but those addresses don't remain stable over time. A service groups a set of pods and provides a stable endpoint or fixed IP for them. For example, if you create two sets of pods called front end and back end, and you put them behind their own services, back end pods may change, but front end pods are not aware of this. They simply refer to the back end service. You can run the kubectl get services command to get the public IP to hit the nginx container remotely. To scale a deployment, run the kubectl scale command. In this case, three pods are created in your deployment and they're placed behind the service and share one fixed ID. You could also use auto-scaling with all kinds of parameters here's an example of how to auto-scale the deployment to between 10 and 15 pods when CPU utilization reaches 80%. So far I've shown you how to run imperative commands like expose and scale. This works well to learn and test Kubernetes step by step. But the real strength of Kubernetes comes when you work in a declarative way. Instead of issuing commands, you provide a configuration file that tells Kubernetes what you want your desired state to look like and Kubernetes figures out how to do it. Let me show you how to scale your deployment using an existing deployment configuration file. To get the file, you can run a kubectl get pods command like the following. And you'll get a deployment configuration that looks like the following. In this case, it declares you want three replicas of your nginx pod. It defines a selector field, so your deployment knows how to group specific pods as replicas and you add a label to the pod template so that it gets selected. To run five replicas instead of three, all you do is update the configuration file, and run the kubectl comply command to use the config file. Now look at your replicas to see their updated state. Then use a kubectl get pods command to watch the pods come online. In this case, all five are ready and running. And check the deployment to make sure the proper number of replicas are running using either kubectl get deployments or kubectl describe deployments. In this case, all five pod replicas are available. And you can still hit your end point like before using kubectl get services to get the external IP of the service and hit the public IP from a client. At this point, you have all five copies of your nginx pod running in GKE and you have a single service that's proxying the traffic to all five pods. This allows you to share the load and scale your service in Kubernetes. The last question is, what happens when you want to update a new version of your app? You want to update your container to get new code out in front of users, but it would be risky to roll out all those changes at once. So you use kubectl rollout or change your deployment configuration file and apply the changes using kubectl apply. New pods will be created according to your update strategy. Here's an example configuration that will create a new version of your pods one by one and wait for a new pod to be available before destroying one of the old pods.

Getting Started with Kubernetes Engine
There are a lot of features in Kubernetes in GKE we haven't even touched on, such as configuring health checks, setting session affinity, managing different rollout strategies, and deploying pods across regions for high availability. But for now, that's enough. In this module, you've learned how to build and run containerized applications, orchestrate and scale them on a cluster, and deploy them using rollouts. Now you'll see how to do it in a demo and practice it in a lab exercise.

Lab: Getting Started with Kubernetes Engine
[Autogenerated] In this demonstration, I'll create a kubernetes engine cluster and will deploy a load balance service to it. I'll scale the service and we'll see what happens first. Let's use the G c P Consul to confirm that the AP eyes we need are enabled in the products and service is menu. We scroll down to AP eyes and service is we're looking for the Kubernetes engine, a p I and the container Registry a p I. There's the kubernetes engine, a p I. It's enabled there's container registry. It's also enabled. Now we're ready to start. A cluster for this activity will use the command line in the cloud shell for convenience, I'm going to define an environment variable that contains my preferred G C P zone. Now launch a kubernetes cluster. In that zone. The cluster will have two nodes. Now the cluster is ready. Let's confirm what version of kubernetes the cluster is running. It's version 1.8. When you launch a kubernetes cluster, you may see a newer version. Remember that kubernetes cluster nodes are compute engine virtual machines. Let's go back to the G c P. Consul, interview them in the products and service is Menu Scroll to compute engine and click on VM instances. There are cluster nudes. We can also view the cluster in the kubernetes engine console. The council reports the cluster name, its location and its size. Now let's return to Cloud Shell. Let's run a Web server in our cluster. We've created a Cooper Netease deployment called in Genetics. The deployment consists of a single pod. Let's confirm that it's running. There's our pod. Now let's expose the deployment we created so that clients from outside KUBERNETES can access it. Now let's view the new service. It takes a moment for an external I P address to be assigned. Now an external AP address has been assigned. Let's attempt to visit this I p address. Using a Web browser, we see the engine Ex Home page, our Web server running inside of a group. Entities deployment is accessible from the Internet. Now let's scale up our deployment. We would do this if load were rising. We named the deployment and specify the new number of replicas. Now let's look at the new number of pods. There are additional pods. Now they're in the running state. Let's also confirm that the external I p address did not change the extra L. A P address is the same. Let's refresh the home page. We have confirmed that the Web server deployment continues to work in this lab. I created a kubernetes engine cluster. I deployed a load balance service to it and we tried to scaling operation, and we saw how seamlessly it worked.

Lab Link
Applications in the Cloud
Introduction to App Engine
(introduction) So, we've discussed two GCP products that provide the compute infrastructure for applications, Compute Engine and Kubernetes Engine. What these have in common is that you choose the infrastructure in which your application runs based on virtual machines for Compute Engine and containers for Kubernetes Engine. But what if you don't want to focus on the infrastructure at all, you just want to focus on your code? That's what App Engine is for. I'll tell you more about it in this module. Let's start with PaaS. Recall that a PaaS is a Platform as a Service. The App Engine platform manages the hardware and networking infrastructure required to run your code. To deploy an application on App Engine, you just hand App Engine your code and the App Engine service takes care of the rest. App Engine provides you with the built-in services that many web applications need, NoSQL databases, in-memory caching, load balancing, health checks, logging, and a way to authenticate users. You code your application to take advantage of these services, and App Engine provides them. App Engine will scale your application automatically in response to the amount of traffic it receives, so you only pay for those resources you use. There are no servers for you to provision or maintain. That's why App Engine is especially suited for applications where the workload is highly variable or unpredictable, like web applications and mobile back end. App Engine offers two environments, standard and flexible. I'll explain what each is and how to choose.

Google App Engine Standard Environment
Of the two App Engine environments, standard is the simpler. It offers a simpler deployment experience than the flexible environment and fine-grained auto scaling. Like the standard environment, it also offers a free daily usage quota for the use of some services. What's distinctive about the standard environment though is that low utilization applications might be able to run at no charge. Google provides App Engine software development kits in several languages so that you can test your application locally before you upload it to the real App Engine service. The SDKs also provide simple commands for deployment. Now, you may be wondering, but what does my code actually run on? I mean, what exactly is the executable binary? App Engine's term for this kind of binary is the runtime. In App Engine standard environment, you use a runtime provided by Google. We'll see your choices shortly. App Engine standard environment provides runtimes for specific versions of Java, Python, PHP, and Go. The runtimes also include libraries that support App Engine APIs, and for many applications, the standard environment runtimes and libraries may be all you need. If you want to code in another language, standard environment is not right for you. You'll want to consider the flexible environment. The standard environment also enforces restrictions on your code by making it run in a so-called sandbox, that's a software construct that's independent of the hardware, operating system, or physical location of the server it runs on. The sandbox is one of the reasons why App Engine standard environment can scale and manage your application in a very fine-grained way. Like all sandboxes, it imposes some constraints. For example, your application can't write to the local file system, it'll have to write to a database service instead if it needs to make data persistent. Also, all the requests your application receives has a 60-second timeout and you can't install arbitrary third-party software. If these constraints don't work for you, that would be a reason to choose the flexible environment. Here's a diagram of how you will use App Engine standard environment in practice. You'll develop your application and run a test version of it locally using the App Engine SDK. Then, when you're ready, you'll use the SDK to deploy it. Each App Engine application runs in a GCP project. App Engine automatically provisions server instances and scales and load balances them. Meanwhile, your application can make calls to a variety of services using dedicated APIs. Here are a few examples. A NoSQL datastore to make data persistent, caching of that data using Memcached, searching, logging user login, and the ability to launch actions not triggered by direct user requests, like task queues and a task scheduler.

Google App Engine Flexible Environment
Suppose you've decided that the restrictions of App Engine standard environment sandbox model don't work for you, but you still want to take advantage of the benefits of App Engine. That's what App Engine flexible environment is for. Instead of the sandbox, App Engine flexible environment lets you specify the container your App Engine runs in. Yes, containers. Your application runs inside Docker containers on Google Compute Engine virtual machines, VMs. App Engine manages these Compute Engine machines for you. They're health checked, healed as necessary, and you get to choose which geographical region they run in. And critical, backward compatible updates to their operating systems are automatically applied. All this so that you can just focus on your code. App Engine flexible environment apps use standard runtimes, can access App Engine services such as Datastore, Memcached, task queues, and so on. Here's a side-by-side comparison of standard and flexible. Notice that standard environment starts up instances of your application faster, but that you get less access to the infrastructure in which your application runs. For example, flexible environment lets you SSH into the virtual machines on which your application runs. It lets you use local disk for scratch space, it lets you install third-party software, and it lets your application make calls to the network without going through App Engine. On the other hand, standard environment's billing can drop to 0 for the completely idle application. Because we mentioned App Engine's use of Docker containers, you may be wondering how App Engine compares to Kubernetes Engine. Here's a side-by-side comparison of App Engine with Kubernetes Engine. App Engine standard environment is for people who want the service to take maximum control of their application's deployment and scaling. Kubernetes Engine gives the application owner the full flexibility of Kubernetes. App Engine flexible edition is somewhere in between. Also, App Engine environment treats containers as a means to an end, but for Kubernetes Engine, containers are a fundamental organizing principle.

Google Cloud Endpoints and Apigee Edge
I've mentioned application programming interfaces, APIs, several times in this course. Let's be precise about what an API is. A software service's implementation can be complex and changeable. What if, to use that service, other pieces of software had to know internal details about how they worked. That would be a mess. So instead, application developers structure the software they write so that it presents a clean, well-defined interface that abstracts away needless details, and then they document that interface. That's an API. The underlying implementation can change, as long as the interface doesn't. And other pieces of software that use the API don't have to know or care. Sometimes you have to change an API, say to add or deprecate a feature. To make this kind of API change cleanly, developers version their APIs. Version 2 of an API might contain calls that version 1 does not. Programs that consume the API can specify the API version that they want to use in their calls. Supporting an API is a very important task, and Google Cloud Platform provides two API management tools. They approach related problems in a different way and each has a particular strength. Suppose you're developing a software service in one of GCP's back ends. You'd like to make it easy to expose this API. You'd like to make sure it's only consumed by other developers whom you trust. You'd like an easy way to monitor and log its use. You'd like for the API to have a single, coherent way for it to know which end user is making the call. That's when you use Cloud Endpoints. It implements these capabilities, and more, using an easy to deploy proxy in front of your software service, and it provides an API console to wrap up those capabilities in an easy to manage interface. Cloud Endpoints supports applications running in GCP's Compute platforms, in your choice of languages, and your choice of client technologies. Apigee Edge is also a platform for developing and managing API proxies. It has a different orientation, though. It has a focus on business problems, like rate limiting, quotas, and analytics. Many users of Apigee Edge are providing a software service to other companies, and those features come in handy. Because the back-end services for Apigee Edge need not be in GCP, engineers often use it when they are taking apart a legacy application. Instead of replacing a monolithic application in one risky move, they can instead use Apigee Edge to peel off its surfaces one by one, standing up microservices to implement each in turn until the legacy application can be finally retired.

Lab: Getting Started with App Engine
[Autogenerated] In this demonstration, I'll create a simple APP engine application using the cloud Shell local development environment. And then I'll deploy that application to APP engine so we can play with it. Let's start working with AP Inge and using Cloud Shell. I'll start by cloning the source code for a simple APP engine application from Get Hub. Now that I have the source code, let's go into the tree in which it is stored. Let's examine the file ap dot yeah mo This file specifies the configuration of the application. The AP Thiemo file specifies the runtime for this application as well as handlers and required libraries, handlers or the Urals on which it responds. Let's run the application locally here in Cloud Show, using the built in APP engine development environment. Now the application is running locally. Let's use cloud shells built in previewing capability to preview it. Here's a local instance of our application. It appears to work. Recall that the application is running locally here in Cloud show. We can confirm that by going to the APP engine dashboard in the products and service is maybe we squirrel down to app engine and choose dashboard noticed that no applications air deployed. Let's return to cloud shone now. Well, press control C to abort the development server. Now we're ready to deploy our application to the real app. Engine service were prompted for the region. We'll choose the one that's geographically close to us. Where s to confirm the deployment APP? Engine files were stored in Google Cloud storage. Now let's go back to the APP engine Dash War. Let's refresh There's a link to our application. A top right. This is the deployed version of the application. When we're done with this application, we can disable it using the APP engine console. We scrolled on two settings and click Disable application were asked to confirm the applications. I d. Now the application has been disabled. In this demonstration, I created a simple AF engine application. I started off in the cloud shell local development environment and then I deployed it toe app engine so he could see it in the management interface

Lab Link
Developing, Deploying, and Monitoring in the Cloud
Development in the Cloud
People create great applications in the Google Cloud. Popular tools for development, deployment, and monitoring just work in GCP. You also have options for tools that are tightly integrated with GCP, and in this module, I'll explain them. Let's start with talking about development. Lots of GCP customers use Git to store and manage their source code trees. That means running their own Git instances or a using a hosted Git provider. Running your own is great because you have total control. Using a hosted Git provider is great because it's less work. What if there were a third way? Maybe a way to keep code private to a GCP project and use IAM permissions to protect it, but not have to maintain the Git instance yourself? That's what Cloud Source Repositories is. It provides Git version control to support your team's development of any application or service, including those that run on App Engine, Compute Engine, and Kubernetes Engine. With Cloud Source Repositories, you can have any number of private Git repositories, which allows you to organize the code associated with your cloud project in whatever way works best for you. Cloud Source Repositories also contains a source viewer so that you can browse and view repository files from within the GCP console. Many applications contain event-driven parts. For example, maybe you have an application that lets users upload images. Whenever that happens, you need to process that image in various ways. Convert it to a standard image format, thumbnail it to various sizes, and store each in a repository. You could always integrate this function into your application, but then you have to worry about providing compute resources for it, no matter whether it happens once a day or once a millisecond. What if you could just make that provisioning problem go away? It would be great if you could write a single-purpose function that did the necessary image manipulations and then arrange for it to automatically run whenever a new image gets uploaded. That's exactly what Cloud Functions lets you do. You don't have to worry about servers or runtime binaries, you just write your code in JavaScript for a Node.js environment that GCP provides, and then configure when it should fire. There's no need for you to pay for servers, either. You just pay whenever your functions run in 100-ms intervals. Cloud Functions can trigger on events in Cloud Storage, Cloud Pub/Sub, or an HTTP call. Here's how setting up a Cloud Function works. You choose which events you care about. For each event type, you tell Cloud Functions you're interested in it. These declarations are called triggers. Then, you attach JavaScript functions to your triggers. From now on, your functions will respond whenever the events happen. Some applications, especially those that have a microservices architecture, can be implemented entirely in Cloud Functions. People also use Cloud Functions to enhance existing applications without having to worry about scaling.

Deployment Infrastructure as Code
Setting up your environment in GCP can entail many steps, setting up compute, network, and storage resources, and keeping track of their configurations. You can do it all by hand if you want to, taking an imperative approach. In other words, you figure out the commands you need to set up your environment the way you want. If you want to change your environment, you figure out the commands to change it from the old state to the new. If you want to clone your environment, you do all those commands again. This is a lot of work. It's more efficient to use a template. That means a specification of what the environment should look like. It's declarative rather than imperative. GCP provides Deployment Manager to let you do just that. It's an infrastructure management service that automates the creation and management of your Google Cloud Platform resources for you. To use it, you create a template file, using either the YAML Markup language or Python, that describes what you want the components of your environment to look like. Then, you give the template to Deployment Manager, which figures out and does the actions needed to create the environment your template describes. If you need to change your environment, edit your template, and then tell Deployment Manager to update the environment to match the change. Here's a tip. You can store and version control your Deployment Manager templates in Cloud Source Repositories.

Monitoring Proactive Instrumentation
You can't run an application stably without monitoring. Monitoring lets you figure out whether the changes you made were good or bad. It lets you respond with information rather than with panic when one of your end users complains that your application is down. Stackdriver is GCP's tool for monitoring, logging, and diagnostics. Stackdriver gives you access to many different kinds of signals from your infrastructure platforms, virtual machines, containers, middleware, and application tier, logs, metrics, and traces. It gives you insight into your application's health, performance, and availability, so if issues occur, you can fix them faster. Here are the core components of Stackdriver, Monitoring, Logging, Trace, Error Reporting, and Debugging. Stackdriver Monitoring checks the endpoints of web applications and other internet-accessible services running on your cloud environment. You can configure uptime checks associated with URLs, groups, or resources such as instances and load balancers. You can set up alerts on interesting criteria, like when health check results or uptimes fall into levels that need action. You can use Monitoring with a lot of popular notification tools, and you can create dashboards to help you visualize the state of your application. Stackdriver Logging lets you view logs from your applications and filter and search on them. Logging also lets you define metrics based on log contents that are incorporated into dashboards and alerts. You can also export logs to BigQuery, Cloud Storage, and Cloud Pub/Sub. Stackdriver Error Reporting tracks and groups the errors in your cloud applications, and it notifies you when new errors are detected. With Stackdriver Trace, you can sample the latency of App Engine applications and report per-URL statistics. How about debugging? A painful way to debug an existing application is to go back into it and add lots of logging statements. Stackdriver Debugger offers a different way. It connects your application's production data to your source code so you can inspect the state of your application at any code location in production. That means you can view the application state without adding logging statements. Stackdriver Debugger works best when your application source code is available, such as in Cloud Source Repositories, although it can be in other repositories too.

Lab: Getting Started with Deployment Manager and Stackdriver
[Autogenerated] In this demonstration, I'll use deployment manager to create a G C P deployment and maintain it. I'll also view resource usage in a virtual machine using Google Stack driver. I'll work with deployment manager using cloud shell For the sake of convenience, I'm going to define an environment variable containing my preferred G C P Zone. I already have an environment very well containing my gcb project i d. Now I will create my deployment manager template using a text editor. I've created my template in another window and I'll paste it in here. Now let's take a look at what we have. This template defines a virtual machine is a resource. It's named my VM. This virtual machine is defined tohave a startup script apt to get update, which calls for its database of applications to be updated. Beware that the syntax of deployment manager requires you to specify the preferred zone in a few places and the G C B project I D. In a few places. Well, substitute them into this file using the said command. All right, my file out to disk. Now let's use said to substitute in our project I d the said command has no output when it is successful, similarly will use the said command to substitute in our preferred zone. Let's look at the file again and see our results. R G C P Zone has been substituted into the template, as has our G C P Project I D. Now we're ready to build a deployment. From this template, the template will create a new deployment in Compute Engine. It'll be called My First Devil, and it will use the template. My deployed, our demo. It's done. We can confirm it status using the G Cloud Command. There's our deployment, my first Epple. Now let's look at the resulting virtual machine. We'll go to the Compute Engine VM Instances Page. There's our virtual machine, my dash V M. We click on its name to open its details and scroll down, and there's a start of script. I have to get update. Now let's make a change to our template. I'm gonna add a command to the startup script. This new command installs the engine X light Web server. Now we'll update our deployment. Now that the update is complete, let's go back to our virtual machine. Instance detail page And now look again at the startup script. The startup script has been updated. Now let's put some CPU load on the VM we created and monitor it using stack driver. First, we log into the virtual machine by connecting to it using S S H. This Lennox command creates an artificial CPU load on the virtual machine. In essence, it forces the CPU to continuously attempt to compress random data. Now let's return to the G C P console and set up stack driver monitoring in the products and service is menu. We scroll down to stack driver monitoring. We confirmed that we want to create an account and we'll confirm that we wish to monitor a G C P project. We don't want to monitor any AWS accounts, so we'll click Skip eight of us set up for this particular activity. There's no need to install the stack driver monitoring agent, so we just click continue. We don't need to get reports by email for this demonstration. So we click No reports. Now we click launch monitoring. We'll click. Continue with the trial. Notice that our CPU utilization increased sharply few minutes ago when we started our artificial load in this demonstration I used to G c P Service is deployment manager and Google Stack driver using deployment manager. I created a deployment and I maintained it. Using Google Stack driver, I viewed resource usage inside of a virtual machine.

Lab Link
Big Data and Machine Learning in the Cloud
Introduction to Big Data and Machine Learning
Google believes that in the future every company will be a data company because making the fastest and best use of data is a critical source of competitive advantage. Google Cloud provides a way for everybody to take advantage of Google's investments in infrastructure and data processing innovation. Google Cloud has automated out the complexity of building and maintaining data and analytic systems. In this module, I'll tell you about Google's technologies for getting the most out of data fastest. Whether it's real-time analytics or machine learning, these tools are intended to be simple and practical for you to embed in your applications so that you can put data into the hands of your domain experts and get insights faster.

Google Cloud Big Data Platform
Google Cloud Big Data solutions are designed to help you transform your business and user experiences with meaningful data insights. We like to call it an integrated serverless platform. What does that mean? Serverless means you don't have to worry about provisioning compute instances to run your jobs. The services are fully managed and you pay only for the resources you consume. The platform is integrated so that GCP Data Services work together to help you create custom solutions. Apache Hadoop is an open source framework for big data. It is based on the MapReduce programming model, which Google invented and published. The MapReduce model is, at its simplest, means that one function, traditionally called the map function, runs in parallel with a massive dataset to produce intermediate results. And another function, traditionally called the reduce function, builds a final result set based on all those intermediate results. The term Hadoop is often used informally to encompass Apache Hadoop itself and related projects such as Apache Spark, Apache Pig, and Apache Hive. Cloud Dataproc is a fast, easy, managed way to run Hadoop, Spark, Hive, and Pig on Google Cloud Platform. All you have to do is request a Hadoop cluster. It will be built for you in 90 seconds or less on top of Compute Engine Virtual Machines whose number and type you control. If you need more or less processing power while your cluster's running, you can scale it up or down. You can use the default configuration for the Hadoop software in your cluster, or you can customize it, and you can monitor your cluster using Stackdriver. Running on-premises Hadoop jobs requires a capital hardware investment. Running these jobs in Cloud Dataproc allows you to only pay for hardware resources used during the life of the cluster you create. Although the rate for pricing is based on the hour, Cloud Dataproc is billed by the second. All Cloud Dataproc clusters are billed in 1-second clock time increments, subject to a 1-minute minimum billing. So, when you're done with your cluster, you can delete it and billing stops. This is a much more agile use of resources than on-premise hardware assets. You can also save money by telling Cloud Dataproc to use preemptible Compute Engine instances for your batch processing. You have to make sure that your jobs can be restarted cleanly if they're terminated and you get a significant break in the cost of the instances. At the time this video was made, preemptible instances were around 80% cheaper. Be aware that the cost of the Compute Engine instances isn't the only component of the cost of a Dataproc cluster, but it's a significant one. Once your data is in a cluster, you can use Spark and Spark SQL to do data mining. And you can use MLlib, which is Apache Spark's Machine Learning Libraries, to discover patterns through machine learning.

Dataflow
Cloud Dataproc is great when you have a dataset of known size or when you want to manage your cluster size yourself. But what if your data shows up in real-time, or it's of unpredictable size or rate? That's where Cloud Dataflow is a particularly good choice. It's both a unified programming model and a managed service, and it lets you develop and execute a big range of data processing patterns, extract, transform, and load, batch computation and continuous computation. You use Dataflow to build data pipelines, and the same pipelines work for both batch and streaming data. There's no need to spin up a cluster or to size instances. Cloud Dataflow fully automates the management of whatever processing resources are required. Cloud Dataflow frees you from operational tasks, like resource management and performance optimization. In this example, Dataflow pipeline reads data from a BigQuery table, the Source, processes it in a variety of ways, the Transforms, and writes its output to Cloud Storage, the Sink. Some of those transforms you see here are map operations and some are reduce operations. You can build really expressive pipelines. Each step in the pipeline is elastically scaled. There is no need to launch and manage a cluster. Instead, the service provides all resources on-demand. It has automated and optimized work partitioning built in, which can dynamically rebalance lagging work. That reduces the need to worry about hotkeys, that is, situations where disproportionately large chunks of your input get mapped to the same cluster. People use Dataflow in a variety of use cases. As we've discussed, it's a general purpose ETL tool and its use case as a data analysis engine comes in handy in things like fraud detection and financial services, IoT analytics in manufacturing, healthcare and logistics, and clickstream, point of sale and segmentation analysis in retail. And because those pipelines we saw can orchestrate multiple services, even external services, it can be used in real-time applications, such as personalizing gaming user experiences.

BigQuery
Suppose instead of a dynamic pipeline, your data needs to run more in the way of exploring a vast sea of data. You want to do ad hoc SQL queries on a massive dataset, that's what BigQuery is for. It's Google's fully managed, petabyte scale, low cost analytics data warehouse. Because there's no infrastructure to manage, you can focus on analyzing data to find meaningful insights, use familiar SQL, and take advantage of our pay-as-you-go model. It's easy to get data into BigQuery, you can load it from Cloud Storage or Cloud Datastore or stream it into BigQuery at up to 100, 000 rows per second. Once it's in there, you can run superfast SQL queries against multiple terabytes of data in seconds using the processing power of Google's infrastructure. In addition to SQL queries, you can easily read and write data in BigQuery via Cloud Dataflow, Hadoop, and Spark. BigQuery is used by all types of organizations from startups to Fortune 500 companies. Smaller organizations like BigQuery's free monthly quotas. Bigger organizations like its seamless scale and its available 99.9 % service level agreement. Google's infrastructure is global, and so is BigQuery. BigQuery lets you specify the region where your data will be kept. So, for example, if you want to keep data in Europe, you don't have to go set up a cluster in Europe. Just specify the EU location where you create your dataset. U.S. and Asia locations are also available. Because BigQuery separates storage and computation, you pay for your data storage separately from queries. That means, you pay for queries only when they are actually running. You have full control over who has access to the data stored in BigQuery, including sharing datasets with people in different projects. If you share datasets that won't impact your cost or performance, people you share with pay for their own queries, not you. Long-term storage pricing is an automatic discount for data residing in BigQuery for extended periods of time. When the age of your data reaches 90 days in BigQuery, Google will automatically drop the price of storage.

Pub/Sub And Datalab
Whenever you're working with events in real-time, it helps to have a messaging service. That's what Cloud Pub/Sub is. It's meant to serve as a simple, reliable, scalable foundation for stream analytics. You can use it to let independent applications you build send and receive messages. That way they're decoupled, so they scale independently. The Pub in Pub/Sub is short for publishers, and Sub is short for subscribers. Applications can publish messages in Pub/Sub and one or more subscribers receive them. Receiving messages doesn't have to be synchronous, that's what makes Pub/Sub great for decoupling systems. It's designed to provide at least once delivery at low latency. When we say, at least once delivery, we mean that there is a small chance some message might be delivered more than once, so keep this in mind when you write your application. Cloud Pub/Sub offers on-demand scalability to 1 million messages per second and beyond. You just choose the quota you want. Cloud Pub/Sub builds on the same technology Google uses internally. It's an important building block for applications where data arrives at high and unpredictable rates, like Internet of Things systems. If you're analyzing streaming data, Cloud Dataflow is the natural pairing with Pub/Sub. Pub/Sub also works well with applications built on GCP's Compute Platforms. You can configure your subscribers to receive messages on a push or pull basis. In other words, subscribers can get notified when new messages arrive for them, or they can check for new messages at intervals. Scientists have long used lab notebooks to organize their thoughts and explore their data. For data science, the lab notebook metaphor works really well because it feels natural to intersperse data analysis with comments about their results. A popular environment for hosting those is Project Jupyter. It lets you create and maintain web-based notebooks containing Python code, and you can run that code interactively and view the results. And Cloud Datalab takes the management work out of this natural technique, it runs in a Compute Engine Virtual Machine. To get started, you specify the virtual machine type you want and what GCP region it should run in. When it launches, it presents an interactive Python environment that's ready to use and it orchestrates multiple GCP services automatically, so you can focus on exploring your data. You only pay for the resources you use, there's no additional charge for Datalab itself. It's integrated with BigQuery, Compute Engine, and Cloud Storage, so accessing your data doesn't run into authentication hassles. When you're up and running, you can visualize your data with Google Charts or Matplotlib. And because there's a vibrant, interactive, Python community, you can learn from published notebooks. There are many existing packages for statistics, machine learning, and so on.

Google Cloud Machine Learning Platform
Machine learning is one branch of the field of artificial intelligence. It's a way of solving problems without explicitly coding the solution. Instead, human coders build systems that improve themselves over time through repeated exposure to sample data, which we call training data. Major Google applications use machine learning like YouTube, Photos, the Google Mobile app, and Google Translate. The Google Machine Learning Platform is now available as a Cloud service so that you can add innovative capabilities to your own applications. Cloud Machine Learning Platform provides modern machine learning services with pretrained models and a platform to generate your own tailored models. As with other GCP products, there's a range of services that stretches from the highly general to the pre-customized. TensorFlow is an open source software library that's exceptionally well-suited for machine learning applications like neural networks. It was developed by Google Brain for Google's internal use and then open sourced so that the world could benefit. You can run TensorFlow wherever you like, but GCP is an ideal place for it because machine learning models need lots of on-demand compute resources and lots of training data. TensorFlow can also take advantage of Tensor Processing Units, which are hardware devices designed to accelerate Machine Learning workloads with TensorFlow. GCP makes them available in the cloud with Compute Engine Virtual Machines. Each Cloud TPU provides up to 180 teraflops of performance, and because you pay for only what you use, there's no upfront capital investment required. Suppose you want a more managed service. Google Cloud Machine Learning Engine lets you easily build machine learning models that work on any type of data of any size. It can take any TensorFlow model and perform large-scale training on a managed cluster. Finally, suppose you want to add various machine learning capabilities to your applications without having to worry about the details of how they are provided. Google Cloud also offers a range of Machine Learning APIs suited to specific purposes, and I'll discuss them in a moment. People use the Cloud Machine Learning Platform for lots of applications. Generally, they fall into two categories depending on whether the data to work on is structured or unstructured. Based on structured data, you can use ML for various kinds of classification and regression tasks, like customer churn analysis, product diagnostics, and forecasting. It can be the heart of a recommendation engine for content personalization and cross-sales and up-sales. You can use ML to detect anomalies, as in fraud detection, sensor diagnostics, or log metrics. Based on unstructured data, you can use ML for image analytics such as identifying damaged shipment, identifying styles, and flagging content. You can do text analytics too, like a call center log analysis, language identification, topic classification, and sentiment analysis. In many of the most innovative applications for machine learning, several of these kinds of applications are combined. What if, whenever one of your customers posted praise for one of your products on social media, your application could automatically reach out to them with a customized discount on another product they'll probably like? The Google Cloud Machine Learning platform makes that kind of interactivity well within your grasp.

ML APIs
The Cloud Vision API enables developers to understand the content of an image. It quickly classifies images into thousands of categories, sailboat, lion, Eiffel Tower, detects individual objects within images, and finds and reads printed words contained within images. Like the other APIs I'm describing here, it encapsulates powerful machine learning models behind an easy to use API. You can use it to build metadata on your image catalog, moderate offensive content, or even do image sentiment analysis. The Cloud Speech API enables developers to convert audio to text. Because you have an increasingly global user base, the API recognizes over 80 languages and variants. You can transcribe the text of users, dictating in an application's microphone, enable command and control through voice, or transcribe audio files. The Cloud Natural Language API offers a variety of natural language understanding technologies to developers. It can do syntax analysis, breaking down sentences supplied by our users into tokens. Identify the nouns, verbs, adjectives, and other parts of speech, and figure out the relationships among the words. It can do entity recognition. In other words, it can parse text and flag mentions of people, organizations, locations, events, products, and media. It can understand the overall sentiment expressed in a block of texts. It has these capabilities in multiple languages, including English, Spanish, and Japanese. Cloud Translation API provides a simple, programmatic interface for translating an arbitrary string into a supported language. When you don't know the source language, the API can detect it. The Cloud Video Intelligence API lets you annotate videos in a variety of formats. It helps you identify key entities, that is nouns, within your video, and when they occur. You can use it to make video content searchable and discoverable. At the time this video was produced, the Cloud Video Intelligence service was in beta, so check the GCP website for updates.

Lab: Getting Started with BigQuery
[Autogenerated] In this demonstration, I'll take a somewhat large log file and loaded up into be query. Using Big Query, I'll perform sequel queries on the data In order to gain some insight about the patterns represented in the log, we'll start by loading data from cloud storage into Big Weary in the G C B consuls products and service is menu. I'll scroll down to Big Query inside my G C P project. I'll create a new data set. I'll name my data, said Log Data. And I'll choose for its location, the location that's geographically closest to me. Now that the data said has been created Going to add a table for the source location. I'm going to give a U R L That points to a C S V file I staged into a cloud storage bucket. The C S V file contains Web server log access data for my destination table name. I'll specify excess log. I'll tell big query to automatically detect the scheme off, I'll accept. The rest of the defaults include Create table. It takes a moment for the load to occur. Now the load is complete. I can click on the table name to view it schema. This chemo was automatically created by examining the types of data in the fields. We can click preview to look at some of the data. Now let's perform a query on this table using the big query Web user interface. We click composed Query will taste in this sequel Query and run it. We can browse the query results. We can also run a query using the B Q Command from within Cloud Shell because this is our first use, were asked to confirm which G. C P project we're using. Here are our query results. In this demonstration, I loaded a long filing to be query, and I performed sequel queries to gain insight about what the log represented.

Lab Link
Summary and Review
Course Review
In this short module, I'll look back on what we've covered in this course. Remember the continuum that this course discussed at the very beginning? The continuum between managed infrastructure and dynamic infrastructure. GCP's compute services are arranged along this continuum, and you can choose where you want to be on it. Choose Compute Engine if you want to deploy your application in virtual machines that run on Google's infrastructure. Choose Kubernetes Engine if you want instead to deploy your application in containers that run on Google's infrastructure. In a Kubernetes cluster, you're defining control. Choose App Engine instead if you just want to focus on your code, leaving most infrastructure and provisioning to Google. App Engine Flexible Environment lets you use any runtime you want and gives you full control of the environment in which your application runs. App Engine Standard Environment lets you choose from a set of standard runtimes and offers finer grain scaling and scale to 0. To completely relieve yourself from the chore of managing infrastructure, build or extend your application using cloud functions. You supply chunks of code for business logic and your code gets spun up on demand in response to events. GCP offers a variety of ways to load balance inbound traffic. Use global HTTPS load balancing to put your web application behind a single Anycast IP to the entire internet. It load balances traffic among all your back-end instances in regions around the world, and it's integrated with GCP's content delivery network. If your traffic isn't HTTP or HTTPS, you can use the global TCP or SSL proxy for traffic on many ports. For other ports or for UDP traffic, use the regional load balancer. Finally, to load balance the internal tiers of a multi-tier application, use the internal load balancer. GCP also offers a variety of ways for you to interconnect your on-premises or other cloud networks with your Google VPC. It's simple to set up a VPN and you can use Cloud Router to make it dynamic. You can also peer with Google at its many worldwide points of presence, either directly or through a carrier partner. Or if you need a service level agreement and can adopt one of the required network topologies, use Dedicated Interconnect. Consider using Cloud Datastore if you need to store structured objects or if you require support for transactions and SQL-like queries. Consider using Cloud Bigtable if you need to store a large amount of single key data, especially structured objects. Consider using Cloud Storage if you need to store immutable binary objects. Consider using Cloud SQL or Cloud Spanner if you need full SQL support for an online transaction processing system. Cloud SQL provides terabytes of capacity while Cloud Spanner provides petabytes and horizontal scalability. Consider BigQuery if you need interactive querying in an online analytical processing system with petabytes of scale. I'd like to zoom into one of those services we just discussed, Cloud Storage, and remind you of its four storage classes. Multi-regional and regional are the classes for warm and hot data. Use multi-regional especially for content that's being served to a global web audience, and use regional for working storage for compute operations. Nearline and Coldline are the classes for, as you guess, cooler data. Use Nearline for backups and for infrequently accessed content, and use Coldline for archiving and disaster recovery.

Next Steps
[Autogenerated] if you're a cloud architect, a dev ops person or any other kind of I t professional who deploys, migrates and maintains applications in the cloud. Continue with specialization. Architect ING with Google Cloud Platform If you're an application programmer or any other kind of software engineer who writes code for the cloud, continue to the developing applications with Google Cloud Platform.

Course author
Author: Google Cloud	
Google Cloud
Google Cloud can help solve your toughest problems and grow your business. With Google Cloud, their infrastructure is your infrastructure. Their tools are your tools. And their innovations are your...

Course info
Level
Intermediate
Rating
4.8 stars with 20 raters(20)
My rating
null stars

Duration
2h 55m
Updated
17 Jan 2019
Share course

