Test-driven Development: The Big Picture

by Jason Olson

Test-driven development is a development practice that helps runaway maintenance costs, and enables developers to build higher quality software. This course shows you why test-driven development is important and what problems it can solve.

Developing software can be expensive. Surprisingly, much of this cost is found in the maintenance of the software. Test-driven development aims to help with these runaway costs, as well as enabling developers to build higher quality software. In this course, Test-driven Development: The Big Picture, you'll learn the basics of test-driven development, a development practice aimed at building high quality software. First, you'll explore what test-driven development is and why it exists. Next, you'll learn the different terminology and tools of the trade when doing test-driven development. Finally, you'll learn how to utilize test-driven development by working through a series of test-driven development examples. When you're finished with this course, you'll have the necessary knowledge to start building software using your test-driven development skills, and improve the quality of software.
Course author
Author: Jason Olson
Jason Olson

Jason Olson is a software engineer passionate about distributed computing and cloud-based technology. He is a full stack developer at Concur, and formerly a Technical Evangelist and Program Manager...
Course info
Level
Beginner
Rating
4.5 stars with 160 raters(160)
My rating
null stars
Duration
1h 50m
Released
14 Apr 2017
Share course

Course Overview
Course Overview

Hi everyone. My name is Jason Olson, and welcome to my course, Test-driven Development: The Big Picture. I'm a senior software engineer at Concur Technologies where I build and maintain distributed systems everyday with a focus on high availability and fault tolerance. So developing software can be expensive. Surprisingly, much of this cost is found in the maintenance of the software. Test-driven development aims to help with these runaway costs, as well as enabling developers to build higher quality software in the first place. This course is a quick introduction to test-driven development, and no prior experience with TDD is required. Some of the major topics that we will cover include what TDD is and why it exists, the different terminology and tools of the trade when doing test-driven development, how to practice test-driven development by working through a series of TDD examples, and the gotchas you need to watch out for, and other topics you should know when discussing TDD with other developers. By the end of this course, you will have the necessary knowledge to start building software using test-driven development skills and to help improve the quality of software you build. I hope you'll join me on this journey to learn test-driven development with this Test-driven Development: The Big Picture course, here at Pluralsight.
What Is Test-driven Development? (TDD)
Introduction

Hi. This is Jason Olson, and this module is the first module of a course to help you understand the big picture of test-driven development. Test-driven development is a software development process aimed at delivering software that end customers need and delivering it with very high quality. In this first module, I want to give you the context needed to understand not only what test-driven development is, but why it provides value in the first place. In future modules, we will cover different ways of testing software, take a look at test-driven development in action, strategies and techniques for effectively testing code, and potential gotchas you are likely to run into as you start practicing test-driven development. So with that said, let's get started.
Getting the Most from This Course

First, prerequisites. This course is a basic course introducing test-driven development. No prior knowledge on test-driven development is required. You are likely to get more value from this course if you have some experience writing code. We will be showing code snippets in both C# and JavaScript, but a passing familiarity with any C syntax-based language is likely to be enough. The primary audience for this course are those that are involved in the day to day of creating and shipping software. This includes not only developers and testers, but also development managers. To get the most from this course, feel free to join the discussion. If you have any questions, you can find the Discussion tab on the Pluralsight page for this course. I am also available on Twitter @jolson88. I would love to hear about your experiences with this course.
Software Development Challenges

To understand why test-driven development exists and what value it provides to software developers and businesses, we first need to look at the challenges facing developing software today. There are many phases to creating and delivering software, starting with analysis and design of what needs to be built, through development and testing, integrating with other systems or features, and then maintaining the software and features once they are release. And I'm not talking about waterfall versus agile here. Agile often just means this cycle is compressed into smaller and faster cycles with a focus on iterative development in delivering a value. The same constraints exist in both agile and waterfall, but are some phases more expensive than others? As developers, we tend to overly focus on the design or implementation of a feature, but other phases actually cost more in the long run. In the grand scheme of things, analysis, design, and implementation are some of the lowest costs associated with software development. Testing and integration is more expensive than those, and maintenance, above all, is the most expensive cost in developing software. You read that right. It is estimated that as much as 65% of software costs come from maintaining the software that we develop. Why is maintaining the software we build so expensive though? There are three constraints that tend to accompany software development that contribute to the cost of maintaining our software. First, like many other areas in life, code suffers from entropy over time. As systems around our code change, or get updated, our code left untouched tends to become brittle. It also becomes more rigid as it doesn't change with the growing needs of the business. Just like a human muscle can atrophy without use, wasting away into nothingness, so can our code. Another factor impacting a business's capability of maintaining software is isolated code ownership. Applications, services, and even features tend to be owned by a given team or a given developer. That owner becomes an expert in that domain and is the only one that knows all the ins and outs of the problem space. This presents challenges if anything happens to the owner, preventing them from working on the software. This can be from the developer leaving the company or as simple as the team's priorities changing as the business around them changes. Ultimately, this leads to a lack of empowerment for other people to update or change the software as needed, and so the software usually starts lagging behind and ultimately succumbs to ongoing technical debt or a continued lack of investment. Another challenge is that software is often validated quite infrequently. Some companies only validate software during the lead up to a new release. This process might take hours, days, or even sometimes weeks. Unfortunately for some software, focus is spent on verifying feature's work rather than the features not breaking. So not as much emphasis is put on identifying when software is explicitly broken, especially for edge cases and boundary conditions. This can contribute to software that is very regression prone, where one change in one part of the system ends up regressing or breaking a different part of the system. This leads to there being a very high risk for code changes. Developers and management become less interested in making any changes to the code because of the high risk it entails. These are all challenges that we want to address to help us lower the cost of maintenance and help make the development of software cheaper and less error prone than it would otherwise be. A lot of these challenges are typically associated with what we call legacy code. Of course, if you ask five developers what legacy code is, you'll likely get five different answers. There are some common definitions that you tend to find though. Some people consider legacy code to be source code inherited from somebody else. It is code that you aren't familiar with and code that you haven't worked with before. Other people consider legacy code to be source code from an older version of their software. It is likely written using outdated tools and platforms. Sometimes it was created with software that isn't even supported by the software vendor anymore. Either way, rarely do developers consider legacy code to be code that was just written last week or last month, especially if it was code that they personally wrote. My favorite definition though of legacy code comes from the book Working Effectively With Legacy Code. Author Michael Feathers' definition of legacy code is any code that doesn't have automated tests for it. Code without automated tests is often code that is more brittle, more error prone to breaking when changed, and is likely validated very infrequently because it requires manual test suites, or perhaps no test suites at all. Code without automated tests suffers from most of the shortcomings that make maintaining software much more expensive than it really needs to be.
What Is TDD?

So what is test-driven development? To understand test-driven development, we first need to understand what a test is. Short story even shorter, a test is something that verifies your code works as it is expected to. This can be verification of quality, performance, or even reliability, as well as functionality. Primarily, this is done before the software is released into staging, production, or some other widespread use. To get started, you can think of tests as needing to verify three things. Does the code satisfy the requirements as specified by the customer? In other words, is the code solving the problem it was intended to solve? Tests also verify that the code responds properly to all inputs. This can also mean that the code should fail appropriately if invalid input is given to it. Remember, don't focus on just the positive, happy path test cases. Finally, tests can verify that the code has acceptable performance and reliability characteristics. So how do these tests extend to doing test-driven development? Test-driven development is a software development process. Just like it sounds, this process is driven by the process of writing tests. It has a reputation for a very short development cycle. As the focus is always on the tests, capturing the needs of the customer, it provides a good vehicle to focus on continuing to deliver customer value. The main mantra that you are likely to hear over and over about test-driven development is red/green/refactor. Red/green/refactor is a cycle. You are always continuing this cycle as you are developing software. First, you write a test that fails. This test is verifying one part of proper behavior for the system under test. You write the test, which will fail immediately because, of course, the feature itself hasn't been implemented yet. Next, you do the minimal work necessary to make the test pass. For initial tests, this might even include hardcoding a return value. The goal is to only write the code necessary to make the system behave as expected. As more tests are created and made to pass, the behavior of the system will become more robust. Many developers are surprised that the solutions this process often delivers results in solutions that are much less complicated than originally thought. Finally, you refactor the code you wrote to clean it up while having the test continually pass after each refactoring. This is the phase of making the code maintainable and flexible as compared to just make it work while we are getting the test to pass. This is the main process when using test-driven development. You are continually getting feedback on the behavior of the system and iteratively improving the software with each successive test. Red/green/refactor.
History of TDD

So with a number of developers talking about test-driven development these days, you might wonder whether test-driven development is a new thing or not. Believe it or not, test-driven development has actually been around for quite a while. Many people point to the book extreme Programming explained by Kent Beck as the first time that test-driven development was discussed. The book was released originally in 1999 and followed up with another book in 2002 called TDD: By Example. When people think of the lineage of TDD, these are the primary resources they think about. Believe it or not though, NASA was using a test-focused approach all the way back in the 1960s for Project Mercury. In Project Mercury, the tests were developed in parallel with the code. When you think of the cost of a bug in a space program, this makes perfect sense, but that still wasn't TDD in the purest sense of the phrase like we know today. However, in 1989 the practices of TDD were starting to formalize a bit in the Smalltalk programming community. In 1994, Kent Beck released a paper called Simple Smalltalk Testing that discussed the framework SUnit. This is the genesis of what we now refer to xUnit frameworks as that are so prevalent in TDD today. Even the framework JUnit that was leveraged in the extreme Programming book was a near-direct port of SUnit into Java by Erich Gamma of Gang of Four Fame and Kent Beck. So while TDD is a very popular practice in writing software today, and you find xUnit frameworks in nearly every programming language and platform, it also has a long and storied history, and is a process that has been well vetted and used in the development of a lot of software over the last 30+ years.
Why Practice TDD?

As a developer, why should you actually practice TDD? Yes, it has an established history and aims to solve many software development challenges, but perhaps you feel your situation is different, or maybe your team has different challenges that you feel TDD can't solve. Well, there are many different reasons on why practicing test-driven development can be beneficial. First of all, there are a series of benefits to the business itself through the use of TDD. It provides a way to verify the requirements of the software that is being developed by writing tests and adhering to red/green/refactor, requirement verification is built in to the software development process itself. TDD also provides a way to catch regressions where previously working functionality stops working with a newer release. When the tests verify the requirements of the delivered software, regressions end up causing test failures, catching the problems as early as possible. Of course, not all regressions are going to be caught, but another aspect of TDD is that when a bug or regression is found, the first thing that happens is to write a test that fails, confirming that the software isn't behaving as expected. When the test is made to pass, and the software no longer regresses, future regressions will be prevented as there's now a new test to catch that form of regression. Finally, when you factor in this requirements verification and catching failures as soon as possible, it helps lower the maintenance cost of the software for the business. There are also several benefits to developers doing TDD. These are some of the reasons that I've been a long-time fan of TDD myself. First, using TDD we have a design-first mentality. By writing failing tests, we need to think about how interactions with our software needs to happen in order to make our features possible. You'll often find yourself surprised by how simple the design can be compared to your original thoughts. This also helps prevent us from over-engineering our code. By focusing on what is needed to make tests pass and to satisfy user expectations, we help keep ourselves on the rails and not get lost in architecture design or thinking of the best possible abstracted solutions that may not actually provide much value themselves. The third benefit is increasing your developer momentum. Rather than struggling for days to implement a single part of a feature, the encouragement to break features down to tests that leverage the red/green/refactor flow, allows you to be making rapid progress and establish a circle of success. Tests are continually made to pass, and it is much easier to track exactly where you are and what is left to do. As red/green/refactor becomes much more natural, you often feel you are always making forward progress. It also enables your success to snowball into bigger and bigger wins. It's very motivating as a developer. Also, you gain a lot more confidence working with code when you have a large suite of tests backing you up. You feel empowered to change code as necessary, even refactoring difficult and complicated parts of the system in order to make it easier to integrate new features. It is incredibly liberating as a developer to have the backing of a very simple, straightforward, and complete testing solution behind you when working with code. Once you experience this feeling, you will never want to go back. Another way developers can leverage test-driven development is through the practicing of code katas. Code katas are derived from the standard use of katas in martial arts. It's a way to have individual training exercises that help reinforce key aspects of the system through repetitive motions and the building up of good habits through the application of smaller components. Developers can use these katas to practice the usage of their existing skills. They can practice being more efficient with their current tools or practice the application of different designs and implementations of known problems. Another really great usage of code katas is as a tool to learn new things. Perhaps you are learning a new algorithm or programming language concept, or even a brand- new programming language in general. By leveraging code katas, you will have a repeatable system that provides you the structure to learn these new techniques and where you can apply that knowledge to existing problem sets. We will take a look at performing a code kata in a later module in this course. More information on code katas can be found at codekata. com. Test-driven development also helps us keep focused on the needs of the customer. This is a common problem we experience in the software industry. After chatting with the customer or doing other research, a spec is created, or a plan form, to deliver the software equivalent of a delivery truck to meet the needs of the customer or target audience. But what ends up actually getting delivered to the customer is, well, an engine and a steering wheel, to the user, not very useful for their daily needs. That's not a problem if the customer wanted to build their own vehicle, but what if all the customer wanted all along was a simple car to take their kids to and from school activities. Test-driven development, especially when applied within an agile process, keeps us focused on delivering software that meets the real needs of the customer. Combined with rapid-release cycles, we are always identifying what needs to change with our software and having the confidence to be able to change our software as necessary as customer requirements change.
Summary

To recap, in this module we took a look at the challenges we face with software development today, what test-driven development is, the history of test-driven development, and why practicing test-driven development helps address the constraints we face when developing software. In the next module, we'll take a look at the different ways that software can be tested and verified.
Different Ways of Testing Applications
Introduction

Welcome to the second module in this course on Test-driven Development: The Big Picture. This is Jason Olson. In the first module, we took a look at challenges faced by businesses and developers when creating software. We also dove into what test-driven development is and the benefits it provides. In this module, we are going to take the next step and dive into the different ways of testing applications and services.
Types of Testing

Let's take a look at the different types of testing. There are many different ways to test software. Software, whether an application or a service, is ultimately composed of units of computation. Depending on the programming language you are using, these could be different things. They could be classes when using an object-oriented language. They could be functions when doing functional programming. Some units may even be dependencies, like partner services that your software needs to interact with to implement its own functionality. Unit testing verifies each one of these units is behaving properly in isolation. Integration testing verifies compositions of units are behaving properly together. Many bugs arise from the contracts between units. One unit might expect its return value to behave one way, while the unit calling it may expect that return value to behave differently. Integration tests aim to catch these issues that aren't caught with isolated unit testing. Acceptance testing verifies the software from the user's point of view. Does the application as a whole behave in ways the customer expects? That's what acceptance tests aim to verify. Does this mean that all unit tests work with classes or functions, or that all acceptance tests are through a user interface? No. It's all a matter of context and depends on what type of software is being created. If you're creating a simple class library, the acceptance tests might actually be testing the usage of a specific class directly. A unit test might be testing an individual method within a class if testing a specific algorithm. It all depends on the software under test and the level of granularity needed by the user. So are one group of tests more important than others? No. They each have different purposes, strengths, and weaknesses. A good testing strategy takes all of these test types into account, and test-driven development isn't limited to any one of these types of tests either. People often associate test-driven development with unit testing, but TDD can also include integration and acceptance tests as well. There are even times when those tests are preferable to unit tests. So what does it mean for a good testing strategy to be balanced? Often, it entails having a mixture of each of these types of tests. Unit tests are extremely fast. By testing each unit in isolation, they are much quicker to set up for testing and to test as well. Unit testing is usually part of the inner cycle of development. It allows the developer to rapidly develop tests and to make them pass and provide lots of motivation and momentum for the developer creating the software. However, unit tests don't verify that all-important aspect of the communications that happen between implementations of different units. Integration tests excel at ensuring multiple units are working together. This will help test that the agreed-upon behavioral contracts between units are working as expected. But unlike unit tests, they tend to be a bit slower, as multiple pieces need to be set up and used together. It's not uncommon to find integration tests run externally as part of a continuous integration process on a build server. This is because it is very important to keep the inner cycle of red/green/refactor as fast and responsive as possible for developers. Having to wait 30 to 45 seconds, or even 5 to 10 minutes between test runs makes it much more difficult to get the true power out of test-driven development. Acceptance tests are the ultimate customer satisfaction tests. Is the feature properly implemented from the user's point of view? But in larger software systems, it's not realistic to drive tests with only acceptance tests, as a large number of components underneath need to be all stood up at once before the acceptance test will light up and turn green. When this extends for days or weeks, it can be demoralizing for developers and slow down the overall progress being made while developing a system. It's through a good combination of all of these types of tests where testing really shines. There is no one right answer for what the correct balance is either. It all depends on the type of software being developed, and it's something that comes with experience. Another aspect of testing to examine is the perspective of the tests themselves. Oftentimes, these testing styles and perspectives are referred to as black box tests versus white box tests. Black box testing views the component being tested as an opaque box. The test has no insight into the inner workings of the component. The tests aren't allowed to reach inside the component to verify what is happening within the component. With black box tests, we aim to verify the behavior of software by giving it different inputs and only observing the output from the software. It's like the component's inner workings are locked within a strong safe. The only thing we have access to are the provided ways to interact with this safe. There is no way to view inside the safe or to even know how it works. It doesn't matter whether the system is responding by some algorithm or some super advanced human intelligence that exists in a different dimension. We simply know we get certain responses when we provide different inputs. When we use black box testing, our tests become much less brittle. Since we are only testing the components from the outside, the internals of the component can change all day long, and our tests won't be impacted. This is very important, but it comes with a downside. If we are working in a system with a lot of dependencies, it can become much more difficult to test with proper isolation. A lot of the dependencies have to be stood up properly in order to put the component under test. On the other side, we have white box testing. Instead of the component being within a locked safe, we can treat the inner workings as having no barrier between us and them. We can monitor the communication happening within the component itself and even inject test-specific dependencies for the component to communicate with instead of the real word dependencies it thinks it is communicating with. This is a very powerful technique when testing systems with lots of dependencies and interconnected components. However, it also comes with a down side. The more invasive we are within the component, the more we risk testing how a component gets its works done instead of verifying that the work itself is correct. If we need to change the implementation of the component later, many of these types of tests will often break as well because the hooks they have sunk into the component are no longer compatible with the newly refactored code. This can make the software a lot more difficult to change and negate a lot of the benefits we are trying to achieve with test-driven development. So as a recap, there are three primary types of tests we use when testing software, unit tests, integration tests, and acceptance tests. When writing these types of tests, there are two different ways we can think about creating the tests, using black box testing or using white box testing. The fun part is finding the right mixture of tests and testing strategies when testing a piece of software. There are many other types of testing though that we won't cover in this course. We will remain focused on the primary types of tests used during test-driven development.
Testing Frameworks and Tools

There are many testing frameworks and tools that exist out there to use while doing test-driven development. Whether you are doing unit testing, integration testing, or acceptance testing, there is a framework out there for you. The first type of framework that developers often think about when discussing test-driven development are the xUnit family of testing frameworks. As we mentioned while discussing TDD history in the previous module, this family of frameworks is named after its primary parent, SUnit, the unit testing framework developed in Smalltalk. JUnit is the Java testing framework that is the direct descendant of SUnit, and through its usage in the widely popular JVM, there has been a rapid deluge of xUnit testing frameworks over the years for nearly every programming language and platform out there, from. NET and Ruby to C++ and Erlang, Perl, PHP, and many others. Even when it isn't named after xUnit, other frameworks and test runners are often inspired and inherited from this family of test tools as well. Basically, there is no reason not to get into test-driven development with the benefits that we see and the number of widely available testing frameworks. These tools can often be used for integration tests as well. There are also a number of testing frameworks and products that can be used for acceptance testing of user interfaces. From frameworks like Selenium and Watir to products aimed at user interface testing like Visual Studio Coded UI Test, Test Studio, or Silk Test. There are many others that exist out there as well. We will not be discussing these frameworks in this course. There are also frameworks you can use to test the overall system as well. Perhaps the most well-known example of a system-level testing framework is the Simian Army suite of open source tools released by Netflix. Perhaps the most referenced tool within this Simian Army is Chaos Monkey. Chaos Monkey is a tool that was originally developed with cloud-based software in mind. While the system is running, Chaos Monkey terminates different major pieces of the system in order to verify that the system is highly available and resilient to failures. This helps identify single points of failure in a distributed system. Latency Monkey introduces artificial delays between components in the system and verifies that back pressure and throttling behave in the system to prevent cascading failures when the system experiences performance degradation. There are also many other tools in the Simian Army like Janitor Monkey, Doctor Monkey, Security Monkey, and so on. No matter what level of testing you're wanting to do, there are likely several solutions out there for you. It doesn't matter whether it's unit testing, acceptance testing, or system-level testing that you are performing.
Testing Concepts

We've talked about the different types of testing, and we've briefly discussed different testing frameworks. Now let's talk about the very simple concepts and terminology you need to know to use these frameworks. The great thing is that the vast majority of testing frameworks use this same or similar terminology. Of course, the most basic building block we've already discussed is the test. Several tests are packaged into a test suite, and there can be several test suites as well. You can have a BeforeEach hook that runs before every single test within the test suite. This is used for any setup that needs to happen before every test is executed. You can also have an AfterEach hook that runs after every single test within the test suite. This is used for any cleanup that needs to happen after every test is executed. You also have before and after hooks that can do set up and tear down around the entire test suite. If there are multiple tests within the test suite, these hooks will only be run a single time, and those are the basic testing framework concepts that are good to know to get started. To recap, you have an individual test and before/after hooks that run around every single test, you also have test suites that are a logical group of tests with before/after hooks that run once around every single test suite, very basic and very straightforward. The primary verification concept we need to know for test-driven development is the assertion. An assert is what allows us to tell the test what values we expect and is the fundamental mechanism that will determine whether a test passes or fails. There are many different types of assertions you might make depending on the test framework you are using. You can assert if a value is true or false or assert whether a value is null. You can assert that two values are equal or even assert that an object is contained within a collection. You can assert whether a string starts with a given value, and many, many others. Some test frameworks also use a technique called fluent programming. The assertions are largely the same, but as you can see, they read in a much more natural way and aim to help non-developers guide and review tests that are being written. We also need to think about how tests are executed. So we have our tests that have been written using a testing framework. Now we need to execute our tests. Tests are executed using a test runner. Sometimes this test runner is built into the testing framework we choose. Other times, the test runner is its own framework or application that can be used for tests written in any number of different supported testing frameworks, the key being that you have your tests, and you have the separate test runner that executes those tests. When running our tests using the test runner, the main thing we need to understand is how the tests are run. Are the tests executed synchronously or asynchronously? Some test frameworks execute every single test synchronously, meaning that only a single test and test suite is executed at any single time. Other test frameworks only execute a single test at a time within a test suite, but are able to execute multiple test suites at the same time. And yet, other testing frameworks run all tests and test suites completely independent of each other at the same time, completely asynchronously. This is important because it has a direct impact on the way we are or aren't able to structure our tests, especially when it comes to the set up and tear downs of tests, and the way we assert functionality is working correctly. Let's take a look at an example. We are developing an ecommerce site, and we are working on functionality for a user's shopping cart. We have several tests that are going to execute. One of them adds an item to a user's cart, and the other one removes an item from the user's cart. When these two tests are executed synchronously, everything works as expected. But problems arise if tests are run asynchronously and both tests run at the exact same time. If they are sharing the same instance of a cart and they run at the same time, the test will conflict with each other, and the assertions may run at different times and experience different results. This is known as a race condition. The worst part about race conditions is that they cause unpredictable behavior. Your tests may pass 80% of the time and fail 20% of the time at totally random intervals, or they may only occasionally fail. It's not a fun situation to be in and to be trying to debug. This can be compounded if test suites run asynchronously as well. This can be avoided when doing unit tests through the creation of new instances of objects or new state used by functions. The problem becomes more ambiguous and difficult to track down in integration or acceptance tests though as more dependencies are brought in and the connectedness and race conditions aren't so easily visible anymore. So these are things to keep in mind when you are writing tests. This is a lot to remember though, so don't feel bad if it takes a while for it to become second nature. Writing tests at all is more important than writing the perfect test. Don't let perfection be the enemy of the good.
Insights from Testing

There are other benefits to testing software that you gain from practicing test-driven development as well. This includes insights you can get into the software you are developing. Of course, the obvious insight that we've already discussed is to be able to give a thumbs up or thumbs down on whether to release your software. But there are many more insights. Let's look at a common banner you see in many open source projects on GitHub. There are several insights outside of whether the build and tests are passing or not, like how much of my code is actually being tested by my tests? Is the other software I depend on working as well? Or how well is my code written itself? Does it adhere to my own coding style guidelines? Perhaps more important though are the business-level insights you can gain from tests. As we've mentioned before, you can check to see if there are regressions. Did any of my previously working features break? How does our new code perform under load? Or, were there any breaking changes made that would prevent partner's code from working? Test-driven development has the built-in feature of helping provide a robust set of tests that can verify all this behavior and provide you all these insights as part of your routine development process.
Summary

To recap, in this module we took a high level look at the different ways of testing applications and services, the various testing concepts you need to know to practice test-driven development, and the insights we can gain from test-driven development. In the next module, we'll take a look at what test-driven development looks like in action by working through a couple of examples of practicing test-driven development.
Test-driven Development in Action
Introduction

Welcome to the next module in this course on Test-driven Development: The Big Picture. This is Jason Olson. In a previous module, Different Ways of Testing Applications, we took a look at the different ways of testing applications and services. In this module, we're going to see test-driven development in action by working through some basic examples. First, we will use the common exercise of FizzBuzz to see the basics of TDD in action. Then, we will use a simple stack example to practice a code kata. We will use this to do multiple implementations of the stack to practice our various development techniques. So let's get started.
Setup

For our exercises in this module, we will using C# as our language and MSTest as our testing framework. Specifically, we'll be using Visual Studio Code as our code editor and. NET Core as our runtime. This enables these exercises to work whether you are on Windows, Mac OS X, or Linux. If you're not planning on following along yourself, these tools aren't necessary. Feel free to just watch the videos, but if you do wish to follow along, let's talk about what needs to be set up first. If you aren't familiar with Visual Studio Code, Visual Studio Code is a lightweight code editor similar to the likes of Atom or Sublime. It is completely free, open source, and runs on multiple operating systems. If you don't have Code installed, you can find the install instructions at this URL. For our runtime, we will be using. NET Core.. NET Core is a cross platform. NET runtime maintained by Microsoft and the. NET community on GitHub. It can be used for many scenarios, and for us here, cross platform is the key, whether you are using Mac OS X or Windows. You can find setup instructions for. NET Core on the web for your operating system of choice at this URL. First, we will be using a tool called Dotnet Watcher. This allows us to watch our code and re-run all of our tests anytime a code file is changed and saved. If you wish to get a barebones project to follow along with, you can start with the article at this URL. This article covers unit testing with MSTest and. NET Core and is from the official Microsoft documentation. Or, you could pull the GitHub repository for this course, and copy the Start section for FizzBuzz to get started. Once you have everything installed, you should find yourself being able to use Visual Studio Code to have the following setup for these examples. We're able to develop our C# code directly in Visual Studio Code, and use the built-in terminal to watch our code and execute our tests anytime code changes using the Dotnet Watch test command. Now with all of this out of the way, let's go ahead and dive right in.
FizzBuzz TDD

To see TDD in action, we're first going to take a look at implementing FizzBuzz. What is FizzBuzz? Well FizzBuzz is a simple algorithm. When given a positive number N, if that number is divisible by 3, it returns Fizz. If the number's divisible by 5, it'll return Buzz. If it's divisible by both 3 and 5, it returns FizzBuzz. Otherwise, we simply return the number. It's a very simple and straightforward algorithm that serves as a good basic introduction to TDD, so let's dive into some code. Following the setup we discussed before, we have a basic template project for testing a library with. NET Core. We have the file FizzBuzzTests. cs in our test project that contain our tests, and we have FizzBuzzService. cs in our library project that will have our implementation of FizzBuzz. Let's create our initial test setup in FizzBuzzTests. cs. We will have our test class setup create an instance of our FizzBuzz service that we will be testing. We will start by testing for the default behavior of returning the number that we give FizzBuzz. Let's test that it prints back 1 if we give it a 1. The first step after we write the test is to see it fail. We will open up the terminal, navigate to our test project, and run the test using Dotnet Watch. It will fail because the project can't compile. So before we can see our red test, we need to get the project to compile. We create the FizzBuzzService in our library project with a print method. For now, to see our test fail, we will simply return a default value of string. Empty. Once we save it, we can see the Dotnet Watcher re-run the test, and the test fails as expected. It was expecting a 1 to return, but it failed because of our empty string. Now that our test is red, the next step is to make it green. In Kent Beck's book, Test-Driven Development By Example, he discusses three primary ways to quickly make a test green. We can fake it, where we return a constant and gradually replace constants with variables until the real code exists, we can make it using the real implementation if it's immediately obvious, or we can use triangulation, only generalized code when there are two or more examples. This is especially useful if the solution isn't immediately obvious. To make our test pass, we want to do the absolute least amount of work possible. The simplest thing I can think of doing is to simply return a value of 1 from our function. We save our file, our test re-runs, and we can see it now passes. This may feel very unnatural at first. We're so used to thinking about the right way or good design, but unlike code that is architected first, we are always starting from code that works and tests that are passing. We're not done yet though obviously. We need to refactor. As another saying goes, make it run, then make it right! So what's the primary problem with the code we have? Yes, it's not a general solution, but what's the root cause? There's actually a dependency between our implementation code and our tests. We can't change one without changing the other. To show that, let's change from 1 to 2 for the value we are passing in to the FizzBuzz function. Our test fails as we expect. So let's make things right. Since we faked it to make it pass, we're going to simply replace our temporary constant in our Print method with returning the string value of our number. We save the file, the tests run, and we can see everything is passing again. So time for another test. Let's test that we print Fizz correctly. Fizz should be returned anytime a number is divisible by 3. We save our file, the tests run, and we see our new test fail. For this test, it seems pretty obvious what the implementation is to make it pass, so let's just go ahead and make it instead of faking it. We will check whether the number is divisible by 3. If it is, we return Fizz. If it isn't, we return the number. Let's see our tests pass. We save the file, the tests run, and we see everything is passing again. Onto the next one! The momentum continues. Let's test if numbers divisible by 5 return Buzz. Of course, we expect this test to fail. We save our file, and we see the test fail as we expected. What's the next thing we do? The test is red, so we make it green. In this case again, the implementation feels quite obvious, but let's fake it in this case, so we can see triangulation in action. If the number passed is a 5, we return Buzz. After saving the file, we see the test is passing. We're now green, so we need to refactor and make it right. Triangulation can help in situations where it isn't clear what the solution is. Triangulation is when we use several test values to help us derive the right solution. In this case, we will add another assertion to ensure we get a Buzz when we pass 10 in. We can see that the test fails as we expected, because we only hardcoded the solution to return Buzz for 5. So let's fix that. Instead of returning Buzz when the number is 5, we will return Buzz when it is divisible by 5. It's a very simple solution. Unfortunately, it doesn't quite show the true power of triangulation, but it does show an example of how you can triangulate using test values to derive a solution. We can save the file and then verify all our tests are passing again, red/green/refactor, and we continue on. We have our last and final test for this demo. When a number is divisible by both 3 and 5, we should return FizzBuzz. That's true for the numbers 15 and 30. Once we've added the new test case, it fails. Again, the implementation seems obvious. So to get the test to pass, we're just going to make the obvious implementation by checking that the number is divisible by both 3 and 5. Then we can save our file, the Watcher will run the tests, and all our tests are passing. We have some duplicate code here, so let's do some final refactoring. First, we'll extract the check for being divisible by 3 to its own IsDivisibleByThree method, and call that method directly from both locations. We save the results, the Watcher runs the tests, and we verify all our tests are still passing. Good, we didn't break anything. Now we do the same for the divisible by 5 check. Extract the method, and we will call that newly extracted method, and save our file. The Watcher re-runs the test, and they are all still passing. We've now completed our simple algorithm of FizzBuzz and seen how we can use TDD to implement it. As a recap, we reinforced how red/green/refactor is at the heart of test-driven development. We also learned a couple of common approaches we can take when making a test pass or when refactoring code. We can fake it. This is when we return a constant and gradually replace constants with variables until the real code exists. We can make it where we use the real implementation if that implementation is immediately obvious. Or, we can triangulate a solution. With triangulation, we only generalize code when there are two or more examples. This is especially useful if the solution isn't immediately obvious. For more detailed examples of TDD in action, you can find them in Kent Beck's great book, Test-Driven Development By Example, a must read for anybody wanting to practice TDD.
Stack Code Kata

Let's do some more TDD. We're going to practice some code katas when implementing a basic stack. With our basic stack, there's just a handful of key scenarios we're going to implement. First, we want to verify that we can pop an item off in the stack. We also want to be able to put multiple items on the stack and get them back out in last in, first out order. If the stack is empty, we don't want it to throw an exception if we try to pop an item off of it. Finally, we want to verify that an exception is thrown if we try to push an invalid value onto the stack. Remember, code katas are not about getting the right answer. Doing a code kata is about improving our skills and exploring different solutions and designs. The goal is the practice, not the solution. So with that said, let's get started. The first thing we're going to do is create a test to verify we can pop an item off our stack. Of course, this won't compile as we can see when we execute dotnet watch test. So let's define the MyStack class so that our code will compile and the test will fail. Save the file, check back in with our terminal, and we will see the test is failing as we expect. So let's make the test pass. We'll do the simplest thing possible and just return foo for now. Save the file, and we verify in our terminal that the tests are passing. So we've done red, we've done green, so let's do a quick refactor. We will substitute our constant of foo for a private variable that gets set in the Push method. We just declare our private member, set it in the Push method, and then return it in our Pop method. Save the file, and make sure that the test still passes. So onto our next test. We want to test that our stack can push and pop multiple items. It should fail because we are only handling a single item, but let's save our file, and verify the test fails. Yes, it fails. So we need to make a pass. We're going to implement our stack using an array of objects. The two pieces of information we will need are the array and the index of the current item to push to. Here we face our first interesting design question. We could set current index to -1 to start with and increment it before pushing the item. However for now, let's just say 0 is the default value of current index. To push the item, we will set the current element in the array to the item being pushed and then increment our index so it's ready for the next time an item is pushed. Finally, we need to pop the item off our stack. We retrieve the item that was at the last index that was pushed to, decrement our index since we've popped that element off, and then return the item we retrieved. With the implementation done, our test should now pass. So we save our file, switch back to our terminal, and verify the tests are passing as expected. So two tests left in our first code kata. Next, we move on to testing that we can pop from an empty stack. We don't want this to throw an exception, it should just return a null for now. The code should already compile, so we just save the file, switch to our terminal, and make sure the test fails. To make our test pass, we're going to simply return null if the current index is 0. That's all we need to do. Save the file, and we should see all the tests passing now in our terminal. Our final test is ensuring an exception is thrown if we try to push null. We want to make sure to test the negative cases, as well as the positive cases. We will use MSTest's ability to provide a lambda function to execute and ensure a specific exception type is thrown. Most xUnit libraries have a similar feature you can use. After saving, the test will fail as we can see because we aren't throwing an exception yet. So in our Push method, we will simply check if the item is null. If it is, we throw an exception, else, we will push the item as usual. After saving the file, all our tests should be passing again. And that's it, that's our first, simple, simple code kata using TDD to implement every step along the way. But writing code isn't always about having the right answer. The best answer will depend on how the code is going to be used and what other types of requirements it has. The great thing is that we can use code katas to explore other solutions in a guided fashion. We used a vanilla array this time, so let's try it again using generics to build our own generic stack.
Generic Stack Code Kata

Let's say that we are new to generics, and we want an exercise that helps us learn them. So for this iteration of our code kata, we will explore creating a stack that uses generics and allows us to store any item in a strongly typed way. First, we make sure we reset back to the starting project with no tests and no implementation yet. We start again with our test to verify an item can be popped off the stack. Other than the way we construct our test, the rest of the test is the same as before. For the sake of this video, we will skip forward a bit and go straight to making this test pass. Like last time, we will store the item in a member variable when it is pushed, and retrieve that item when we pop off the stack. We save the file, switch back to our terminal, and verify that the test passes. Now our second test is to push and pop multiple items with a stack. Of course, we expect this to fail. So after saving the file and checking in our terminal, we see the test fails as expected. Instead of using a normal array like last time, we're going to use a generic list this time. This will give us practice using the generic parameter to our type in our methods, as well as private member variables. Here we have an inflection point in our kata. Are there any steps we could take to make this move even more fine grain? It was a fairly large step with lots of typing. These are the kinds of questions we should be asking ourselves when practicing a code kata. Always challenge yourself. Don't just go through the motions. For now though, we will save the file and move along. We can see in our terminal that the tests are now passing. For the final test in this video iteration, let's just stick with making sure we can pop from an empty stack without an exception being thrown. To make the test pass, we need to return a default value when there are no items on the stack. In the previous video, we simply returned null. This won't work with the vanilla generic without something called a type constraint, because it's possible the user could be declaring a type that is not nullable. Instead, we will use the default keyword to simply return the default value of the type. In the case of strings, this will be null. Now we save the file and verify in the terminal that all our tests are passing. So, while we're using this kata as practice, maybe we want to try a fluent interface when pushing items onto the stack. We can use a technique called method chaining to do this where we make multiple push calls together back to back. This is made possible by having push return the instance of our stack. It's very easy to make this change in code. The method signature of push changes to return MyStack of T, and after the item is added to the stack, we simply return our instance. Voila, more experiments done! All the tests should still be passing. So, let's move on to a little more fun experiment for a code kata.
Immutable Stack Code Kata

As a more advanced example, let's do the stack code kata again, but we will focus on building an immutable stack. If you aren't familiar with immutability, the basic concept is that an object can't change its state. Any time a method would change the state of the object, like pushing an item onto the stack, it needs to return a new copy of the object with the change. The previous instance remains unchanged. This is very common in functional programming languages and helps us write concurrent code more easily. Going back to our first test to pop an item off the stack, we are faced with our first immediate problem. How do we push an item onto a stack without changing the stack's state? Well, we don't change the stack's state, we return a new instance of the stack. We change our code to reflect this, and we will add two new assertions to verify the old stack remains unchanged. We immediately have another challenge though. Our Pop method also changes the state of the stack since it removes the top element. If operations that change the stack need to return a new instance of a stack, what do we do for the Pop method that already has a return value returning the item being popped? We could use an out parameter to return the item from the Pop method while having the Pop method directly return the stack instance representing the new state. I don't like this though. It makes me feel kind of weird. It feels like a bad thing to do. So it has a design choice. We could split it into two operations. We could introduce a new Peek method that simply returns the top element without removing it from the stack. To prove that the stack is immutable, we can call Peek multiple times and always get back the same result. Our Pop method then simply returns a new instance of the stack with the top element removed. We'll verify the Pop method doesn't mutate our stack state either by checking the count of items in each stack. Now to implement this new MyStack class, we need to store the list of items currently in our stack, and we will expose the count of items in the stack so our test will compile. Our Peek method is really simple. Since no state is changed, all it needs to do is return the top item of the stack, very straightforward. Now, when we Push an item, we need to create a new instance of our stack class. We will copy the list of items into a new list and add the item to that new list. Then we will initialize a new instance of our stack using this newly modified list. That way, the local items member variable in our current instance remains unmodified since we need our stack to remain immutable. Now for popping, we will do something similar. We will copy our list, remove the top item that was popped from the list, and return a new instance of our stack with the new modified state. We can then save our file and see our test passing in the terminal. I think we were perhaps a bit over-eager and built a test that was too big. It didn't allow us to do the smallest solution possible and led us to this more complex code. If you notice, we had to implement the entire class in order for our single test to pass. That's not a habit we want to get into with TDD, so let's try it again. This time we will focus on smaller steps by breaking our tests into smaller chunks and see what kind of design it leads us to. Believe it or not, this is a very common thing that can happen when practicing TDD, and you always want the skill to step back and try again if you can. Instead of the first test being about popping an item, let's back up and make a smaller test simply to verify that an item can be pushed onto the stack. Now we will do the simplest thing possible to make the test pass. We primarily need a count property. We also need to return a new instance of the stack when the item is pushed. To get the most minimal solution to make the test pass, we need to specify a count of 1 when we create the instance of the stack returned from our Push method. This means we need a constructor that will accept the count value for the new stack. We will also have a default constructor that will initialize the count to 0, so our initial stack creation in our test continues to compile. We save our file, switch to the terminal, and we should see our test passing now. We have the test passing, so let's refactor. Instead of passing in the constant of 1, we will use the private count variable and add 1 to it. We save our file, and then we can see the test still passing in the terminal. So moving onto the next test. We will verify we can Peek the item that is currently at the top of the stack. We should be able to Peek multiple times without the value changing. We create a Peek method. To make this test pass very easily, it would be great to just return the item that was last passed in when Push was called. So let's store this item in a private member variable. The place this is passed in is via our Push method. Since Push returns a new instance of our stack, we need to be able to pass the item into our private constructor. So we add the item to our private constructor and specify the default value of the item in our default constructor as well. We save the file, and we can now see all the tests passing in our terminal. This is looking much more simple than before. We now get to popping an item off the stack. We're going to spice up this test a bit to push multiple items onto the stack. We will then pop twice to make sure both elements come off. But how do we implement our Pop method? My problem is that I can't immediately see how we can implement this. Where do we get the previous item that was pushed, and the previous one before that? But I have an idea. Let's take a sidetrack to think about how lists in some functional programming languages promote immutability. In C-based languages, we're used to thinking of lists as a contiguous block of memory. All the items are together in one package of sorts. Functional programming languages, especially those that make heavy use of recursive programming models, have a different way to think of lists though. The list is considered a recursive data structure that contains a single element that is the head of the list, and another element representing the rest of the list, also known as the tail. Now, if we look at the tail, that list is also comprised of a head and a tail, and if you look at the tail of that list, it's also comprised of a head and a tail, all the way down. It makes immutability and recursive processing much, much simpler. Perhaps there's a way we can use this pattern for pushing and popping our stack's state. We can think of immutable stacks the same way, but instead of a head and a tail, we use item and the previous stack. Then if we dig into our previous stack, it has its own item and reference to its own previous stack, and so on. So if we have a reference to the current item and have a reference to the previous stack, we should have everything we need to implement our immutable stack, so let's do it. Looking at the Pop method again, we couldn't figure out how to create a new instance of our stack. Now though, if we have a reference to the previous stack, we can just return that reference. Of course, we need a reference to the previous stack, so in our private constructor, we will accept the previous stack instance as a parameter and set the private member variable to it. Then in our Push method, we will pass in our current stack instance as the previous one when a new stack is created. This allows all the pushes to be unwound through our Pop method. We save the file, switch to the terminal, and we can see all our tests passing again. You can see that this time there are no list of items to collect, no worries about possibly mutating a list, and each, individual method is much more straightforward to reason about. Of course, this solution does have some drawbacks as well. If a stack needs to support storing thousands or even millions of items, we're likely to run out of memory on some machines, as every single item has an entire instance of the stack class along with it. This is why it's important to understand the desired usage of the code you write.
Summary

So we used a code kata to practice TDD, and we also used it to try different implementations with generics, fluent interfaces, and immutability. You can also use katas for a bunch of other ideas as well. Remember, we use code katas to improve our skills as developers, everything from our basic sense of design to our execution ability. There are many different ways you could challenge yourself within the context of a code kata. You could use a kata as a way to learn more about working with asynchronous code, perhaps by interacting with the file system or over the network. You may want to use it to practice efficiency, like perhaps doing an entire kata without ever touching the mouse a single time. Learn all your main editor's keyboard shortcuts you would need to do this. Perhaps you want to learn a new editor, trying out Sublime or Atom or even Emacs or Vim. And you might even want to learn a new framework, perhaps it's a new dependency injection framework or a new testing framework. Remember, code katas are all about intentional practice. You should always be pushing the limits of what you are capable of doing. Code katas enable us to do this in a somewhat controlled environment, so we can work on improving ourselves as developers, and test-driven development lies at the heart of this and allows us to practice this in a repeatable way. So to recap, in this module we took a look at test-driven development in action. We worked through examples of both FizzBuzz and a code kata of implementing a stack. We saw how easily our designs can get out of control if we rush forward in the TDD process. In the next module, we're going to take the next step and discuss different strategies and techniques for testing code.
Strategies and Techniques for Testing Code
Introduction

Welcome to the next module in this course on Test-driven Development: The Big Picture. This is Jason Olson. In the previous module, we took a look at test-driven development in action. We also practiced a code kata. In this module, we're going to take the next step and discuss different strategies and techniques for testing code. There are three specific areas we will dive into in this module, dependency injection, a technique that allows us to use common test practices, test doubles, a common testing practice that frequently uses dependency injection, and best practices for test-driven development.
Dependency Injection

Let's talk about dependency injection, a technique commonly used when practicing test-driven development. What is dependency injection? Simply put, dependency injection is a technique where an object is supplied with the dependency it needs, rather than creating the dependency directly itself. There are two pieces we typically think of. First, we have the dependency. It is an object that is used by a different object. Second, an injection is what we do to provide the dependency to the object that needs it. We are effectively injecting the dependency into the object. To understand what this looks like in code, let's take a look at a common problem we run into with test-driven development. We have a simple Greeter class, which uses a nameService. The Greeter calls out to the nameService's GetName method to complete the greeting. Perhaps a bit naive in this example, but it's a pattern you will see quite often when interacting with external services, like databases or file systems. In this case, the nameService is the dependency. So what's the problem? What if we wanted our test to verify what happens if the nameService crashes or throws an exception? It's very difficult for our test to force the nameService to crash, and since the nameService is created directly by the Greeter class, our test has very few options to get at it to change its behavior. Dependency injection is the technique we use to be able to provide the Greeter class with the nameService to use instead of the nameService being created by the Greeter class. We can then force the nameService to have different behaviors for testing purposes. We'll talk about this more in a later video. There are primarily three types of dependency injection. The first is constructor injection. Constructor injection is used to provide dependencies through a class's constructor. Second, we have property injection, or setter injection in languages that don't support properties directly. Here we use a Property or a Setter method to inject dependencies. Lastly, we have interface injection. This is perhaps the least frequently used. With interface injection, the dependent object defines an interface that describes how its dependencies are injected into it. This type can be used when there are multiple dependencies that need to be injected, or if the process of injecting a dependency is more complicated or requires a sequence of method calls, like a protocol. Let's take a look at each of these in action with our simple Greeter class. We've taken our Greeter class from before and are now using constructor injection in it. In the Greeter's constructor, we are now passing the nameService as a parameter. This allows an external consumer of the Greeter class, like a test, to pass in any instance it wishes. To enable this, we've also created an interface called INameService that our Greeter is dependent on instead of the concrete instance class of NameService itself. Now the Greeter class doesn't care if the name is getting retrieved from a file, from a database, from a randomly generated list, or some other mechanism. For our testing purposes, we create a TestNameService where our test can explicitly set the name to be returned from the service. This allows us to set the name to Jason, give the testing-oriented nameService to our Greeter class, and verify the greeting that is generated is correct; pretty straightforward. Dependency injection makes testing much easier than it would otherwise be, and enables us to use some powerful techniques when testing different types of code. Property injection is very similar, but instead of injecting the dependency into the constructor, we use a property to inject our dependency. This is useful when the dependency might need to change more than a single time over the lifetime of the object being tested. Testing it is very similar to our test for constructor injection. We create a new instance of our TestNameService, but this time, we set the dependency via the property. Call the SayHello method, and verify its proper return value. Interface injection is an extension of property injection. With interface injection, the Greeter class implements an interface that describes how injection happens. As mentioned before, this is very useful when there are multiple dependencies needing to be set, or the act of setting the dependency is more complicated, like a protocol. Testing with interface injection is the same as property injection in this simple case, but could be more involved depending on the definition of the dependency interface. So all these examples are objected oriented. Does that mean dependency injection is only really used in object-oriented programming languages? Not at all. Dependency injection is also commonly used when testing code in functional programming languages as well, but the injection mechanism is a bit different. Here we have some simple JavaScript code. Similar to our previous examples, we have a Greet method that retrieves a name using the getName method, and uses it to generate a greeting. We still have a direct dependency here. It's the getName method itself, rather than a separate object. A pattern that is used in functional programming is that the dependency is provided as a parameter to the method and called from within that method. This allows us to provide any method as the parameter to determine how the name is generated, similar to the way we were injecting dependencies before. If you want to learn more about how this technique is possible in functional programming, you can research the concept of higher-order functions. We will not be covering this in this course though. There are some trade-offs of dependency injection we should keep in mind. Like any other topic, it's not a silver bullet, and it comes with some drawbacks. On one end of the spectrum, we have completely self-contained code where dependency injection is never used. On the other end of the spectrum, we have code where dependency injection is used everywhere for all dependencies. Self-contained code is much easier to understand. You can see exactly what a class is using when it is using it. But as we've discussed with testing problems, this kind of code is much more difficult to test. On the flipside, we have code that is making heavy use of dependency injection. Yes, it's now much easier to test and is extremely flexible. However, when debugging or otherwise trying to understand what the code is doing, it's much more difficult to understand. It is not clear from the class itself what its dependencies are doing, or even which instances of which dependencies are being used. So always think about the balance between testing and ease of understanding when using dependency injection.
Test Doubles

An important concept for us to understand to test our code is that of test doubles. So what are test doubles? The term test double was popularized in the book xUnit Test Patterns, even though the concept of a test double has been around much longer. A test double is a term for any object used during test to replace a real object with a testing-specific object. And though you may not know it, you've already seen examples of test doubles in action in this module. Let's take a look at the code we discussed in the previous video. We have a Greeter class. To get the name of the person we are saying hello to, we are given a nameService to use to retrieve the name with. As we discussed, this is named dependency injection. Using dependency injection, it is now much easier for us to test the code. We can give the Greeter service a nameService that we have created specifically for testing. We can give this test-specific nameService any name we wish, so we can verify the Greeter class is behaving properly when we tell it to say hello. This nameService class is known as a test double. Think about it similarly to a stunt double in movies. It is a stand-in object that we have given to the component to replace the real object. There are several different types of test doubles. The two primary ones you will commonly find yourself using are stubs and mocks. There are also fakes and spies. However, we will focus on stubs and mocks. A stub is a test double that provides canned answers when certain methods or properties are called during the test. The test name service we just looked at was a stub. They are simple, and they are straightforward. They are a very powerful tool to use when doing test-driven development. Mocks, on the other hand, are kind of like stubs on steroids. Not only can they provide specific answers, but they are also preprogrammed with their own expectations and assertions that are verified after a test is run. How are stubs and mocks different in action? Let's see. Here's the TestNameService class we created previously. This is a stub test double. It's an object that provides a simple, canned answer when the getName method is called. What would a mock look like instead? At the most basic, it looks quite similar to a stub. We us a generic mock object to create a test double of the INameService interface. However, there's one key difference. We also give it a set of expectations to verify. In this case, the getName method is called at least once. It is this addition of expectations and assertions that make it a mock test double. We've mentioned in previous modules to be careful about testing implementation details of a component though and how that can make your code more brittle. So if it makes your code more brittle, why would you ever use mocks? Well, mocks are great when it comes to testing more involved communication paradigms, like protocols. When you expect methods to be called in a specific order and in specific ways, mocks can be very useful to ensure your code is exercising the protocol correctly, but they do come with the drawbacks that we need to be aware of. So what do we mean though that testing implementation details makes your code more brittle? We have our code on one side and the tests that verify that code on another side. Let's consider these two different things largely independent of each other. We don't want excessive dependencies between our code and our tests. If we change the product code, we want the test to remain untouched in passing. It's the core tenant of how we make sure our changes to the source code didn't break any functionality. The tests continue to pass. On the flipside, if we are cleaning up or updating our test code, we don't want our product code to change. If we change our product code and the tests need to change as well, or vice versa, how do we actually know we didn't introduce any breaking changes? Were any bugs introduced on either side during our changes? With both sides changing, we lose our common baseline or control element that helps guide and contain the changes we are making. If you have to change your test code when you change your product code, your test code is brittle. If you have to change your product code when you update your test code, then your product code could be brittle. It's a code smell that you should look out for when starting to implement test doubles, so you don't introduce excessive dependencies between your test code and your product code.
Best Practices

Let's talk about some common best practices for testing code that you should keep in mind when beginning to practice test-driven development. One of the first things you should keep in mind is that test code should be treated like production code. This means you should aim to write readable and maintainable test code. Test code is not throw-away code. It is code that will need to be maintained and improved on over time. So don't fall into the trap of treating your test code like a second-class citizen compared to your other code. We also need to remember to write tests that address both positive and negative test cases. Many times in software, it is simple to get positive test cases passing. It can become much more difficult to debug software when things start failing. To avoid the frustration and confusion that comes with cascading failures, save yourself some trouble, and put in the effort to test the negative cases as well. What does your code do when somebody uses it the wrong way? What if somebody puts in incorrect values? Finally, be aware of code duplication. For example, separate your common setup and teardown logic so it's not spread throughout all your different tests. Treat your test code as production code. Just as you strive for removal of code duplication in your production code, so should you drive to remove it from your test code. Another practice to keep in mind is to only focus on the necessary values and results. If you aren't testing for a given value, don't assert on it. Being overly aggressive with asserts and tests can lead to brittle test code. When your application or service code needs to change, unnecessary asserts will make the changes more involved than they need to be, since you will often need to change large portions of your tests as well. Finally, as you and your team are practicing TDD, you will want to continue to review and define your testing practices with your team. This includes discussing what techniques you are finding to be really useful in your software. Lessons learned by one person, or pair of people, can then help educate other team members, improving the skills of the entire team. It's also good to catch bad habits before they become ingrained into the team. What areas are causing pain for the team while practicing TDD? Are there times when you find yourself cutting corners or jumping ahead? Find them and drive to remove them from your process. You'll also want to discuss common challenges. If there are challenges that are preventing you or the team from fulling embracing TDD, brainstorm solutions for those challenges. Are the tests taking too long to run and interfere with your red/green/refactor cycle? Well, you should spend some time investigating why the test takes so long and what you can do to improve their execution time. Are there parts of your code that frequently break when new code or features are added? Take the time to clean up the broken code and make it more resilient to those types of changes you're experiencing. Through TDD, you should also be striving to leave your code and tests in better shape than when you started your task.
Summary

To recap, in this module we took a look at different strategies and techniques for testing code. We looked at dependency injection, test doubles, both mocks and stubs, and best practices when testing code. In the next module, our last module, we'll take a look at common test-driven development gotchas you are likely to run into when practicing TDD or discussing TDD with others.
Looking out for Test-driven Development Gotchas
Introduction

Welcome to the fifth and final module in this course on Test-driven Development: The Big Picture. This is Jason Olson. In the fourth module, we took a look at different strategies and techniques for testing code, like dependency injection and test doubles, as well as best practices when testing code. In this final module, we'll take a look at common test-driven development gotchas you are likely to run into when practicing TDD. We'll touch on TDD anti-patterns, TDD limitations, and common questions that come up when discussing TDD with others. We'll then conclude this course with a brief recap and some next steps you may want to take in your development journey.
Anti-patterns

Let's discuss some common anti-patterns that you're likely going to run into as you start practicing test-driven development. These are things you'll want to try to avoid and be aware of. One area that can become very frustrating is when there are dependencies between tests. There are some things you need to keep in mind to avoid this pain caused by these interdependent tests. First, the execution order of tests shouldn't matter. All tests should be able to run in any order and work all the time. If you find that the order of your tests matter to make the tests pass, there's likely a subtle, or not so subtle, dependency between a number of tests. When this occurs, you may be faced with intermittent test failures that happen when new tests are written or previous tests are changed. Sometimes, this is very difficult to track down and can waste a lot of development time and effort. One of the ways that these problems can be very difficult to track down is because these interdependent tests can cause cascading failures. This may mean that the actual failure occurred several tests ago, but only result in a later-run test failing. This type of cascading failure can make it even more difficult and frustrating to track down test failures. Finally, you will need to know whether your testing framework executes tests and/or test suites in parallel or in serial. Parallel execution has the advantage of making your total test package run quite a bit faster, and will make it more obvious when two tests are directly dependent on each other. But testing certain components in a parallel fashion can also be incredibly difficult at times. So there is a cost associated with parallel execution you will have to keep in mind. So as you begin to practice TDD more, always be aware of how your tests are being executed. Another testing anti-pattern to watch out for is when you find yourself testing implementation details. Tests should focus on what is trying to be solved, not how it is being solved. Over-aggressive usage of mocks can be a code smell that means you are testing how something is done instead of testing the results themselves. This is bad because testing the implementation details of code can make your tests incredibly brittle. If you refactor the internal implementation details of a component and your tests break, it is usually a sign that you're being overly aggressive and testing the implementation itself. Try to avoid this to have tests that are more robust and allow you to more easily change your code over time. It's easy to end in a position where developers don't want to change a certain component because they know that they will have to go change a whole slew of tests when they do so. Remember, our goal is make changing code easier and to give us the confidence to do so when needed, so we can reduce the ongoing maintenance costs of our software. One last beginning anti-pattern to keep in mind during your TDD journey is to be aware of slow-running tests. Slow-running tests aren't always bad, but they can be a code smell for larger problems that are lurking below. A large problem of slow-running tests is that they prevent rapid red/green/refactor cycles, which, in turn, slows down progress on the development of software. It's frustrating when trying to do a set of sequential refactorings and have to wait several minutes between each step to verify you didn't break anything. Slow-running tests serve as a warning sign that your code might be too coupled. If you are having to stand up significant amounts of the system to test basic scenarios, your tests will take longer and longer to execute as more tests are added. This will then compound with any other existing issues impacting your red/green/refactor cycles. Slow-running tests can also be a warning sign that your code may not be very testable. Perhaps this is because your code is too tightly coupled as we just discussed, or perhaps your code isn't broken down into fine grained enough of chunks. This can then contribute to more difficulty in reusing different parts of your code when implementing new features. So why should excessively slow tests concern us? Primarily because as tests become slower, we can lose more and more of the benefits that we are wanting to get out of test-driven development. Our confidence can diminish as we don't get very quick feedback when making changes to the system. We can't be as confident in our changes as we would otherwise be. As tests take longer and longer to execute, it slows down our ability to rapidly release software. As we have to wait longer to run the full test suite, our release cycles also slow down. Finally, we end up being less agile. We will stop making changes to certain parts of the code because we know then that we'll have to run very long-running tests in order to verify we didn't break anything, or, we end up not being as aggressive as we want to be when changing any code. When we have a rapid red/green/refactor cycle, we can be very aggressive in changing code, since the tests are continually running and giving us feedback. So these are some basic anti-patterns to look out for when you are starting to practice test-driven development.
Limitations of TDD

Test-driven development is not a silver bullet. It does come with some limitations that you should be aware of. First of all, it's possible to have holes in your tests. One of my favorite quotes is from a talk by Joe Armstrong, the co-creator of Erlang. What does every bug in production have in common? Well, two things really. First, they have all passed the compiler checks, and second, they all pass the tests that were run against the software. So while TDD gives us a lot of benefits, we should also avoid over-confidence in our test suites. Practicing test-driven development won't prevent bugs from being written entirely. Another thing to remember is that TDD by itself is not sufficient. TDD certainly gives us great benefits as part of a development process, but doesn't satisfy all the needs we have for verifying our software. We also need to account for deployment verification. When a new version of our software is deployed, we still need to make sure it works after deployment. Perhaps a production dependency has changed, or perhaps some other aspect of our production machines have changed. Network changes could also prevent our deployed software from working. Our test-driven development process doesn't directly address these types of changes that can break our software. There are also integration problems that may not happen until we are running our software against production dependencies, or perhaps a partner dependency deploys a new version of their own software that contains breaking changes, which causes problems in our software. It's an important constraint we need to keep in mind and that TDD doesn't directly address. To be successful with TDD, it is also important to have management buy-in and support. For managers and developers not familiar with TDD, it can be difficult to see the long-term benefits and to overly focus on any possible short-term pain in adopting TDD. With constant pushback against the adoption of TDD, it will become even more difficult to put TDD in the practice and to fight the uphill battle every step of the way.
Common Questions

There are some common questions or comments you may hear from other TDD practitioners or people wanting to get into TDD. First of all, to practice test-driven development, do you also need to be practicing agile? TDD is certainly a main staple of practicing agile, but agile itself isn't a requirement to be practicing TDD. Let's review the business goals we achieve when practicing TDD. We are aiming to have the verification of requirements built in to our development process. By using test-driven development, we aim to develop a holistic suite of automated tests that will help us catch any possible regressions in our software, and we do a lot of this to help lower the maintenance costs of our software. So can you use TDD while working in, let's say, a waterfall system? Yes, of course you can. Will you get the full benefit that TDD can bring you? Possibly not. TDD shines brightly when leveraged within an agile process because it allows us to make the cycle between a customer ask and the software being delivered smaller and smaller. With other project management styles outside of agile, you risk losing this key benefit. The next controversial question that people often argue about is whether you have to write your tests first. Some will say that to be practicing TDD properly, you absolutely must write your tests first before you write any implementation code whatsoever. It's one of the key tenants of the red/green/refactor cycle. However, I don't think it's necessarily that cut and dry. Let's remember what development benefits we are trying to achieve with TDD. We want to avoid over-engineering our code. Writing our tests first help us with this, but so would, let's say, rapid prototyping first before writing the tests, as long as we remember to treat our prototype not as releasable code, but code that we're willing to throw out. We also want to reap the benefits of increased momentum and higher confidence when making changes to our software. Can you achieve these benefits if you aren't writing your tests first? Of course you can. You may not be reaping all the benefits of TDD through an efficient red/green/refactor cycle, but you will still be getting many, many benefits. It's not about the dogma of pure TDD. It's about the end goal, the value we are delivering to the business and to the customer. So a key to remember is to keep an eye on the benefits that test-driven development is trying to deliver. A key aspect of any properly agile process is to customize it to be your own. You want to make the process work for you, your team, your business, and your customers. We are aiming to improve the value and cost of the software we are delivering. There is no one right solution, and there is no silver bullet. It is all about having retrospectives and constantly seeking to improve your process with each successive release.
Course Summary

So, we've reached the end of this course on The Big Picture of Test-driven Development. Let's recap really quickly. What is test-driven development? Test-driven development focuses on the writing of tests. We want these tests to verify our code satisfies customer requirements, responds correctly to all input, and is reliable, and has acceptable levels of performance. Test-driven development then is the software development process built around writing these tests and has a very short development cycle. Customer requirements are turned into test cases, which are then made to pass by improving our software. This inner cycle of development is known as red/green/refactor and is the mantra that lies at the heart of test-driven development. For the business, we are aiming to reduce the costs of delivering software today. TDD helps us achieve this by keeping us focused on the customer and the customer's needs. Every time we are implementing a feature, we are writing tests and code from the customer's point of view. This, in turn, helps us avoid over-engineering our software. This will help the software remain flexible to the ways it will need to change in the future based on the changing needs of the customer and the changing needs of the business. Finally, by having a continually growing set of customer-focused tests and a rapid red/green/refactor development cycle, we gain a lot more confidence as developers working with source code, and gain increased momentum when developing new features because of the lessened impact when working with existing code. And that's a quick recap of The Big Picture of Test-driven Development. So what are some next steps we can take in our journey? First of all, focus on continuous improvement. The goal is that day by day, week by week, we are getting better as software developers. It will take time to put these new activities into practice. Keep at it, and don't worry if you have occasional setbacks. We want to drive our code from legacy code to fully supported code. Remember, we define legacy code as code without automated tests. If you come across some code that doesn't have automated tests, implement just a single test at first. Every single test that gets added to support legacy code puts us in a better and better position. Finally, share your journey with others. It could be team members, your management chain, or other developers you know from user groups or perhaps just starting a blog or Twitter. It's great to network with others that are on this journey and realize that we are all in this journey together. We often face common problems, and learning from others is a great way to continually improve yourself as a developer. Finally, there are many related topics that you can dig into next if you're interested. You may want to learn more about agile development, or dig into details of doing test-driven development in your language of choice. You can learn more about refactoring as a way to have repeatable steps to improve your code. Or, you may want to dig into other areas that aim to help us improve our code in a more rapid fashion, like continuous integration and continuous deployment. As a reminder, if you have any questions, comments, or other feedback on this course, you can find a Discussion tab on the Pluralsight page for this course. I am also available on Twitter @jolson88. I would love to hear about your experiences with this course. So thanks again for joining me on this journey of learning about The Big Picture of Test-driven Development. Until next time, this is Jason Olson signing off.
Course author
Author: Jason Olson
Jason Olson

Jason Olson is a software engineer passionate about distributed computing and cloud-based technology. He is a full stack developer at Concur, and formerly a Technical Evangelist and Program Manager...
Course info
Level
Beginner
Rating
4.5 stars with 160 raters(160)
My rating
null stars
Duration
1h 50m
Released
14 Apr 2017
Share course


