Architecting Big Data Solutions Using Google Dataproc
by Janani Ravi

Dataproc is Google’s managed Hadoop offering on the cloud. This course teaches you how the separation of storage and compute allows you to utilize clusters more efficiently purely for processing data and not for storage.

When organizations plan their move to the Google Cloud Platform, Dataproc offers the same features but with additional powerful paradigms such as separation of compute and storage. Dataproc allows you to lift-and-shift your Hadoop processing jobs to the cloud and store your data separately on Cloud Storage buckets, thus effectively eliminating the requirement to keep your clusters always running. In this course, Architecting Big Data Solutions Using Google Dataproc, you’ll learn to work with managed Hadoop on the Google Cloud and the best practices to follow for migrating your on-premise jobs to Dataproc clusters. First, you'll delve into creating a Dataproc cluster and configuring firewall rules to enable you to access the cluster manager UI from your local machine. Next, you'll discover how to use the Spark distributed analytics engine on your Dataproc cluster. Then, you'll explore how to write code in order to integrate your Spark jobs with BigQuery and Cloud Storage buckets using connectors. Finally, you'll learn how to use your Dataproc cluster to perform extract, transform, and load operations using Pig as a scripting language and work with Hive tables. By the end of this course, you'll have the necessary knowledge to work with Google’s managed Hadoop offering and have a sound idea of how to migrate jobs and data on your on-premise Hadoop cluster to the Google Cloud.

Course author
Author: Janani Ravi	
Janani Ravi
Janani has a Masters degree from Stanford and worked for 7+ years at Google. She was one of the original engineers on Google Docs and holds 4 patents for its real-time collaborative editing...

Course info
Level
Beginner
Rating
0 stars with 7 raters
My rating
null stars

Duration
2h 17m
Released
1 Nov 2018
Share course

Course Overview
Course Overview
Hi. My name is Janani Ravi, and welcome to this course on Architecting Big Data Solutions Using Google Dataproc. A little about myself. I have a Master's degree in electrical engineering from Stanford and have worked at companies such as Microsoft, Google, and Flipkart. At Google, I was one of the first engineers working on real-time collaborative editing in Google Docs, and I hold four patents for its underlying technologies. I currently work on my own startup, Loonycorn, a studio for high-quality video content. In this course, you'll learn to work with managed Hadoop on the Google Cloud and the best practices to follow for migrating your on-premises jobs to Dataproc clusters. We'll study in some depth how separation of storage and compute allows you to utilize clusters more efficiently purely for processing data and not for storage. We start off by creating a Dataproc cluster and configuring firewall rules to enable us to access the cluster manager UI for our local machine. We'll execute map reduce jobs in the cloud using the web console, as well as the command line. We'll add additional compute capacity to our cluster using preemptible VMs and monitor our cluster using Stackdriver. We'll then study how we can use the Spark distributed analytics engine on our Dataproc cluster. We'll work with the PySpark shell on our cluster, as well as submit Spark jobs using the web console. We'll also see how we can write code to integrate our Spark jobs for BigQuery and cloud storage buckets using connectors. We'll then use our Dataproc cluster to perform extract, transform, and load operations using Pig as a scripting language and work with Hive tables. At the end of this course, you should be comfortable working with Google's managed Hadoop offering and have a sound idea of how to migrate jobs and data on your on-premises Hadoop cluster to the Google Cloud.

Introducing Google Dataproc for Big Data on the Cloud
Module Overview
Hi, and welcome to this course where we'll study how we can architect big data solutions using Google's Dataproc. We'll start off by understanding what exactly Dataproc is and how you can move your big data computations to the cloud using GCP's managed Hadoop offering. Now Hadoop is a ubiquitous big data processing technology that has been around for over a decade and is very mature and is widely used. Hadoop can be thought of as a software framework that allows you to run computations on a number of different nodes where the nodes belong to a cluster. Hadoop clusters tightly couple storage, as well as compute. You store data on different machines in the cluster, and you run processes on them in parallel. The original concepts behind Hadoop were developed at Google in order to process and index its web search results. This was breakthrough technology at the time, and its open source version that is Hadoop is now widely available and used by almost every organization. However, this tight coupling of storage and compute leads to utilization and cluster sizing inefficiencies. You end up paying for more than what you need. Time has brought about improvements in the products that are available as well. Managed Hadoop allows us to solve for these inefficiencies by splitting storage and compute. Where you store the data and where you perform computations are completely separated. This allows us to efficiently use our cluster. Dataproc is Google's managed cloud Hadoop offering, and that's what we're going to study here. Dataproc allows you to set up clusters and perform processing on huge amounts of data in a pay-as-you-go manner.

Prerequisites, Course Outline, and Spikey Sales Scenarios
Before we move on to understanding and working with Cloud Dataproc, let's see some of the prerequisites that you need to have in order to make the most of your learning. Now this course assumes that you're familiar with cloud computing. You've worked with cloud computing platforms before, either the GCP or AWS or Azure. If you haven't, then this course here, Choosing and Implementing Google Cloud Compute Engine Solutions, would be the right course for you to take before you move on to this course. This course also assumes that you're familiar with the basics of big data processing in the Hadoop ecosystem. If not, here is a course that you can take on Pluralsight, The Building Blocks of Hadoop -- HDFS, MapReduce, and YARN. In order to be completely comfortable understanding the concepts and working with the demos in this course, you need to have a basic understanding of cloud computing. It need not be on the GCP though. If you work with Azure or AWS, that's fine too. You need to understand how VMs work. GCP uses its virtual machines in order to run managed Hadoop on the cloud. You need to understand the basics of Hadoop, Spark, Hive, Pig, and other ecosystem technologies. All of these technologies run on top of the basic Hadoop framework. You don't need to have in-depth knowledge, but you have to have some kind of understanding of how they work. We start this course off by understanding what exactly is Google's Dataproc for managed Hadoop. We'll see why it's better than on-premises datacenters and why you might want to run your big data processing jobs on managed Hadoop. Once we've understood the concepts, in the module after that, we'll focus on hands-on demos. We'll see how we can run MapReduce jobs on Dataproc. We'll see how we can monitor our jobs using Stackdriver monitoring and use initialization actions to set up our VMs. If your organization is working with big data, chances are your developers use Spark as a data analytics engine. We'll see how we can work with Spark on Dataproc. And, finally, we'll see how we can use Dataproc in order to perform extract, transform, and load operations using Pig and how we can use Hive on Dataproc as our data warehouse. Through all of the demos in this course, we'll assume that your part of the engineering team at SpikySales. com. Spikey Sales is a hypothetical online retailer. They are an e-commerce site, which focuses on flash sales of trending products, which means they tend to have very large spikes in user traffic. At other times, traffic to their website is fairly calm and quiet. Their engineering team wants to move from an on-premises data center to the GCP. They're exploring if GCP as cloud computing is a perfect fit for their use case. It's pay-as-you-go, and they have no idle capacity during the off-sale periods. They want to move the analytics and other data that they have from their on-premises data center to cloud storage buckets. Cloud storage allows them elastic storage, pay as you go, as well as global access. The engineering team currently has a large on-premises Hadoop cluster, and they have utilization concerns. As they get more popular, their Spikey Sales tend to have large utilization. If they buy additional machines to add capacity, those machines tend to be largely un-utilized when there are no sales. Administering this on-premises cluster is also proving to be a heavy burden. They want to scale the big data processing requirements that they have. The on-premises Hadoop cluster is also expensive to scale, and they have many solutions which use Spark, Hive, and Pig. So they'll use the entire suite of technologies available in the Hadoop ecosystem. They're especially excited about Cloud Dataproc on the GCP because they want a way to deal with spikes in workloads. The cluster is mostly idle for them when they use their on-premises solution. They want their cluster to be utilized completely. They also need to be able to quickly scale up their cluster processing on sale days.

Distributed Processing
Let's first talk about what exactly Hadoop is and why it's useful for big data processing. Now back in the late 90s, most of the systems that we built were monolithic. There are two ways to build a system, and in the 90s, they built large monolithic systems with components that call into each other. But as the 2000s rolled about and Google started web search at a massive scale, it started looking for distributed solutions. A distributed system basically involves a cluster of machines. There are many off-the-shelf components which make up this distributed cluster. A single machine in this cluster is called a node, and a group of machines working together to achieve a single objective is called a cluster. When you have a number of machines working together in this manner, you require software that will coordinate processes across all of these machines. And this software was what Google developed back in the early 2000s. Google's original software was made up of two basic components. It involved a distributed file system called Google File System, and it involved a computation framework that ran processes on this distributed file system. This was called MapReduce. Google engineers wrote a paper on this breakthrough framework, and HDFS is the equivalent of the Google File System that was developed in the open-source world. And MapReduce is the open-source equivalent of the MapReduce framework originally from Google. These technologies came together to form Hadoop, HDFS, that is, a distributed file system. This is what we use for storage of our big data. MapReduce is the compute, the software that runs on top of the distributed file system to process your data in parallel. And YARN is the cluster manager that coordinates the running of this software. YARN is responsible for resource allocation across the cluster for multiple jobs. It runs the MapReduce parallel programming framework so that data that is stored on HDFS, the distributed file system, is operated on in parallel. This is how Hadoop achieves its massive big data processing capabilities. A developer working on the Hadoop platform simply writes code in terms of map and reduce operations, and these operations are then passed in to the cluster. YARN, the cluster manager, is responsible for triggering this job on the cluster, running these map and reduce operations in parallel across all of the nodes in the cluster. YARN figures out where and how to run the job and stores the result in HDFS. Hadoop is the basic underlying framework over which all of these other technologies run, Hive, HBase, Pig, Flume and Sqoop, Spark, and Oozie. If you've worked with any of these technologies, you're basically using Hadoop behind the scenes for your big data processing.

Storage in Traditional Hadoop
Before we understand why exactly Google's Cloud Dataproc is such an improvement over the on-premises Hadoop that we worked on so far, we need to understand how storage and compute work in traditional Hadoop. Here are the three components that make up Hadoop. HDFS, which stands for Hadoop distributed file system, is the storage component. MapReduce is the parallel programming framework. And YARN is for coordination. Let's talk about storage first and figure out where exactly data is stored in traditional Hadoop. Now the Hadoop distributed file system is a file system that spans multiple machines in your distributed cluster. The actual file system resides on different nodes within your cluster, and all of these nodes are built on commodity hardware. These clusters are highly fault-tolerant. Nodes fail really often; data is often replicated on multiple nodes in order to enable recovery. HDFS has been built for batch processing operations. The data access pattern tends to favor high throughput rather than low latency. As you add more machines to your cluster, you increase the size of data that you can store, which means HDFS is capable of supporting very, very large data sets. Assume that you have a very large data set. It's made up of 100 million records. Now HDFS will manage the storage of this data across multiple disks. These disks will belong to different machines. Each of these disks will be associated with a different node in your cluster. You have a cluster of machines for your big data processing. These disks are associated with the different machines in that cluster. Now one machine here represents one node in your cluster, and a single node in your cluster is denoted as the master node. The master node in your cluster is also often called the name node in your cluster, and the remaining nodes in your cluster are called data nodes, also referred to as worker nodes. Now if you have your large data set comprised of 100 million records, the name node does not actually store the data. It's responsible for managing the overall file system. It stores the directory structure and the metadata of all of the files that make up your data set. The name node is responsible for knowing which part of your data is stored on which machine. Let's say you've split your data into five. There are 20 million records on each of your file nodes. The name node will know which record is stored where. The data node is where your data is actually stored. This is comprised of the physical disks where the records, each of your 100 million records, will be stored. This is what a traditional Hadoop cluster looks like. You have multiple data nodes within the cluster, and each of these data nodes have blocks of storage where your data will be stored. And the name node will have a mapping knowing where exactly a particular file is stored, in which block and which node in your cluster.

Compute in Traditional Hadoop
Now that we know how data is stored in a distributed manner, let's talk about how the MapReduce parallel programming framework allows us to operate on our data in parallel. MapReduce is just a programming paradigm that allows us to take advantage of the inherent parallelism that is present in data processing. Let's say you have a data set with 100 million records. Such data sets are actually the norm nowadays, they're not the exception. Now the MapReduce framework processes this data in two stages. The first of these stages is called the map operation, and the second stage is called the reduce operation. The map operation runs in parallel across multiple machines on your distributed cluster, so the map process runs on multiple machines, and it works on that subset of data that is present on that machine. Once the map operation is completed, the output of the map operation is passed on to the reduce operation. The reduce process then aggregates the output of the mappers together to give some meaningful output. Map and reduce are the only two functions that a programmer needs to define. And the rest of the detail, making sure the data is processed on correctly, making sure the output of the mapper is passed to the reducer, all of this is taken care of by Hadoop. The map process looks at every record in your data set. Multiple map processes run on different nodes of your cluster, and they process only those records that are present on that node. The map process looks at one record and produces one key-value pair as the output. The reduce process can be thought of as some kind of aggregation operation which takes the output of several mappers together and combines them together in some meaningful way. So the output of several mappers is passed to the reduce phase, and then you get the final output as the result. So if you want to summarize map and reduce, map is a step that is performed in parallel on multiple machines where your data is stored, and reduce is a step which combines the output of the mapper that is the intermediate results to give you the final result. As an example, let's assume that you have a very large file in petabytes, and this has been stored across multiple machines on your cluster. It's managed by HDFS. How do you go from this very large file to counting the word frequencies in this file? You want to know how many times each word occurs in this file. This file that we start off with, which makes up our data set, is typically very, very large in petabytes. Now this file will be split up, and different subsets of the file will be stored on different machines in your cluster. So you have all of these machines which hold subsets of your data. Each partition or each subset of your data is given to a different mapper process, which operates on that data. So you can imagine a mapper process is operating on each individual subset here. The mappers work in parallel. They perform the same operation but on that subset of the data that is present on that machine. So within each mapper, the rows in your files are processed one after the other. But all the mappers work in parallel. Each mapper will process just one line at a time. It'll split the line into words, and output the word itself and a count of 1. So for every word, a count of 1 is associated with that word. So each row or line will be split into as many key-value pairs as there are words in that line, and that is the output of every mapper. The output of every mapper is then passed on to the reducer. The reducer is where the actual counting happens. Behind the scenes, Hadoop will group all of the key outputs of the mapper along with the associated values, and the reducer will simply sum up the values associated with each key. And it'll get the final word frequencies. The Hadoop framework is responsible for ensuring that all of the values associated with the same key are passed on to one reducer. This combination of mappers and reducers together form the basic building blocks of big data processing.

Separating Storage and Compute with Dataproc
We'll now move from the world of traditional Hadoop to Cloud Dataproc and see why Cloud Dataproc is so much more powerful and flexible. The first significant advantage that you'll encounter when you move your big data processing systems to the cloud is the fact that you can create and take down your Hadoop clusters on the fly. If you're working with the traditional on-premises data center, you're probably already aware that provisioning Hadoop machines and setting up your cluster is an onerous, time-consuming process. This is no longer true when you're using managed Hadoop on the cloud. This is the most obvious reason to choose cloud computing platforms. But before that, let's talk about the drawbacks of tight coupling between storage and compute that you have to live with if you're using traditional Hadoop clusters. Now we know that each node contains data blocks, and the node also runs processes, mappers and reducers that work on these data blocks. Storage and compute are tightly coupled. They run on the same node. The first problem that you have with this setup is that you have to keep your cluster up and running even when there is no processing happening. The cluster is required to store your data. The cluster has to be provisioned. Your machines have to be up and running in order for data to be persistent. Now if you encounter a situation where you have to use this cluster to store even more data, the size of your data has expanded significantly, then you need to resize this cluster by adding more machines. And resizing the cluster requires all of your data to be re-sharded and re-distributed across the nodes in your cluster. For a rapidly growing organization, this is a major pain point. Another issue is that of utilization. For an organization such as Spikey Sales where the utilization of their cluster varies significantly from day to day, sizing the cluster, figuring out the right number of machines in the cluster, and figuring out the best possible utilization of these machines become very difficult. If you try to over provision your cluster in order to meet the demands of traffic on sale days, then the cluster becomes too big. The utilization of your hardware is far too low. At the other end of the spectrum, if you try to increase utilization and cut down on the fixed cost of your machines, you end up with a cluster that is far too small. Then the latency and the performance of that cluster might be unacceptable. And for many organizations, this happens to be the major driving force behind moving their big data processing to the cloud. Cloud-based Hadoop offerings for managed Hadoop allow you to control the size of your clusters on the fly. You can dynamically create and use the clusters whenever you require them. You can scale the clusters very, very fast. You can add more virtual machines to increase the size of your clusters on sale days. On other days where your organization might experience low traffic, you can simply decommission those bits of hardware. You can simply turn down those machines. When you use GCP's managed Hadoop offering, the administrative overhead of performing these operations is completely abstracted away from you. All you have to do is run scripts on a command line or click on the web console. Here is a visual representation of the tight coupling between storage and compute in a traditional Hadoop cluster. The data nodes that perform processing also store the data. In the case of a Dataproc cluster, though, you'll have VM instances, which are responsible for running processes. You'll store your data in cloud storage buckets. And this is a significant reason why Cloud Dataproc is so powerful. You can store your data on cloud storage buckets where it can be accessed by other GCP services. You'll perform all of your processing on your Hadoop cluster. Low latency, high throughput connections between cloud storage and your cluster machines make this the preferred way to operate. The nodes that make up your Hadoop cluster on the cloud are GCE VM instances, Google Compute Engine virtual machine instances. The master node, as well as all of the worker nodes which run your big data processing operations are GCE VM instances, and all compute is performed on these instances. The map and reduce operations that we spoke of earlier are processes on these VMs. You can configure one or more VM instances as the master node. On the GCP, it's a simple configuration setting to run your cluster in the high-availability mode with multiple masters. So if a master fails, another one can take over easily. All of the data on which you want to run big data processing won't be on the actual nodes themselves. Instead, you'll use cloud storage buckets for storage. Cloud storage buckets on the GCP allow object storage just like AWS's S3 or Azure's blob storage. This is elastic storage. The storage expands as you add more data, and you only pay for what you use. You don't provision storage upfront and pay fixed costs. You only pay for the data that you actually store in a bucket. This is infinitely scalable. You just add more data as you need to. You don't need to add additional machines. And more significantly, storage is completely decoupled from compute. It's scalable independent of your compute option, so you can have a very small cluster and a huge data set, or you can have a large cluster and a small data set. Here are the basic components that make up a traditional Hadoop framework. Here we've switched storage over to Google Cloud Storage. Now this is preferable. It's not that you can't use HDFS when you're using Cloud Dataproc. However, you would prefer to store your data on Google Cloud Storage thus effectively separating compute from storage. Instead of provisioning real machines with software installed to run MapReduce, you'll use Google Compute Engine VM instances. And instead of YARN, you'll use something called a Dataproc service. YARN is just one component within Dataproc service. You can think of the Dataproc service, what you use for coordination within your managed Hadoop cluster, as YARN++.

Hadoop vs. Dataproc
If your organization is already using an on-premises Hadoop cluster, then the chances are you're storing your data on HDFS. If you're using Cloud Dataproc, though, the preferred solution is to use Google Cloud Storage to store your data. Now HDFS is still available on all of your cluster nodes. It still runs on the persistent disks of your cluster VMs. We should avoid using it, though, so that we don't tightly couple storage and compute. This basically means that when here using Dataproc, you don't have to instantiate your cluster machines if you're not actually running any processing. You don't have to instantiate those clusters only to store data. Store data on cloud storage buckets. This is what Google prefers, and this is what is recommended in order to reduce your costs. You should not pay for VM instances if they don't need to be active. Let's take a look at some of the differences between traditional on-premises Hadoop versus Cloud Dataproc. In the traditional Hadoop setup, clusters stay in existence forever. You add machines, you install the software, you provision your cluster. In the case of managed Hadoop with Dataproc, you can create your clusters dynamically when you need them. Google's managed Hadoop offering does not do away with HDFS. HDFS runs on the persistent disks of VMs, and you can use it if you want to. You'll just have to make sure that your cluster is instantiated if you want persistent storage. When you're working with traditional Hadoop on-premises, the data for jobs that you run will be stored in HDFS. In the case of Dataproc, though, ideally you want your data stored in Google Cloud Storage buckets, which offers elastic storage and fast access. When you store data in your cluster, your cluster encapsulates state. It holds state. The cluster with Dataproc is stateless. It's only used for processing. In addition to loosely coupling storage and compute, when you store data in cloud storage buckets, there are other advantages to using bucket storage versus persistent disk storage. Persistent disk is the storage that you're familiar with. It's block storage of data. Buckets offer object storage. The data itself is treated differently. When your data is in a bucket, you can configure fine-grain policies for version management, as well as lifecycle management for this data. A persistent disk associated with a Compute Engine VM on the GCP can be a maximum of 64 TB in size. Buckets, though, are infinitely scalable. You can add as much data as you want in a bucket. Persistent disks are pay what you allocate, so if you have a 3 TB disk, you pay for the entire 3 TB no matter if you use just 100 GB from within it. Buckets are pay what you use. You only pay for the data that you've stored. Persistent disks are closely tied to GCE VMs and can be only accessed by processes on those VMs. Buckets are completely independent of GCE VMs, and they can be accessed by other Google services as well. Persistent disks can be accessed from within a particular zone. A zone can be thought of as a single data center or a failure domain within GCE or can be accessed from within a region in certain cases. In the case of buckets, it allows regional, as well as global access. A region on the GCP can be thought of as a geographical area, and global basically means across the world.

Using the Cloud Shell, Enabling the Dataproc API
Let's take a little break from concepts and log on to our GCP account and enable the APIs that we require to work with Dataproc. You can create an account on the Google Cloud Platform using a Gmail account or a G Suite account that is associated with your organization. If you're logging in for the very first time, you probably don't have a project yet. You can click on the drop-down on top and create a new project or choose an existing project. A project on the GCP can be thought of as a logical unit that holds your resources. Every resource on the GCP belongs to a particular project. A project is an independent billing unit and is typically associated with a particular team in your organization. Spikey-dataproc is the project that the Spikey Sales organization engineers are using in order to test out Dataproc. This should take you to the dashboard for the spikey-dataproc project. Make sure that you have billing enabled for this particular project so that you can instantiate VMs that make up your Dataproc cluster. So long as you're good about deleting the resources that you create, the charges that you'll incur when you perform the demos in this course should be under $5. If you're working on the GCP, the cloud shell is a very handy feature. The cloud shell is basically a terminal window on an ephemeral VM that Google creates for you on the cloud. This VM comes preinstalled with all of the utilities that you need to work with Google services such as the gcloud SDK, gsutil to work with cloud storage buckets, and any other tools that you need. You can explore this dashboard and other options that are available here. On the top right, you can see that I've signed in with a G Suite account, Spikey Sales. Use the hamburger icon on the top left in order to access all of the products and services that GCP has to offer. If you click on this, it'll open up a navigation menu, and you can use this to navigate to the service that you're interested in. We're going to start off with the APIs and Services where we'll enable the APIs that we need to work with Dataproc. We've come to the API library here. This gives us a search box, which allows us to search for the API that we're interested in. We are interested in the Dataproc API. This gives us one search result. Click through here, and click on the Enable button. Once this API is enabled, you can set up Dataproc clusters on the GCP. You can click on the Back button here at the top in order to go back to the main page. Here you'll see links to manage this API and another button that will take you to a tutorial that allows you to try this API. And at any point in time, clicking on the Google Cloud Platform link at the very top will take you to the main dashboard for your project. Activate your cloud shell by clicking on the cloud shell icon on the top right. Here you have your cloud shell terminal window. You can see that the current project is spikey--dataproc. You can ensure that the current session of your cloud shell is always set to this project by calling gcloud config set project spikey-dataproc. The gcloud is a command line utility, which is the SDK that Google offers to work with GCP services. Gcloud comes preinstalled on the cloud shell. If you're working on a local machine, you need to download the gcloud SDK. Google also offers a nifty code editor on your browser. You can click on the edit icon there and launch code editor beta. The three-dot menu will show you additional actions that you can perform on your cloud shell such as upload a file to cloud shell, download a file from that VM, and so on. You can use the cross button at the very right in order to close this terminal window.

Dataproc Features
So far, we've mainly spoken of the decoupling of storage and compute when we use Dataproc as opposed to traditional Hadoop. Let's see what other features Dataproc has to offer. Now there are different types of clusters that you can create when you use Dataproc. You can create a regular single-master cluster, you can have a single-node cluster as well with the master and the worker in the same node. This is useful when you're prototyping and developing your big data processes. Or you can set up your cluster in high-availability mode with more than one master. Once you've created your cluster, accessing your cluster in order to run jobs on it or to administer it is very straightforward. You have a variety of ways in which you can connect to this cluster. You can use the web console, or you can script using the gcloud command line utility. You can also use REST APIs in order to administer your cluster, or you can use client libraries that give you programmatic access. The advantage of using a cloud platform such as Google is that cluster actions such as resizing are very, very fast. You can reset your cluster even when you have processes running. You don't need to pause or stop your jobs just because you need to add or remove a few nodes. You can increase and decrease the number of workers that you have enabled on the fly. GCP allows you to gracefully decommission the downsized cluster nodes. If you plan to decommission a node, processes that are running on that node will first be completed. Only then will that node be removed. You might want to scale your cluster for a variety of different reasons. For example, the Spikey Sales organization has found that on sale days, they need analytics reports quickly. This allows management to analyze how the sales are doing and to tweak their product offerings. Now the batch processes that are run to generate these reports can be run much faster with additional compute nodes. You might also want to scale if you're storing your data on HDFS and require additional storage. Because clusters can be tweaked so easily, large clusters are completely fine when working with Dataproc. Large clusters can be scaled up or down based on your requirements. In fact, there is brand-new functionality available from Google to enable you to auto-scale your clusters. Based on YARN cluster metrics, nodes will automatically be added to or removed from your cluster within bounds that you specify. You can also configure your cluster very explicitly by specifying what kind of OS images you want to run on your cluster VMs. You can switch between different versions of the various technologies, a different version for Spark, Hadoop, Hive, Pig, and so on. Google is responsible for the version management of these technologies and will upgrade them separately. All of this is managed completely by the GCP. If you want your cluster VMs configured with additional software, you can specify them in the form of initialization actions. You can specify separate initialization actions for the master node, as well as worker nodes. We've already discussed the auto-scaling of clusters that is currently in alpha. Another cool feature is auto-zone placement. If you want your cluster nodes to be within a particular region, you can specify auto-zone placement, and GCP will choose the right zone for you for the different nodes. Scheduled deletion is a feature that you can enable on your cluster nodes in order to ensure that you don't inadvertently incur costs for a cluster that you no longer use. Jobs running on your cluster can also be configured to be restartable. Scheduled deletion of cluster nodes is a very cool feature that allows you to get a tight grip on utilization and costs. You can delete the cluster after a specified idle period, at a specified future time, or after a specified interval after you've created the cluster.

Migrating to Dataproc
Dataproc's advantages have convinced your organization that moving to the cloud is the right thing to do. Dataproc is basically Hadoop on the cloud, so you'll find that migrating to Dataproc is very easy, but using it well in order to extract all of the efficiencies that it offers requires a big change in mindset, and that's where you might trip up. Let's take a look at some of the design decisions that you might need to make so that when you move to the cloud, you're set up to utilize all of the benefits. Dataproc supports almost all of the technologies in the Hadoop ecosystem, so migrating should be relatively straightforward. There is built-in support for MapReduce, Spark applications written either in Scala or in Python, Pig for extract/transform/load operations, and Hive to use as your data warehouse. Here are some of the mindset changes that you might need to make when you're migrating to Cloud Dataproc. Now when you're on the cloud, clusters are no big deal. You can create clusters on the fly. You can make the clusters as large as you want them to be. When you're working with on-premises Hadoop, clusters are really big deal. You have to specially order machines, provision them for Hadoop, and so on. When you move to the cloud, you need to start thinking of clusters as something that is a lightweight compute option. You create and tear down clusters on the fly. When you have the on-premises mindset, though, your cluster size and setup is set in stone. When you're working on the cloud, don't worry about the size of the cluster and the number of nodes that you need to provision when you first create them. You can resize and tweak your cluster once you've seen the traffic patterns and your processing requirements. With the on-premises mindset, cluster properties are typically set in stone and cannot be tweaked. When you're using Dataproc on the cloud, you need to trust GCP. You need to trust that GCP will handle all of the orchestration, administration, and setting up and scaling of the cluster for you. If you have the on-premises mindset, you'll try to tightly configure and control everything simply adding to your burden. When you're working with the cloud platform, you won't keep your cluster running at all times and incur unnecessary costs. You'll bring your cluster up only when you want to explicitly run some kind of job on the cluster. If you have the on-premises mindset, you'll end up keeping your cluster up at all times and paying for the VM instances that you have running. Remember that clusters on the cloud can be brought up very easily. If you require multiple clusters, and you have jobs that need to be isolated, go ahead and create multiple clusters. If you have the on-premises mindset, you'll always use the single cluster that has been set up and provisioned right. This makes it much harder for you to control the resources that are allocated to specific jobs on that cluster. Administering of clusters in the cloud is very simple using the web console. You can submit jobs using this console, monitor them, and even cancel them if you want to. If you have the on-premises mindset, you'll continue to use the on-premises procedures such as logging into the master machine to submit and manage jobs. Based on the kind of workloads you want to run, you can configure your cluster exactly the way you want it to be. You can configure the machine types that make up the individual nodes. If you're running a job that requires high compute capacity, you can configure your cluster with high CPU machines. If your job has large memory requirements, your cluster can be made up of high memory machines. Do not try to match your cluster with what you currently have on your on-premises data center. Tweak your cluster based on your requirements. When you're working on the cloud, don't be afraid to provision very large clusters to have your job run quickly. You can shut down the cluster once your job has completed. If you're working with the on-premises mindset, you'll try to match your cluster size on the cloud to what you have on-premises. And that doesn't really work that well. Think of the number of nodes you have in your cluster kind of like a configuration parameter that you have to tweak on the fly. Scale your clusters up and down as needed. If you continue with your on-premises mindset, you'll try to keep the cluster size constant at all times to avoid provisioning new machines. And here is a really cool feature that you can use on the GCP. You can use preemptible VMs in order to add compute capacity to your cluster. Preemptible VMs are kind of like Amazon's spot instances, but you don't require reservations upfront. Preemptible VMs should be used as additional compute capacity when you really need it. This is an instance that you can create and run at a much lower cost than normal instances. This is because the GCE might terminate these instances at any point in time, and your preemptible instance will be terminated at least once in a 24-hour period. Preemptible instances are a low-cost way to get additional compute capacity. They're not used for data storage. If the preemptible instances that you've added as worker nodes to your cluster are reclaimed by Google Compute Engine, then the jobs that you're running on those preemptible instances will slow down. But those jobs will not be stopped. They'll continue running, albeit more slowly, on your permanent instances. If you're working with Dataproc on the Google Cloud, make sure you use the default Hadoop distribution. If you try to shoehorn a third-party distribution to GCP, you might have to deal with issues that are very specific to you. If you want to tailor your VMs in the Hadoop framework specific to your use case, you should use initialization actions and cluster metadata to do so. Previously when you were on on-premises Hadoop, you might have done this using config files or third-party configuration tools.

Dataproc Pricing
If you're moving all of your data and compute to the cloud, how much is Dataproc going to cost you? Now the cost for Dataproc is primarily the price you pay for the VMs that run your compute operations plus a small additional charge. There is a small additional charge for the Hadoop software or Spark, Hive, or Pig, or any of the other technologies that you use from the Hadoop ecosystem. They're all the open-source versions. The main cost component in your Dataproc cluster is the Compute Engine instances that you need to keep up and running. GCE VMs come in a variety of flavors. There are standard, predefined machine types, and there are custom machine types as well. Over and above the VM prices, you get sustained discounts for continuous usage of these VMs. And if you commit upfront to a certain period of usage, committed discounts apply as well. If you choose to add additional virtual CPUs for more compute capacity, there is an additional fee per vCPU per hour. This additional fee for each vCPU can range from 1 cent to 64 cents. Remember, this is per vCPU per hour. Now if you really want to understand Dataproc pricing, you need to understand GCE VM pricing, and this is discussed extensively in the prereqs for this particular course, which talks about Google Compute Engine solutions. If you're storing your data on cloud storage buckets as opposed to HDFS, the charges for this data are completely separate and are based on the kind of storage that you're using, whether it's a multi-regional bucket, a regional bucket, and so on. If you use other GCP services from Dataproc, you'll be billed for those separately, for example, Stackdriver monitoring for your Dataproc jobs. If your Dataproc jobs connect to BigQuery or BigTable, which are other cloud storage technologies on the Google Cloud, you'll be billed for those separately as well. And on this note, we come to the end of this introductory module on Cloud Dataproc. We studied how Hadoop works first, which is a ubiquitous big data processing technology. One drawback of Hadoop is that it tightly couples storage and compute. Data is stored on the Node on which it's processed. This can lead to utilization inefficiencies and cluster sizing inefficiencies because if you have to scale storage, you end up scaling compute as well, and vice versa. Managed Hadoop offerings on cloud platforms allow you to decouple storage from compute and, thus, allow you to independently scale either of these, and Dataproc is Google's managed cloud Hadoop offering. In addition, we saw that Dataproc has other features such as automatic cluster resizing, scheduled deletion of your cluster, which gives it powerful advantages over on-premises Hadoop. In the next module, we'll get really hands on. We'll create our Dataproc cluster and use it to run MapReduce jobs.

Running Hadoop MapReduce Jobs on Google Dataproc
Module Overview
Hi, and welcome to this module where we'll see how we can set up our first Dataproc cluster, and then use it to run Hadoop MapReduce jobs. We've already seen that Dataproc is Google's managed Hadoop offering on the cloud, which means that the MapReduce jobs that you have running on your on-premises cluster can be migrated to Dataproc on the cloud fairly seamlessly. In this module, we'll see how we can create a Dataproc cluster, and once the cluster has been created, we'll see how we can connect to it in a variety of ways. We'll use the web console, as well as work from the command line using the gcloud command line utility. We'll also see how we can use Stackdriver monitoring in order to monitor the jobs that we have running on our cluster including cluster metrics themselves. We'll also see how we can use initialization actions in order to configure the nodes on our cluster. Initialization actions on the GCP allow us to configure and initialize a software that we need to customize our VM.

Creating a Dataproc Cluster Using the Web Console
In this demo, we'll use the web console in order to create our very first Dataproc cluster, and then we'll see how we can log in to the master node using SSH. Log on to your GCP account, and we'll start off in the main console of our spikey-dataproc project. We are in the dashboard right now. Use the hamburger icon on the top left in order to view the products and features that GCP has to offer. You can scroll down, and under Big Data, you'll find the Dataproc option. GCP gives you the option to pin commonly used services to the very top of this menu. So you can click on the pin button and have Dataproc show up at the top of the list of services that you see here. Click on Clusters within Dataproc, and let's create our first cluster. Our Clusters page is completely empty here. We've enabled the Dataproc API, so we can click on the button here to create our cluster. If you haven't enabled the Dataproc API, you'll see an option here to enable the API first before you create your cluster. Here is the configuration page on the web console where you can set up what your cluster looks like. You can give your cluster a name. I'm going to call it spikey-cluster-one. Remember, you can create clusters on the fly for specific workloads. You ought to name your cluster based on the workload that you plan to run here. The next option that you see here asks you to specify a region for the nodes in your cluster. A region on the GCP refers to independent geographic areas, and these geographic areas are made up of zones. In addition to a region, you also have the option to specify specific zones where your cluster nodes will be located. A zone can be thought of as a single deployment area within a geographic region. A zone is a single failure domain on the GCP. Notice that the default option for region has been set to global. Global is a special multi-region endpoint. And when you use this endpoint for your cluster, your cluster resources can live in any user-specified zone, and your cluster can interact with these resources. This is a special multi-region namespace. You also have the option here to indicate that you want all your cluster resources to be within a specific region, and here are the various regions that are available in the US, in Europe, in Asia, and so on. When you choose a specific region from this list, you isolate your cluster resources to that region. This includes any VM instances that your cluster instantiates and any cloud storage buckets that you use. The advantage of isolating your cluster resources to a specific region is improved performance. Because all your nodes are located in the same region, they can communicate with each other with low latency. Once you've chosen a specific region, you can use this drop-down to choose a specific zone within the region. We'll have all our cluster nodes be in asia-east1-b, but our cloud storage buckets can live in any other region because of the global namespace. Below that, you have a drop-down for the different kinds of clusters that you can instantiate. The standard configuration is when we have 1 master and N workers. That's what we're going to choose here. You can have a single node cluster or configure your cluster for high availability with three masters as well. If you scroll down a bit, you'll see that you can choose the kind of VM instance you want to set up your master node. Here I've chosen 2 vCPUs. These are predefined machine types that you see here on this list. You can use the customize link to customize your machine type as well. You can then move on to configuring the size of the persistent disk that you want attached to your master node. I'm going to change the size to be 10 GB. I'm not planning to use it to store very much data. For your master node, you can choose to go with a standard persistent disk, or you can choose an SSD persistent disk. Solid-state drives are for when you want higher performance and lower latency for your reads. SSDs are, of course, more expensive than standard persistent disks. Once you're done with the master node, you can then go on and configure the kind of machine that you want for your worker nodes. I'm going to go with the less powerful, predefined machine type here with just 1 vCPU. I'll reduce the size of my persistent disk to 10 GB. Remember, persistent disks are pay as you allocate. So if you allocate a larger sized disk, you'll end up paying for it as well. It's preferable to go with the lower sized persistent disk in order to reduce your costs, especially if you plan to store your data on cloud storage buckets. We'll accept the default options for all of the other settings. We are creating a cluster with one master and two workers. Two worker nodes are the minimum requirement. Click on the Advanced options link that you see here onscreen, and let's explore what other options are available to us. You can see at the very top that you can add preemptible worker nodes using the UI when you create your cluster. We'll leave it 0 now. We'll see how to create a cluster with preemptible nodes later on using the command line. When you create a project on the GCP, a VPC is created for you by default, and it's preconfigured with some standard firewall rules. This VPC is referred to as the default network, and all of the cluster nodes that you instantiate will by default be on this network. The default network on the Google Cloud comes with a bunch of preconfigured firewall rules. These firewall rules enable VMs on the network to communicate with each other and allow only limited external traffic such as SSH connections to your VM. There are other options you can configure here as well. You can specify a cloud storage bucket that processes on Dataproc can use for staging. You can also use the web console to configure initialization actions that allow you to customize the software that you have installed on your cluster VMs. Let's hide these advanced options here by clicking on Less. And you can see here at the bottom that you have a command line option. If you click on the command line, this will give you the gcloud command that you can run from any terminal window in order to create this cluster that you've just configured. You can use this gcloud command in order to write a script that will create your cluster before you run your workload. If you don't want to use the gcloud command line utility, you have other options as well such as the REST API. You can use the REST API. You can see the POST request that you need to use for cluster creation right here on the screen. Close this window, and let's click on the Create button and create our very first Dataproc cluster. You might have to wait for a couple of minutes for all of the cluster nodes to be up and running. And here it is, our very first Dataproc cluster.

Using SSH to Connect to the Master Node
Now that we have our first Dataproc cluster, let's click through on our Cluster dashboard page and see what this cluster looks like. You can see a number of tabs on top here which gives you information on this cluster. The first tab is the Overview tab. This will give you graphical information on various cluster metrics. They haven't been turned on yet because you haven't done anything with your cluster. If you click on the Jobs tab, that will show you what jobs have been submitted to your cluster, what jobs are currently running. The Configuration tab will give you configuration details for the cluster that you just created, the number of master nodes, the worker nodes, the kind of machines that you set up for the master, as well as worker nodes, and other such information. You can see an interesting thing at the bottom here. A default cloud storage staging bucket has been created for you along with the cluster. This has been done by GCP under the hood. If you click on the VM Instances tab that you see here onscreen, this will give you information of all of the nodes that make up your cluster, the master node, as well as the worker nodes. You can see here that SSH has been enabled by default only for the master node. Your firewall rules have been configured such that you can only SSH into the master node, not into worker nodes. You can use the links on top to perform other administrative actions as well, such as deleting nodes from your cluster, viewing logs from your cluster, or refreshing this page to get the latest updates. There are different ways that you can use to SSH into the master node of your cluster. You can use the gcloud command line utility, or you can use another SSH client that you have installed on your machine. The simplest way is to open the SSH terminal window in a new browser window, and you can simply click on the SSH button here, and that will open up another terminal window on the browser. This is the master node of your Hadoop cluster, and a bunch of software that Hadoop requires to run has already been enabled. Java has already been installed, and you can see the current version of Java by running java --version. You'll see that jdk 1. 8 is powering this cluster. The master node also has Python installed. The current version of Python is Python 2. 7. In addition to Hadoop, your cluster also has Spark running. PySpark, Sparks Python shelf, is also available here. You can check the version of PySpark. You can see it's version 2. 2. 1. Your cluster also comes with Pig installed. You can write Pig Latin scripts to perform extract, transform, and load operations. If you are currently using Hive as your data warehouse on your on-premises setup, Hive is available here as well. You can see that Hive version 2. 1. 1 has already been preinstalled. The version of Hadoop on the GCP at the time of this recording is version 2. 8. 4. GCP is responsible for managing and updating the versions of all of the software that's installed on this cluster. Hadoop, Hive, Pig, Spark, all of these will be independently updated. The exit command will allow you to exit this SSH connection and go back to your cluster dashboard. Now let's go back to our main dashboard on the Google Cloud Platform. Under Resources, you can see that we have three compute engine instances running. These correspond to the three nodes of our cluster, and we have one cloud storage bucket. If you click on the three compute engine instances, you'll be taken to the VM instances dashboard. This will show you all of the VMs you have in this project. Right now we only have the cluster VMs corresponding to the nodes in our Dataproc cluster. Let's head back to our main project dashboard page. And under Resources, let's click on the cloud storage bucket. Remember, we didn't start off with a bucket, but once you go in here, you can see that Dataproc clusters automatically create a staging bucket that they can use. In addition, clusters also have installed a cloud storage connector, which is software which allows them to work with cloud storage buckets, connect to cloud storage buckets, get data from them, add data to them.

Creating a Firewall Rule to Enable Access to Dataproc
When you're working with Hadoop, you need access to YARN's resource manager UI, as well as the HDFS web UI in order to monitor your cluster. You need to configure firewall rules so that you can access information from your cluster, and that's what we'll do in this demo. Click on the hamburger icon on the top left to access all of GCP services. And under Networking, go to the VPC network option. Under here, let's go to Firewall rules. This is where we can create new firewall rules. Even though we haven't explicitly set up a single firewall rule for our project so far, you can see a number of rules are already available here. These are preconfigured firewall rules for the default VPC that has been automatically created. Notice that these rules allow SSH and Remote Desktop access to our VMs. By default, these rules also allow all VMs on the same network to communicate with each other. Click on the Create Firewall Rule on top in order to create a new firewall rule that will apply to our cluster. If you're interested in firewall rules on the GCP in general, here is a link that you can study. It's good to give your firewall rule a meaningful name so that you know what it represents. I'm going to call it default-allow-dataproc-access, allow us access to the Dataproc cluster. Give it a meaningful description if you want to. This rule that we're going to create applies to the default network because that's where our cluster nodes live. That's the only network available in our project at this point in time. Firewall rules can be used to allow or deny access to network resources. And every firewall rule is associated with a priority. Lower numbers indicate higher priority. I'm going to assign a priority of 1000 for this firewall rule. This gives me some room to play around. Higher priority rules will have lower values than 1000. Lower priority rules will have higher values than 1000. Firewall rules are explicitly configured applying to ingress or egress traffic. Ingress traffic applies to incoming traffic, traffic incoming to your cluster nodes. And egress traffic talks about traffic outgoing from your cluster nodes. We'll apply an ingress rule here to allow certain kinds of traffic to reach our cluster nodes. You then specify the kind of action that you want to apply to this ingress traffic. You can choose to allow specific kinds of traffic or deny specific kinds of traffic. I'm going to choose an allow rule here. I want to explicitly allow certain kinds of traffic. You can fine-tune your firewall rules further by specifying the kind of targets to which you want your firewall rules to apply. You can have your firewall rule apply to all instances that live on this network, to instances with specific tags, or to instances which use a specific service account. In order to keep things simple, we'll apply this firewall rule to all instances that are on this network. If you want this firewall rule to apply to only traffic from a specific source, that's possible as well. You can specify a source filter to make your rule more specific. In the case of egress rules, you can specify a corresponding destination filter as opposed to a source filter. For our ingress firewall rule, we can filter based on IP ranges, subnets, source tags, or service accounts from where the traffic originated. Here I want to be a little specific. I want to allow traffic only from my local machine to this cluster. So I'm going to filter based on IP ranges. In order to figure out what my IP is, I'm going to search for my IP on the browser. Here is my public IP address. Your public IP address will be a little different. Copy this IP address over, and we'll specify this in the IP range. I'll be able to connect to instances on this network only from my local machine, not from any other machine. You can choose an additional source filter if you want to. For the purposes of this demo, we'll just stick with one. I only want to enable traffic which follows certain protocols to certain ports on our cluster. I want to use TCP to connect to the resource manager UI on my Hadoop cluster, which is present on port 8088 and to the HDFS UI present on port 9870. We are now finally ready to create the firewall rule allowing us access to our cluster. Click on the Create button and wait for a couple of minutes for the firewall rule to be up.

Accessing the Resource Manager and Name Node UI
We now have the firewall rule configured that allows us access to the cluster nodes from our local machine. We are now on the Dataproc dashboard page. I'm going to click through to spikey-cluster-one, and I'm going to click on the VM Instances tab. You can see our one master and two worker nodes here. Click through to the master node, and you can see from the VM instance details page that we have access to the external IP of this master node VM. This is the IP address that we can use to connect to this VM instance from our local machine. So copy over this IP address, and let's open up a new browser window and specify the 8088 port on this VM instance. This is the port on the master node where the Hadoop resource manager is available. We are now going to access this web console from our local machine. And you can see this familiar web console onscreen. This is the same web console that you'd use with on-premises Hadoop. It's available for you on Dataproc as well. This is the same UI that you have on your on-premises cluster. If you click on the About link on the left, this will give you all of the information you need on the cluster. If you click on the Nodes link to the left here, you'll see that this cluster has two data nodes, exactly what we configured. Both of these nodes are currently in the running state, and you can see that the node addressed for each of these nodes is a GCP-specific one, one that GCP has assigned. In addition to the resource manager web console, our firewall rule was also configured to allow our local machine to access the NameNode web console available at port 9870. Accessing this URL brings up the familiar NameNode web console for Hadoop. Once again, this UI should be very familiar to you. You can scroll down here and get a bunch of different pieces of information. You can see that this cluster has two live nodes. You can click on the Datanodes tab on top in order to get information on your worker nodes. Having these familiar web consoles available to your Hadoop developers and administrators makes migration to cloud Dataproc easier.

Upload Data and MapReduce Code to Cloud Storage
With our cluster set up, let's see how we can run our first MapReduce on Dataproc. When working with on-premises Hadoop, one standard practice that organizations might follow would be to log in to the master node to kickstart their MR jobs or MapReduce jobs, and that's exactly what we'll do here. Later we'll see how we can use the gcloud command line utility to kickstart MapReduce jobs without logging in to the master node. When we migrate to the cloud, though, we want to store our data, as well as our scripts on cloud storage buckets. Go to the hamburger icon, and click on Storage and the Browser option within storage, and this will show you all of the buckets that we have created. This is the staging bucket that our cluster created. We're going to create a brand-new one where we'll store the scripts and the data for our MapReduce. I'm going to call this bucket spikey-data. It's a multi-regional bucket, and this is fine because our cluster is in the global namespace. It should be able to access data in a multi-regional bucket that's based in the US. Go ahead and click on the Create button, and create your spikey-data bucket. Within the spikey-data bucket, I'm going to create a folder where I'll store my MapReduce scripts. I'm going to call it mapr_scripts. This is what will hold our mapper and reduce our code, which we'll write in Python. Create another folder here called mapr_input. This is the folder where we'll store the input data that our MapReduce will act on. And here is the data that we're going to upload to this cloud storage e-commerce bucket. This is some sample data that we have from our Spiky Sales organization containing e-commerce transactions. Every record here has the following bits of information, the time at which a particular purchase was made, the location of the customer, what the customer bought, the product category, the price, and how the customer paid, Amex, Visa, or MasterCard. This is typical information for a sale day, and our MapReduce operation will calculate how much customers spent on each product. Here is the code for our mapper, which reads in one line at a time. It splits the data in that line, extracts that information, and outputs just the item and the price. Here is the code for the reducer, which is a script written once again in Python. We are tracking the total_price paid per item. The keys are the products or the items that the customer purchased. The output of the mapper is fed into the reducer, extracts the product and the price at which it was purchased, and then performs an aggregation. This is the sum aggregation to calculate how much money customers spent on a per-item basis. The exact structure and functionality of this code is beyond the scope of this particular course. You have the mapper and reducer Python files available for download, though, if you want to try them out. Let's switch over to our GCP web console and go to the mapr_input folder within our spikey-data bucket. We'll upload the input data set, the e-commerce data set that we saw earlier. Our sample sale data for a particular day is available in itemData. txt, and I'm going to go ahead and upload this. Once the data has been uploaded, it should be available in your cloud storage bucket and should be accessible by a MapReduce operation running on your cluster. Go back one level to the spikey-data bucket, and within here, let's go to mapr_scripts. Here is where we'll upload the Python code that runs our mapper, as well as our reducer. Click on Upload files, choose mapper. py and reducer. py, and upload them. We now have our MapReduce code written in Python, as well as our input data available in cloud storage buckets. And that's where we're going to access them when we run our MapReduce on our Dataproc cluster. Observe that we follow Dataproc best practices. We've separated storage from compute. We're storing our data on cloud storage buckets. We're going to run our processes on our Dataproc cluster.

Running MapReduce on Dataproc
We're now ready to run MapReduce on our spikey-cluster-one. Start off in the Dataproc dashboard, click through to spikey-cluster-1, and click on VM Instances. We'll SSH into the master node in order to run MapReduce. When you're using Dataproc on the cloud, HDFS is available to you on all of the cluster nodes. If you run the hadoop fs command, you'll see that HDFS is available, and you'll see the directories that are available in HDFS. You can choose to use HDFS to store your data and to store your scripts as well, but that is tightly coupling your storage with compute. We'll store our data on the cloud. We'll use our cluster only for compute. And here is the Hadoop command that you would run to kickstart your MapReduce, the Hadoop JAR. We'll reference Hadoop's streaming JAR that comes preinstalled on your master node. This is the Java archive that will kickstart the MapReduce operation on your cluster. Now you need to specify where your files are located. Use the --files command line argument to specify where your mapper and reducer scripts live. You can point to a cloud storage bucket. There is no additional configuration required for Hadoop to work with cloud storage. The cloud storage connector comes preinstalled with your Dataproc cluster. We've written our mapper and reducer code in Python. Use the --mapper and --reducer command line options to specify the command to kickstart your mapper and reducer processes. These are both Python processes. We use the Python shell command for this. The --input command line argument points to where our input data is located on which we want the MapReduce operation to run. You can point to a cloud storage bucket using gs://. Specifying data on cloud storage buckets as opposed to HDFS should simply involve changing your hdfs:// to gs://. We also want the output of our MapReduce operation to be stored on cloud storage buckets rather than on HDFS. You can simply point to the folder where you want the output to be placed. Mapr_output doesn't exist yet. Dataproc will automatically create this folder on cloud storage. Go ahead and hit Enter, and wait for your MapReduce process to complete. You'll see that the mapper process runs first. You can see the progress of your operation. Based on the logs that you see on your screen, map has gone up to 87%. Now mapper has reached 100%, the reduce operation has started and completed. And you can see the final result in your cloud storage bucket. Let's switch back to our GCP web console. Click on the hamburger icon, choose Storage and Browser. Here is where all of our buckets are listed. Let's click through to the spikey-data bucket. Here is the mapr_output folder. This did not exist before. This was created by Hadoop when it ran our MapReduce job. And within this folder here are the results of our MapReduce operation that completed successfully as you can see from the _SUCCESS file that was placed here in this cloud storage folder. Let's switch back to where we SSHed into the master node of our cluster. You'll see here that we can use the hdfs dfs command with cloud storage buckets exactly as we would with HDFS. We use the -cat command here to list the contents of our MapReduce output. This is the first file that MapReduce outputted, and here are the contents. You can see the products and how much customers spent on those products for that sale day. You can see from this example that the change to managed Hadoop on the cloud is not intrusive at all. If you've been using on-premises Hadoop, you can move to cloud storage buckets and work with them in exactly the same way as you did with HDFS.

Running MapReduce Using the gcloud Command Line Utility
If you're using Dataproc on the GCP, you won't typically log in to the master node in order to kickstart your MapReduce jobs. Instead you'll use the gcloud command line utility that allows you to do this from any terminal window that can connect to your Dataproc cluster. Click on the hamburger icon on the top left, and go to the navigation menu that shows you all of the services. Let's go to Dataproc, and go to the Jobs page that will allow us to monitor the jobs that we submit to our cluster. Once we're in this page, activate the cloud shell. We'll use this as our terminal window in order to use the gcloud command line utility. In order to use gcloud, you need to make sure that you're authenticated. And you can do this by a gcloud auth login. Now if you're working on the cloud shell, you don't really need this step. But if you're working with gcloud on your local machine, you need to authenticate yourself. So I'm going to go ahead and click on Yes to continue. This will pop up a link. Click on this link to get the authentication token. You need to sign in with your GCP account. Yes, indeed, I do want gcloud to perform all of these actions. I'm going to click on Allow, and this will generate a token that I need to copy/paste into my cloud shell window. If you now click on Enter, gcloud has now authenticated you as spikeysales@loonycorn. com. We're ready to get started. Using the gcloud command line utility, I'm going to set my current project as spikey-dataproc. This is the project that I want my current cloud shell session to use. All Dataproc jobs can be submitted using the gcloud command line. Note that we are working from any terminal window. We haven't logged into our master node. Gcloud dataproc jobs submit hadoop will allow us to submit Hadoop MapReduce jobs. Specify the cluster where you want to submit this job, that is spikey-cluster-one, our only cluster at this point in time. For this demo, we'll run one of the standard Java examples that are present when your cluster is set up. This JAR file is located on the master node of the Dataproc cluster that we set up and can be accessed locally by our MapReduce job. This JAR contains the word count MapReduce that comes preinstalled with your Dataproc cluster. You can point to any other JAR which has your MapReduce code. You can point to JAR files that you uploaded to cloud storage as well using gs://. JAR files can also live on HDFS. We want to run the word count MapReduce from within this JAR FILE, and we want to count the number of words in itemDated. txt that is located in our cloud storage bucket. We want the output of this MapReduce operation to be stored to our spikey-data bucket in mapr_java_output. Our MapReduce job has been successfully submitted to the Dataproc cluster. It's running here as you can see from the log output. If you click on Refresh within your Jobs page, you'll be able to see the current status of your job here. This job might take a minute or two to run. Instead of viewing logs on the console window, you can click through to the job here on this Jobs page, and you can see logs right here on your web browser. Just wait a couple of minutes more, and you'll see that your job will complete, job output is now complete. Let's go back to our main Jobs page. You can see from the green checkmark that the job has completed. Let's go to our storage bucket to view the output. Within the spikey-data bucket, we have the mapr_java_output folder that has been created to store the final results of our word count operation. You can see that our MapReduce completed successfully. On your cloud shell terminal window, you can use the gsutil command to view the contents of this file. Gsutil is a command line utility that you can use to work with cloud storage buckets. Here we are going to view the output of our MapReduce here, and you can see the word counts associated with the words in that itemData. txt. If you're scripting your MapReduce operations, prefer to use the gcloud command line. It's simple and easy to use.

Creating a Cluster with Preemptible Instances Using gcloud
If you want to add additional compute capacity to your Dataproc cluster, and you want to do so cheaply, the easiest way to do this is using preemptible VMs. In this demo, we'll see how we can use the gcloud command line utility in order to create a Dataproc cluster, and we'll have this Dataproc cluster have preemptible instances. We are on our main project page, and we have cloud shell activated and have set a current project to be spikey-dataproc. You can use gcloud dataproc clusters create spikey-cluster-two to create a brand-new cluster named spikey-cluster-two. The num-preemptible-workers flag allows you to indicate how many preemptible instances you want for this cluster. We'll choose just one here. We want this cluster to be created in asia-southeast1-a. We'll use the n1-standard-1 predefined machine type for our master node, and our master-boot-disk is 10 GB in size. We'll have two permanent worker nodes in addition to our preemptible worker node, and the machine type here is once again n1-standard-1 for these worker nodes. And, finally, the worker-boot-disk size is also 10 GB. Hit Enter and wait for a minute or so for our cluster nodes to come up. We'll go to the navigation menu and go to Dataproc to view our clusters. We already have spikey-cluster-one, and here is spikey-cluster-two being created. Click through to the spikey-cluster-two Dataproc cluster, and here on the Configuration page, you can see the current configuration of your cluster. You can see that the preemptible worker nodes are equal to 1. And if you click on the VM Instances tab, you can see that along with the one master and the two permanent workers, we also have a preemptible worker. And our preemptible worker has already been preempted. You can see that it's currently in the stopped state. Preemptible workers are lower cost VM instances, but they can be reclaimed at any point in time by GCP, which is why you never store data in preemptible workers, you only use it for compute. Let's restore our cloud shell window here, and we'll see how we can use the gcloud command line to edit the configuration settings of our cluster. Here I'm going to set the number of preemptible workers to 0. I'm going to get rid of the one preemptible worker that our cluster has. On our Cluster details page, our UI hasn't yet updated. If you click on the Refresh button, you'll find that our preemptible instance has disappeared. We only have the one master and two workers now. I can also use the gcloud dataproc command in order to delete this entire cluster. I'm going to delete spikey-cluster-two. It shows me a warning. Yes, I do want to go ahead and delete it. Click on Yes here, and then your cluster will be deleted. If you hit Refresh on your Dataproc Clusters page, we'll find that we are left with just the one cluster now, spikey-cluster-one. Cluster two has been deleted.

Monitoring Clusters Using Stackdriver
We have our Dataproc cluster, and we know how to run MapReduce jobs on this cluster. Let's see how we can monitor the cluster and its metrics using Stackdriver. Stackdriver monitoring collects metrics, events, and other metadata from your cloud platform resources and allows you to view this information graphically and configure alerts based on predefined thresholds. In your navigation menu, go to Monitoring under Stackdriver. Wait for Stackdriver to instantiate for your current project. Choose the project, here I'm choosing spikey-dataproc, and create a new Stackdriver account. This is needed if you're using Stackdriver for the very first time on this project. My current project here, spikey-dataproc, has been selected. I'm going to click on Continue and move on to the next step. Stackdriver monitoring also works with AWS resources, so you can add an AWS account and have Stackdriver monitor that as well. I'm going to go ahead and skip AWS and move on to the next step. If you want to collect additional information in the form of events and metrics from your VM instances that you are to monitor, then you can install Stackdriver agents on your VMs. We're going to skip this step as of now. I'm going to hit Continue and move on. I'm not really interested in Stackdriver reports at this point in time. I'm going to hit Continue and move on. Wait for a bit while Stackdriver configures monitoring for your project. You can now click on the Launch monitoring button. This takes us to the main monitoring dashboard for our spikey-dataproc project. You can explore this dashboard and see what all is possible to monitor using Stackdriver. For our case, though, we're going to create a new dashboard to monitor our Dataproc cluster. Our dashboard is empty at this point in time. Click on the Add Chart button to the top right to add a new chart that you'll used to monitor your cluster. We'll call this the Log_Chart. We'll use it to track logging information. You can choose the resource type and the metric you want to track for that resource. I'm going to choose Cloud Dataproc Cluster here. There are a variety of other options available. You can use Stackdriver to track metrics on your storage buckets, VM instances, any GCP resource. Once you've chosen the Dataproc cluster as an option, here are all of the metrics that you can track within your cluster, the HDFS capacity, the DataNodes, storage utilization, log bytes, log entries. There are a whole host of metrics available here. For our demo here, we'll choose to track the number of user-defined log entries. So that's our option. Once you've made your choice, you can see that the chart for this is immediately available for preview on the right. In addition, I'm going to now filter this by cluster name. I only want to track my spikey-cluster-one, not other clusters that I might have. Click on Apply, and notice how the graph on the right changes now to track just spikey-cluster-one. There are other options available here for grouping, as well as aggregation. I'm just going to go with aggregation none for now. I want to count the number of log entries, I don't want other aggregations to be performed. Go ahead and hit Save. And here is our first Log_Chart. The spike in log counts that you see at the center here corresponds to when we ran our MapReduce jobs. You can get additional details on the log count if you click on the icon here at the top right. And here are the individual log counts for info logs, name node logs, and so on. You can add multiple charts to your dashboard to track all of the metrics that you find interesting. I'm going to now track some HDFS-specific metrics. The HDFS_Chart is going to track the Cloud Dataproc Cluster as before. And this time I'm going to track HDFS storage utilization. We'll track HDFS storage utilization for spikey-cluster-one. We want to ensure that our developers don't use HDFS beyond a point. They should be using cloud storage. This time around, we'll perform an aggregation. We want to track average storage utilization. We'll use the mean aggregation. Go ahead and click on Save, and we have our second chart. Clicking on the details icon at the top right of this chart will show us that our storage_utilization is very low. So that's good. Once you have your custom dashboard, you can rename this dashboard to be something meaningful, Spikey-Dataproc Dashboard. And you can view information for different periods of time, for example, for 6 hours, 1 day, 1-hour periods, and so on. Stackdriver monitoring makes a host of cluster metrics available to you out of the box.

Stackdriver Monitoring Groups and Alerting Policies
Stackdriver monitoring has a feature called groups that allows you to monitor a set of resources as a single entity. In this demo, we'll see how we can set up a monitoring group to monitor our cluster resources. We want to be able to collect additional metrics and events from our cluster nodes. And for this, we'll set up a Stackdriver monitoring agent using initialization actions on our nodes. We start off in the main Monitoring Overview page. This shows us our Spikey-Dataproc Dashboard that we set up earlier. On the left navigation pane, click on the Groups link, which will take us to monitoring groups. Here is where we'll set up a new group. Click on the Create Group option. The cool thing about using monitoring groups to monitor a group of resources is the fact that this group is dynamic. You can set up membership criteria so that you know which resources will belong to this group at any point in time. Based on whether they meet this membership criteria, resources can fall in and out of this particular group. Once I get to the Create group page, go ahead and give this group a name. I'm going to call it the spikey-clustergroup. And you can specify the criteria to determine membership within this group. The default filter condition allows us to monitor any resource that matches any of the rules that we're going to set up here. Here our very first rule is that the name contains spikey-cluster-two. You can modify this rule. Instead of Contains, you can have other options configured as well, such as Does Not Contain, Equals, Starts With, Ends With, and so on. There's a special checkbox here to allow you to customize your monitoring when you're dealing with nodes that belong to the same cluster. If there is one or more nodes in this cluster which differ significantly in performance from others, that will be flagged. Go ahead and save this group. We've created a new monitoring group. A very useful feature of monitoring is the setting up of alerting policies. Alerting policies can highlight significant changes in your resource performance. We are going to create a new alerting policy, which will be triggered under specific conditions. Click on the Add Condition button here, and this will bring up a page that will allow you to configure the condition under which your alert will be triggered. There are two broad categories based on which your alert can be triggered. Either a metric has crossed a particular threshold, or a metric is entirely absent. We're going to choose the Metric Threshold alerting policy for now. Go ahead and click on the Select button here. The first configuration setting is to specify the target of this alerting policy, which is the resource that you want to monitor. There are a number of resources you can choose from. We want to specifically monitor the consumed API, how many API calls are made from our Dataproc cluster. If the number of requests made by the processes running on our Dataproc cluster exceed a certain threshold within a certain time period, we want our engineering team to be alerted. So go ahead and choose Consumed API. Now let's set up our metric. If a metric goes beyond a certain condition here, if request count goes beyond a certain condition here, we'll choose the condition to be above. And here, the threshold we set to 1000 requests. So if the number of requests made by the processes on our cluster go beyond 1000 requests per minute, then we want to be alerted. Accept the default values for all of the other configuration settings. If we go down to the PROJECT_ID field, we want to explicitly monitor resources in the spikey-dataproc project, and that's what we're going to choose here. We can also specify the service where we want to track the consumed APIs. We want to track APIs at dataproc. googleapis. com. That is our choice here. Once you've set up your configuration, you'll notice that a visualization tracking this metric pops up here at the bottom. Go ahead and click on Save Condition, and let's go on with setting up our alerting policy. The next step is to specify how we want to be alerted when this threshold is exceeded. Under Notifications, go ahead and click on the + button here that will allow us to add a method of notification. We want to be notified by email to spikeysales@loonycorn. com. And, finally, we'll give this alerting policy a meaningful name so that we can identify what exactly it tracks. This is the APIConsumption_AlertPolicy. Click on the Save Policy button. We've created an alerting policy for our cluster group. You can always click on the Groups link on your left navigation pane in order to get quick access to your monitoring groups. Under Groups, click on Groups Overview, and this will take you to a dashboard that will list all of the monitoring groups that you have. Here is our spikey-clustergroup. Click through to this group, and you'll notice something. There are no matching criteria, no events are being monitored. This is because when we set up this monitoring group, our filtering criteria basically said we want to match those resources which have the name spikey-cluster-two. We just have spikey-cluster-one. There is no resource which matches the membership criteria for this group. There is nothing to monitor here. Let's fix that in the next clip.

Configuring Initialization Actions for Dataproc
In this clip, we're going to create a cluster called spikey-cluster-two, which we'll then monitor using Stackdriver. For every node in spikey-cluster-two, we want to customize the software that is installed in the VMs that make up this cluster so that the Stackdriver monitoring agent is installed. The monitoring agent can be installed using commands in a shell script, and we'll have this shell script run as an initialization action. So whenever a VM on this cluster comes up, this initialization action will be executed. Here are the shell commands that we'll use to install the Stackdriver monitoring agent. This can be for any VM instance. We'll apply it to the VM instances that make up our cluster. This command uses the curl utility that all our VMs come preinstalled with to download the monitoring agent and then install the monitoring agent on the VM. These shell commands are present in the stackdriver. sh file. We'll now switch to our GCP web console. We start off in the Dataproc Clusters page. We have the one cluster here, spikey-cluster-one. Click on the Activate Cloud Shell icon to the top right so that we can work with the terminal window. We set the current project on this cloud shell session to be spikey-dataproc. We'll use the gcloud command line utility to create a new Dataproc cluster. We'll call this cluster spikey-cluster-two. Remember, this cluster will match the membership criteria of our monitoring group. This cluster will be created within the spikey-dataproc project. That's where we're doing all of our work. And we have specified initialization-actions for all of the VMs on this cluster. We want to customize those VMs to install certain software. The GCP has set up a public cloud storage bucket with commonly used initialization actions, and installing the monitoring agent is one such script file. We'll access this script file directly from the dataproc-initialization-actions bucket. We need to enable certain OAuth scopes on the cluster machines so that Stackdriver monitoring allows publishing metric data to the GCP projects. The scope that we want here is the monitoring. write scope that will enable our monitoring agent to write data to our GCP project. The other configuration parameters that you see here relate to the cluster creation itself. And you are already familiar with these. Hit Enter, and go ahead and create this cluster in the zone of your choice. I'm going to create this cluster in the asia-southeast1-a zone. Creating this cluster might take a minute or so. Observe the progress of your cluster creation on the UI, and wait for your cluster to be up and running. We now have spikey-cluster-two. Now let's go back to our Stackdriver monitoring page. And if you hit Refresh on this spikey-clustergroup, you'll see that we now have monitoring enabled for all of our nodes in spikey-cluster-two. The monitoring agent that we set up on the cluster thanks to our initialization action made these metrics available to Stackdriver. You can click through to individual cluster nodes from the Stackdriver monitoring group UI, and you can view the API requests made by this particular node to your Google Dataproc APIs. These metrics are only available because we have the monitoring agent installed via our initialization action. And with this, we come to the very end of this hands-on module. We saw how we could work with managed Hadoop on the Google cloud using Dataproc clusters. We started off by creating a cluster using the web console. We also saw how we could create and manage clusters using the gcloud command line utility. We saw how we could submit MapReduce jobs to the cluster by SSHing into the master node, as well as using the gcloud command line. We connected to cloud storage buckets to access data, and we worked with preemptible workers. In addition, we learned how we can use Stackdriver monitoring in order to monitor our cluster resources. We also saw how we could customize our Dataproc cluster nodes with specific software using initialization actions. In the next module, we'll see how we can create and run Spark applications on our Dataproc cluster.

Working with Apache Spark on Google Dataproc
Module Overview
Hi, and welcome to this module where we'll see how we can work with Apache Spark on Google's Dataproc cluster. Apache Spark is a great general purpose computing engine that is built on top of basic Hadoop framework. Apache Spark uses HDFS, YARN under the hood. It does not use MapReduce, though it has its own analytics computation engine that allows for big data processing. Apache Spark is simple and easy to use. It's very intuitive because it completely abstracts away the concept of multiple machines on the cluster from the developer, unlike MapReduce operations. Dataproc on the Google Cloud supports Spark using the YARN cluster manager. Spark applications that you might have been running on your on-premises datacenter can be lifted and shifted to the cloud very easily. There is built-in support for PySpark, that is a Python shell for Spark, as well as Spark Scala. In addition, GCP provides connectors so you can connect to BigQuery, Cloud Storage, and other GCP technologies in order to access your data from within Spark applications.

Spark for Distributed Processing
Before we move on to using Spark on Dataproc, let's briefly talk about Spark as a data analytics engine and understand why it's so popular. Spark is one of a suite of technologies that is built on top of the Hadoop ecosystem, but it's by far the most popular and very widely used. One important reason for this is the fact that Spark makes it very easy to develop and prototype big data applications on Hadoop. This is a distributed computing engine that is built on top of Hadoop, but it has an interactive shell that allows you to quickly process datasets. You can develop your big data processing applications in a programming language of your choice using something familiar such as the Jupyter iPython notebook for Python code, as well as Scala. In addition to a distributed processing framework, Spark offers a number of built-in libraries in different languages for very common use cases, such as machine learning, processing streaming data, graph processing, and so on. These built-in libraries make it very easy to get up and running with your application. We've already spoken of the three main components that make up the Hadoop distributed processing framework. We have HDFS, which is a distributed file system to manage data storage. We have MapReduce, which is the framework to define a data processing task. And we have YARN, which is a cluster manager or a framework, which executes the data processing task that we've defined. Now Apache Spark is built on top of YARN and HDFS. The Spark Core engine can be thought of as a replacement for MapReduce. Spark Core is what takes care of general purpose computing on a distributed platform. Spark Code can use YARN as its cluster manager to allocate resources across the different processing tasks. It can use other cluster managers as well, such as Mesos or Spark Standalone, which runs on a single node. When you run Spark on top of Hadoop, it continues to use HDFS in order to store data in a distributed manner. Where Spark really differentiates itself and wins over basic Hadoop, though, is in its libraries. There are libraries to perform SQL transformations on your data. There are streaming libraries, machine learning libraries, and libraries for graph manipulation, for social network graphs for example. If you talk to a developer or data scientist today, chances are that they use Spark for all of their big data processing requirements. So what does Spark have to offer? Spark gives us real-time, as well as batch processing of data. Hadoop is mainly for batch processing. Spark also gives us an interactive read/evaluation/print/loop environment, a REPL environment as it's called. This environment gives us fast feedback allowing us to build prototypes very quickly. And, lastly, Spark supports a variety of common programming languages. Python, Java, Scala, and R are among the few. Hadoop MapReduce operations always act as data in the form of files. These files can be stored on cloud storage in the case of Dataproc, or they'll be on HDFS. All operations in Spark are performed not on files but on in-memory objects called RDDs or resilient distributed datasets. Working on in-memory objects is what allows Spark to be so performant and efficient. An RDD's not a single object. Instead, it can be thought of as a collection of entities. These entities can be thought of as rows and records of the RDD, and these rows and records are partitioned and spread across multiple machines on your cluster. All of these are in the memory of your cluster machines, not on disk. One of the most important characteristics that you have to remember about RDDs is the fact that it's partitioned or split across data nodes in the cluster. This is what allows for parallel processing on multiple nodes. RDDs are immutable. Once created, they cannot be changed. You can only apply transformations to an RDD and create a new RDD with new information. And, finally, RDDs are resilient. That's why they are called resilient distributed dataset. If the node on which a portion of our RDD lives crashes, the RDD can be reconstructed from scratch. It has a lineage; it knows where it came from.

Running a Spark Scala Job Using the Web Console
In this demo, we'll see how we can submit a Spark application to the Dataproc cluster using the web console. This Spark application will be built using Scala. We start off in the Dataproc cluster state, and here is the spikey-cluster-one that we created earlier. Click through, and let's take a look at the VM instances in the cluster. What we want to do here is to SSH into our master node and create a new Scala Spark application. A master node comes preinstalled with Scala. If you type Scala on our terminal window here, we'll get into the Scala interactive shell. Let's define a very simple Hello, world! Scala object here, which prints Hello, world! to screen. This will be our Scala application. Let's save this object in the Scala file. We'll call it HelloWorld. scala, and then go ahead and quit this Scala interactive shell by hitting :q. Compile the Scala file by calling scalac HelloWorld. scala, and if you run an ls command here, you'll see that there are. class files available here in the current working directory. We know that Java is already installed on our master node here. Let's use the jar command in order to create HelloWorld. jar. Once our Java archive has been created, we can now copy this JAR file over to our spikey-data bucket. We copy the JAR file over using the gsutil cp command. We can now head back over to our GCP web console, use the hamburger icon, go to the navigation menu, and let's take a look at our storage buckets. If you look within the spikey-data bucket that we have here, you can see that HelloWorld. jar is present. Let's submit this Spark Scala application to our Dataproc cluster. Go to Dataproc, Jobs, and here you'll find a link to submit a job from the web console. Click on Submit job, and it'll take you to a place that will allow you to configure your job settings. Specify a unique ID for your job. We are going to use the global endpoint, and we are going to submit this job to spikey-cluster-one. There is a drop-down here that allows you to specify the kind of job you want to run on your cluster. You can choose from Hadoop jobs, Spark jobs, PySpark jobs, and so on. For a Scala Spark application, the job type should be Spark. For Spark job, you need to specify the JAR file or the main class where your Spark application can be invoked. Point to the HelloWorld. jar that's present in your spikey-data bucket. If you have other command line arguments that you need to pass into your JAR file, you can specify those here as well. We have nothing, so we can leave all of these fields empty. Scroll down and click on the Submit button, and you will have successfully submitted your Scala Spark application to your Dataproc cluster. If you go over to the page that lists all of your jobs, you can see that your sparkScala job is just starting up. You can monitor the status of your job here. If you're interested in viewing logs, you can just click through to the job, and you'll see your log files right there. There's just one log file entry here. Hello, world! has been printed to screen. A job output is complete.

Executing a Spark Application Using gcloud
In this demo, let's see how we can run a simple Spark application on the Dataproc cluster. We'll submit this job to the cluster using the gcloud command line utility. We start off in the spikey-dataproc project, and we are in the Dataproc Clusters page, which lists all of the clusters that we have up and running. We just have the single cluster, spikey-cluster-one, at this point in time. We've deleted any additional clusters that we had created earlier. We'll run a Spark Python application, which is available as a part of the examples that GCP provides to us in order to test out Dataproc. The Python code for this Spark application is available in the Dataproc staging bucket that was created when we set up our spikey-cluster-one. I'm going to use gsutil here and list the contents of this bucket. And we'll find the right Python script that we want to run. Within the source folder here on our staging bucket, there is a folder named PySpark, which contains a few Python examples. The specific example that we're looking for is the hello-world. py. This is the Spark application that we'll execute on our Dataproc cluster. As its name implies, this is a very simple Hello, world! Spark application. I'm going to call gsutil cat in order to list out the contents of this Python file. You can see the Python code right here onscreen. We import the PySpark library, set up a SparkContext, parallelize a new RDD with the words hello and world, and then print it out to screen. You can use the gcloud dataproc jobs command in order to submit a PySpark job. We want to submit PySpark, and we want to submit this job to the one cluster that we have up and running, that is spikey-cluster-one. You'll also need to specify the Spark Python program that we want to execute on our Dataproc cluster. We simply point to hello-world. py on our staging bucket. When you're executing this code on your local machine, remember that your staging bucket will have a slightly different name. You need to copy over the right name for this to work. Our job has been successfully submitted to the Dataproc cluster. We can head over to the web console and click on the Jobs link to see the status of its progress. You can see onscreen here that our PySpark job is currently running. It has been running for about 15 seconds. Wait for a little bit for this application to run through to completion. You can see here onscreen Hello and world! have been printed out. Our Spark job has executed successfully.

Creating a BigQuery Table
In this demo, we'll build our own Python Spark application, which shows how we can connect to other GCP services in order to access data. We'll connect to BigQuery, as well as cloud storage from within Spark. We'll start off by creating a new BigQuery table where we'll store our input data that we want to process using a Spark application. BigQuery is Google's data warehouse, and it's comparable to Redshift on Amazon. One big advantage of BigQuery, though, is the fact that it's serverless. You don't have to instantiate machines that will hold your data. Your data is always available to you in a serverless manner. You only pay for data storage only for what you use and for processing. Clicking on the BigQuery link on your navigation pane will open up the BigQuery UI. We are within the spikey-dataproc project. All tables in BigQuery live within a dataset. A dataset can contain one or more tables. Let's create a new dataset where we'll set up our table. Click on the Create dataset button here, and specify the spikeysales_dataset. You have the option to choose where you want your data to be located in case you have jurisdiction issues. The options are United States, European Union, and Tokyo. We'll go with the default option here, which should be the US. Go ahead and click on the Create dataset button. That'll create a new BigQuery dataset for you. Click on the Go to dataset link here, and this will show you our spikeysales_dataset that we just created. We'll now create a table within this dataset, and our data will be stored within this BigQuery table. Click on the Create table button, and this will open up a UI allowing you to create your first BigQuery table. You can specify where the data in this table comes from. You can choose to leave the table empty, but we're going to upload a CSV file with our data. So I'm going to choose the Upload option here on this menu. I have some sales data available here on my local machine. Click on the Browse button. This will allows you to browse your computer. And I'm going to choose salesdata. csv. This contains CSV records for the sale of various products on the Spiky Sales site. You can indicate to BigQuery the format of your data. It's in the CSV format. I'm going to choose the CSV option from this menu. This table is in the right project and the right dataset. I'm going to now name this table. It'll be called the salesdata table. You can click on this checkbox here that allows BigQuery to auto detect the schema for your table. It will set up this schema based on the records in your CSV file. Go ahead and click on the Create table button. Wait for a little bit for your data to be uploaded to your newly created table. This is in the form of a job, so if you click on spikeysales_dataset, you'll find that you now have this salesdata table within it. Observe that the column names from our CSV file have been picked up by BigQuery. That is the schema for our table. There are five columns here, time, location, item, cost, and mode. This is the same sale-specific information that we've seen earlier but now in a CSV format. You can preview the table as well, and here is a little preview of the data that stored in this BigQuery table. The Spiky Sales organization is seriously considering moving its analytics data to BigQuery, and having Spark applications process this data easily is an important factor in their decision-making.

Pyspark Application Using BiqQuery and Cloud Storage Connectors
And here is the Python code for our Spark application. This Spark application will connect to BigQuery, read in sales data, perform some kind of aggregation operation on it, and write the results out to BigQuery. In order to work with BigQuery tables, we'll use a BigQuery connector. The Spark application will take the data from BigQuery, place it in a cloud storage bucket, perform its operations, and write the final results from cloud storage back to BigQuery. In this way, we'll see how Spark integrates both with BigQuery, as well as cloud storage. We start off by instantiating a SparkContext, which is our bridge to the Spark environment. We need to point Spark to the bucket created by our Hadoop cluster, the default staging bucket. The name of the staging bucket is available as a property in the hadoopConfiguration file, the fs. gs. system. bucket, and that's the property that we are accessing here. We then access the current project ID to which the cluster belongs. That is our spikey-dataproc project. This is also present in the hadoopConfiguration file of our Dataproc cluster. In order to use the BigQuery connector from Spark, you need to specify a bunch of information to BigQuery, which you do via a config Python dictionary. These are BigQuery-specific configuration settings, all of the information that BigQuery needs to connect to the right table within the right dataset in the right project. We connect to BigQuery within our spikey-dataproc project, specify the staging bucket and the input_directory within the staging bucket where data will be temporarily stored. When we read in data from BigQuery, we'll temporarily store data in the staging bucket. And here is information for the BigQuery connector on where the input data is present in the salesdata table within spikeysales_dataset. The output of our Spark application will be written out to the spikeysales_dataset in the totalcost_output table. You can use Spark's newAPIHadoopRDD function in order to create an RDD from data that's present in BigQuery. Just specify the right input arguments as you see here onscreen. We specify that the BigQuery input is in a JsonTextBigQueryInputFormat. This is a format that our BigQuery connector on Spark Dataproc understands. BigQuery is the input format. Here are classes which represent the output key-value pairs. Keys are LongWritables, values are JsonObjects. Make sure you specify a config object that you set up earlier for your BigQuery connection. And here are some simple Spark transformations on our input data. This can be anything based on your use case. We are calculating here the sales per location. We'll write the output temporarily to cloud storage before we move it from cloud storage to BigQuery. The output generated by Spark will be written to our temporary staging bucket within the pyspark_output folder within this temporary staging bucket. We'll use the sql_context in Spark to generate output in a JSON format, which we'll then write out to our cloud storage bucket. We can then execute a shell script on our cluster to spawn off a process on the command line. We'll load the JSON result from cloud storage buckets into a BigQuery table. Bq load is the BigQuery command that will run on the command line. Bq is a built-in command line utility available as a part of the gcloud SDK. The bq load command takes the source data, which is in a JSON format in our cloud storage bucket, and places it into a BigQuery table. This is the output table that we have specified earlier. The output_files will be placed within our Dataproc staging bucket in the pyspark_output folder. And, finally, once our data has been safely stored within a BigQuery table, we can cleanup all of the cloud storage folders that we used, all of the extra folders that were generated in our staging bucket.

Executing a Spark Application to Get Results in BigQuery
We have our input data in BigQuery. We have our PySpark code. We are now ready to submit this Spark job to our cluster. Activate the Cloud Shell, and launch the code editor beta. This is what we'll use to edit configuration files and other little bits of code from within the browser. Here is code editor beta. It's a nifty new feature that you should definitely try out. I'm going to upload a new file by clicking on the three-dot menu on the top right. This time, we'll store the Python program which contains our Spark application on the local machine on our Cloud Shell VM. So I'm going to upload totalcost. py, which is on my local machine here, to the Cloud Shell VM. This code editor points to the home directory on my Cloud Shell. If you click on File and then Refresh, you'll see that totalcost. py is present here. You can click through to this file and see that your Spark application has been successfully uploaded to the home directory in your Cloud Shell. You can now run this application on your Dataproc cluster using a gcloud command. We'll first set the current project on our Cloud Shell session to be spikey-dataproc, and we'll call gcloud dataproc jobs submit to submit our PySpark job. Point to totalcost. py that is located in this current folder, that is our home directory, and the cluster to which we want to submit this job is spikey-cluster-one. Once the job has been submitted, we can use the navigation menu to go to Dataproc, Jobs in order to monitor the status of its progress. You can see from this web console here that our job is currently running and has been running for about 36 seconds. If you click on the running job, you'll be able to monitor logs that it's generated from your Spark application. Here are the logs from our BigQuery PySpark application. If you switch back to Cloud Shell, you'll be able to monitor the job from the command line as well. You can see some of the output that we've printed out from within our Spark application right here on screen. Wait for a little bit for the job to complete execution. You can switch over to the Jobs dashboard and see that the job has run through successfully as you can see from the green checkmark here. The results of our Spark processing application should now be available in BigQuery. Click on BigQuery from your navigation menu, and let's see if we can query the results. Under the spikey-dataproc project, we have the spikeysales_dataset and the salesdata table. The output table doesn't seem to be available yet. I'm going to click on Refresh on my browser here to see if the table has been updated. And if you click on the spikeysales_dataset now, you'll see that totalcost_output is now available. It has been populated with our final results. You can click on this table and observe the schema. We have the location and the sales per location. And you can click on the Preview link in order to see a few rows from this table. We integrated our Python Spark application with BigQuery in order to calculate the total sales per location for our sale days.

Monitoring Spark Jobs on Dataproc
We are already familiar with how Stackdriver monitoring works. Let's use Stackdriver to monitor our Spark applications. Start off in your main project dashboard page. We'll use the navigation menu to go to the Monitoring option. And here within Monitoring, we'll choose the Spikey-Dataproc Dashboard that we had set up earlier. Here are the two charts that we had set up earlier. We're going to add a new chart this time to monitor Spark applications. Click on the Add Chart button in the top right and set up a PySparkJob_Chart. We are going to continue monitoring our Dataproc cluster, that is our resource type. We'll choose Cloud Dataproc Cluster here. With the cluster chosen, the next step is to choose a metric that we want to monitor within the cluster. There are several job-related metrics that you could choose from. One metric is job duration to monitor how long your jobs take to complete. You can also monitor submitted jobs, running jobs. You can monitor jobs based on their status. Here is how you'd monitor your submitted jobs, and you can choose the running jobs as an option as well. This is the metric that we'll choose to monitor, our running jobs. We can specify additional filtering conditions for the jobs that we want to monitor. Our filter condition here is going to be based on cluster_name. We're only interested in those jobs that are running on spikey-cluster-one. We've configured our chart. Go ahead and click on the Save button, and you'll have a new chart added to your Spikey-Dataproc Dashboard. You can click on the details link here, and this will show you all of the jobs that are currently running in your cluster. Now the values that you see might be a little different based on the jobs that you have running at this point in time. And with this we come to the very end of this module where we saw how we could use Dataproc to run our Spark applications on the cloud. Now Spark is a great general-purpose computing engine, and it's one of the most popular big data processing frameworks. Dataproc supports Spark on the cloud. It uses YARN as its cluster manager. Dataproc has built-in support for Spark jobs which are written in Python, as well as Scala. You can use the web console or the gcloud command line utility to submit your jobs to the cluster. The Spark applications that you run on Dataproc have access to other GCP services as well. You can connect to BigQuery, as well as cloud storage, in order to access the data that you want to process, and to write out your final results. In the next module, we'll see how we can work with Pig, as well as Hive on cloud Dataproc.

Working with Pig and Hive on Google Dataproc
Module Overview
Hi, and welcome to this module where we'll see how we can use Google's Cloud Dataproc to work with Pig and Hive. Now Pig and Hive are important technologies in the Hadoop ecosystem. In fact they're built on top of Hadoop, and whatever script you write in Pig or query you right in Hive actually runs MapReduce jobs under the hood in order to process your data. Pig is a scripting technology that runs in the Hadoop ecosystem. You write code in Pig Latin, and you use Pig for extract, transform, and load operations where you clean up unstructured data in order to load it into a data warehouse. Hive is a data warehouse that you use with Hadoop. Hive offers a SQL interface to any data that you have stored within HDFS. This allows data analysts with no knowledge of MapReduce to extract information from Hadoop. When you create a Dataproc cluster on the GCP, it comes built in with support for Pig, as well as Hive. You can use these technologies out-of-the-box.

Pig for Extract Transform Load
Every technology that runs on top of the Hadoop ecosystem has been built for a special use case, and Pig is no exception. Pig is what you'd use when you want to script your extract, transform, and load operations in order to clean up the data before you store it in a data warehouse. Pig can be thought of as a data manipulation language, which transforms unstructured data, which might be generated by sensors or log files into a structured format. The Pig Latin scripting language has been tailored to work with raw, unstructured data. It allows you to extract, parse, clean up this data, and store the structured data into a data warehouse such as Hive. Once data is present in Hive, it's accessible to your data analysts who can then extract this information using languages such as HiveQL. Data which might be collected from log files, sensors, or even your website might be in an unstructured format. The schema might be unknown. The data might be incomplete. The data might be inconsistent. When you're working with data that has these characteristics, Pig is what comes to the rescue. Apache Pig is a high-level scripting language that has been explicitly tailored to work with data that has an unknown or inconsistent schema. The exact term for this is that Apache Pig is used for extract, transform, and load operations. This is where you pull unstructured, inconsistent data from another source, you clean up this data, extract the relevant information, and place it in another database where the data can be analyzed further. This is what a typical log file looks like. You have a bunch of text messages that are stored somewhere on file. Now this log file, if you see, has some format even though it's unstructured. Here you have the server IP address. You have the date and time at which this log was generated. You have the request type that was made to your website or application. You have the URL that was called. Pig can work with data of this type, extract all of the relevant parts of the information from these log messages, and place it into a tabular format where it can then be queried. The scripting language that you use to work with Pig is called Pig Latin. This is a procedural dataflow language to extract, transform, and load data. It has a series of well-defined steps to perform data transformations. It does not have if statements or for loops. Pig Latin runs MapReduce jobs under the hood in order to process your data. Data from one or more sources can be read, processed, and stored in parallel. You'll use Pig to clean your data, perform common aggregations, and perform other pre-computations that will help you before you store this data in a data warehouse. Here we have a visual showing the basic components of Hadoop. Pig is a technology that runs on top of Hadoop. It uses the Hadoop distributed computing framework in order to perform its operations. Pig processes data that is stored in files on HDFS. Any intermediate results produced by Pig are also stored in HDFS. The final output will also be written to HDFS. Pig runs MapReduce tasks in order to perform its data transformation tasks. Pig scripts can run very fast because they leverage the parallel computing framework that MapReduce offers. Pig has built-in MapReduce implementations for common operations, which are run very, very efficiently.

Running Pig Scripts on Dataproc
In addition to Hadoop and Spark, Dataproc comes preinstalled with Pig. Let's see how we can run Pig scripts on our Dataproc cluster. Here is the data that we're going to process using a Pig script. This contains some product data from our Spiky Sales e-commerce site. We have the original product category, such as clothing, the actual product. Then we have the MSRP and the discounted rate and the brand for that particular product. The brands that are available here are Puma Nike. Switch over to our GCP web console dashboard, click on the hamburger icon, and use the navigation menu to go to our cloud storage buckets. We're going to upload this file into the spikey-data bucket that we created earlier. I'm going to create a new folder here and call it pig_input. This is where the input files that our Pig scripts will process will be placed. Click through to pig_input and use the Upload files button in order to upload itemdetails. txt to the pig_input folder. Once the file has been uploaded successfully, we can activate the Cloud Shell using this button on the top right, and then run our first Pig job on the Dataproc cluster. We'll use the gcloud command line utility as we've done before, gcloud dataproc jobs submit pig and specify that we want to submit this Pig job to spiky-cluster-one. The --e or the --execute option for gcloud dataproc jobs submit allows you to specify the Pig script right here on the command line. And here are the different lines of our Pig script. We first load the data from itemdetails. txt in our spikey-data bucket and specify a field name for each column. Along with the field name, we specify the field data type as well. This can be chararray, float, int, and so on. We then perform a GROUP BY action on the data that we read in. We want to group by the category of product, which is specified in the type field, and we want to dump the groups out to our terminal window. This we do using the DUMP GROUPS command in Pig. Hit Enter. The Pig job will be submitted to our Dataproc cluster. You can go to the navigation menu, go to Jobs under Dataproc, and you should see the job running right there. You can see its status is running. It has been running for about 18 seconds. And you can see that the type of jog is a Pig job. Click on the job ID, and you can view the log outputs from your Pig script. Notice that under the hood, your Pig scripts run a MapReduce job. You can see the mapper and reducer running here, and you can see the same thing on your Cloud Shell terminal window. You'll find that the job will run through successfully in a minute or so. You've successfully submitted your first Pig script to your Dataproc cluster.

Storing Pig Output to Cloud Storage
We'll continue with our previous example and run a Pig script on our Dataproc cluster once again, but this time we'll specify the script in a file. We'll write our Pig script using code editor beta. Launch code editor beta and wait for it to be up and running. Click on the File button, and click on the New option in the menu to create a new file. Pig scripts have the. pig extension. We'll call this file itemdiscounts. pig. We'll write the code in our Pig script here. We'll load data from the itemdetails. txt file that is stored in our cloud storage bucket. We then filter out the header row from this file. We only want the data; we don't want the header. Extract each of the fields from the file that we've read in, and add a new column which calculates how much of a discount we got off the retail price. That's the last column here. Filter our data to only keep the Nike and the Puma brands, and then group this resulting information by category or type. This time we want to store our scripting results back to our spikey-data bucket in the pig_output folder. Itemdiscounts. pig is present in the home directory of our cloud shell. We can now submit this Pig script to Dataproc using gcloud dataproc jobs submit pig, specify that you want to submit this to spiky-cluster-one, and point to our itemdiscounts file. This file is present in the current working directory. Hit Enter, and this job will be submitted to your cluster. You can go to your cluster jobs web console, and you can see that this job is currently running. Wait for the script to run through successfully. If you scan the cloud shell terminal window, you'll find that we've successfully stored two records in our spikey-data/pig_output folder. Go to cloud storage using the navigation pane on the left. This will show you all of the buckets that we have on the cloud. Within spikey-data, you should now see the pig_output folder. And if you click through to pig_output here, you'll find that our MapReduce job, which was triggered by our Pig script, has been successfully completed, and the output generated as well. You can see here the technologies in the Hadoop ecosystem have been seamlessly integrated with your cloud storage buckets, thus, effectively separating your compute from storage.

Hive to Query Big Data
In all of the demos we've seen so far, we've seen how easy it is to work with open-source Hadoop technologies on Google's Cloud Dataproc. And the same thing is true for Hive as well. Hive is a data warehouse that has been built on top of Hadoop, and Hive provides a SQL interface to data that has been stored in files in HDFS. Hive can be thought of as a way to perform great data processing in Hadoop for folks who don't have exposure to object-oriented programming in Java. You can use MapReduce on Hadoop by just writing a few queries in Hive. Hive in a way serves to democratize MapReduce by eliminating the need to write Java code. Just like other technologies in the Hadoop open source ecosystem, Hive is built on top of Hadoop. Hive uses HDFS, MapReduce, and YARN in order to run queries. Data that you store in Hive is stored in the form of files. These can be text files or binary files. All of these files are partitioned across machines in the Hadoop cluster. The files are replicated for fault tolerance, and any processing tasks that you run using Hive is parallelized across multiple machines in your cluster. All of the processing that Hive performs on your data is run in the form of MapReduce jobs under the hood. As you already know, MapReduce jobs are run in parallel across all cluster machines, and they work on subsets of your data. The best thing about using Hive, though, is that you can write MapReduce jobs using queries without actually writing any MapReduce Java code. Instead of working with MapReduce code, you'll write structured queries using HiveQL. HiveQL else stands for Hive Query Language, and it's a SQL-like interface to the underlying data that's stored on HDFS. Data that is stored in the form of files in HDFS is exposed to the user in Hive _____ tables. So Hive actually creates tables and structures the underlying file data so that it can be queried by the user in a tabular format. As a user of Hive, you'll write a SQL-like query in order to operate on the underlying data. You'll do this in HiveQL, and you'll then submit this to Hive. Hive will then translate this query to MapReduce jobs under the hood, which it'll run in parallel across the Hadoop cluster. MapReduce will process the files that are stored in HDFS in Hadoop and then return the results to Hive, which will then display these results to the user. Hive completely abstracts away the details of the underlying MapReduce jobs that are run to process data from the user. You'll work with Hive almost exactly like you would with a traditional relational database.

Executing Hive Queries on Dataproc
In this demo, we'll see how we can run Hive queries on our Dataproc cluster. We start this demo off in the Dataproc Cluster page on the GCP web console. We are currently viewing spikey-cluster one. SSH into the master node. Hive has a simple command line shell that can be invoked by calling the hive command. Calling Hive will take you to the Hive interactive shell where you can run queries. Let's create a new Hive table here to store our e-commerce items information. We'll create this table if it doesn't already exist, and we specify the columns for this table. Remember that this Hive table is simply an interface or a bridge on top of the data that is stored typically in the form of a text file or maybe even binary files. Specify the format of the file that holds the underlying data. Our Hive table has been successfully created here. If you're using Hadoop on-premises, and your running Hive on top of Hadoop, your text file and other files will be stored on HDFS. In our case, though, our file lives in cloud storage buckets, specifically the spikey-data bucket. Inside spikey-data in the pig_input folder, we have the itemdetails. txt file, and this is the data that we'll use to load into our Hive table and query using the HiveQL language. Let's switch back to our terminal window here where we are logged into the Hive shell. I'm going to load data into the table that we just created. The table name was called items. Our Hive table will simply point to the data that is stored in this cloud storage bucket in the itemdetails. txt file. Once the data has been loaded into our table successfully, we can query this using HiveQL. SELECT * FROM items will display all of the items in our itemdetails. txt file. Notice that HiveQL queries are very similar to SQL queries. You can specify more complex queries as well using the WHERE clause, the ORDER BY, and so on. Observe that when you execute these queries, Hadoop runs MapReduce jobs under the hood in order to give you the results. You can also run Hive commands without having to log in to the master node of your cluster. You can run from the command line using gcloud exactly like you did with Pig scripts, Spark jobs, and Hadoop MapReduce jobs. Here is what a gcloud command to execute a Hive query on your Dataproc cluster would look like. Hive queries can be specified as a command line argument, or if there are more complex queries, they can be specified in a file. Let's clean up after ourselves. I'm going to go ahead and drop the items Hive table that we just created. Working with Hive on Dataproc is very similar to working with Hive on an on-premises Hadoop cluster. In fact, it's easier because you can have your data on cloud storage.

Summary and Further Study
And this brings us to the very end of this module where we saw how we could use Pig, as well as Hive on our Dataproc cluster. Pig and Hive are extremely important technologies in the open source world, and if you're migrating to cloud Dataproc, it's important that you have equivalents available, and that's what Google offers. Hive offers a SQL interface to data that is stored in Hadoop. It democratizes the process of MapReduce so that big data processing is available to those handlers who do not write Java code. Pig, on the other hand, is an important scripting mechanism that allows you to perform extract/transform/load operations on your data. This allows you to clean up your data so that it can be stored in a data warehouse such as Hive. Cloud Dataproc offers built-in support for both Pig and Hive, which makes migrating to the cloud a simple decision. As you work through the demos of this course, we created Dataproc clusters running on GCP VMs. Now if you leave these clusters running, you will incur charges for your clusters, so it's better to turn them down when you're not using them. And on this note, we come to the very end of this course on Google's Cloud Dataproc. It's time for us to say goodbye. But before I head out, here are some other courses on Pluralsight that you might find interesting. If you're interested in big data processing on other cloud platforms, Big Data on Amazon Web Services will show you how you can use elastic MapReduce on AWS. If, instead, you want to learn about Azure's managed Hadoop offering, HDInsight Deep Dive is a good course that you might want to see. If you're interested in the GCP, and you want to learn about other Google technologies, here are some courses on Pluralsight that you might want to watch. Creating and Administering Google Cloud SQL Instances will show you how you can move your relational databases to the Google Cloud. Or if you're interested in cloud storage solutions, Architecting Google Cloud Storage Configurations is the course for you. And that's it from me here today. Goodbye, and thank you for listening.

Course author
Author: Janani Ravi	
Janani Ravi
Janani has a Masters degree from Stanford and worked for 7+ years at Google. She was one of the original engineers on Google Docs and holds 4 patents for its real-time collaborative editing...

Course info
Level
Beginner
Rating
0 stars with 7 raters
My rating
null stars

Duration
2h 17m
Released
1 Nov 2018
Share course

