Algorithms and Data Structures - Part 2
by Robert Horvick

A look at the advanced data structures and algorithms used in day-to-day applications.

In this course we will look at some advanced data structures and algorithms used in everyday applications. We will discuss the trade-offs involved with choosing each data structure and algorithm and see some real world usage examples. This is part 2 of a two-part series of courses covering algorithms and data structures. In this part we cover data sorting, string searching, sets, AVL trees and concurrency issues.

Course author
Author: Robert Horvick	
Robert Horvick
Robert spent nearly 10 years at Microsoft creating software that made it easier for everyone else to write software. Most recently Robert worked on Team Foundation Server on the...

Course info
Level
Intermediate
Rating
4.5 stars with 459 raters(459)
My rating
null stars

Duration
2h 30m
Released
11 Jun 2012
Share course

Sorting Algorithms
Introduction
Hello. Welcome to the Pluralsight Algorithms and Data Structures 2 course. I'm Robert Horvick. And in this module, we'll be looking at several different algorithms for sorting data. First, we're going to have a quick conceptual overview of what data sorting is, and then we'll talk about how to measure the performance of various algorithms. This will help give us a common framework to compare the various algorithms and to learn when to pick one over another. Next, we're going to look at the five sorting algorithms that are being covered in this course. We'll start with the most basic sorting algorithm, the bubble sort, and we'll end with one of the most commonly used algorithms, the quick sort. Finally, we'll look in the sample application that will help visualize and compare the performance of each of the algorithms presented in this module.

Sorting Overview
Before we discuss how to sort data, let's start by agreeing on what sorting data means. What I'll be talking about is the process of rearranging data and the collection, so that the objects that are the smallest are at the start or the left of the collection. And the objects that are the largest are at the end, or the right of the collection. The sorting can be performed on any type that supports a notion of the quality comparisons. These include greater than, less than, and equality. Sorting algorithms fall into two broad categories; linear and divide and conquer. Linear algorithms process the unsorted collection of data as a whole. What I mean by that is if you are sorting an array of data, the entire array is being processed at once. The sorting problem is not being reduced into smaller units of work. Divide and conquer algorithms on the hand reduce the problem of sorting by dividing the data in a way that allows the problem to be reduced into smaller and smaller pieces, each of which can be independently sorted. When looking at the performance characteristics of sorting algorithms, there are two measurements that can help us quantify and compare the different algorithms. The first is the number of comparisons that are performed during the sorting process. In this case, what I'm talking about is any time the values of any two items in the collection of data are compared using one of the three equality operations. The second is the number of swaps that are performed. Swaps occur when data at two points in the collection are exchanged. In the example shown, there is an array with the values 1 and 0. And after a swap operation occurs, the values in the array are now 0 and 1. So we have swapped the values. The reason we measure these two metrics is that both comparisons and swaps have an associated cost. If one algorithm does fewer comparisons and swaps than another algorithm, it is very likely that it will complete the sorting process in less time. Similarly, one algorithm might do fewer swaps but far more comparisons than another. In that case, the overall cost of the sorting will depend on the relative cost of each comparison and each swap. For each algorithm we look at, I will discuss the performance cost for best, worst and average cases. This is because algorithms often have different performance characteristics, depending on the size and structure of the data presented to them.

Bubble Sort
The first sorting algorithm we're going to look at is the bubble sort. Bubble sort is the simplest sorting algorithm. The bubble sort works by processing the data and the collection in multiple passes. The data is processed from start to end, or left to right. The current value in the collection is compared to the next value. Starting at the first value in the collection, the value is compared to the next value. And if the current value is larger than the next value, they are swapped. So now, we've moved the largest value that we've seen so far to the right. This comparison and swap operation is now repeated for each value in the collection. So at every comparison, we're moving the larger value further and further right. Once the whole collection has had the compare and swap operation performed once, the process is repeated over and over until there are no swaps performed. Once no swaps are performed, the collection is known to be sorted. I want you to think for a moment about what the collection of data looks like at the end of each pass. After the first pass, the largest value in the collection will have made its way to the end of the collection. This is because every time a comparison is made against the largest item, it will be swapped with the smaller value, continually moving it from the left, or the start of the collection, to the right, or the end of the collection. So after the first pass, all we know for sure is that the largest value is at the end of the collection. Now when the next pass finishes, if any swaps were performed, we now know for sure that the second largest item is in the second to last slot in the collection. And after each subsequent pass continues, we can know that the array is becoming more and more sorted with the largest values at the right. Now, let's take a look at this in action. Here, we have an array of data that is unsorted. And we're going to perform pass 1 one of the bubble sort algorithm. We start with the first value, 3. 3 will be compared with 7, and since 7 is larger, no swap is performed. Now, we move to the next value, 7. 7 is compared with 4. 7 is larger than 4, so a swap must be performed. Once we've performed that swap, 7 has made its way further to the right. Now, this doesn't tell that us that 4 is in the right position. It just tells us that at this moment, 7 is working its way to the right. Now again, 7 is compared with the next 4, and a swap is needed. 7 is compared against 6, and a swap is needed, against 5, and a swap is needed. And now, 7 is compared against the last value in the collection, the 8, but a swap is not needed. So 8 is the last value in the collection, and that pass is now done. Now, we can see several swaps were performed, so we do need to perform another pass. So we start with the value 3. We compare it with the 4, no swap is needed. We compare 4 against 4, and no swap is needed, 4 against 6, no swap. Now 6 against 5. In this case, a swap is going to be needed. So the 5 and 6 are swapped. Now the 6 can be compared against the 7, the 7 against the 8, and we're done with this pass. One swap was performed, so we need to perform another pass. We compare the 3 to the 4, and as we can see, no swaps are needed all the way through, and eventually, we make it to the end of the array. And since no swaps were performed, we know the data is sorted. Bubble sort is a very simple algorithm, both conceptually and when implemented in code. But this simplicity comes at a cost. The performance of bubble sort means it might not be an appropriate choice in many problem demands. The worst case performance of bubble sort is about as bad as it can get. For an array with N items, it can take N squared comparisons and swaps for the data to become sorted. This might not be a problem for small datasets. For example, 10 items would require 100 operations. But as the dataset grows, the number of operations required to sort grows exponentially. Sorting 1000 items could take 1 million operations. This means that bubble sort is generally not an appropriate sorting algorithm for a large unsorted datasets. To make matters worse, the average case is just as bad as the worst case. N squared operations would be required to sort the collection of data. But it's not all bad news because the best case, or nearly best case, is actually very good. This makes sense. In the case where the data is already sorted, each item will be compared to its neighbor one time, and no swaps will be performed. This means that in the case where the data is already or nearly already sorted in advance, bubble sort can actually perform very well, even for large datasets. There's a risk though, and that is if your large dataset ever deviates from being nearly or completely sorted, the performance of the algorithm very quickly deteriorates to N squared performance. One final measurement that is worth observing is that the bubble sort operation was performed entirely in place. No extra allocations were performed. This can be very important if you're working in a domain where the operations need to be performed in line, or where memory might be at a premium.

Insertion Sort
The next algorithm we are going to look at is the insertion sort. Insertion sort works by sorting each item in the array as it is encountered. Now, like the bubble sort, the insertion sort works by processing the data in the collection from the left to the right. But the similarities end there. Insertion sort works by performing a single pass through the collection of data, whereas bubble sort use multiple passes. Each item in the collection is evaluated from left to right. And all of the data to the left of the item currently being evaluated is known to be sorted. And all of the data to the right is unknown and considered to be unsorted. The item being evaluated for sorting is inserted into the left, or already sorted part of the collection, at its appropriate sort order, relative to the other already sorted items. Now, this might sound a little tricky, but it's quite easy to understand when demonstrated graphically. So here, we have an array of unsorted data, and we're going to start at the second item. And the reason I do this is because the 5, or the first item in the collection, when you start, there's nothing to the left of that item, so it itself is considered to be sorted. So we'll start with the second item, the 3. It needs to be inserted to the left in its appropriate sort order. Now we can see that 3 is less than 5, so we're going to pull the 3 out, move the 5 over, and put the 3 in place. Now, the 3 and the 5 are a sorted array of data, and the rest of the items to the right are an unknown or unsorted data. Next, we look at the 4. Now the 4 needs to be inserted to the left in its appropriate place, so we pull it out, and we find its place and stick it in. So now, we have 3, 4, 5. We move to the next 4, we free its slot, and we find its appropriate insert place. Now, we'll continue this operation with the next item, the number 8. Now, because the largest item in the sorted portion is a 5, and an 8 is larger than 5, we know we don't need to do any data shifting. The 8 is already at its appropriate sort location. The next item is a 6, which is less than 8, so it needs to be inserted. So we pull out the 6, we find its appropriate insertion location, and now, that data is sorted. And finally, we have the 7, which is also less than 8, so we pull it out, and then shift the data, and insert it into the right location. So now, in a single pass, we have sorted the entire array of data using the insertion sort. Despite behaving very differently, insertion sort and bubble sort have very similar performance characteristics. In the worst case, N squared comparisons and swaps will need to be performed. Despite being a single pass algorithm, the process of selecting the right insertion location and shifting the data around during the insert operation means the number of comparisons and swaps, can grow exponentially to the data size. Now, you might say, "But we aren't swapping data, but we are moving data. " And when we're talking about swapping, we're talking about moving data and memory, that movement has cost. So whether we're moving the data as part of the swap, or whether we're moving data in order to shift it to the right in memory, it's still a swap, it still has a cost, and we can't simply discount that cost. Like the bubble sort, the average case is just as bad as the worst case, with N squared operation is been required in general. The best case performance is where insertion sort shines. That's quite a tongue twister. With already sorted data, the algorithm will not need to perform any swap operations. With nearly sorted data, the number of swaps and comparisons is also quite small. This makes it a reasonable candidate when working with small datasets, or with nearly sorted large ones. And lastly, no additional space is required to perform an insertion sort. This means that it's a good candidate for sorting data when minimizing memory space is important, and the dataset is either small, or a nearly sorted large one.

Selection Sort
Next, let's take a look at selection sort. Selection sort is another linear algorithm, and is a kind of hybrid between bubble and insertion sort. Selection sort works by finding the smallest item in the collection, and moving it to its appropriate location. And this might sound kind of like the opposite of bubble sort which move the largest items to the end of the collection, and in that sense, it is similar. But like insertion sort, it sorts the data from smallest to largest, and it does it without revisiting the already sorted data. The algorithm for selection sort is pretty simple conceptually. You enumerate the array from the first unsorted item to the last, or from left to right. While you're doing that, you find the smallest item. And once you found that smallest item, you pull it out of the array, and you insert it at the first unsorted location. So you're just plucking out the smallest items and moving it to the front of the array. So let's take a look at this in action. So here we have an array of 8 items. The selection sort is going to be pretty conceptually simple, but I want you to think about all the different comparisons and swaps going on while this is happening. So let's first enumerate from left to right and find the smallest item. So we'll start at the 3, and we're going to have to go all the way to the 1 which we can see is the smallest, but we don't know that. We have to go all the way to the end. So we're going to compare all the way to the end and find that 1 is the smallest item. So we're going to swap that item with what was first in the array. It doesn't matter if it was 3 or 2 or whatever. It's just-- the point is the smallest item has to go to the front. Now, we're going to do this again. So we're going to start at the 8, and we're going to walk all the way to the end of the array. And during that process, we're going to learn that 2 is the smallest item. Since 2 is the smallest item, we're going to swap it with the 8. Now let's repeat this one more time. We'll start at the 8 and go all the way to the end, and find that 3 is the smallest. So we'll swap and we'll continue this process. We'll next, find that 4 is the smallest item, and we'll swap it with the 8. Now here is where it gets a little interesting, because now we're going to go from left to right starting at the 5, and we'll find that 5 is the smallest item. And in doing that, no swap is necessary because it's in the right location. Now, we'll start with the 8 again, find that 6 is the smallest, move it, and then we'll swap the 7 and the 8, and we're at the end of the array, so we're done. Now, we didn't do that in many swaps. We in fact, we don't do more swaps than we have items in the array. But we did a whole lot of comparisons. So let's go talk about the performance of selection sort. Selection sort performance is pretty similar to bubble and insertion sort. Worst case performance is N squared. So like we've talked in the past, this means it's not appropriate for large unsorted datasets. Average case performance is also N squared, so it's not appropriate for large unsorted datasets. But in practice, it typically performs better than the bubble sort, but worst than the insertion sort. And the best case performance, it's also N squared. And you might be saying, "Wait hey, we didn't do that many swaps. " Well, you're right, we didn't do that many swaps, but we did a whole lot of comparisons. There's no way to avoid them. The only way to find the smallest item is to walk all the way to the end of the array. Now there might be optimizations you can do if you have a finite alphabet of values. If you know that there is a minimum value under which there could be no smaller item, then once you encounter that item, you could just fall to the front because you know there can't be another one. So there are certain optimizations you might be able to get. But in general, you know, if you're talking about a large set, like 32 bit integer space, or just arbitrary values, that could be anything, there's really no optimization you can do with selection sort to avoid doing all those comparisons. But if you're on a system where comparisons are really, really cheap, and swaps are really, really expensive, then selection sort may have better wall clock performance than algorithms that do more swaps but fewer comparisons, for example, merge sort. And the space required is whatever the number of items in the array is. The selection sort operates directly on the input array, meaning it's a great candidate when minimizing space is important. But because of the other performance characteristics, it's usually not a great candidate for other reasons.

Merge Sort
Now we're going to look at our first divide and conquer algorithm, merge sort. Merge sort works by recursively splitting the data in half. For example, an array of 8 items would be split in the middle in the 2 arrays of 4 items. The splitting continues until each split array has only 1 item in it. Since the array now only has 1 item in it, that split array is known to be sorted. Now at this point, the arrays are reconstructed. But the values are put back together in sort order. And after each reconstruction, the sorted array doubles in size. And this continues until the array is all reconstructed and fully sorted. It's a little bit tricky to understand with words, so let's take a look at it in action, because I think it's actually a little simpler than you might imagine. Here we have an array with 8 items that are unsorted. So, let's start at the beginning. The array is recursively split in half. First, it's splitting the 2 arrays of 4. And at each of those arrays of 4 is split into arrays of 2. And then all of it is split up, so now, we're left with 8 arrays of 1 item. Now because there's only 1 item in the array, it's sorted. So let's take that data there and let's bring it up to the top and look at the next pass, the reconstruction pass. So first, we're going to reconstruct each of these single arrays back into arrays of 2. And when we do that, we're going to sort them. So the 3 and the 8 are reconstructed into an array with 3 and 8. They're already sorted. The 2 and the 1 are reconstructed, but their values need to be flipped. Now they're sorted as 1 and 2. 5 and 4 reconstructed into 4 and 5; and 6 and 7 are reconstructed as 6 and 7. So what are we going to do next? Well, we're going to reconstruct these arrays of 2 back into arrays of 4, and we're going to sort them as we do that. So let's pull the arrays of 2 up top and look at the next reconstruction pass. The 3 and the 8, and the 1 and the 2 will now be reconstructed into an array of 4 items that are sorted 1, 2, 3, 8. So now see what we've done here is we've taken the process of sorting and decomposed it into smaller and smaller problems, and now, we're building it back up slowly. We all know how to take 2 items and see if they're sorted and put them back together, we've seen 2 algorithms that can do that. But with merge sort, we're doing that just on a smaller scale repeatedly. So 4, 5, 6, 7, they're already in sort order. So when that array is put together, nothing changes. Now, we have our final reconstruction phase. We're going to take our 2 arrays of 4 and make them a single array of 8. So let's bring it up top and look at the final reconstruction. And I think you can see where this is going. The 8 in the first array of 4 is clearly in the wrong place, but everything looks pretty close other than that. So, put the data all together in the right order. The performance of merge sort is impressive, but it's fixed, and that's something that we'll talk about in a moment, but it's important to understand. In the worst case, only N log N comparisons and swaps are needed. This is substantially less than the N squared that we were seeing at the bubble and insertion sort. And this makes merge sort a reasonable candidate for large unsorted datasets. And another important part is that as we're splitting that data, we're splitting the arrays into subsequently smaller arrays, and we're only looking at those arrays in their split parts. We're not considering all the other split arrays. This means we can take the sorting operation and run it in parallel on multiple different processors, or multiple different course. So the data splitting doesn't just give us the advantage of reducing the number of operations we perform. It gives us the advantage of being able to perform those operations in a different way. So if you have to perform a million comparisons and that million comparisons takes 2 seconds on the clock, if you're able to perform those in parallel, even though you're still doing a million comparisons, you're able to do it in 1 second, or perhaps slightly more with the overhead of threading. But the point is that the restructuring from linear to divide and conquer has given us a very important optimization potential. Now like I said, the performance is impressive, but fixed. So the average case is also N log N. This still makes it appropriate for large datasets, even in the average case. But the best case is N log N. Still appropriate, but you might be wondering why is that. Why do bubble sort and insertion sort improve their performance exponentially in the best case, but merge sort doesn't? Well, it's because merge sort has to perform its data splitting, all the comparisons and the reconstruction the same way regardless of if the data is already sorted or not. The cost is basically fixed. This means that merge sort is a very predictable algorithm, and this can be very desirable in an algorithm because the data size is the only factor that really influences the performance. And finally, the space required to perform a merge sort is a little bit tricky. It can be done in line, but often, it's not. Often times, the arrays that are being split are actually new arrays that are allocated, and the data is copied from the larger array into the smaller array, and then copied from the smaller arrays back into the larger arrays. And these extra allocations increase the memory footprint required to sort the data. And it can do it quite dramatically. If you have an array with a million items, every time you split, you're still going to have a million items. You're just going to have it in smaller and smaller arrays. So however many splitting phases occur, is how many duplicate allocations of the original size you're going to need. These extra allocations increase the memory footprint, and they also have some other effects on data locality, and the heap can become populated, and more garbage collections can be occurring, and there's all kinds of things can happen because of this. So when you're looking at merge sort as a candidate algorithm, you really have to understand, is merge sort being implemented in place, or is it being implemented with array allocation at every split in phase? And if it's being implemented in place, you will see, you know, that O N space, which is exactly what you want, it's comparable with the other algorithms, it can't get better than that. You can't store N items in less in the amount of space, N items take to store. So that's the ideal. But it can get pretty bad pretty quick if you're not being careful. So understand how the algorithm is implemented not just how it works in the case of merge sort.

Quick Sort
The last algorithm we're going to look at is the quick sort. Quick sort is a divide and conquer algorithm, and it's one of the most commonly used general purpose sorting algorithms in computer science. Since it's divide and conquer algorithm, it should come as no surprise to you that we are going to be partitioning the data into smaller sets. Where quick sort differs from merge sort, is that the arrays are not necessarily split in half. Rather, a pivot value is picked based on either a fixed rule or some heuristic. Now, there's many school that taught on how to pick the best pivot value, and ultimately, there's no right or wrong choice. There are just better choices based on the characteristics of the data. Once a pivot value is picked, all the values in the array that are smaller than the pivot value are placed on the left of the pivot. And all the values that are larger are placed to the right. So we've now partitioned the data, so that we know that all the values below a threshold or on one side, and above the threshold are on another. So we haven't sorted the data, but we've split it up in a way that we at least understand its relative sorting. Now this pivot and partitioning operation is performed again. This time on the smaller and larger value partitions created in the previous step. And each partition will be itself partitioned further with the values moving to one side of the partition or the other, depending on whether they are larger or smaller than that pivot value. And this is repeated over and over again until the data is fully sorted. So let's take a look at this. Here we have an unsorted array of data. Now the first thing we need to do is pick a pivot point. Here, we're going to pick the value 5. Now how the value 5 was picked isn't important. So what we're going to do now is take all the values that are less than 5 and put them to the left of the 5, and take all the value that are greater than 5 and put them to the right. So, what we can see is that the 4 and 8 need to swap spaces. And there we go. We now have 2 partitions. The one on the left has all the values less than 5, the one on the right has all the values greater than 5. And what's important is that because we know that all the values above and below 5 are relatively larger or smaller, we also know that 5 is in the right spot. So, let's go pick another value. This time, let's start on the left. So our partition value we pick this time will be 2. Now, we can see that everything to the left of 2 is larger than 2, and everything to the right is smaller. So what do we need to? We'll need to pull the 2 out of the array, we need to move all the values around to their appropriate locations, and we need to put the 2 back in the array. And what we know now is that everything to the left of 2 is smaller, everything to the right is larger, and 2 is on the right location. So let's pick another pivot. This time we pick the 1. There's only 1 item in that partition, so it itself is known to be sorted. Now, we move on to the other side of the partition, the 3 and the 4. We'll pick a pivot, we pick the 3. Well, we know that the 3 can be pulled out. Everything smaller than 3, put to the left, everything larger, put to the right. We have a fully sorted partition now. So let's move over to the other side. These are the values that were greater than 5. We'll pick a pivot which will be 7 in this case. We'll pull the 7 out, we'll put all the values to the appropriate side, the left or the right. We've now put 7 where it belongs, and 6 and 8 are where they belong. So as you can see what we did was we would pick that partition or that pivot point, and we would make sure that that pivot value is in the right place, and all the values less than or greater than were relatively in the right place, and then we just keep doing that over and over and over again. So now that we know that quick sort works, let's talk about its performance and see why it's an attractive algorithm. Well, the quick sort performance is pretty good. And I think what we'll see here is that except in the worst case, and by worst case, I truly mean pathologically worst cases. Except in that case, it's a good algorithm. In the absolute worst case, it can be N squared. Now, N squared is no worse than any other algorithm we've looked at, which means that even in the pathologically worst case, this is still a pretty good algorithm relative to the other ones. But in the average case, it's N log N. This means it's appropriate for large datasets. And in the best case, it's N log N, which means it's still appropriate for very large datasets, it's very good best case performance. The space required, it's N. It's N line. We aren't creating other arrays like we were at merge sort. Now there is a little bit of space overhead that you have to consider with the respect to this being a recursive algorithm. And when you make a recursive call, you are creating new stack entries. So just consider how your environment and what the cost of a call is when you're doing this. You don't want to think that you're about allocating more than you are, but then find out that your environment is doing it implicitly for you. But this is a recursive algorithm that operates in place in general, so you have very good space characteristics combined with very good performance characteristics which makes quick sort a very attractive general purpose sorting algorithm.

Sorting Demo Overview
Now I'd like to take a look at our demo application. This application will be used to graphically compare the cost of sorting various datasets. The demo will allow us to control how many items are being sorted, and this is useful for seeing how the performances of algorithms are affected by data size. We'll be able to control the general characteristics of the sample data, either providing presorted, reverse sorted or random data. And this will help us see how best, worst and average case performance looks. And we'll be able to measure both comparisons and swaps, or on the case of merge sort, to the assignments of data and the split arrays. The sample application will contain a single window with the data characteristic and operation selections on the bottom. A reporting window will be populating most of the window, and a button to execute the sorting algorithms will exist. After the sorting is completed, we'll be able to see the cost of each algorithm compared to the others. So let's go take a look at this application in action.

Sorting Demo
Our visualizer has the item count, the data order dropdown, the operation dropdown, the execute button, and the graph that we'll render. Though what's in here right now is just the designer look that it gives you. This isn't actually data that I have in there. So, what we really want to look at then I guess is the execute button. When we code execute, what happens? What happens is we clear the chart, we set weight cursor, and we graph all of our points. And if we want to look at what we're doing there, we figure out are we random, are we sorted, are we reversed, we get the points we want. We have an array of the algorithms we're going to use. And then for every algorithm we're going to use, we sort a copy of the data. And then we take the chart series which is something if you want to learn more about chart series, go ahead and learn about the chart controls, I have a reference to it at the very end of the module. And we'll look and see, are we tracking comparisons or swaps, and we render the appropriate series based on that. So here we go. We have our form up. Let's look at 10 items. And I would like the data to be random, and we are going to compare our swaps. So when we execute, we're going to see bubble sort, insertion sort, merge sort, they, you know, they did quite a few swaps, less than 40. Selection sort didn't do quite so many. Quick sort, fair at the midline. So, what if we bump this value up to a thousand now? Whoa, okay. Now, it's becoming little bit clearer why bubble sort and insertion maybe aren't our favorite algorithms. What if we check comparisons? See now, selection sort came back up. Now that's kind of interesting. Insertion sort generally having better performance than selection sort. And selection sort generally having better performance than bubble sort. And if you look at this, you'd say, but look, insertion sort does so many swaps and not a lot of swaps here for selection sort. But when you switch to comparisons, boy, look at that. It's not even close. Selection sort is doing a lot more than insertion sort. And that's where I'm talking about kind of that relative cost of things becomes really important. The relative cost of the swaps and the comparisons and the context of the data that's being looked at, all factors into this. So if we look at sorted data, we'll see that selection sort just goes crazy. It does a ton of comparisons. And there's nothing you can do about that to get rid of them. If you look at swaps, there it's merge sort. Remember, we talked about how in merge sort its assignments, and there's nothing you can do to get rid of those assignments. You know, the assignments are going to happen with merge sort regardless of what happens because it has to break apart those arrays, and it has to put them back together. All right, now with the sorted data, you'll notice that bubble sort, insertion sort and selection don't even show up for swaps. And why is that? Well, it's because they don't do swaps they don't need to. But if we come here and look at comparisons, well like selection sort, there's nothing you can do about it. It's going to do a ton of comparisons, because it has to go through and find that value to put in the right location. And it doesn't know where that value until it's gone through and seen that everything is right. And it's going to do that over and over and over and over again. So yeah, here is that example of where knowing the characteristics of your algorithm, and knowing how comparisons and swaps are different, about how they all factor together to look at the overall performance, that's why there's something like this is important. It's important to be honest about what's being done, about the swaps being performed, the comparisons being performed. So that's out sample app. I want you to be comfortable now to be able to go in and dig a little bit deeper into the code while you're looking at the module and the clips again to be able to sit side by side and understand, okay, I see this happening graphically, now, let me look at the code. All right, I understand that now. In general, these aren't the algorithms you're going to write a lot. These are algorithms you're going to use. But you want to know which one to pick.

Summary
In this module, we started by first having an overview of what sorting was, and how we we're going to measure performance. Next, we looked at five different sorting algorithms; bubble sort, insertion sort, selection sort, merge sort and quick sort. And then finally, we looked at a visualization application that allowed us to compare the various sorting algorithms, and see how they responded to different data characteristics such as the data set size, the order of the data, and whether we are tracking swaps or comparisons. There are some references I think you might want to look at. Wikipedia is always a great source. You know, anytime you're looking at an algorithm, of course, start with your algorithms book if you have one. You know, there're really good books out there. Some are free, some you might still have from school. But if you don't have a book, Wikipedia is a great place to go to. There's a lot of explanation, not just on how the algorithms work at Wikipedia, but there are some decent visualizations. And they really break down some of the math behind it, so you can understand more about the performance characteristics at a theoretical standpoint. And also, because I introduced the data visualizer in one of the charts, I want to put a link to that. If you're interested in learning about the charting APIs that we used in the sample application, they exist under these system; windows, forms, data visualization, charting, chart class. It's a great place in MSDN to start. It's going to take you through all-- you know, all the related classes will be linked from it. And you can find that using the MSDN link I have available there. And if that link ever expires, just look for the class by name on MSDN, and you'll be able to find exactly what you're looking for.

Set Collection and Algorithms
Introduction
Welcome to the Algorithms and Data Structures 2 course. My name is Robert Horvick, and in this module, we'll be learning about the set data type and some commonly associated algorithms. We'll start by first learning about what a set is. Next, we'll see how that might be translated into a set class. This class will look and feel like a typical dot net framework collection. After that, we'll look at the four algorithms commonly associated with sets. These are the union, intersection, set difference and symmetric difference algorithms. We'll look at how a set class can be used in a sample application. And we'll briefly look at set classes and algorithms in the dot net framework and in the C plus plus standard template library.

Set Overview
Let's start by defining what a set is. A set is a collection of objects. Any type of object can be stored in a set, as long as the objects are comparable. Let's look at some examples using integers. Here, I'm showing five sets of integers. The odd set contains all of the odd integers. And the even set, includes all of the even integers. The negative and positive sets include all of the negative and positive integers, respectively. And the neutral set contains only the integer value zero. You can see in these examples, that the same value can exist in multiple sets. For example, the value 2, exist in both the even set and the positive set. And we can also see that at least in theory, sets can contain an infinite number of values, or in the case of the neutral set, unlimited number of values. Of course in practice, the number of items that could be added to a set class would be limited via constraints such as available memory. So let's go take a look at an actual set class that we'll be implementing during this module.

The Set Class
Over the next several slides, we're going to be looking at the framework of a set class that we'll be implementing during this module. Before we dive into the individual methods, let's take a broader look at the class. First, we can see that this class is a generic class. And as such, it takes a type of argument, T. A constraint exist on the class that ensures that the type T implements the I comparable interface. This ensures that we can compare instances of the type T using the compare to method. We can also see that the class implements the IEnumerable interface. This will allow us to use the class in context wherein a numerator is needed. For example, in a for each loop. Let's start by looking at the class construction. Since we are storing objects in the set, we need to have a place to put them. This could be any, more basic data structure. And in this case, I have chosen a dot net framework list class. Of course, it would be possible to use an array instead of a list. But doing so would require adding logic to the class that would not help teach the set concepts but would require additional time to explain. If you're interested in seeing more of the nuts and bolts of item storage, the first algorithms and data structure course contains the implementation of several collection classes including a length-list class that could be used here. So first, we have a constructor for the empty set. And we also have a constructor that accepts an I IEnumerable of items to add to the set. This allows us to easily create a set from any IEnumerable such as from other sets. Once we have an instance of a set in memory, there are several operations we might want to perform to manage the items within the set. The first is to add new items to the set. The set class we're creating will allow add in a single item to the set. As you can see, if the item already exist in the set, an exception will be thrown. This is because our set class does not allow duplicate items. You can also add multiple items using a method named add range. Again, because this method defers to the add method, duplicate items are now allowed just as important as adding items is removing them. Our set class will support a single remove method. This will take the item to remove, and will return a Boolean indicating whether or not the item was found and removed. In addition to basic item management, we also want to know certain facts about items in the set. We would like to be able to determine the count of items or the cardinality of the set. In this case, it is simply the count of items in our backing list. We would also like to determine if a specific item exist in the set. In this case, we're simply deferring to the contains method of the backing list. The last non set-specific methods we're going to look at are the enumeration methods. In both cases, we're simply deferring to the backing lists get enumerator methods. If you want to know more about how these might be implemented, I would encourage you to review the link list module of the algorithms and data structures part 1 course. This module dives pretty deep into how it get a numerator might be implemented for a custom class.

Union
The first set algorithm we are going to look at is the union algorithm. The union algorithm compares two sets and produces a third set that contains all the unique items of both sets. It's a bit of a mouthful so let's look at what I mean. In this example, there are two input sets of integers. The first set contains the integer values 1, 2 and 3. The second set contains the values 3, 4 and 5. This means that when the sets are unioned together, the produced set will contain the unique items of each set. So the produced set contains 1, 2, 3, 4, and 5. Notice that 3 appeared in both input sets but only appears once in the output set. This is because 3 needs to be unique in the output set to not violate the class constraint that duplicates are not stored in the set. So let's look at the code for union. The first thing we do is create the result set. And since we know the result set, it has to contain all of the items that are unique in both sets. We pass in the current sets items to the constructor. So immediately, the result set contains all of the items in the current set. Now, what we need to do is add the unique items from the other set into the result set. Now notice there's a new method here being called and it's not add range but rather it's add range skip duplicates. And this is because as in the example, the number 3 shows up in both sets. But we don't want to throw an exception. We just want to skip the second 3. So, when result is allocated the first time, the items 1, 2 and 3 are passed in and that result contains 1, 2, and 3. When add range skip duplicates is called with the set containing 3, 4, 5, 3 is skipped and 4 and 5 are added. And the result set containing 1, 2, 3, 4, 5 is returned. So now, we haven't looked at add range skip duplicates but all it does is defer to add skip duplicates which instead of throw in like the add method does when a duplicate is encountered, it just skips it. These are private methods and they're only used in the context of these algorithms where we need to be able to produce an output set without throwing an exception understanding that there might be duplicates on the input.

Intersection
The next set algorithm we're going to look at is the intersection algorithm. The intersection algorithm compares 2 sets and produces a third set that contains all of the members that exist in both sets. These are the values that are said to intersect between both sets. Let's look at an example. Here, we see 2 input sets. The first contains the values 1, 2 and 3. And the second contains the values 2, 3 and 4. Our output set should contain all the items that exist in both sets. In this case, that is the values 2 and 3. They are the only items that exist in both sets. So looking at the code, we can see that again, we create the result set first. But this time, we use the empty set constructor, because we don't yet know which items will exist in both sets. And next, we enumerate over all the items in the local items list. And if the item also exists in the other set, we add the value to the result set. You may notice that in this case, we're using the add method rather than add skip duplicates. And this is fine, because the item we're adding is the result of an enumeration of a single set, which means, it's not possible for there to be a duplicate. For there to be a duplicate would mean that the current items list would contain a duplicate. And that would be a violation of a constraint that sets only contain unique items.

Set Difference
The set difference algorithm is the third set algorithm we are going to look at. Set difference works by taking two input sets and returning all of the items at the first set that are not members of the second set. Let's look at an example. In this case, we have two sets of integers. The first set contains the values 2, 3 and 4 and the second set contains the values 3, 4, and 5. When we apply the set difference algorithm, we want to find all of the items in the first set that do not exist in the second set. In this case, it's only the value 2. Note that the value 5 exist in the second set, but not the first. But it's not in the result set. This is an important behavior of set difference. Imagine I asked you, for the list of students who are in math class but not in world history. The output set should only contain students who are in math not students who are in world history but not in math. So now let's look at the code. The ways that difference works is first allocate the result set and include all of the items from the local list. Next, we enumerate over the other set and remove from the result set any items that exist in both. So let's go back to our example and walk through the code thinking about the values in the sets 2, 3, 4 and the sets 3, 4, 5. So we start by adding 2, 3 and 4 to the result set. Now, we start enumerating the set that contains 3, 4, 5. First, we find the value 3. But it exists in the result set which contains 2, 3, 4. So we remove it. Now the result set contains 2 and 4. We enumerate the items again and now we get to the item 4. Well, the result set contains the item 4. So we remove it. Then we enumerate the third item which is the value 5. The result set does not contain the value 5. So the remove method returns false but nothing happens. So we're left with a result set that contains only the value 2 and that's what we return.

Symmetric Difference
The final set algorithm we're going to look at is the symmetric difference algorithm. The symmetric difference algorithm is similar to set difference. But now the output set will include all of the items that exist in only one of the two input sets. An interesting property of the symmetric difference is that it's the set difference of the union and the intersection of the input sets. That's a lot to take in. So let's look at an example that might make it clearer. We want to find the symmetric difference of two sets where the first set contains the values 1, 2, and 3. And the second set contains the values 2, 3 and 4. Now, the output set should contain all the items that only exist in the first or second set. So recall that the symmetric difference is the set difference of the intersection and the union of the input sets. Let's look at that piece by piece. We start by getting the intersection of the two sets. Well the intersection of 1, 2, 3 and 2, 3, 4 is 2 and 3. So t intersection set contains 2 and 3. The union of 1, 2, 3 and 2, 3, 4 is the set containing 1, 2, 3, 4. So now we want to find the set difference. Between the union set and the intersection set. So what are the values that exist in the union set that do not exist in the intersection set? And those are the values 1 and 4. And if you look at the code for symmetric difference, you can see, we really are just doing the one that I described. We get an intersection set, we get a union set and then we perform the set difference operation between the two. Now, this still might be a little bit unclear. So let me try and compare set in symmetric difference, where set difference was used to figure out what students were taking math but not world history. Symmetric difference can be used to find out which students are taking math or world history, but not both. So it's a subtle but very important distinction between set and symmetric difference.

Sample Application
Now, we'd like to take a look at a sample application. But the sample application kind of takes off a little bit further on some of these student class assignment examples I have been using in the algorithm descriptions. The application is going to have several data sets. It's going to have all the male students, all the female students and then some information on class enrollment. We're going to be able to answer some questions such as what women are taking math? Or what men are not taking writing? So let's go look at the sample application and see how these set algorithms can be used to answer some pretty practical and real-world questions. Okay, what I want to do now is bring up the sample application for sets. And what this sample application has is the ability to take two data sets, compare them using a set algorithm, and then produce the output set. Now, it's going to use that theme of students and class assignment for us to answer some interesting questions. Let's start with a real basic one. I want to find all the students. So I'll take all the male students, all the female students and if I want to find all of them, I'm going to use a union. And let's evaluate. So this is the list of all the students. Now, what if I wanted to find all the students who are both male and female? Now that would be intersection. Now, this is going to be a very short list and in fact it's empty. And we can use some of the difference algorithms, for example, set difference. This is going to find all of the items in the men's list that are not in the women's list. And it's all the men. And then symmetric difference is going to find all the men that aren't in the women's list and all the women that aren't in the men's list. But that's not really useful, the last couple of examples there but I wanted this to show that they are doing what we think what they should do. So let's answer some more interesting questions. Let's find all of the men who are taking reading. So this is going to take the men's list and perform an intersection with the reading list. Let's evaluate that. So we see James and Robert are taking reading. Okay, great. Let's go a little bit further and say, who is taking reading and writing? Robert and Elizabeth are taking both reading and writing. Now, let's answer the question of, who is taking reading or taking writing but not taking both? Remember, this is symmetric difference and we can find out that James, Mark, Aimee and Evelyn are taking reading or writing but not both. Now, set difference, remember, that would be students who are taking reading but not writing. So James is the only member in the reading class who's not also taking the writing class. So I hope you can see here that these four basic set algorithms allow you to answer some really interesting questions, when you're looking at real-world data sets. So this is just a very brief demonstration of the set algorithms in a kind of a semi practical setting. But I hope that it makes it clear that these are worthwhile computer science concepts that are worth the investment in time to understand.

Other Implementations
In this module, we've gone through all of the code necessary to create a simple set class. And while this is a great learning tool, it's nice to use a production quality implementation on writing production quality applications. So in the dot net framework, the hash set class implements the set behaviors we just learned about. But they have slightly different names. As you can see in the table, union is called union with. The intersection algorithm is called intersect. The set difference is performed with the except method and the symmetric difference is performed with the symmetric except with method. So the names may be different. The concepts and behaviors are the same as what we've seen. I also wanted to take a quick look at how the set algorithms are exposed in the C plus plus language. The C plus plus takes a different approach by providing implementations of the algorithms that exist independent of the collections they operate on. Now, this might seem a little unconventional if you've not spent much time working with the C plus plus standard template library, or the STL. But it's quite an elegant solution. Because it means that the set algorithms can be performed on any class that exposes an iterator not just a set class. I have some sample code here. And what its showing is an intersection that is going to find the people who are taking both gym and math class. So first I add the names Biff and Marty to the gym class vector. Then I add Emmet and Marty to the math class vector. I then call the set intersection function, passing in the gym and math class iterators and populated an output vector called people and gym and math. That output vector ends up containing a single value, Marty, the only student who's taking both math and gym. So it looks a little bit different that the C sharp code we've seen, but the set algorithms function the same way.

Summary
In this module we've learned all about the set class and four basic algorithms. We started by taking an overview of sets and just trying to figure out what they were without worrying about code. Then we moved on to the class. We started by looking at some of the kind of peripheral methods that are necessary for the class to really be useable, things like adding and removing items and enumeration or contains. These are things that are helpful but don't really have anything to do with sets in specific and they're just general collection methods. Next, we look at the set specific algorithms. We looked at union, intersection, set difference and symmetric difference. And at each step along the way, we looked at some examples and we went through the code for the algorithm. And we looked at the sample application. And with a sample application, we were able to look at several different sets of students, partition by class enrollment and by gender and then answer some questions, such as who's taking math but not writing? Finally, we looked at how the dot net framework provides the hash set class and how C plus plus exposes the set algorithms independent of a class. Now, there are a few things you can look at if you want to lean more about sets. There's actually quite a bit out there and the reason is that set theory isn't just a computer science concept. It's actually heavily written mathematics. So, when we start looking at sets on sites like Wikipedia, you need to start looking at set theory and then sets from a computer science standpoint and then sets from a mathematics standpoint, and there are some differences between them. So, if you're really interested in sets in set algorithms, these are some great places to start. And I also wanted to provide links to the reference types for the dot net framework, the hash set class, and for C plus plus, where the set algorithms are contained in the algorithm header file. So those are some links to MSDN that should help you find out more about that, see some more sample code and maybe dive a little bit deeper into those implementations.

AVL Tree
Introduction
Hello. My name is Robert Horvick and welcome to the Algorithms and Data Structures 2 Course. In this module, we will be learning about the AVL tree data structure. We will start this course by reviewing the binary tree data structure and comparing at a high level the differences between a balanced and an unbalanced binary tree. Next, we will start looking at the base implementation of the AVL tree. We will see how it is very similar to the binary tree structure and we'll learn about how it differs. With the general tree structure understood, we will move on to learning how tree balancing works by reviewing the four balancing algorithms the AVL tree employs. Finally, we will look at an example where we will visualize both binary and AVL trees to see how they differ structurally when given identical data. ( Pause )

Binary and AVL Trees
Let's begin by reviewing the binary tree data structure. Now, this will be a brief overview and we'll not dive into the implementation of the binary tree structure. If you would like more information about binary trees, please consult the first algorithms and data structures course where the binary tree data structure is covered in detail. A binary tree is a collection that stores data using a tree structure. Each node in the tree contains a value. This is the data that has been contained within the collection. Each node also has a left and a right pointer. Now, these pointers are used to navigate around the tree using a few simple rules. The rules are that given a node with the specific value. If you navigate to the left child, it and all of its children will contain values less than the current node's value. Conversely, if you navigate to the right child, it and all of its children will contain values that are equal to or greater than the current node's value. Because a binary tree is data collection, we require some basic operations. These include inserting new values, deleting existing values, searching for values, clearing the entire collection, and enumerating all the values in the collection. Now, again, all of these topics are covered in detail and the binary tree module of the first algorithms and data structures course. Now that we've recalled the basics to the binary tree structure, let's learn a bit about the AVL tree. AVL trees are a self-balancing tree. They were invented in 1962 by the Russian mathematicians Adelson-Velskii & Landis. Their initials are the basis of the name AVL tree. AVL trees are very similar to binary trees. They follow all the constraints of the binary tree, namely, that smaller values are on the left and equally larger values go to the right. Because they follow these same rules, search and enumeration are identical to the binary tree structure. Because of that, we won't look at these methods in detail. Insertion and deletion are where the AVL and binary trees differ. In an AVL tree, whenever a node is added or removed from the tree, operations are performed if necessary to keep the tree in balance. This happens behind the scenes from the user which is why the tree is said to be self-balancing. The AVL tree introduces a few new concepts that we will spend more time looking at. Balancing of course will be looked at in depth. For balancing the function correctly, some new structural information is needed. This information includes the height of the tree, the balance factor and whether or not a tree is left or right heavy. It is this new pieces of information that are used to determine when and how to perform the balancing operations.

Unbalanced Trees
In the first algorithms and data structures course in the binary tree module, we created an unbalanced binary tree. While a binary tree is a very useful data structure, when unbalanced, it comes with a significant performance issue. An unbalanced binary tree can, depending on the order of the values are added to the collection, become a linked list. And when this happens, we don't realize the primary benefit of the structure, reduced search time. Let's look at an example of an unbalanced binary tree becomes a linked list by adding increasing integer values into the tree. So we'll start with an empty tree and then we add the value one. The value one is now the value of the tree's right node. Next, we'll add the value two, and since two is larger than one, it becomes the right child of one. And we'll add three and four. Now, even with only four nodes, it's easy to see that this tree structure looks a lot more like a linked list. And as long as we keep adding larger and larger values, they will continue this pattern of being added as the rightmost node's right child, continuing our linked list. Now that we have a small unbalanced tree, let's search for the value four. We'll start by searching for four at the tree's written node. Four is greater than one so we walk to the right. Four is greater than two and three. And eventually, we get to the node with the value four. Now, we can see that four comparisons were necessary. And we're in a tree with four nodes. So the performance of our unbalanced binary tree search was O(n) exactly the same as a linked list. Now, you might say that this is not likely to happen. But it's not hard to come up with scenarios where it could. Imagine you wanted to store every word in the English dictionary in memory so that input could be checked against the dictionary words quickly. A tree structure would make sense since it offers quick search characteristics in general. However, if you built the structure by loading the words in the English dictionary from the letter A to the letter Z which is likely how they appear in the input file, and then unbalanced tree would end up storing the items in exactly the structure, a linked list. And this is precisely what you want to avoid when search speed is the primary goal of the structure.

Balanced Trees
Now that we understand the problems associated with an unbalanced binary tree, let's look at how a self-balancing binary tree addresses them. The main point is that after any operation that changes the tree's contents, for example, node in search insertion or deletion, the tree performs a balancing operation. During the balancing operation, a height constraint is enforced. The constraint says that the height of the left and right sub-trees or the distance from the root node to its deepest children will always differ by at most one. Let's take a look at what we mean. What we have here is the binary tree we saw in the previous slide. This is an unbalanced tree which has become effectively a linked list. The root node has no children on the left. But on the right, it has a child with the value two. So relative to the root node, that child has height of one. The value two has a child whose height relative to the root node is two. And its child has a height relative to the root node of three. So the left height of the root node is zero as it has no children. And the right height of the root node is three. The difference between these is three. And since we said earlier that the balancing operation will ensure that the difference between the height of the left and right sub-trees will differ but most one, we know that this tree is unbalanced. So what would a balanced tree look like? Well, let's create a balanced tree using the same increasing integer values we saw on the previous slide. One through four, and will insert these values in the same order, one, two, three and four. We start by adding the root node of the value one. Since this node has no children, its left and right height are zero, and this means the tree is balanced. Next, we add the value two. Now, the root node has left height of zero and a right height of one. Since they differ by only one, the tree is still balanced. When we add the value three, things start to look a little different. Now, we can see that the tree has a left height of zero and a right height of two. Since a self-balancing tree ensures that the height difference will never be more than one, we know the balancing algorithm needs to run. And when it does, we have a tree with the same values one, two, and three, but which is balanced. The tree has a left height of one and a right height of one. Now, you can see that the value two has replaced the value one as the root node. And we'll discuss more about how that happens in a future slide, but for now, you can just assume the balancing operation made that occur. When we add the value four, the value finds its place as the rightmost node and the tree now has a left height of one and a right height of two. And the difference between them is only one so the tree is said to be balanced. Now, let's search for the value four within our balanced tree. So we start at the root node. The root node is two and two is less than four, so we go to the right. Three is also less than four so we go to the right, and then we find our match at the node with the value four. We can see that three comparisons were necessary. In an unbalanced tree, four comparisons were necessary. Now, this might not seem like a huge difference, but as more and more nodes are added to the tree, the height of the tree will remain relatively stable. And the benefit of the balanced tree will become more and more pronounced as search operations are performed.

Balanced Insertion
Insertion is the first tree operation that can cause tree balancing to occur. The mechanics of the node insertion are identical to what was seen in the binary tree module of the first algorithms and data structures course. Lesser values are added to the left and greater or equal values are added to the right. After the insertion operation completes, the balancing operation runs for the inserted node and for each of its parents. Let's look at a quick example where we add the values four, two and three to a self-balancing binary tree. Four is the first node added and becomes the root. Because four has no children, its left and right height are zero, so it is balanced. Because it has no parent, the balancing operation ends. Two is added and becomes a left child of four. Because two has no children, it is balanced. Two's parent, four, has one child, a left height of one and a right height of zero, so it is also balanced. Now, we add three to the tree. Three becomes a right child of two. Three is balanced because it has no children. Three's parent, two, has a right height of one and a left height of zero, so it is balanced. Two's parent, four, has a left height of two and a right height of zero, so it requires balancing because the difference between its left and right heights is two which is greater than one, which is our threshold for balancing. Since we know that four needs to be balanced, the balancing operation will run at the node four. Once the balancing operation is complete, three is the new root node and the tree is now balanced.

Balanced Deletion
Deletion is the second operation that can cause the balancing operation to occur. Like insertion, deletion is identical to what was seen in the binary tree module of the first algorithms and data structure course. The node to be deleted is found and its children are moved to retain the tree's rules. Once the deletion operation is complete, the balancing operation runs for the parents of the deleted node and for all successive parents up to the tree root. Let's look at an example where we are deleting the value four from an existing tree structure. The algorithm we're going to follow is at first, the node of the value four will be found. The node with the value four will be deleted and then the balancing operation will be performed for the parent node up to the root. When we start, we have a tree whose root node has a left height of one and a right height of two. Since the heights differ by only one, the tree is balanced. The value four is found using the binary tree search algorithm. Once the node is found, it will be deleted. With four gone, we look to its parent node, the node of the value five. And this node now has a left height of zero and a right height of two. Since these heights differ by two, the tree is unbalanced and needs to be balanced. The balancing operation is going to rotate the tree in a way that results in the tree being balanced. When complete, the tree has a left height of two and a right height of one. And you may notice the root node has changed. It was previously five and it's now seven. The algorithm by which this occurred will be discussed later in this module.

AVL Tree Class Overview
At this point, we have a pretty good understanding of what it means for a tree to be balanced. So now we can start learning how to make that happen, specifically by looking at the implementation of an AVL tree. Now, what we have here is the rough outline for an AVL tree class. We can see the class is named AVL tree and it's a generic class and it's a generic class taking a generic argument type T. Now, T is constrained to be IComparable, and this is necessary because we want to be able to know if a value is greater than, equal to, or less than other values in order to know how to find it, add it, or delete it from a tree. We also, for convenience, make the collection IEnumerable. Within the class definition, we have a couple of classes of methods. First, we have methods that allow us to modify the contents. For example, add and remove. Next, we have some informational methods. For example, contains will tell us if a value is contained within the tree, this is our search algorithm. Finally, we have the traversal and enumeration methods. These provide traversal such as the in order, pre order and post order traversals. Now, majority of these methods are completely unchanged. They are exactly the same as they were in the binary tree module of the first algorithm data structure course. So we won't be going over their code because we've already done it in-depth previously. I would encourage you to refer back to that course if you have any questions about how these methods work. Where there are differences we'll be diving in deeper and looking at the code more closely. A majority of the differences between an unbalanced binary tree and a self-balancing AVL tree occurs within the AVL tree node class. The AVL tree node class is similar to the binary tree node class but it provides the self-balancing operations. Like the AVL tree, it's a generic class that takes a generic type argument for the type being contained within the tree node. Additionally, the generic argument type is constrained to be IComparable. The AVL tree node class contains many of the binary tree properties, for example, left and right, these are the left and right children, additionally, a value. Now, there is a new one, and that is parent, and this is a pointer from any node to its parent node. If there is no parent, what should mainly the root node? It would be null. Now, with the balancing operations, the balancing methods we can see are left and the right rotation and left-right rotation and right-left right rotation, and we'll look at those in greater detail in this module. Finally, we have some support properties, and we'll learn what each of these mean and we'll see the code for them. But for right now, you can see they define things like left height and right height, those are things we've looked at in this module already. But then we also see balance factor in state, these are things that we'll talk about in just a moment. So what I'd like to now is start breaking these methods and properties down piece by piece and learning exactly how they function.

AVL Tree Properties
Let's start digging into the AVL tree by examining some of the new properties and methods on the AVL tree node class. Like the binary tree node, the AVL tree node contains left and right properties. These properties refer to the left and right children of the node. The difference between the left and right in the binary tree node class and the AVL tree node class is that the AVL tree node class needs to set the parent property of the child when the child is updated. The value property is unchanged from the binary tree class and simply returns the node's value. The parent is new to the AVL tree class and returns a pointer to the parent AVL tree node. As we saw earlier, node height is the distance from the current node to its farthest child node. The AVL tree class defines two private properties, left height and right height, which represent these values. Both properties defer to a helper method named MaxChildHeight which returns zero if the node argument is null, for example, if the left or right child are null, otherwise, it returns one plus the maximum height of the left and right children recursively. So if you had a tree that had a null left child, when MaxChildHeight was called, it would see that the left node was null and return zero. When right height were looked at, if it had a single right node, MaxChildHeight would be called with the right node being passed in. Since the node is not null, it would return one plus MaxChildHeight called recursively for its left and right nodes. If both of those nodes were null, those would return zero, so we would return one plus zero and we would get back a height of one. Take a little bit of time and understand how this works because it's really fundamental to understanding what a tree needs or it doesn't need to be balanced. Another new property is the balance factor. Now, we've been using balance factor in this module but we've not yet given yet a name. The balance factor of a node is the difference between its right and left node heights. When we've been saying that a tree is balanced if the difference between the node heights is no greater than one, what we've been talking about is the balance factor. Now, one thing to notice is that the balance factor can be positive or negative. And so far, we've only been talking about the absolute value of the difference. So for example, if we had a tree whose right height was zero and the left height were six, we would have a balance factor of negative six. Previously in the module, we would have just said a height difference of six. So why do we care about the sign of the value? Well, the problem with using the absolute value is that it does not tell you if the left or right side of the tree is unbalanced, only that the tree is unbalanced. Knowing which side is very important information. So we have a new property called state which returns an enumeration value that indicates if a node is left heavy, right heavy or balanced. A left heavy tree would mean that an unbalanced tree would have a height in the left that was more than one greater than the height on the right. And a right heavy tree has a right height that is more than one greater than the left height. And if the tree is not right or left heavy, it must be balanced. So what we've seen here is we know how to find the node's height by recursively looking at its children to determine what the right and the left node heights are. Then we take those two values and we figure out the balance factor. And from there, we're able to figure out if the tree is left or right heavy. So now we know a lot of information about the tree. Specifically, we know how to figure out if any node has unbalanced or balanced children. And once we know how to determine if a node or the tree is balanced or unbalanced, all we need to be able to do now is perform the balancing operations.

Node Rotation Introduction
Once we have added or removed a node from the tree and determined that the tree is now unbalanced, we need to balance the tree using one of the balancing operations. Balancing is performed using the technique known as node rotation. Now, as we've seen in the earlier slides, rotation occurs at the point of the insertion or deletion and is repeated for the node parent up to the root node. The goal of rotation is to change the physical structure of the unbalanced tree to make it balanced while always respecting the constraints of being a binary tree. Namely, that smaller values go in the left and equal or large values go in right. And over the next few slides, we are going to look at the four types of rotation that are used to balance an AVL tree, right rotation, left rotation, right-left rotation and left-right rotation.

Which Rotation Algorithm?
Before learning how to perform the four rotations, we have to figure out which one to choose. Now, there's four possible options. So what we're going to do is look at the tree that we want to balance. We are going to say, if the state of the tree is that were right heavy, let's look at the right side of the tree. If the right child is not null and the balance factor is less than zero, so what we're saying here is that we have a right heavy tree with a left heavy right child. We're going to perform a left-right rotation. Otherwise, the tree is just right heavy. And if it's right heavy, we do a left rotation. Conversely, if we have a left heavy tree and the left child is right heavy, we're going to perform a right-left rotation. Otherwise, if we simply have a left heavy tree, we'll just perform a right rotation. Now, as we see each of these rotations in action, we'll see why need to choose a right-left versus a right. We'll see cases where performing simply a right or a left rotation will actually leave the tree unbalanced just like it was before but with a different structure. And we'll see how the right-left and left-right rotations help us get out of those scenarios where we have an unbalanced to an unbalanced rotation and take us from unbalanced to a different type of unbalanced and then to balanced.

Rotation Algorithms
The first type of rotation we will look at is right rotation. Right rotation is an algorithm that rotates a node and its children to the right. Doing this has three operations. First, the left child rotates to the right, replacing its parent in the tree. It is now the new root of the tree structure being rotated. The right child of the new root is assigned to the left child of the old root, and this is done to keep the values in the proper order within the tree. And finally, the previous root, which we know has a greater value than the current root, becomes the new right child of the root node. That's a lot to take in, so let's look at visually. Here, we have an unbalanced tree. At the root node, we can see we have a left height of two and a right height of zero. This means the tree is left heavy and we need to perform a right rotation. So there's three steps we need to perform. Let's do them now. First, we find the left child. This child will become the new root. Now, you can see that the node that was the previous root, four, is kind of floating off to the side, it's no longer really a part of the tree. Now, we need to take the right child of the new root and assign it to be the left child of the old root. Now, the left child of the old root used to be the value two, so the left child is free to be assigned too, so we'll do that. Now, notice within these two sub-trees, all the binary tree constraints are still enforced. three is less than four so it's on the left, and one is less than two so it's on the left. Finally, the previous root, which is the value four, becomes the new root's right child. So, we can see here the right rotation is complete and we now have a tree, which contains the original four values, but whose left height is one, whose right height is two, and is now, considered balanced. Left rotation is pretty much the opposite of right rotation. We're going to take the right child and it becomes the new root. The left child of the new root is assigned to the right child of the old root and the previous root becomes the new root's left child. Again, it's a lot to say, so let's it visually. Here, we have pretty much the exact opposite case of the right rotation example. We have a node with the value one that has a left height of zero and a right height of two. This makes the node right heavy which means that a left rotation is necessary. So let's go through our three rules. First, the right child becomes the new root or the child is the node with the value three, so let's make that the new root. The node with the value one is now floating off to the side, waiting to be placed back into the tree. The next step, the left child of the new root is assigned to the right child of the old root, so let's do that. The left child is the value two, let's assign that to be the right child of the node with the value one. And the third step, the previous root becomes the new root's left child. So the previous root was the value one, so we're going to make that the left child of the value three. And now, we have our tree root node of three, whose right height is one, left height is two, the difference is only one so the tree is considered balanced. ( Pause ) Right-left rotation is where things get a little tricky. Right rotation can leave a tree unbalanced. So let's take a look at that. We have a tree with the values three, one, and 2. If we perform a right rotation, we're going to end up with a tree that is effectively mirrored. We haven't really done anything to balance the tree. We went from having a tree with a left height of two and a right height of zero to a tree with a right height of two and left height of zero. So we went from an unbalanced state to an unbalanced state. So what we need to do is perform a right-left rotation. So the first thing we're going to do is rotate the left child and then right rotate the updated tree. So let's start with our example again. We have our tree root node of three with the value one to its left and the value two to its right. So we are going to left rotate the left child, and the left child is the value one. So, we're going to perform a left rotation. And when we do a left rotation, we are going to end up with the value two as the new root and the value one as its child. And now, we have that linked list thing that we saw earlier in the module. So what's going to happen now if we right rotate the updated tree? Well, the value two is going to become the new root, and the tree is now balanced. So you can see we had a form where the tree was left heavy but we weren't able to do a right rotation to simply solve our problem. We needed to left rotate the left child and then right rotate the updated tree. So we're doing a right rotation of a left rotated tree which is why it's called a right-left rotation. And finally, we have the left-right rotation. Now, it should be pretty clear already, this is the exact opposite scenario, what we just saw. A left rotation can also leave a tree unbalanced. Here, we have a tree with the values one, three to its right and two to the three's left. If we perform a left rotation here, we're going to end up with the tree effectively mirroring its structure. That wasn't helpful. So now, we've ended up with a structure that is, again, unbalanced. So, what we need to do is right rotate the right child and left rotate the updated tree, so the exact opposite of what we did in the right -left rotation. So, let's look at our example and perform our right rotation at the node with the value three. So, we find that node and when we do that right rotation, what's going to happen is everything is going to roll to the right and that two is going to become the new root with three being a child of it. Now, that we have done that, we again have our linked lists sub-tree that we've seen before. And what we're going to do now is perform a left rotation of the entire tree structure. And that is going to put two at the root and one and three as its children. So, just like the right-left rotation and the left-right rotation, we are taking a structure, and straightening it out, and then rotating the whole thing. And this is all to prevent the case where the tree would remain unbalanced using either the right of the left rotations

Demo: Visualizing AVL Trees
to see how the heights between the two trees are affected by balancing versus being unbalanced. We'll look at some cases of random data. We'll look at some manually entered data and we'll look at some pathologically bad data. So what I've brought up is our sample application. And the first thing I want to point out is that the sample application to draw the graphs is using a library from Microsoft Research which is freely available and I'll include references to it later on. This library is known as the GLEE Library. And one of the things about GLEE that will impact the visualization is that it doesn't understand the binary tree rules of smaller values on the left, larger values on the right, it just understands graphs. So, during the visualization process, you may notice times where it appears that the tree is storing smaller values on the right or larger values on the left. When in fact, all we're seeing is an artifact of the graph drawing library. So, I just want to call that out so that if you see that, there's no confusion and just understand that at the moment, this is pretty much the best freely available library I could find that was usable within this demo that everybody watching this can go out and get and try on their own. I didn't want to use a library that people have to either license or use a commercial demo version. This is something anyone is welcome to use anytime. So, what we're going to do is add a few values to our tree. Let's start by adding the values one, two, three, and four. These are values we've seen quite a few times in our examples. So here, we have a tree with the value two is the root, one on its left, three on its right and four. Now, you can see that the straight arrow makes it difficult to tell if the four is on the left or the right, but we know from our understanding of binary trees that it's on the right. So what would happen if we remove the value one? Well, we know if we remove the value one, the tree will go from having a left height of one and a right height of two to having a left height of zero and a right height of two and it will be unbalanced and therefore will need to be balanced. And what I predict will happen is because we have a right heavy tree, we're going to see a left rotation which should leave three as the new root. So let's remove the value one and that's exactly what happened. We perform that rotation and now, we have a root tree of the value three with two and four as its children. So let's clear that out. Now, what I wanted to do is repeat the same exercise looking at a binary tree. So we're going to add the values one, two, three, and four. And you can see we have the values one, two, three, and four in that linked list structure that we expect. Now, we would expect this to draw, when you think about a binary tree, towards the right but because the graphing library doesn't understand those rules, it's drawing it as a straight line. So, let's remove the value one, and we still have a linked list. So given the exact the same inputs, we can see that an AVL tree and a binary tree operate very differently. In here, we can go back and remember what the AVL tree looked like. So let's clear both. And now I'm going to add 100 random values to the tree. So add 100, and we can see that we have a tree that has a very uniform depth. It starts off as a single root node and then it becomes very broad. And as it becomes broad, very few nodes start to become the terminal children. We have a very broad tree but with the binary tree, it's quite a bit different. We can see the height difference is significant. Here, we have a height of one, two, three, four, five, six, seven, eight, nine, ten, eleven, twelve, thirteen, and fourteen. So our maximum height here is fourteen. In our AVL tree, the maximum height is one, two, three, four, five, six, seven, and eight. That's a significant difference. It would take a lot of nodes in an AVL tree to ever hit a height of fourteen. In fact, we can keep adding nodes and seeing this happen. And as we add hundreds of more nodes, the tree gets wider and wider but it doesn't really ever get deeper. Now, we go to our binary tree and we can see it's a different story. So let's clear the tree and let's add a pathologically bad 100 nodes. And what these means is that we're going to add 100 nodes from the value one to the value 100. And in an AVL tree, we see a broad rather shallow structure. But when we look at a binary tree, we see a linked list. With this structure, we can come in and start removing some nodes. For example, I can see this node right here is the value 31. So let's remove that. Now, the tree structure is changed but the overall depth and breadth hasn't, and in the binary, we still have a linked list. So I hope that this sample has helped you kind to visualize the differences between a binary tree and an AVL tree or more generally, between an unbalanced tree and a balanced tree. This sample and all the source code for it is included with the exercise files with this module. If you have access to those, I would encourage you to look at them and see how it works. Additionally, there will be a reference to the GLEE library and the references slide at the end of this module.

Summary
During this module, we've learned how and AVL tree works and seen the differences between balanced and unbalanced trees. We started by looking at the binary tree. We recalled some of its behaviors and we looked at the difference between a balanced and unbalanced binary tree. Next, we moved on to a specific implementation of a balanced binary tree, the AVL tree. Once we understood the basic AVL tree, we looked at its four balancing algorithms, right rotation, left rotation, right-left rotation, and left-right rotation. And finally, we looked at the sample application that showed us to visualize some of the differences between a balanced and an unbalanced tree. For anyone who'd like information about AVL trees or just trees in general, there are some great resources online. Wikipidea has excellent articles on AVL trees, binary trees, and tree rotation. If you like more information about the binary tree class, the first algorithms and data structures course includes an in-depth binary tree overview that covers the binary tree, its algorithms and goes through the source code. And finally, we have the GLEE library which is part of the Microsoft Automated Graph Layout project at Microsoft Research. This is the library I used to visualize the binary and AVL trees within the demo application. If you have any interest in this, I would encourage you to visit the Microsoft research website and look at the project.

String Searching Algorithms
Introduction
Hello, and welcome to the Algorithms and Data Structures II Course. My name is Robert Horvick and in this module, we will be learning about string searching. Before diving into the string searching algorithms, we are first going to look at the string searching API we'll be using. This API will provide a common framework for implementing the search algorithms and for creating a sample application later in the module. The first algorithm we are going to look at is the naive search algorithms. We will start by taking some time to understand how the algorithm works and then look at some code and finally, we'll discuss the performance considerations of using this algorithm. Next, we will look at the Boyer-Moore-Horspool Algorithm. This is a simplified version of the Boyer-Moore Algorithm. Like the naive algorithm, we'll first learn how the algorithm works and then we'll look at some code and finally, we'll discuss the performance considerations. Last, we'll look at a demo application that uses both algorithms to perform string search and replace operations. This will allow us not only to see the algorithms in action but also compare their performance.

API Overview
Since we are going to be implementing multiple algorithms that will be performing a similar operation, in this case string searching. I would like to start by defining an interface that each algorithm will implement. This will allow us to use either or both algorithms more easily when we implement our sample application. This is not necessarily meant to serve as a best practice example of how string search and algorithms should be implemented but it will serve our needs well. Each algorithm will need to implement the IStringSearchAlgorithm interface. This interface defines a single method search. This method takes the string to find, the string to search within, and it returns in I enumerable of the search matches. We also need to define the ISearchMatch interface which is returned by the search method. This interface represents a single instance of a search match and it contains the start index of the match and the link of the match. With these 2 interfaces in mind, we can now implement the string search and algorithms in a uniform manner and this will allow us to use them interchangeably.

Naive Search
The algorithm starts by using a for loop to enumerate the string to search within from start to end. Along the way, the variable start index contains the current string index to search from. Within each iteration of the for loop, a local variable keeps tract of the number of characters that have been matched. Next, a nested loop begins. This loop compares the contents of the string to find against the string to search within. If the character does not match, the while loop exists and the for loop continuous to search at the next character. However, if the character does match, the match count as incremented. If the length of the string to find matches the number of characters matched, then we know the string to find was found. If the length does not match, then we know that we are one step closer to finding the match and we should iterate the while loop one more time. Let's look at this algorithm in action. By searching for the word "drop" in the sentence "Phil dropped his phone. " So we start with the sentence, Phil dropped his phone which is the 2 search string. The string we want to find or the to find string is the word drop. So when the for loop begins, start index will be set to 0. That will be the letter P in the string Phil dropped his phone. Match count will now be set to 0. So when the while loop begins, we're going to compare the start index 0 plus match count 0 so that 2 search string index of 0, the P compared to the to find string with the match count of 0 as the index. That is going to be the letter D. Well, the letter D and the letter P did not match, so the while loop will end and will go back up to the next iteration of the for loop where start index will be set to 1. So now, we're going to compare start index of 1 which is the H, match count would be set to 0 so that to find string is now searching for D again. Well, D is not equal to H and this will continue. D is not equal to I nor to L or to the space but once we get to start index 5, we're going to find that the D in the string to search will match the D which is the start of the string to find. So, we'll enter the while loop. Match count will be incremented to 1 and we'll compare. Well, is the length for equal to match count 1? No, it's not. So let's continue the while loop. Now, we incremented match count to 1 so now, we're going to compare the start index 5 plus match count 1 which is 6 and that is the R in Phil dropped his phone against index 1 of the string drop which is also an R. So match count will be incremented to 2 but the lengths still don't match. So we'll start the while loop again. The O will match but the length still won't match and we'll go one more iteration and the P will match. Now match count will be bumped up to 4, the length of the string to find is 4, match count is 4, we have found a match. So that's how the naive search algorithm walks forward one character at a time in the outer for loop and then uses a nested while loop to do the comparisons. Now, this nesting of loops should give you a little bit of concern and we'll talk about that when we talk about the performance of this algorithm. Let's go take a look at some code. The first thing you should notice is that we're using the search method signature from the IStringSearchAlgorithm interface that was introduced earlier in this module. The algorithm starts with some basic validations. First, that the parameters are not null and next, that the parameters contain data. Once these basic checks have been performed, the for loop we saw in the previous slide begins. Now, there's one subtle difference in the code, however. In the code, there is a simple optimization where start index is being compared against the difference between the length of the string to search and the length of the string to find. This is done because there's no reason to continue searching for a string whose length is longer than the remaining characters of the string to search within. For example, if the string to find is 10 characters long but there're only 9 characters left to search within, then it is impossible for a match to occur. So the loop can end. Once inside the for loop, things should look very familiar. The match count variable is declared and set to zero, the while loop begins and compares the current character of the string to find against the current character of the string to search within. If a match is found, the match count is incremented and if the length and math count are equal, the match is yielded to the color via the I enumerable result. The start index is now incremented so that the matched value is skipped over in the next iteration of the for loop and this is where the while loop ends. Now, in case you're wondering, the reason the start index is incremented by match count minus 1 is because start index will be incremented by 1 when the loop continues. So to skip just the matched value, we want to subtract 1 and then we'll add that 1 back on in a moment.

Naive Search Performance
The naive search algorithm is simple to implement but as we've seen in the past, that simplicity can come at a performance cost. On average, a single match has a cost of O(n plus m) where n is the length of the string to search within and m is the length of the string to find. Now, what we're measuring here is the number of comparisons that need to be performed on average before the first match is found. Worst case performance however is O(n times m) and it should be easy to see how this number can get very large very quickly. And one important characteristic of the naive algorithm is that it does not require any preprocessing to operate. We will see what the Boyer-Moore-Horspool algorithm that this is not always the case. Now, because of the performance factors described above, the naive search algorithm is most appropriate when the string toSearch and the string toFind are both relatively small.

Boyer-Moore-Horspool Search
The Boyer-Moore-Horspool algorithm is a simplification of the Boyer-Moore algorithm developed by Robert Boyer and J Moore. At a very high level, the search algorithm tries to minimize the overall cost of the search by skipping as many characters as possible. This algorithm does this by operating in 2 stages. The first stage is the preprocessing stage. During this stage, a table is built that contains the number of characters that can be skipped any time a mismatch occurs. This table is known as the bad match table and we will see how to build it shortly. And the classic Boyer-Moore algorithm generates the bad match table as well. However, it additionally generates a good suffix table. The good suffix table is also used to determine the number of characteristic that can be skipped when a mismatch occurs. If you are interested in learning more about how this table is built and used, a reference to more information will be provided later. Once stage 1 is complete and the table is built, stage 2 will perform the search algorithm by comparing the string to fine against the string to search for. Only instead of comparing the string to find from left to right, it will compare it from right to left. This allows more for efficient bad match character skipping when a mismatch is found. The bad match table is what determines how far the string to find can slide when a mismatch occurs. The algorithm is very straightforward. First, the length of the string being found is stored as the default length to skip when a mismatch occurs. Next, each character is added to the bad match table such that the character index returns the value that is the number of characters that can be skipped upon the next mismatch. Let's look at building the table for the example word "truth. " First, we set the default value to the length of the pattern. In this case, the length is 5. So looking at the table we can see that anytime we get an index that we don't recognize represented by the question mark, we can skip the entire length of the string being found. Next, we allocate the dictionary that will hold the remainder of the table and finally we walk the string from left to right and set the distance to skip to be the difference between the pattern length and the current index minus 1. So let's work through the word truth. The first pass through adds the value T to the table, T being the first letter in the word truth. Now, it gets the index value of 4 and it gets that because the pattern length is 5 minus I which the first iteration through is 0 minus 1, so 5minus 0 minus 1 is 4. Next, we add the value R with an index value of 3. In this case, I is 1, pattern length is still 5, so it's 5 minus 1 minus 1. U gets added with the index value 2 and now we encounter T again. This time the value for T is 1, pattern length of 5 minus the index 3 minus 1. So we overwrite the value 4 in the table and we insert the value 1. Now, you might notice that the table does not contain the letter H, the last character in the word truth. This is because if a mismatch occurs on the last character and if the character only appeared once in the pattern, then the string should slide by the total value of its length. Since that is the default behavior, we simply omit it from the table. Now, with the table built, we can go see an example of the search algorithm.

Boyer-Moore-Horspool Example
In this example, we are going to use the Boyer-Moore-Horspool algorithm to search for the word truth in the phrase, "We hold these truths to be self-evident. " Previously, we saw how to build the bad match table for the word truth. We will now be using that table. The algorithm starts by aligning the word truth to the start of the phrase. As I mentioned earlier, the comparison is not done from left to right but rather from right to left. So we start by comparing the H in the word truth with the O in the word hold. These characters did not match. The bad match occurred with thick letter O in the word hold so we can solve the bad match table and we see that O does not exist in the table. So the default value of 5 has returned and we skipped the word truth forward 5 characters. Now, let's take a second to understand why that works. Since O does not occur anywhere in the word truth, there's no reason to ever compare O to any character in truth so we can skip all the way pass it and in doing so, we also get to skip pass all the characters that preceded. So you can see we've been able to perform one comparison operation and skip ahead 5 characters. It would have taken the naive algorithm 5 comparisons to get to the standpoint. Now we have a more interesting case. Again, we compare the string to find from right to left. So we see that the H in truth matches the H in these. So we move one character to the left and see that the T in truth matches the T in these. Going once more to the left, we see that the U in truth does not match the space character so we have a bad match. When we consult the bad match table, we see that the space character does not exist. The default value 5 has returned and we skipped forward 5 characters. Now, we are comparing the letter H in truth with the letter T in truths. Now, this is a mismatch but unlike before, the value T does exist in the bad match table with the value of 1. What happens now is that we are aligned in the mismatch T with the T in the string truth. So we'll slide 1 character to the right. With the Ts aligned, we start comparing at the end of the word truth again, this time comparing the H in truth with the R in truths but this is a mismatch and again, the R exist in the bad match table so we look at the value, we find the value 3 and we slide everything forward 3 characters aligned in the Rs. With the Rs aligned, we now match the letter H and then we match the letter T, U, R, and T and we've now used the Boyer-Moore-Horspool algorithm to match the word truth in the phrase we hold these truths to be self-evident.

Boyer-Moore-Horspool Code
Let's take a look at the Boyer-Moore-Horspool algorithm code. This is the stage 2 algorithm we discussed earlier. First, the bad match table is created. We looked at the constructor for this class earlier to see how it was created. Once the table is created, the outer loop that controls the algorithm flow started. Inside the loop the number characters left to match is set and the inner matching loop starts. This inner loop is performing the right to left matching of the string to find against the string to search. Every time a match is found, the variable character left to match is decremented. Once the loop exits, there are 2 possible cases. The first is that the character left to match has gone negative. This indicates that a complete match was found and should be returned to the caller via the yield statement. The other case is that a mismatch was found. In this case, the bad match table is consulted and the current start index is adjusted by the offset returned. This loop continues to run repeatedly until the entire string to search has been searched.

Boyer-Moore-Horspool Performance
The performance of the Boyer-Moore-Horspool algorithm has some interesting characteristic. The performance improved with the length of the string being searched for and this islogical when you remember that the default shift length is the length of the pattern. So the larger the pattern, the more potential that exist for skipping more characters. The best case performance is O(n divided by m) where n is the length of the string to search and m as the length of the string to find. Worst case performance is O(n times m). Unlike the naive algorithm, it is possible to create pathological bad scenarios. So when it's all set and done, the Boyer-Moore-Horspool algorithm and really any member of the Boyer-Moore family is acceptable as a general purpose string search in algorithm.

Demo: Search and Replace
Now, I'd like to take a look at a sample application that uses both the naive and Boyer-Moore-Horspool algorithms to perform a search and replace operation. The sample application will also measure the number of comparisons that occur which should help make the performance difference between the 2 algorithms clearer. Let's start by reviewing the phrase we've been using throughout this module. We hold these truths to be self-evident. What we're going to find is the word truth and we will replace it with the word truth in upper case. I'll click the replace button and we can see that in both cases with the naive and the Boyer-Moore-Horspool algorithm that the lower case word truth was replaced with the upper case word truth. We're also able to see that 37 comparisons were made in the naive algorithm and only 15 in the Boyer-Moore-Horspool algorithm. So as you can see both algorithms produced the same output but the naive algorithm performed more than twice as many comparison operations. Now, let's try a larger data set. In this case, I will use the entire declaration of independence. The text at the phrase we've been using comes from. Now, for reference, the text I'm pasting is 9, 108 characters long including spaces. So we will again be searching for the word truth and replacing with the upper case word truth. Now, we can see by scrolling down, that the word truth was replaced with the upper case word truth and we can also see that 10, 007 naive comparisons were made whereas only 2, 236 comparisons were made by the Boyer-Moore-Horspool algorithm. So you can see that the search and replace function correctly. Now, at this time, the na�ve algorithm made more than 4 times as many comparisons. Now, let's try one more thing. Let's lengthen the string that we're going to find. Recall that the Boyer-Moore-Horspool algorithm generally improves its performance as the string to find gets longer. So now, we'll search for the phrase "truths to be self-evident" and we simply live truth as our replacement text because that's not really relevant to the performance characteristics. So we will again click replace and we can see that "truths to be self-evident" was replace with the word truth but the performance's work is interesting. The naive comparison was nearly 10, 000 whereas less than a thousand Boyer-Moore-Horspool algorithm comparisons were done. Again, we can see the algorithm succeeded but this time, the Boyer-Moore-Horspool algorithm performed 92 percent pure comparisons than the naive algorithm.

Summary
In this module, we have learned about string searching using the naive and Boyer-Moore-Horspool algorithms. We started by reviewing the interface that both algorithms would implement. Next, we look at the naive search algorithm. We saw how the algorithm worked. We looked at the source code and saw live example and then we learned about its performance characteristics. After that, we looked at the Boyer-Moore-Horspool algorithm. We saw the algorithm in action. We learned how the bad match table was created and we discussed the performance characteristics. Finally, we looked at the sample application that performed the search and replace operations using both the naive and the Boyer-Moore-Horspool algorithms. We're able to see that the algorithms have identical behaviors but vastly different performance characteristics. More information about the naive, Boyer-Moore, and Boyer-Moore-Horspool algorithms can be found online.

Collection Concurrency
Introduction
Horvick: Hello. Welcome to the algorithms and data structures collection concurrency module. In this module, we're going to be learning about how concurrency issues affect collection usage, and we will see several ways to address concurrency issues. We will start this module by briefly discussing what concurrency is, and then we will look at a specific issue where a q collection can return the same item twice due to a race condition. Next, we will look at several different solutions to collection concurrency. We will look at caller synchronization, synchronization using monitor locks, and synchronization using reader/writer locks. Finally we will look at what. Net offers in terms of concurrent collections.

Multi-Threaded Overview
Let's start by reviewing what concurrency is. Concurrency is when multiple instructions execute at the same time. Now there are three common scopes at which concurrency issues exist. The first is when a single process uses multiple threads. It is important to understand that it does not matter if this process is running on a multiprocessor or a multicore machine or not. Even on a single processor system where only one thread can execute at a time, concurrency issues or race conditions can occur. Multiple processes on the same machine can also have concurrency issues. While it is unlikely though not impossible that multiple processes are sharing the same in-memory collection, it is more common for multiple processes to share a common resource, such as a file. A third situation that involves concurrency is when multiple independent systems each have a dependency on a common resource, for example, accessing a common table in a database, and the key point here is that concurrency is not an issue that only affects a single process. Concurrency issues abound in nearly every type of application. This module, however, will focus specifically on the scenario of a multithreaded application accessing a shared in-memory collection from multiple threads. Let's start by taking a quick look at single threaded execution. In this example, we are going to see jobs created added, to a queue, and then processed as they are dequeued. The code starts by creating a queue of jobs. Next it creates 1, 000 jobs and adds them to the queue. The job class in this example simply records the job ID, and in the process method, it does a very simple spin wait to kill some time. Now a spin wait is done rather than a sleep to keep the processing thread active rather than releasing it for another thread to execute. This more accurately simulates a job performing some compute intensive operation. Finally, while the queue still contains jobs, the jobs are dequeued one at a time and processed. In this example there are no concurrency issues. The queue is not shared among multiple threads, and the job class does not use any external resources. Now let's take a look at a multithreaded example. This time we will create the queue of jobs just like before, only we will create multiple threads that execute the jobs in parallel. There are two main changes in this example from the single threaded example. First, multiple threads will be created. So we need to track these threads, and we will need to execute the job processing within a delegate that the thread executes. Second, the job class has been modified. We will see that the job class will now detect if a job has been executed twice. Finally, we will see that race conditions exist when multiple callers access the queue at once. These are the same code snippets we saw in the single thread example only with modifications from multithreaded execution. The queue was created and populated with jobs. This is unchanged from the single threaded example. An array of threads is allocated, and the thread delegate used for job processing is defined. You can see that this delegate contains the same loop that checks the queue to see if there's anything in it and then dequeues and runs the job. The threads are allocated and added to the array of threads and then started. This gets the job processing moving forward. Last, the threads are enumerated and the join method is called. The join method blocks until the thread has finished executing, and when all the threads are finished executing, the job processing is complete. Finally we can see the modified job class. An object to be used as a synch lock has been added as well as a Boolean that indicates that the job has been processed. When process is called, the lock is taken to enforce exclusive access to the enclosed code. If the job has already been processed, an exception is thrown. But if the job has not been processed, the spin wait function executes to simulate work, and the processed field is set to true to indicate that this job has been processed. So what's the problem? Well, let's walk through an example. We start with a queue that has 4 jobs in it with job ID's 1, 2, 3, and 4, and we have two threads running, each of which is executing the loop defined in the thread callback delegate. Thread 1 starts executing and dequeues job 1. Then it is interrupted by the thread scheduler, and thread 2 takes over. Thread 2 dequeues job 2, and it is then interrupted giving control back to thread 1. Thread 1 processes job 1, and then thread 2 processes job 2. So far, so good, right? Well, let's take a look at what dequeue does. The code for dequeue comes from the queue module of the first algorithms and data structures course. This is an example of the queue backed by an array. If you want to learn more about this collection type, a URL of the course is given on the references slide at the end of this module. Now thread 1 begins executing and starts by starring off the queue's current head item into a local variable named value. This is the job that will be returned from the dequeue method. The code that follows adjusts the value of head so that the next call to dequeue will return the next job in the queue. Thread 1 then continues on to the if statement before being interrupted by the thread scheduler. With thread 1 interrupted, thread 2 now calls dequeue and starts by saving off the head value from the items array into a local variable. This is the job that thread 2 will have returned to it from dequeue, and we can see that both thread 1 and thread 2 are going to dequeue the same item because thread 1 was interrupted before it got a chance to update the value of head. This is a very serious race condition. Now thread 2 has not been interrupted so it continues executing, and it processes job 3. Thread 1 then takes over, and when it attempts to process job 3, the job detects that it has already been processed and throws an exception. Now this is just one example of a race condition that exists in the code shown on the slide. There is actually another race condition. Now if you don't see it, don't worry. We'll cover it later in this module, but I'll give you a hint. It has to do with the thread delegate while loop.

Caller Synchronization
Now that we've seen that multithreaded access to the queue collection can cause race conditions to occur, what can we do about it? Well, if we cannot modify the queue class, for example if we're using the. Net framework queue, we're going to need to perform synchronization on the caller or the collection consumer side. When using caller synchronization the collection makes no effort to be thread safe. This is often done to simplify the design of the collection, and also because deferring to the caller imposes no overhead on the collection, especially when it's being used in a non-concurrent manner. So what options does the caller have? In the. Net framework, there are several. Monitors exposed via the lock key word in C sharp are one very popular option. Additionally a mutex, (inaudible), or reader/writer lock slim could be used. Notice I suggest the reader/writer lock slim instead of the reader/writer lock. Now this is based on guidance from Microsoft, and there are several factors involved. The reader/writer lock class has a more complex upgrade and downgrade interface, and it is more prone to certain deadlock conditions, and it's also significantly less performant than the reader/writer lock slim class. So the reader/writer lock slim class should be favored in all new code. Now that we know what our options are, let's see an example. In this example, we're going to use a monitor via the lock key word to protect the collection from race conditions. The benefits of this approach is that non-thread safe collections can be used safely in a multithreaded environment. The downsides though are that the caller is responsible for all thread safety issues and that when using monitors, there's no distinction between actions that modify the collection and actions that only read from it. The effect of this is that operations that might be inherently safe to execute in parallel without locking, such as peak, will be serialized due to the locking. Let's see how we might implement caller synchronization using a monitor. The first thing we need to do is allocate an object that will service our locking object. Next, we're going to modify the code to add a lock statement to protect the dequeue operation. You can see on the right in the code snippet, I've modified the inside of the loop to add a lock statement that occurs after the while but before the dequeue operation. This will serialize the calls to dequeue and prevent multiple threads from attempting to dequeue at once. This code has some problems though. The first is the bug that I alluded to earlier in the module. The problem is that count needs to be included in the lock scope. Now why is this? Well, it's because any time you're performing an action based on a decision such as checking a collection count, you need to ensure that the decision point cannot change between when it was checked and when the action is performed. It is possible that after count was checked, another thread would take control of the collection and remove the last item from it. Now when this thread resumes, it will attempt to call dequeue and there won't be anything left in the collection. There's also a performance issue. The performance issue is that the call to process is occurring within the lock scope, and this means that we're holding the lock for longer than we need to. The lock should be held for only as long as necessary, and in this case, that means only for the time it takes to check the count and dequeue the item. So this code won't work for us. We need to change the scope of the lock to include the count, and we need to not hold the lock for longer than is necessary. Now in this modified code, notice that the very first thing the while loop does is check the count, but we're not inside of a lock. Now you might be saying, "Hey, you just told me I had to check that inside of a lock. " But don't worry. What we're doing here is ok. This is actually a common pattern called double checked locking. Now if this first check in the while loop passes, we'll enter the while loop. Once we're inside the while loop, we'll take the lock, and then inside the scope of the lock, we will then make the check again. So it is possible that when the while loop executes, the count of 1 will be returned and will enter the while loop. Then we'll take the lock, and maybe that count has been decremented to 0 now because another thread had control. So in the if statement, we'll check the count one more time, and if we're still greater than 0, we will dequeue the item. Once we've dequeued the item, we start into a temporary variable. We no longer process right off the dequeue operation. Then we leave the lock scope. So within the scope of the lock, all we did was check count and dequeue. We didn't perform any processing. Now that we've left the scope of the lock, we check to see if job to process is not null. The case where it would be null is where another thread got in and emptied the queue before we had a chance to call dequeue. If the job was dequeued successfully, so if job to process is not null, we will call the process method, and then we'll go and start the while loop over. Now checking that count again outside the lock scope, like I mentioned before, is safe in this context because we're using double checked locking. So here we've seen how monitors can be used to safely enforce caller side synchronization of a non-thread safe collection.

Monitor Synchronization
Synchronization can also be performed within a collection itself. In this example, we'll look at providing thread safety within the collection using a monitor. Locking will be done at the method level, for example, peak inqueue and dequeue. Now the benefits of this approach is that the caller does not need to implement method level blocking, but the cons are that it is deceptively simple, and that just like with caller synchronized code, readers will block other readers. Now to make the collection thread safe using a monitor, the first thing we'll need to do is allocate the object that will be our locking object. Next we add method level locking to the queue collection methods. For example, here we've added the lock statement to the dequeue method. This same lock would be used in the other methods such as inqueue and peak. This will ensure that only one thread is accessing the collection at a time, but as was mentioned previously, this is deceptively simple. Method locking does not mean the caller can stop thinking about concurrency issues. Just like in the caller synchronize example, the usage of count to determine whether to call dequeue still presents a problem because the lock is not held from the time count is called until dequeue completes. In the window where is not held, another thread could modify the queue contents. Now we will see how the. Net framework addresses this issue later in the module.

Reader Writer Lock Synchronization
When a collection will have a large number of readers and not many writers, a monitor might not always be the right choice. In this example, we'll look at providing thread safety within the collection using a reader/writer lock, specifically using the. Net framework reader/writer lock slim class. As with monitor locking, locking will be done at the method level. Now the benefits of this approach is that the caller does not need to implement method level blocking and that readers will no longer block other readers, but again, the cons of that is that it's deceptively safe, just like the monitor implementation, and also the reader/writer lock solution, even using the more performant reader/writer lock slim class, may still be slower in certain circumstances. This will really become more noticeable in the very fast read only methods such as peak where the cost of allocating and releasing the reader lock may end up being greater than the cost of performing the entire peak operation. To make the thread collection safe using a reader/writer slim instance, the first thing we need to do is allocate the lock instance. Next we need to add method level locking to the queue collection methods. For example, here we've called the inter write lock method and then added a try-finally where the finally block releases the write lock, and just like the monitor example, the usage of count to determine whether to call dequeue still presents the problem because the reader/writer lock is not held from the time count is called until dequeue completes. In the window word where it is not held, another thread could modify the queue. Again, we'll see how the. Net framework addresses this issue later in the module.

.NET Framework Concurrent Collections
If you have written much multithreaded code using the. Net framework, you are probably keenly aware that the common collections such as list and queue are not thread safe. In. Net4, however, the. Net framework was changed to include several new collection types that are thread safe. These include the concurrent dictionary, a key value pair similar to the dictionary class, concurrent queue and concurrent stack, a first in-first out and last in-first out collection similar to the queue and stack classes, and concurrent bag. This is an unordered collection similar to a hash table. While these classes are similar to existing classes, they are not drop-in replacements. For example, concurrent queue has try dequeue rather than dequeue. Now why might this be? It's because of the pattern we have seen throughout this module where the queue count is checked and then dequeue is called. This pattern required client side locking even when using a thread safe collection. By adding a try dequeue method, that logical pattern of dequeuing if an item exists can be implemented using a single thread safe call without any client side locking. When writing new code that requires thread safe access to collections, these new types should generally be favored. Now interesting note is that concurrent queue and concurrent stack are both lock free collections. Now what this means is that they do not use a locking mechanism such as a monitor or a reader/writer lock to provide thread safety. Rather they use a combination of interlocked operations to perform operations atomically and spin locks rather than locks that might release the thread context to perform waiting operations. Understanding and implementing a concurrent collection is a major undertaking, and there are some excellent books written on the topic. I will list some references at the end of this module. So this final example, we can see how the. Net framework concurrent queue class is used to implement the sample we've used through this module. We can see that the queue type has been replaced with the concurrent queue type, and that within the while loop, no locking is performed and dequeue has been replaced with try dequeue. These two changes were all that was necessary to make use of a performant and tested thread safe collection.

Summary
In this module, we discussed concurrency issues and how they relate to collections. We started by getting a better understanding of concurrency issues and looking at single threaded versus multithreaded processing. Next we looked at 3 different approaches to addressing concurrency issues-caller synchronization and method level collection synchronization using the monitor and reader/writer locks. Finally we discussed the Newton current collection classes in the. Net framework, and we saw how the concurrent queue class can be easily used where a thread safe access to a queue is needed. If you have an interest in learning more about writing or using thread safe collections, there are several places you can start. The MSDN documentation for the monitor and reader/writer lock slim classes have many practical examples that can get you started. Additionally Microsoft has provided documentation for the Newton collection current classes. The queue implementation used in this module is covered in more detail in the first algorithms and data structures course, and the book, "Concurrent Programming in Windows, " by Joe Duffy, is probably the best guide to understanding concurrency issues on Windows and creating lock free collections in the. Net framework. The website listed for Joe Duffy's book is actually his personal website. If you go to the (inaudible) URL, bluebytesoftware. com, you'll find his blog where for the past several years he has discussed all the concurrency issues I have talked about and gone into great detail on lock free implementations. If you're looking for an online resource, I would definitely recommend that as a starting point.

Course author
Author: Robert Horvick	
Robert Horvick
Robert spent nearly 10 years at Microsoft creating software that made it easier for everyone else to write software. Most recently Robert worked on Team Foundation Server on the...

Course info
Level
Intermediate
Rating
4.5 stars with 459 raters(459)
My rating
null stars

Duration
2h 30m
Released
11 Jun 2012
Share course