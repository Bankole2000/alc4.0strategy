Elastic Cloud Infrastructure: Containers and Services
by Google Cloud

In this course, Elastic Cloud Infrastructure: Containers and Services, You will differentiate between the GCP options and tools available for development and collaboration.

In this course, Elastic Cloud Infrastructure: Containers and Services, learners explore services provided to applications built on GCP that enhance their scalability and maintainability. First, you will work with services like Google Cloud Pub/Sub and Google Cloud Functions to make applications more efficient. Next, you will explore the use of containers on GCP. Finally, you will discover the functions of the Kubernetes Engine, App Engine, and Containers on Compute Engine. By the end of this course, you will have foundational knowledge and skills in Elastic Cloud Infrastructure's Containers and Services.

Course author
Author: Google Cloud	
Google Cloud
Google Cloud can help solve your toughest problems and grow your business. With Google Cloud, their infrastructure is your infrastructure. Their tools are your tools. And their innovations are your...

Course info
Level
Intermediate
Rating
0 stars with 8 raters
My rating
null stars

Duration
0h 42m
Updated
17 Jan 2019
Share course

Welcome to Elastic Infrastructure: Containers and Services
Elastic Cloud Infrastructure: Containers and Services Course Intro
Hello, and welcome. My name's Jasen Baker with the Google Cloud Platform. This course is Elastic Cloud Infrastructure: Containers and Services. So what does that mean? Well, let's start with containers. Essentially, containerization is something that Google's has been doing for the last 12 years. It's essentially that hybrid between virtual machines and pure Platform as a Service. Now you can deploy your code in a very nice little package with its dependencies. But the great thing is, it's portable. It can run on-premise. It can run in other cloud providers. Especially, it runs really well in Google Cloud Platform. So with containerization in general, we're going to introduce something we call Kubernetes, which is essentially an outsourced version of the internal system we call Borg. Borg essentially finds a home for your containers to live. Kubernetes helps you to automate and orchestrate where your containers live. If you want to take it even further, you can install Kubernetes. You can go to kubernetes. io. It's an open-source project. You can install that on-prem or other individual cloud providers as well. We have a managed service version of that, and that's called Google Kubernetes Engine. So now we've automated a lot of that infrastructure process, but we've added some additional enhancements. Since Kubernetes has to run anywhere, it can't really integrate as well with native cloud provider features. Now if you took some of our other courses, you may have learned about autoscaling, our firewall rules, load balancers, etc. Google Kubernetes Engine integrates into all of those. And so it takes advantage of our global network. It can set up the firewall rules for you, load balancing, autoscaling rules, and you're also doing this with containers, so you really get a huge benefit running on Google Container Engine. Now then, what about our services? What are they? Well, first of all, they can be the application infrastructure. This could be our Platform as a Service. These are where you're deploying your own code: Java, Python, Go, PHP, you name it. Perhaps you want to take advantage of microservices, so you have Cloud Functions. We also have Cloud Endpoints, in which you can publish your APIs. And plus, there's a number of other managed services, Google Cloud Pub/Sub for messaging services between your applications, and thousands of individual APIs that are available to you and your applications, such as Google Maps API, Google Analytics, being able to pull calendar events from Google Calendar. And of course, a number of different Cloud APIs from machine learning APIs, vision APIs, sound, etc. You can go ahead and use our API Explorer to find out more, but there is going to be a great deal covered inside of this individual course.

Application Infrastructure Services
Module Overview (Intro)
Hello, and welcome. This is Mylene Biddle with the Google Cloud Platform. I'm a Google Cloud course developer, and this is module one, Application Infrastructure Services. Application infrastructure means technologies that are commonly needed by any distributed, cloud-based, globally scalable application. Rather than require you to create this part of your application yourself, GCP offers it as an integrated service. For example, every distributed application needs a method for the separate parts to communicate with each other. GCP offers Cloud Pub/Sub to provide said communication without requiring you to install, configure, and maintain software that does this. In this module, we will cover Cloud Pub/Sub, API management, Cloud Functions, Cloud Source Repositories, and specialty APIs. There is no hands-on lab in this module, but there will be solution-driven demonstrations throughout.

Cloud Pub/Sub
Cloud Pub/Sub is a fully managed real-time messaging service that allows you to send and receive messages between independent applications. Cloud Pub/Sub is an independent, scalable, managed messaging queuing service that will guarantee delivery of all of those individual messages. It will hold on to that data for up to seven days. It does not guarantee first-in, first-out, so you're not guaranteed to get things in order. Cloud Pub/Sub can be the single ingest for all of your different data points. Imagine you had a number of different IoT devices. They could all publish data to Cloud Pub/Sub, and then you could actually subscribe to and pull down these messages as needed. Here are some of the benefits of Cloud Pub/Sub. It is a globally managed service with extremely low latency. You can dynamically rate limit so you can throttle exactly how often or how much Pub/Sub will push those messages. Pub/Sub provides end-to-end reliability because we're going to acknowledge at least one guaranteed delivery and receipt of each individual message. Everything is completely encrypted as well, and its maintenance is free, or you simply just pay for exactly what you use. Cloud Pub/Sub uses two levels of indirection between the publisher and the subscriber. This decouples the sender's transmission of the message from the receiver's receipt of the message. Starting here at 1, a publisher wishes to publish a message. A message is simply data in transit through the system. The message consists of a payload and optional attributes that describe the payload. The message is published to a specific topic. A topic is a feed of messages. The topic stores the message, ensuring availability and reliability. The message is transmitted to one or more subscriptions. A subscription is an entity that represents interest in receiving messages. The subscription determines which subscribers are registered to receive the message and queues up the messages to be sent. Subscribers can either receive the message through pull or push. The subscribers either receive messages by Pub/Sub pushing them to the subscriber's endpoint or by pulling them from the service. pull subscribers using HTTPS requests to Google APIs. Push subscribers use webhook endpoints that can accept post requests over HTTPS. The message arrives at the subscriber, where it is consumed. After the message is consumed, the publisher sends an acknowledgment to the subscription. The subscription registers each delivery. When all of the deliveries are complete, it removes the message from the queue. Here's a workflow including multiple publishers and multiple subscribers. Two publishers publish messages on a single topic. Publisher 1 sends message A. Publisher 2 sends message B. The topic aggregates their messages. The messages are stored before they are delivered. The messages are sent to specific subscriptions. In this example, there are two subscriptions, 1 and 2. Each subscription forwards the information to the subscriber and only the information requested. Subscriber 1 gets message A. Subscriber 2 gets message B. Subscriber 3 gets both messages A and B. For each message received, the subscriber sends an acknowledgement here, here, and here. The subscription responds to the acknowledgement by removing the message from the queue. This diagram illustrates the flow of messages through connections. The system that receives the message from the publisher on behalf of the topic is called the publishing forwarder, and the system that receives the message from the publishing forwarder and ensures delivery to subscribers is the subscribing forwarder. This diagram illustrates the different integrations for publishing and subscribing. The many possible integrations allow you to create the appropriate messaging solution for your needs. On the slide, we see some common Cloud Pub/Sub use cases. Balancing workloads and network clusters. For example, a large queue of tasks can be efficiently distributed among multiple workers, such as Google Compute Engine instances. Implementing asynchronous workflows. For example, an order processing application can place an order on a topic from which it can be processed by one or more workers. Distributing event notifications. For example, a service that accepts user signups can send notifications whenever a new user registers, and then downstream services can subscribe to receive notifications of the event. Refreshing distributed caches. For example, an application can publish invalidation events to update the IDs of objects that have changed. Logging into multiple systems. For example, a Google Compute Engine instance can write logs to the monitoring system to a database for later querying and so on. Data streaming from various processes or devices. For example, a residential sensor can stream data to back-end services hosted in the cloud. Also, reliability improvement. For example, a single zone Compute Engine service can operate in additional zones by subscribing to a common topic to recover from failures in a zone or region.

API Management
Let's learn about API management using GCP services. Oftentimes, applications will communicate with each other, especially without having to authenticate with Active Directory or other strong means of authentication. So the application wants to provide a service, and that service will only be available through an endpoint, so we can expose an API. Google Cloud Endpoints helps you create, deploy, protect, monitor, analyze, and serve your APIs using the same infrastructure Google uses for its own APIs. Use any language and framework to write an API. Add an OpenAPI Specification or gRPC API configuration, and Cloud Endpoints will monitor and protect your API. Here's an example for a mobile backend. Let's say you have users that need to do very small, synchronous interactions with your application. Basically, they can publish or communicate with that Cloud endpoint, which can then be processed say by App Engine and a number of other different data sources to get back to that individual customer. Cloud Endpoints can be the forefront of where your communications will start, and then, of course, it can publish to other back-end application servers. The nice thing here is you've got that mobile backend API, so you don't need to set up an entire environment just to handle all of the incoming connections from your different locations. It is a managed service. It automatically scales up and down for you, and this way, you have a back-end service constantly running with your Cloud Endpoints only available whenever users are trying to interact with it. You don't want to have to depend on creating autoscaling engines just because a couple hundred phones came online, an airplane landed, and then everybody's checking your application as soon as they get on. Cloud Endpoints can be a nice, suitable environment for setting that up. Apigee is an API interface that wraps your APIs and provides analytics and data transformation and validation. The use of Apigee is outside of the scope of this course, but more information can be found in both the online documentation, as well as in the courses in our application development track.

Cloud Functions
Let's learn about GCP's serverless offering, Cloud Functions. Cloud Functions is a microservices architecture. You can think of it as a way to do lightweight event actions using JavaScript. So instead of setting up an environment, imagine if you could just publish JavaScript to do something based on a trigger. Here's the official definition of Cloud Functions. Google Cloud Functions is a lightweight compute solution for developers to create single-purpose, standalone functions that respond to Cloud events without the need to manage a server or runtime environment. Cloud Functions is a lightweight, event-based, asynchronous compute solution that allows you to create small, single-purpose functions that respond to Cloud events without the need to manage a server or a runtime environment. Triggers can be configured using Cloud Pub/Sub, HTTP, and Cloud Storage. You deploy functions from a Cloud Storage bucket, GitHub, or a Bitbucket repo, and they are written in JavaScript running in Node. js. Cloud Functions also benefit from Stackdriver integration. Here is a sample comparison between Cloud Functions and Cloud Endpoints. Cloud Endpoints exposes an array of endpoints or API functions, whereas Cloud Functions exposes a single endpoint. The Cloud Endpoints backend is an app engine backend, so you have a long-running programming environment with full access to complex data and storage services. In Cloud Functions, you have one single piece of code that accepts a limited input, executes rapidly, produces some output, and then exits.

Demo: Cloud Functions
All right, so in this demo, I'm going to show you how to create and deploy a cloud function using the gcloud command-line tool. I'm going to use Cloud Shell. So I've already started Cloud Shell. And so what I'm going to need to do is I'm going to need to set up a Node. js dev environment, so I'm going to install the Google Cloud client library for Node. js. I'm going to do that by typing in this command, then wait for that to install. That's installing everything that I need so that I can start writing a function in Node. js and all of the dependencies required. If you see any warnings, you can go ahead and ignore them. So now I'm going to create a local directory in this Cloud Shell environment for the function code. I'm just going to call it gcf_hello_world. Now I'm going to navigate into that. And then so what I'm going to do next is I'm going to use nano, which is a text editor, to create an index. js file in this directory, and I'm going to paste in some code. So nano index. js. Paste this in. So what this code is doing is it's just a simple function. It's called helloGET. Let me actually go ahead and show you the code. So in the code, this helloGET function, all it does is it responds to a GET request via HTTP, and it shows the text Hello World. There could be many other applications to this as we discussed in the modules of a response to I put a new file into a bucket, and then my cloud function resizes it into a thumbnail based on the HTTP GET. So let me go ahead and X out of this. So now what I'm going to do is I'm going to deploy the function. And if you'll recall, Cloud Functions is in beta, so we have to specify gcloud beta functions deploy. My function name is helloGET. I'm going to specify that the trigger is HTTP. This can take a couple minutes to happen. Take about 2 minutes for this to fully deploy. All right, you can see here it's actually starting to do something. It's telling me it's going to take about 2 minutes deploying the function. It's going to start displaying a couple of parameters as it deploys this cloud function, and what we're looking for is when it's completely deployed, for it to spit out the HTTPS trigger URL so that we can look at that. So we'll wait for that to pop up. It says that the deployment is done. Here is my HTTP trigger, so I'm going to go ahead and copy that value. Create a new tab and click Enter, and you can see my Hello World. Really fancy. So as you can see, it's quite simple to deploy a serverless application using Cloud Functions. Our function responded to a GET request via HTTP, and it responded with the text Hello World with no server management needed.

Cloud Source Repositories
Next, we're going to discuss Cloud Source Repositories. Google Cloud Source Repositories provides Git version control to support collaborative development of any application or service, including those that run on App Engine and Compute Engine. If you are using Stackdriver Debugger, you can use Cloud Source Repositories and related tools to view debugging information alongside your code during application runtime. Google Cloud Source Repositories also provides a source browser that you can use to view your repository files from within the GCP console. You can easily create a repository using the listed gcloud commands. Repositories can be a mirror of a hosted GitHub or Bitbucket repository. Here's additional information about Cloud Source Repositories. In the previous slide, we mentioned that repositories can be a mirror of GitHub or Bitbucket repositories. When you push a change to the connected repository, it automatically syncs to the Cloud Platform repository.

Specialty APIs
The last topic we'll cover in this module is the specialty APIs available to you through GCP. You can easily get amazing functionality into your applications by leveraging the services and APIs already supported in the larger Google Cloud, beyond the Google Cloud Platform APIs. As you can see in the slide, the list is extensive. Using very little coding, you can add cool features to your application by leveraging the machine learning API and specialty APIs. Some demonstrations consist of as few as six lines of code. Specifically, the Cloud Speech API provides speech-to-text functionality in over 80 languages and variants in real-time streaming or batch modes.

Module Review (Outro)
The application infrastructure services we've covered can help you grow your application in directions you might not have considered. For example, if you have a successful application, wrapping the API with security, monitoring, and documentation can enable third-party development on top of your solution, turning it from an isolated application into an extensible platform and enabling business partnerships. We hope you enjoyed the module.

Application Development Services
Module Overview (Intro)
Hello, and welcome. This is Mylene Biddle with the Google Cloud Platform. I am a Google Cloud course developer, and this is module 2, Application Development Services. In this module, we will discuss Google App Engine. App Engine handles all the front-end and back-end scaling transparently, so all you need to do is focus on the application code.

App Engine
Google App Engine is a Platform as a Service and gives you infinite autoscaling. Google App Engine standard environment is a fully managed environment, and it allows you to scale down from nothing to thousands of different modules and instances running at the same time. Google App Engine flexible environment is more flexible because it's based on Docker containers. It gives you VM exposure so you can SSH into the box, and then as a result, you can install whatever language you want because it's now your container. In the flexible environment, you're managing containers instead of the underlying development environment. To follow up on a slide we have used throughout our architecture courses, you can see where App Engine standard environment and App Engine flexible environments are positioned compared to other GCP compute and processing options. App Engine gives you the ability to deploy your own code, but take advantage of the built-in feature set of autoscaling your managed servers. Here's an overview of App Engine. We have a built-in load balancer that's already front-ending your applications. Any type of load balancing configurations you have set with Google App Engine are already built in. You don't necessarily have access to local storage, and so pretty much everything is done through APIs for external access. You could be making calls to other services, such as Cloud Datastore for your back-end infrastructure and perhaps Cloud SQL for relational database tasks. You can take advantage of a number of built-in functions such as task queues, which you can use for launching other external tasks, whether they be in real time or in batch. You can also utilize external Cloud Storage, as well as Cloud Memcache. There's actually a number of other additional APIs you can take advantage of, but these are some of the common architectures that take advantage of the external services available to your front-end application on App Engine. Services are a modular abstraction in App Engine, a way to break up the application into separate parts. Instead of architecting your entire application inside a single App Engine service as shown on the pervious slide, you can implement your application in multiple services. You could have more than one service in a project. You can also use multiple projects to further isolate services. Splitting traffic to different versions enables incremental rollouts and A/B testing. Each service can have multiple versions deployed simultaneously. Code is completely separate in versions, services, and projects. Services in a single project share some resources, for example, Datastore, Memcache, and task queues. Services in separate projects are completely isolated. On the IAM level, you can assign different roles at the project, but not at the service level. When services are in the same project they are isolated in some ways and then share certain resources. Code in one service can't directly call code in another service. Code is deployed independently. Each service can be written in a different language. And then with autoscaling and load balancing and the machine instance type that you saw on the previous slide, they're all independent per service. Here is an overview of features you should consider when choosing between the App Engine flexible and standard environments.

Module Review (Outro)
The Google App Engine flexible environment supports Docker containers. In the next module, we're going to look at Google Kubernetes engine, which is a container-first system. App Engine makes it easier and faster to get an application up and running since it handles the infrastructure and lets you worry about your application instead. If you'd like to learn more about App Engine flexible environment, I urge you to look into the application development track available through GCP training.

Containers
Module Overview (Intro)
(Music) Hello, and welcome. This is Mylene Biddle with the Google Cloud Platform. I am a Google Cloud course developer, and this is module 3, Containers. The agenda for this module is as follows. First, we're going to give you an introduction to containers, then we're going to talk about Google Kubernetes Engine, which is actually a managed version of Kubernetes, an open source container management system. We will then discuss Google Container Registry. There's a sizable hands-on lab you will complete around Kubernetes load balancing. We will finish out the module by discussing how to answer the question of whether you want to use Google Kubernetes Engine, Google App Engine, or simply containers on Google Compute Engine.

Containers
Here's the slide we've seen a few times throughout our architecting courses to depict GCP Compute and processing options. Google Kubernetes Engine is really that blend of both Infrastructure as a Service and Platform as a Service. Benefits include automatic scaling of the cluster with a lot of the flexibility of being able to deploy preconfigured containers. Your developers can package up their applications and dependencies, and you as the administrator can publish or push those, or even delegate that control to them. Kubernetes Engine has a lot more built-in capabilities that we will discuss in this module, and you have less restrictions than you might have with other configurations. So, what is a container? Let's take a look at the history of virtualization. If you remember, most people were buying bare metal servers. That included the entire stack, your applications, dependencies, which sit on a common operating system, on top of some kind of hardware appliance. With this bare metal setup, there were huge amounts of inefficiency, like some people would actually buy a single-core processor with 4 GB of memory and pay 5 to 6 grand for a machine that was only going to run Active Directory. You could easily run many more applications on that hardware setup. But that was kind of the dedicated single server, single application approach. Running multiple applications could sometimes create conflicts between different dependencies, OS versions, etc. So then came along virtual machines, and what they did is they abstracted that hardware requirement so that you could have a single piece of hardware, but many different versions of operating systems with many different dependencies and applications running independently of each other. This allowed you to purchase larger pieces of hardware and still run just as many components as you did before on a much smaller hardware footprint. The problem with this though is the fact that you have this big, bloated operating system, which was often many times larger than the actual applications running on top of it. And what purpose did the operating system really serve except to give access to the underlying hardware? You've got this small hypervisor hardware kind of abstracting that layer between as well. With containers, you're bundling the application code and dependencies into a single unit, abstracting the application from the infrastructure. Containers package your applications so they can run in any environment that supports containerized workloads. Container-based solutions give you the ability to manage applications, not machines, and maintain vendor independence. You can write the code once and run it anywhere. Container-based solutions make it easier to migrate to new platforms and help you decouple applications from dependencies.

Kubernetes Engine
Now we'll learn about Google Kubernetes Engine. Google took upon their 12 years of experience managing containers and made an open source project, which we call Kubernetes. Kubernetes is Greek for helmsman, or pilot, and it allows you to navigate where these containers will exist in the environment. It is based on internal Google systems and is a complex project that we only cover the basics of in this course. In fact, we are only covering Kubernetes Engine in this class. For more information go to kubernetes. io. Kubernetes can run anywhere and it was designed to run anywhere. You can run Kubernetes on your laptop, you can run it on-premise, you can run it in any other cloud provider. Google Kubernetes Engine allows you to run all of your Docker format containers, but we can handle a lot of the autoscaling. We integrate natively with Stackdriver, so you have logging and monitoring. You can integrate with VPN. So if you're running Kubernetes on-premise, we can also run Google Kubernetes Engine, which will manage Kubernetes for you. We can also integrate with Identity Access Management. Google Kubernetes Engine can automate the setup and deployment of an entire Kubernetes cluster, making it much easier to manage because it throws in all of those additional GCP services that aren't natively part of Kubernetes. Here are some more details about how Kubernetes works. At its core, Kubernetes will start with what we call a container cluster. The cluster consists of physical hardware resources. They are going to be virtual machines. A standard minimum cluster is going to have exactly one master to start with. That master is going to be the coordinator. It will be providing the API access to communicate with the individual nodes. Each of these individual nodes is going to be running a Docker runtime. They're going to be running a kubelet agent, which is going to handle the managing and scheduling. They also include a network proxy, so this way they can communicate internally with each other and with other containers inside of the virtual machines. We recommend a minimum of three node instances for standard high availability. Let's explore the Kubernetes master. The Kubernetes master is the cloud endpoint towards the Kubernetes cluster itself. It's running the Kubernetes API server and it handles all of the scheduling. It runs the health checks to ensure everything is running, and it offers all the cloud service integration for you. Google Kubernetes Engine is going to be running this master node on a VM that you do not see and do not manage. In our own environment though, you will set up your own Kubernetes master and individual nodes if you run Kubernetes by itself. When utilizing Kubernetes Engine, Google takes over this management node for you, integrates that with Stackdriver applications with IAM, and other automated processes that we'll discuss a little bit later. How are containers organized? They're organized inside of what we call a pod, which is an abstraction. It's similar to a target group when we discussed load balancers. In this case here, you create a pod where you can bundle multiple containers together that might be tightly coupled. We recommend you usually only run one container per pod because we aren't distributing containers, we are distributing pods across multiple physical nodes. A pod is going to be able to allow you to expose these applications through a public IP address. Containers in the pod share a single IP address and a single namespace so they can communicate with the other nodes in the cluster. A pod also gives you access to centralized storage. There is important information that needs to be shared, perhaps SSL certificate keys for example. Those can be stored on a central storage device. Some type of network attached volumes will be associated with these individual pods, and those containers will have access to them. You might recall that in load balancing you can load balance based on target pools or managed instance groups. In this case here, we will be load balancing using labels. A label is an arbitrary key value pair that you apply to pods and other objects. They are used by Kubernetes Engine to configure orchestration. For example, if you want to shut down all the production pods, you'd look up those labels and shut down matching pods. If you want to expand the pod of the production cluster, you'd do so by looking up labels. You can query based on those labels, giving you the ability to have full command and control over any pods that match those queried labels. Here's an example to explain the Kubernetes Engine service concept. The problem depicted is as follows. An application needs to communicate with a group of pods, A and B. What happens if a pod has to be restarted? When the pod restarts, it will be dynamically assigned a new internal IP. The solution would be the service has a persistent IP, which can be either an internal IP or an external IP. The application communicates with the pods through the service IP. The service is connected to the pods by label. If a pod has the correct label, it will be connected to the service. If a pod restarts, the pod's label causes it to be picked up by the service. Some other important points include services come in different types, they can expose internally, externally, and they can load balance incoming requests to the pods in the group. They combine the functions of a group and a load balancer. This slide depicts a Kubernetes Engine deployment. A deployment gives you the rules for how are you going to autoscale those individual nodes. It drives the number of pods that are actually running towards the ideal state of the number that we want to be running. In this example, we have a service which will sit in front that will create the target pool of instances. Then a deployment group defines exactly how scaling will happen. Here we can automatically spin up additional pods based on health checks, load balancing requirements, or other definitions. In this slide, we can see how pods are scheduled onto nodes. Once you have a deployment, we now specify a particular service we want to deploy, which consists of pods and individual containers. You can start to see the hierarchy. I can deploy a deployment onto individual nodes. The master node is going to be responsible for doing that, but it's going to equally distribute the number of pods across these nodes, depending on what your container cluster looks like. Remember, a container cluster is going to consist of physical nodes. These are going to be actual virtual machines, compute instances that you can go into your console and see. Deployments provide you with built-in resilience. If you have a pod or a physical node failure that has running pods, all of these pods will be eliminated. However, the deployment will have specific rules saying that we have a minimum of two pods running at all times. So if there's only one physical node, fill in the remainder number of containers onto the remaining node. So, when Node 2 fails, the deployment redeploys Pod B to another node in the cluster, in this case, Node 1. Another built-in feature is rolling updates, allowing you to upgrade systems to a new version. In this case, version 1 of the software is running on 3 separate nodes. The deployment is making sure we keep three pods running. We're showing a service in the diagram so you can see the IP changes, as well as the node changes. What we did here is launch a fourth pod to Node 3 with the new version of the application. Once it is operational, the service picks up the pod based on the label and begins serving it. We now have four pods, but only need three. So Deployment Manager shuts down the version 1 pod in Node 3. This process is continued until all three nodes have a pod with the updated version of the application. Another benefit of running Google Kubernetes Engine is the IAM support, so all of the things that you can do on a containerized environment can be command and control using the same level of security that you give to somebody to manage the network, to manage storage, or to manage other APIs. It gives you central access control when it comes to managing the rest of the platform objects. You can design container clusters so that they are spread across multiple zones for resiliency. When you enable multi-zone container clusters, the container resources are replicated in the additional zones and work is scheduled across all of them. If one zone fails, the others can pick up the slack. In this case, any single zone is capable of running the entire application. Node pools are instance groups in the Kubernetes cluster. They are much like a managed instance group. All of the VMs in a pool are going to be exactly the same. Pools can contain different virtual machines from one another, so pools could also be in different zones. The cool thing is Google Kubernetes Engine is node pool-aware, so based on the labels that you provided the pool, we can manage those accordingly inside of Kubernetes Engine. Kubernetes Engine will replicate all node pools and multi-zone container clusters, but you should be aware of any quotas associated with the region you're running in. Here are some more Kubernetes Engine features. Cluster federation applies to multi-region cluster containers. As an example of how you'd utilize cluster federation, if you're going to be running environments in Asia, Europe, and the Pacific, or even through other cloud providers, cluster federation will allow you to manage and deploy this in a multi-cloud solution. We also integrate with network load balancing and of course our cluster autoscaler. Here's some more information about cluster federation. Cluster federation is useful when you want to deploy resources across more than one cluster, region, or cloud provider. You may want to do this to enable high availability, offer greater geographic coverage for your app, use more than one cloud provider, combine cloud providers and on-premise solutions, or for ultra-high scalability. Cluster federation is also helpful when you want resources to be contactable in a consistent manner from both inside and outside your clusters, without incurring unnecessary latency or bandwidth cost penalties, or being susceptible to individual cluster outages.

Container Registry
Now we'll briefly discuss Container Registry. Container Registry provides secure, private Docker image storage on GCP. While Docker provides essential registry to store public images, you may want your images to be accessible to the world. In this case, you must use a private registry. The Container Registry runs on GCP, so it can be relied upon for consistent uptime and security. The registry can be accessed through an HTTPS endpoint, so you can pull images from any machine, whether that machine is a Google Compute Engine instance or your own hardware.

Kubernetes Load Balancing (Overview and Objectives)
Kubernetes Load Balancing (Review)
In this lab, you deployed a container application using Kubernetes and Google Kubernetes Engine. Then you tested the configuration of your application. You then undeployed it and redeployed it behind a Google Compute Engine HTTPS load balancer using the Ingress extension to Kubernetes. I hope you enjoyed applying the concepts learned in this module in a real world application.

Kubernetes Engine, App Engine, or Containers on Compute Engine?
Our last topic in this module will cover considerations to keep in mind when making the decision of whether you should deploy using Kubernetes Engine, App Engine, or deploy your containers on Compute Engine. There are a few differentiators you should keep in mind when deciding where to deploy your containerized applications. Containers on Compute Engine are a good choice when you absolutely want control over the VM resources while also benefiting from Docker image development. If you don't need Docker containers in your application, App Engine Standard environment might be a good option for you. You simply deploy the code and there's an infrastructure there designed to scale up as fast as your application requires. You should think of App Engine Flex as a developer-focused, no operations compute infrastructure offering that abstracts the underlying infrastructure to a large extent in favor of developer friendliness. Flex runs Docker containers in VMs under the hood, while providing convenient models of deployment and management. Flex provides an experience that makes it easy to take your code and directly deploy it to a production scale environment. It takes care of capturing logs, scaling, versioned upgrades, and traffic splitting without the developer having to be aware of containers, config, or settings. If your organization is already familiar with Docker and you want to run containerized workloads in production spread across pools of VMs, Kubernetes Engine may be a good fit. It provides deeper awareness of the underlying infrastructure than App Engine Flex, and may require an understanding of container orchestration concepts. Kubernetes Engine is also a great choice if you plan to run Kubernetes on-premises or beyond GCP. The good news is that if you start with App Engine Flexible environment and then decide you want to move to Kubernetes Engine, we make it really easy to generate the Docker file used under the hood by App Engine, which can be built into an image run by Kubernetes. This overview should give you a better understanding of how to choose the right deployment environment for your application.

Module Review
In this module, we introduced you to the concept of containers. We then introduced some basic concepts around Google Kubernetes Engine and Google Container Registry. You completed a hands-on lab where you deployed a container application using Google Kubernetes Engine and configured load balancing. If you'd like to learn more about Google Kubernetes Engine, I urge you to check out the online documentation and our other courses.

Elastic Cloud Infrastructure: Containers and Services Course Outro
This is Jasen Baker with the Google Cloud Platform, and I want to thank you for completing the Elastic Cloud Infrastructure, Containers and Services course. So, what did you learn? Well, you learned how to compute the way Google computes. Our developers write applications in containers. We have a containerization workload. We manage over 2 billion containers per week. You can now deploy your code in an automated fashion directly to the Google Cloud Platform. You also examined monitoring, and you implemented Kubernetes and Kubernetes load balancing. So now you have an idea of how to compute just like Google, plus the big benefit, you know what k8s means, right? You know that it's eight letters in between the K and the S. So now you can answer the trivia question, hey, what is C-3PO's real name?

Course author
Author: Google Cloud	
Google Cloud
Google Cloud can help solve your toughest problems and grow your business. With Google Cloud, their infrastructure is your infrastructure. Their tools are your tools. And their innovations are your...

Course info
Level
Intermediate
Rating
0 stars with 8 raters
My rating
null stars

Duration
0h 42m
Updated
17 Jan 2019
Share course
