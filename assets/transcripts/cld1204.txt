Essential Cloud Infrastructure: Core Services
by Google Cloud

In this course, Essential Cloud Infrastructure: Core Services, you will learn how to deploy solution elements, including infrastructure components such as networks, systems, and applications services, and more.

This course, Essential Cloud Infrastructure: Core Services, introduces you to the comprehensive and flexible infrastructure and platform services provided by Google Cloud Platform. Through a combination of video lectures, demos, and hands-on labs, you will explore and . This course also covers deploying practical solutions including securely interconnecting networks, customer-supplied encryption keys, security and access management, quotas and billing, and resource monitoring.

Course author
Author: Google Cloud	
Google Cloud
Google Cloud can help solve your toughest problems and grow your business. With Google Cloud, their infrastructure is your infrastructure. Their tools are your tools. And their innovations are your...

Course info
Level
Intermediate
Rating
0 stars with 4 raters
My rating
null stars

Duration
1h 53m
Updated
17 Jan 2019
Share course

Welcome to Essential Infrastructure: Core Services
Essential Cloud Infrastructure: Core Services Course Intro
Hello, and welcome. My name is Jasen Baker with the Google Cloud Platform, and this course is the Essential Cloud Infrastructure: Core Services. In this course, we're going to jump into Identity Access Management, or IAM. Here, we're going to cover things such as roles, groups, organizations, folders, and service accounts. Sounds exciting, doesn't it? Well, actually it's quite important because this essentially designs a hierarchy of exactly how your security model's going to be laid out. Such an example, do you want to give access to an entire team of individuals, or perhaps would you like to curate those roles and customize what individuals have access to what? Perhaps you're a large enterprise and you need to organize multiple individual groups under one core set of authority. Now, perhaps this authority doesn't necessarily want to have physical access to all the virtual machines, but they want to have control. Who individually has those? Centralized billing. Perhaps you even just want to automate everything. You don't want humans to have access to the account, so you might create service accounts, which will allow your applications to have the same credentials or act on behalf of a human to do some kind of automated function. Now, once security is in place, you have to determine where is your data going to go. We'll cover things like Google Cloud Storage. Now that sounds very simple; in fact, it is, but there is a number of feature-rich features that we're going to talk about. The first one is encryption. In many different countries, encryption is a big concern, so we have what we call customer-supplied encryption keys. Things like lifecycle management. What happens if data is deleted or we want object versioning? And so what that allows us to do is essentially have any number of backups, and then we can get into lifecycle management, which can determine, as data gets older, do we need to keep it around? Perhaps we might want to move it to a more affordable type of storage or delete it altogether for archive purposes. Object change notification. This is kind of a fun one not a lot of people think about. If you have billions and billions of files, what happens if something changes? Would your applications like to know about it? What if users uploaded 1, 000 pictures all at once, would you like to look for that or would you like to be told about that? Google Cloud Storage really encompasses a lot fantastic features that we're going to deep dive to really kind of make your applications function well. Now with that, we also have to worry about resource management. So this is going to include things like quotas. Anything you do in the cloud is going to cost money, so you need to understand exactly what your effect will have on billing. And so this is why we establish quotas, so essentially you don't shoot yourself in the foot. We always like to say that, you know, we'll give you the gun, but you have to ask for the bullets. Kind of a Google humor thing. But the other thing is labeling. Well, if you have all of these different resources, how are you going to export your bill and understand that these resources went to a specific team? So then we'll talk about resource labeling as well. Then finally, resource monitoring. This is where we can allow you to monitor all your resources in the cloud. You can create automated response systems, customized dashboards, interactive logging; all of this is taken care of through an application we call Stackdriver, and Stackdriver was built on AWS, so one of the benefits there is not only do you get monitoring and logging and all sorts of other great features on Google, but you can also extend this to AWS as well.

Cloud IAM
Module Overview (Intro)
Hello and welcome. I'm Philipp Maier, a course developer with Google Cloud Platform, and this is module 1, Cloud Identity and Access Management, or IAM. Cloud IAM is a sophisticated system built on top of email-like address names, drop tables, and granular permissions. If you're familiar with IAM from other implementations, look for the differences that Google has implemented to make IAM easier to administer and more secure. In this module, I will start by introducing Cloud IAM from a high-level perspective. Then we will dive into each of the different components within Cloud IAM, which are organizations, roles, members, and service accounts. I will also introduce some best practices to help you apply these concepts in your day to day work. Finally, you will get first-hand experience with Cloud IAM through our lab.

Cloud Identity and Access Management (IAM)
So what is Identity Access Management? It is a way of thinking; who can do what on which resource? The who can be a person, group, or an application. The what refers to specific privileges, or actions. And the resources could be any GCP service. For example, I could give you the privilege or role of compute engine viewer. This provides you read only access to get and list compute engine resources without being able to read the data stored on them. Cloud IAM is composed of different objects as shown on this slide. We are going to cover each of these in this module. However, to get a better understanding of where these fit in let's look at the Cloud IAM resource hierarchy. GCP resources are organized hierarchically as shown in this tree structure. The organization node is the root node in this hierarchy. Folders are the children of the organization, projects are the children of the folders, and the individual resources are the children of the projects. Each resource has exactly one parent. Cloud IAM allows you to set policies at all of these levels where a policy contains a set of roles and role members. Let me go through each of the levels from top to bottom, as resources inherit policies from their parent. The organization resource represents your company. Cloud IAM roles grant that at this level I inherited by all resources under the organization. The folder resource could represent your department. Cloud IAM roles granted at this level are inherited by all resources that the folder contains. Projects represent the trust boundary within your company. Services within the same project have a default level of trust. For example, the queue_a application shown here can access bucket_a, as they are part of the same project; project bookshelf. The Cloud IAM policy hierarchy always allows the same path as the GCP resource hierarchy. Meaning, if you change the resource hierarchy, the policy hierarchy also changes. For example, moving a project into a different organization will update the project's Cloud IAM policy to inherit from the new organization's Cloud IAM policy. Another thing to point out is that child policies cannot restrict access granted at the parent level. For example, if I grant you the editor role, for the bookshelf project, and I grant you the viewer role to the queue_a application, then you still have the editor role for that application. Therefore, it is a best practice to follow the principle of least privilege. This principle applies to identities, roles, and resouces. Always select the smallest scope that's necessary to reduce your exposure to risk.

Organization
Let's learn more about the organization node. As I mentioned earlier, the organization resource is the root node in the GCP resource hierarchy. This node has many roles, like the Organization Admin. The organization admin provides a user like Bob access to administer all resources belonging to his organization, which is useful for auditing. There is also a Project Creator role, which allows a user like Alice to create projects within her organization. I'm showing the Project Creator role here as it can also be applied at the organizational level, which would then be inherited by all projects within the organization. The organization itself is created by Google Sales, which includes establishing the organization owners. G Suite Super Admins, which we'll talk more about later, are the only organization owners. These organization owners can assign the Organization Admin role that we just discussed through the G Suite Admin console, which is a separate product. Organization administrators, on the other hand, manage GCP from the Google Cloud console. For security purposes, it's a best practice to always have more than one organization owner. Let's talk more about folders as they can be seen as suborganizations within the organization. Folders provide an additional grouping mechanism and isolation boundary between projects. Folders can be used to model different legal entities, departments, and teams within a company. For example, a first level of folders could be used to represent the main departments in your organization, like department X and department Y. Because folders can contain projects and other folders, each folder could then include other sub-folders to represent different teams, like team A and team B. Each team folder could contain additional sub-folders to represent different applications, like product 1 and product 2. Folders allow delegation of administration rights. So for example, each head of a department can be granted full ownership of all GCP resources that belong to their departments. Similarly, access to resources can be limited by folder, so users in one department can only access and create GCP resources within that folder. Let's look at some other resource manager roles, while keeping in mind that policies are inherited top to bottom. The Organization node also has a Viewer role that grants view access to all resources within an organization. The Folder node has multiple roles that mimic the organizational roles, but are applied to resources within a folder. There's an Admin role that provides full control over folders, a Creator role to browse the hierarchy and create folders, and a Viewer role to view folders and projects below a resource. Similarly for projects, there's a Creator role that allows a user to create new projects, making that user automatically the owner. There's also a project Deleter role that grants deletion privileges for projects. I recommend exploring the documentation link in the notes of these slides for a full list of resource manager roles. I previously mentioned that a G Suite Super Admin works in G Suite. Both G Suite and GCP are part of a single product line called Google Cloud, but they are separate products. The G Suite admin has two functions with respect to GCP. One, the G Suite Super Admin can assign GCP organization owner to an account from within the G Suite Admin console. A GCP organization owner can also create more organization owners by assigning the role to an account within the GCP console. The G Suite Super Admin cannot assign any other GCP roles to accounts from the admin console. Second, the G Suite Super Admin creates users and groups, controls memberships of users and groups, and controls Google hosted domains. From an authorization perspective, GCP uses Google's credential system, which is directly integrated into G Suite. You can sync existing credentials using Google Cloud Directory Sync and optionally implement single sign on. I will talk more about both of these in the coming slides. Using Google's credential system provides many built- in features like session activity tracking, session management tools, security alerts, and even suspicious activity detection. With Google Cloud Directory Sync, or GCDS, the G Suite admin can automatically add, modify, and delete users, groups, and non-employee contacts to synchronize the data in a G Suite domain with an LDAP directory server or MS Active Directory. The data in the LDAP directory server is never modified or compromised, making this a secure tool that helps keep track of users and groups. The G Suite admin uses the GCDS Configuration Manager to customize synchronizations and can perform test synchronizations to find what works best for the organization and then schedule synchronizations to occur when needed. As I mentioned earlier, GCP provides single sign on authentication. If you have your own identity system, you can continue using your own system and processes with SSO configured. When user authentication is required, Google will redirect to your system. If the user's authenticated in your system, access to Google Cloud Platform is given. Otherwise, the user's prompted to sign in. This also allows you to revoke access to GCP. If your existing authentication system supports SAML2, then SSO configuration is as simple as three links and a certificate, as shown in this screenshot. Otherwise, you can use a third-party solution like Ping or Okta. Let's look at some best practices in regards to what we've covered so far. We've already talked about the principle of least privilege. Remember to always apply the minimal access level that is required to reduce your exposure to risk. Since managing permissions for individual users can be cumbersome and error prone, I recommend using groups. For example, you could create a SecOps group for your security operations team that has multiple roles. For example, the Security Admin role to manage firewalls, and the Log Viewer role for auditing. When new members join the team, you just add them to the group. Policies allow you to secure your resources. You also want to make sure you control how additional users gain access to resources through policies and group memberships. Without strict control over policy changes and group memberships, you may inadvertently allow new users more permissions than they need, which also violates the principle of least privilege. Finally, I suggest auditing policy changes. The audit logs should record project level permission changes and additional levels that are being added. In regards to using groups, I recommend to let the G Suite admin handle membership to secure roles. That being said, for high risk areas, assign roles to individuals directly rather than using groups.

Roles
Let's talk more about roles, which define the can-do-what on which resource part of Cloud IAM. There are three kinds of roles in Cloud IAM, primitive roles, curated roles, and custom roles. Primitive roles are the original roles that were available in the GCP console. These are the owner, editor, and viewer roles. The owner has full administrative access. This includes the ability to add and remove members and delete projects. The editor role has modify and delete access. This allows a developer to deploy applications and modify or configure its resources. The Viewer role has read only access. All of these roles are concentric. That is, the Owner role includes the permissions of the Editor role, and the Editor role includes the permissions of the Viewer role. There's also a Billing Administrator role to manage billing and add or remove administrators. Each project can have multiple owners, editors, viewers, and billing administrators. In addition to the primitive roles, Cloud IAM provides additional curated roles that give granular access to specific GCP resources and prevent unwanted access to other resources. These roles are a collection of permissions. Most of the time to do any meaningful operations, you need more than one permission. For example, in this slide, a group of users is granted the Instant Admin role on project A. This provides the user of that group all the compute engine permissions listed on the right. Grouping these permissions into a role makes them easier to manage. Roles represent abstract functions and are customized to align with real jobs. The permissions themselves are classes and methods in the APIs. For example, compute. instances. start can be broken down into service, resource, and verb, meaning that this permission is used to start a stopped compute engine instance. These permissions usually align with the action's corresponding REST API. The third type of roles are custom roles. You can create a cloud IAM role with one or more permissions, but these roles are in a beta release as of this recording. So overall, we have organization, folder, project, and product-specific roles. The product-specific roles are crafted for each resource, and you can learn more about them in their product-specific Cloud IAM documentation. Please keep in mind that some of these roles are in beta, which will be indicated in the documentation. Let's dive deeper into project and product-specific roles. Project roles mimic the primitive roles in that there is a Viewer, Editor, and Owner role. All of these roles are again concentric. That is, the Owner role includes the permissions of the Editor role and the Editor role includes the permissions of the Viewer role. Besides the permissions listed here, a project owner can also invite others to become project owners. Doing so is as simple as sending an email invitation from the GCP console and having the new project owner accept the invitation as show here. Such an email is only set for project owner roles and not for other roles like assigning organization ownership. I already talked about product-specific roles. Here are some examples, like the Compute Engine Instance Admin, that allows to create, modify, and delete virtual machine instances and disks. Each product or service has several of these roles. I recommend to check out the Cloud IAM documentation to determine which of these roles you want to implement in your infrastructure.

Members
Let's talk more about members, which define the who part of who can do what on which resource. There are different types of members. There are users and there are service accounts. As I will cover service accounts later in this module, let's focus on users for now. Users can be Google accounts, G Suite domains, Google groups, or Cloud Identity domains. A Google account represents a developer, an administrator or any person who interacts with GCP. An email address that is associated with a Google account, such as a gmail. com address, can be an identity. A G Suite domain represents a virtual group of all the members in an organization. G Suite customers can associate their email accounts with an internet domain name. When you do this, each email account takes the form of username at mydomain. com. You can specify an identity by using an internet domain name that is associated with the G Suite account. A Google group is a named collection of Google accounts and service accounts. Every group has a unique email address that is associated with the group, like groupname@mydomain. com. Google groups are a convenient way to apply an access policy to a collection of users. You can grant and change access controls for a whole group at once instead of granting or changing access controls one at a time for individual users or service accounts. You can also easily add members to and remove members from a Google group instead of updating a Cloud IAM policy to add or remove users. Now, GCP does not create or manage users or groups. As I explained earlier, the G Suite admin manages users and groups for an organization in a separate product from GCP. If you want to take advantage of using a Google account, but are not interested in receiving mail through Gmail, then you can still create an account without Gmail. For more information on this, explore the link in the notes of the slides.

Service Accounts
As I mentioned earlier, another type of member is a service account. A service account is an account that belongs to your application instead of to an individual end user. This provides an identity for carrying out server to server interactions in a project without supplying user credentials. For example, if you write an application that reads and writes files on Cloud Storage, it must first authenticate to either the Google Cloud Storage XML API or adjacent API. You can enable service accounts and grant read/write access to the account on the instance where you plan to run your application. Then, program the application to obtain credentials from the service account. Your application authenticates seamlessly to the API without embedding any secrets or credentials in your instance, image or application code. Service accounts are identified by an email address, like the example shown here. There are three types of service accounts, user creator or custom, built in, and Google API service accounts. By default, all projects come with the built-in Compute Engine default service account. Apart from the default service account, all projects come with a Google Cloud Platform API service account, identifiable by the email project, dash, number, @cloudservices. gserviceaccount. com. This is a service account designed specifically to run internal Google processes on your behalf, and it is automatically granted the Editor role on the project. Alternatively, you can also start an instance with a custom service account. Custom service accounts provide more flexibility than the default service account, but they require more management from you. You can create as many custom service accounts as you need, assign any arbitrary access scopes or Cloud IAM roles to them, and assign the service accounts to any virtual machine instance. Let's talk more about the default Compute Engine service account. As I mentioned, this account is automatically created per project. This account is identifiable by the email, project number, dash, compute@developer. gserviceaccount. com, and it is automatically granted the Editor role on the project. When you start a new instance using G Cloud, the default service account is enabled on that instance. You can override this behavior by specifying another service account or by disabling service accounts for the instance. Authorization is the process of determining what permissions an authenticated identity has on a set of specified resources. Scopes are used to determine if an authenticated identity is authorized. In this example, applications A and B contain authenticated identities or service accounts. Let's assume that both applications want to use a Cloud Storage bucket. They each request access from the Google authorization server and in return they receive an access token. Application A receives an access token with read_only scope, so it can only read from the Cloud Storage bucket. Application B, on the other hand, receives an access token with read_write scope, so it can read and modify data in the Cloud Storage bucket. These tokens can be short-term privileges. For example, if you want to provide read access to an ebook for 24 hours, then you can create a 24-hour access token that your application service would use. Scopes can be customized when you create an instance using the default service account as shown in the screenshot on the right. These scopes can also be changed after an instance is created by stopping it. Scopes cannot be customized for user-created service accounts, but you can use Cloud IAM roles instead. The reason for this is that access scopes are the legacy method for specifying permissions on your instance. Before the existence of IAM roles, access scopes were the only mechanism for granting permissions to the service accounts. Another distinction between service accounts is that default service accounts support primitive, or project, and curated, or Cloud IAM roles, while user-created service accounts only use Cloud IAM roles. Now, roles for service accounts can also be assigned to groups or users. Let's look at the example shown on the slide. First, you create a service account that has the Instance Admin role, which has the permissions to create, modify, and delete virtual machine instances and disks. Then, you treat this service account as the resource and decide who can use it by providing users or a group with the Service Account User role. This allows those users to act as that service account to create, modify, and delete virtual machine instances and disks. That being said, users who are service account users for a service account can access all of these services for which the service account has access. Therefore, be cautious when granting the Service Account User role to a user or group. Here is another example. The VMs running component_1 are granted editor access to project_B using Service Account 1, whereas VMs running component_2 are granted objectViewer access to bucket_1 using an isolated Service Account 2. This way you can scope permissions for VMs without recreating VMs. Essentially, Cloud IAM lets you slice a project into different microservices, each with access to different resources by creating service accounts to represent each one. You assign these service accounts to the VMs when they are created, and you don't have to ensure that credentials are being managed correctly as GCP manages security for you. You might ask, how are service accounts authenticated? While users require a username and password to authenticate, service accounts use keys. There are two types of service account keys, GCP-managed keys and user-managed keys. GCP-managed keys are used by Google Cloud Platform services such as App Engine and Compute Engine. These keys cannot be downloaded, but Google will keep the keys and automatically rotate them daily. User-managed keys are created, downloadable, and managed by users. When you create a new key pair, you download the private key, which is not retained by Google. With external keys you are responsible for security of the private key and auto management operations such as key rotation, which is illustrated on the slide.

Cloud IAM Best Practices
Let's talk about some Cloud IAM best practices to help you apply these concepts in your day-to-day work. First, leverage and understand the resource hierarchy. Specifically, use projects to group resources that share the same trust boundary. Check the policy granted on each resource and make sure you recognize the inheritance. Because of inheritance, use the principle of least privilege when granting roles. Finally, audit policies using cloud audit logs and audit memberships of groups used in policies. Next, I recommend granting roles to groups instead of individuals. This allows you to update group membership instead of changing a Cloud IAM policy. If you do this, make sure to audit membership of groups used in policies and control the ownership of the Google group used in Cloud IAM policies. You can also use multiple groups to get better control. In the example on this slide, there is a Network Admin group. Some of those members also need read_write role to a Cloud Storage bucket, while others need read_only role. Adding and removing individuals from all three groups controls their total access. Therefore, groups are not only associated with job roles, but can exist for the purpose of role assignment. As for using service accounts, here are some best practices. As mentioned before, be very careful when granting the Service Account User role as it provides access to all the resources for which the service account has access. Also, when you create a service account, give it a display name that clearly identifies its purpose, ideally using an established naming convention. As for keys, establish key rotation policies and methods and audit keys with the serviceAccount. keys. list method. Finally, I recommend using Cloud Identity-Aware Proxy, or Cloud IAP. Cloud IAP lets you establish a central authorization layer for applications accessed by HTTPS so you can use an application level access control model instead of relying on network level firewalls. Applications and resources protected by Cloud IAP can only be accessed through the proxy by users and groups with the correct Cloud IAM role. When you grant a user access to an application or resource by Cloud IAP, they're subject to fine-grained access control implemented by the product in use without requiring a VPN. Cloud IAP performs authentication authorization checks when a user tries to access a Cloud IAP secured resource as shown on the right. Please refer to the documentation link in the notes of the slides for more information on Cloud IAP.

Lab: Cloud IAM (Overview and Objectives)
Lab: Cloud IAM (Review)
In this lab you granted and rewrote Cloud IAM roles first to a user, user name 2, and then to a service account user. Having access to both users allowed you to see the results of the changes you made.

Module Review
In this module we covered Cloud IAM along with its components and best practices. Cloud IAM builds on top of other Google Cloud Identity services. The creation and administration of corporate identities occurs through the G Suite admin interface and is commonly handled by a separate person from the GCP administrator. Group emails are a great way for these two business functions to collaborate. You establish the roles and assign them to the group, and then the G Suite admin administers memberships in the group. Finally, remember that service accounts are very flexible and they can enable you to build an infrastructure-based level of control into your application.

Data Storage Services
Module Overview (Intro)
Hello and welcome. I am Philipp Maier, a course developer with Google Cloud Platform, and this is module 2, Data Storage Services. Every application needs to store data, whether it's business data, media to be streamed, or sensor data from devices. Sometimes we characterize data by the three Vs. Variety, which refers to how similar, structured or variable the data is; velocity, or essentially how fast the data comes; and volatility, which refers to for how long the data retains value and therefore needs to be accessible. From an application-centered perspective, the technology is to store and retrieve the data, whether it's a database or an object store is less important than whether that service supports the application's requirements for efficiently storing and retrieving the data. When you take a look at Google's offerings for data storage services, there are a lot of services to choose from. In this module, we will cover Cloud Storage, Cloud SQL, Cloud Spanner, Cloud Datastore, and Cloud Bigtable. To get you more comfortable with these services, you will get to apply them in three labs. Let me start by giving you a high-level overview of these different services.

Data Storage Services
This table shows the different services that I just mentioned and highlights attributes such as the capacity, access metaphor, and usage. I'm not going to cover the whole table right now as I will cover each column as we cover each service. Speaking of columns, I also listed another service here on the right-hand side, which is BigQuery. I grayed the service out as it sits on the edge between data storage and data processing. You can store data in BigQuery, but the usual reason you do this is to use BigQuery's big data analysis and interactive querying capabilities. For this reason, BigQuery is covered in the Scaling and Automation course of this specialization. Now, if text-heavy tables aren't your thing, I also added this decision tree to help you identify the solution that best fits your application. Let's walk through this together. First, ask yourself if your data is structured. If it's not, choose Cloud Storage. If your data is structured, ask yourself if your workload focuses on analytics. If it does, you will want to choose Cloud Bigtable or BigQuery depending on if you need updates or low latency. Otherwise, check if your data is relational. If it's not relational, choose Cloud Datastore. If it is relational, you will want to choose Cloud SQL or Cloud Spanner depending on your need for horizontal scalability. Now, depending on your application that you might use, you might end up using one or several of these services to get the job done. Before we dive into each of the data storage services, let's define the scope of this module. The purpose is to understand from an infrastructure perspective what services are available and when to consider using them. So we'll be covering things like databases, Bigtable, NoSQL databases, certain circumstances in which you might say, this sounds very data-centric, which it is, but from an infrastructure standpoint you need to understand what the service differentiators are, when would I decide which platform that I would like to bring into my organization and support. Now, you may not know all the intricacies, for example, how to run complex queries, optimizations, etc., but you need to have that basic knowledge for setting everything up, connecting to different services, being able to audit, make sure that everything is online all the time. I recommend looking into the data engineering specialization if you want a deeper dive into the design, organizations, structures, schemas, and details on exactly how data would be optimized, served, and stored properly within those services.

Cloud Storage
Let's start with Cloud Storage. On the left-hand side, you can see Cloud Storage is really designed for scale, specifically petabytes and even exabytes of capacity. Some like to think of this as files in a file system, but it's not really a file system, it's just simply a bucket and you're going to drop objects in that bucket. You can create directories so to speak, but really a directory is just another object that points to different objects in the bucket. You're not going to easily be able to index all of these files like you would in a file system. None of that's really kept track of. You just have a specific URL to access all of these. When it comes to Cloud Storage, there are four different types of storage classes. Regional, Multi-Regional, Nearline, and Coldline. Regional storage enables you to store data at lower cost with trade off of data being stored in a specific regional location instead of having redundancy distributed over a large geographic area. So for example, your data may be stored in us-central1, europe-west1, or asia-east1. Multi-Regional storage, on the other hand, is georedundant, which means Cloud Storage stores your data redundantly in at least two geographic locations separated by at least 100 miles within the multi-regional location of the bucket. Therefore, Multi-Regional storage can be placed only in multi-regional locations such as the United States, the European Union, or Asia, not specific locations like us-central1 or asia-east1. Multi-Regional storage is appropriate for storing data that is frequently accessed, such as serving website content, interactive workloads, or data supporting mobile and gaming applications, whereas Regional storage is appropriate for storing data in the same regional location as Compute Engine instances or Kubernetes Engine clusters that use the data, providing you better performance for data intensive computations. Nearline storage is a low-cost, highly durable storage service for storing infrequently accessed data. This storage class is a better choice than Multi-Regional storage or Regional storage in scenarios where you plan to read or modify your data on average once a month or less. For example, if you want to continuously add files to Cloud Storage and plan to access those files once a month for analysis, Nearline storage is a great choice. Coldline storage is a very low-cost, highly durable storage service for data archiving, online back up, and disaster recovery. However, unlike other cold storage services, your data is available within milliseconds, not hours or days. Coldline storage is the best choice for data that you plan to access at most once a year due to its slightly lower availability, 90-day minimum storage duration, cost for data access, and higher per operation costs. For example, if you want to archive data or have access in the event of a disaster recovery event. Now, let's focus on durability and availability. All of these storage classes have 11 nines of durability, but what does that really mean? Does that mean that you have access to your files at all times? No. What that means is you won't lose your data. You may not be able to access the data, which is like going to your bank and saying, well my money is in there, it's 11 nines durable. But when the bank is closed, I don't have access to it, which is like availability, and differs between the different storage classes. So, Cloud Storage is broken down into a couple of different items here. First of all, there are buckets which are required to have a globally unique name and cannot be nested. The data that you point to in those buckets are objects that inherit the storage class of the bucket, and those objects could be text files, doc files, video files, etc. There's no minimum size to those objects, and you can scale it as much as you want as long as your quota allows it. To access the data, you can use the gsutil command in Cloud Shell, or either the JSON or XML APIs. Once you've created a bucket, you might want to change the storage type of that bucket. You can change the default storage class of a bucket, but you can't change a Regional bucket to a Multi-Regional, and vice versa. As the slide illustrates, Multi-Regional can be changed to Coldline and Nearline, and Regional can be changed to Coldline and Nearline as well. You can also move objects from one bucket to another using the GCP console, however, moving objects to buckets of different storage classes requires using the gsutil command in Cloud Shell. Let's look at access control for your objects and buckets that are part of a project. We can use IAM for the project to say what individual user or service account can see the bucket, list the objects in the bucket, view the names of the objects in the bucket, or create new buckets. For most purposes, Cloud IAM is sufficient and roles are inherited from project to bucket to object. Access control lists, or ACLs, offer finer control. For more detailed control, signed URLs provide a cryptographic key that gives limited access to a bucket or object. Finally, a signed policy document further refines the control by determining what kind of file can be uploaded by someone with a signed URL. Let's take a closer look at ACLs and signed URLs. An ACL is a mechanism you use to define who has access to your buckets and objects, as well as what level of access they have. Each ACL consists of one or more entries, and these entries consist of two pieces of information. A scope, which defines who can perform these specified actions, for example a specific user or a group of users, and a permission, which defines what actions can be performed, for example, read or write. For some applications, it is easier and more efficient to grant limited time access tokens that can be used by any user instead of using account-based authentication for controlling resource access. For example, when you don't want to require users to have a Google account. Signed URLs allow you to do this for Cloud Storage. You create a URL that grants read or write access to a specific Cloud Storage resource and specifies when the access expires. That URL is signed using a private key associated with a service account. When the request is received, Cloud Storage can verify that the access granting URL was issued on behalf of a trusted security principal, which would be the service account, and delegates its trust of that account to the holder of the URL. After you give out the signed URL, it is out of your control. So, you want the signed URL to expire after some reasonable amount of time, as shown in this example, which is set to expire after 10 months. Here's another example of what a signed URL looks like. Your applications could call it, or you just simply make that available to individuals for download. That way, you can ensure that there's going to be an expiration to the access of a file instead of looking at lifecycle management to delete older files.

Cloud Storage Features
There are also a number of different features that come with Cloud Storage. I'm going to cover these at a high level for now as we are about to dive deeper into some of them. In a previous course, we already talked a little bit about custom-supplied encryption keys when attaching persistent disks to virtual machines. This allows you to supply your own encryption keys instead of the Google-managed keys, which is also available for Cloud Storage. Cloud Storage also provides object lifecycle management, which lets you automatically delete or archive objects. Another feature is object versioning, which allows you to maintain multiple versions of objects in your bucket. Now I place a star here because you are charged for the versions as if they were multiple files, which is something to keep in mind. Cloud Storage also offers directory synchronization so that you can sync a VM directory with a bucket. As for object change notification, data import, and strong consistency, we will discuss these in more detail after going into object versioning and object lifecycle management. To support the retrieval of objects that are deleted or overwritten, Google Cloud Storage offers the object versioning feature. You enable object versioning for your bucket. Once enabled, Cloud Storage creates an archived version of an object each time the live version of the object is overwritten or deleted. The archived version retains the name of the object, but is uniquely identified by a generation number as illustrated with g1 over here. When object versioning is enabled, you can list archived versions of an object, restore the live version of an object to an older state, or permanently delete an archived version as needed. You can turn versioning on or off for your bucket at any time. Turning versioning off leaves the existing object versions in place and causes the bucket to stop accumulating new archived object versions. To support common use cases like setting a time to live for objects, archiving older versions of objects, or downgrading storage classes of objects to help manage costs, Cloud Storage offers object lifecycle management. You can assign a lifecycle management configuration to a bucket. The configuration is nothing but a set of rules, which apply to all the objects in the bucket. So when an object meets the criteria of one of those rules, Cloud Storage automatically performs a specified action on the object. Here are some example use cases. You could downgrade the storage class of objects older than, let's say a year, to potentially Coldline storage. You can delete objects created before a specific date, for example, January 1st, 2017. And you could keep only the three most recent versions of each object in a bucket with versioning enabled. Now, updates to your lifecycle configuration may take up to 24 hours to go into effect. This means that when you change your lifecycle configuration, object lifecycle management may still perform actions based on the old configuration for up to 24 hours, so keep that in mind. Object change notification can be used to notify an application when an object is updated or added to a bucket through a watch request. Completing a watch request creates a new notification channel, which is the means by which a notification message is sent to an application watching a bucket using a web hook. After a notification channel is initiated, Cloud Storage notifies the application any time an object is added, updated, or removed from the bucket. For example, as shown here, when you add a new picture to a bucket, an application could be notified to create a thumbnail. Now that being said, Cloud Pub/Sub notifications are the recommended way to track changes to objects in your Cloud Storage bucket because they're faster, more flexible, easier to set up, and more cost effective. Cloud Pub/Sub is Google's distributed real-time messaging service, which is covered in the Developing Applications specialization. The GCP Cloud Storage browser allows you to upload individual files to your bucket, but what if you have to upload terabytes or even petabytes of data? Well, there are three services that address this. Storage Transfer Service, Google Transfer Appliance, and Offline Media Import. The Storage Transfer Service enables high performance imports of online data. That data source can be another Cloud Storage bucket, an Amazon S3 bucket, or an HTTP or HTTPS location. Transfer Appliance, on the other hand, is a high capacity storage server that you lease from Google. You simply connect it to your network, load it with data, and then ship it to an upload facility where the data's uploaded to Cloud Storage. This service enables you to securely transfer up to petabytes of data on a single appliance. Finally, Offline Media Import is a third-party service where physical media such as storage arrays, hard disk drives, tapes, and USB flash drives is sent to a provider who uploads the data. When you upload an object to Cloud Storage and you receive a success response, the object is immediately available for download and metadata operations from any location where Google offers service. This is true whether you create a new object or override an existing object. Because uploads are strongly consistent, you will never receive a 404 not found response or stale data for a read-after-write or read-after-metadata-update operation. Strong global consistency also extends to deletion operations on objects. If a deletion request succeeds, an immediate attempt to download the object or its metadata will result in a 404 not found status code. You get the 404 error because the object no longer exists after the delete operation succeeds. Also, bucket listing is strongly consistent. For example, if you create a bucket, then immediately perform a list buckets operation, the new bucket appears in the returned list of buckets. Finally, object listing is also strongly consistent. For example, if you upload an object to a bucket and then immediately perform a list objects operation, the new object appears in the returned list of objects. To summarize, let's look at this decision tree to help you find the appropriate storage class in Cloud Storage. If you're going to read your data less than once a year, you should consider using Coldline storage. If you're going to read your data less than once a month, you should consider using Nearline storage. And if you're going to be doing reads and writes more often than that, you should consider choosing Multi-Regional or Regional depending on your locality needs. Speaking of that, another reason to choose Regional storage is if you want to ensure that your data stays within a specific geographic or political region to comply with legal requirements, for example. So far, we only considered unstructured data. Structured data services are covered next in this module.

Lab: Cloud Storage (Overview and Objectives)
Lab: Cloud Storage (Review)
In this lab, you learned to create and work with buckets and objects, and you learned about the following Cloud Storage features, customer-supplied encryption keys, access control lists, lifecycle management, versioning, directory synchronization, and cross project resource sharing using IAM. Now that you're familiar with many of these advanced features of Cloud Storage, you might consider using them in a variety of applications that you might not have previously considered. A common quick and easy way to start using GCP is to use Cloud Storage as a backup service.

Cloud SQL
Let's jump into the structured or relational data storage services. First up is Cloud SQL. Cloud SQL is a fully managed or NoOps database service that makes it easy to set up, maintain, manage, and administer your relational databases on Google Cloud Platform. Cloud SQL has a high capacity, capable of handling terabytes of storage. The databases are relational, which means that you're simply going to be running queries such as select statements to read field's data or insert statements to write field's data. Alternatively, you could install a SQL Server application image on a VM using Compute Engine, but there are certain benefits of using Cloud SQL as a managed service inside of GCP. Let's take a look at those benefits. Since Cloud SQL is a fully managed service of either MySQL or PostgreSQL databases, patches and updates are automatically applied. However, you still have to administer MySQL users with the native authentication tools that come with these databases. Cloud SQL supports many clients, such as Cloud Shell, App Engine, and G Suite scripts. It also supports other applications and tools that you might be used to like SQL Workbench, Toad, and other external applications using standard MySQL drivers. Cloud SQL offers two different generations. I recommend using the second generation because it provides up to 7 times the throughput and 20 times the storage capacity of the First Generation, giving you up to 208 GB of RAM and 10 TB of data storage. This generation works with either MySQL 5. 6 or 5. 7, but it only supports InnoDB as the storage engine. The First Generation provides lower memory and storage, and only works with MySQL 5. 5. However, this generation supports connections over both IPv4 and IPv6 addresses and supports the On Demand activation policy. Generations aside, the MySQL functionality provided by a Cloud SQL instance is the same as the functionality provided by a locally hosted MySQL instance. However, there are few differences between a standard MySQL instance and a Cloud SQL instance. For example, user-defined functions are not supported. For a full list of the differences between Cloud SQL and standard MySQL functionality, I recommend looking at the Cloud SQL documentation. Let's focus on some other services provided by Cloud SQL. There's a Replica service, which can replicate data between multiple zones with automatic failover if an outage occurs. Cloud SQL also provides automated and on-demand backups with point-in-time recovery. You can import and export databases using mysqldump or import and export CSV files. Cloud SQL can also scale up, which does require a machine restart, or scale out using read replicas. There are a couple of options when it comes to connecting to Cloud SQL instances. Using the most basic connection, you can grant any application access to a Cloud SQL instance by authorizing the IP addresses that the application uses to connect. This is a great way to get started, but I don't recommend it for production instances. Instead, for a more secure access, you can temporarily whitelist IP addresses to easily connect from the GCP console. This is best for quick administration tasks requiring the MySQL command line tool. You can also configure SSL certificate management for a Cloud SQL instance and connect to the MySQL client using SSL, which you will actually do in the next lab. Cloud SQL also provides instance level access to authorize access to your Cloud SQL instance from an application or client that could be running on Google App Engine or externally, or another GCP service such as Compute Engine. I also mentioned earlier that you can use the native MySQL access privilege system to control which MySQL users have access to the data in your instance. Here's another example of connecting a MySQL client to your Google Cloud SQL instances using the Cloud SQL Proxy rather than over IP. The Cloud SQL Proxy provides secure access to your Cloud SQL second generation instances without having to whitelist IP addresses or configure SSL. Cloud SQL Proxy works by having a local client called at the proxy running on the local environment. Your application communicates with the proxy with the standard database protocol used by your database. The proxy uses a secure tunnel to communicate with its companion process running on the server. To summarize, let's look at this decision tree to help you find the right data storage service with full relational capability. If you need more than 10 TB of storage space, over 4000 concurrent connections to your database, or if you want your application design to be responsible for scaling, availability, and location management when scaling up globally, then consider using Cloud Spanner, which we will cover later in this module. If you have no concerns with these constraints, ask yourself if you have specific OS requirements, custom database configuration requirements, or special backup requirements. If you do, you want to consider hosting your own database on a VM using Compute Engine. Otherwise, I strongly recommend using Cloud SQL as a fully managed service for your relational databases.

Lab: Cloud SQL (Overview and Objectives)
Lab: Cloud SQL (Review)
In this lab, you configured Cloud SQL for use by a client. Then you improved security by requiring SSL certificates. Cloud SQL is a great way to rapidly provision a familiar database environment while offloading many of the administrative activities. This was a good example of a managed service where some parts of the service parameters are yours to configure and some parts are automatically handled by GCP. One thing to keep in mind is where that dividing line is and what your responsibilities are with respect to security of the overall solution.

Cloud Spanner
If Cloud SQL does not fit your requirements because you need horizontal scalability, consider using Cloud Spanner. Cloud Spanner is a service built for the cloud specifically to combine the benefits of relational database structures with nonrelational horizontal scale. This service can provide petabytes of capacity and offers transactional consistency at global scale, schemas, SQL, and automatic synchronous replication for high availability. Natural use cases include financial applications and inventory applications traditionally served by relational database technology. Cloud Spanner offers strong consistency including strongly consistent secondary indexes. It offers SQL support with ALTER statements for schema changes. Cloud Spanner also offers managed instances with high availability through transparent, synchronous, built-in data replication. Let's compare Cloud Spanner with both relational and nonrelational databases. Like a relational database, Cloud Spanner has schema, SQL, and strong consistency. Also like a nonrelational database, Cloud Spanner offers high availability, horizontal scalability, and configurable replication. In essence, Cloud Spanner offers the best of the relational and nonrelational worlds. These features allow for mission critical use cases such as building consistent systems for transactions and inventory management in the financial services and retail industries. Cloud Spanner supports many open standards such as the ones that are listed here. It also supports many workloads like transactional workloads where companies that have outgrown their single instance relational database management system and have already moved to a NoSQL solution, but need transactional consistency or are looking to move to a scalable solution. Cloud Spanner also allows for database consolidation where companies that store their business data in multiple database products with variable maintenance overheads and capabilities need consolidation of their data. To better understand how all of this works, let's look at the architecture of Cloud Spanner. A Cloud Spanner instance replicates data in N cloud zones, which can be within one region or across several regions. The database placement is configurable, meaning you can choose which region to put your database in. This architecture allows for high availability and global placement. The replication of data is going to be synchronized across zones using Google's global fiber network. Using atomic clocks ensures atomicity when you're updating your data. Cloud Spanner has its own set of IAM access roles. This allows you to have the same security mechanisms without having to create something separate for your database. These IAM permissions can be granted to a database, instance, or GCP project. Cloud Spanner offers many different features as you can see here, tables, primary and secondary keys, database splits, transactions, and timestamp bounds. Feel free to explore the documentation to dive deeper into these features. However, because the scope of this course is to understand under which circumstances you would use Cloud Spanner, let's look at the decision tree. If you have outgrown any relational database or sharding your databases for throughput high performance, need transactional consistency, global data and strong consistency, or just want to consolidate your databases, consider using Cloud Spanner. If you don't need any of these, nor full relational capability, consider a NoSQL service such as Cloud Datastore, which we will cover next.

Cloud Datastore
If you're looking for a highly scalable, NoSQL database for your applications, consider using Cloud Datastore. You can think of Cloud Datastore as a persistent hash map that offers terabytes of capacity. One of its main use cases is to store structured data from App Engine apps. Cloud Datastore is paired with a Memcached service to increase performance for repeatedly read data. Typically in development, the App Engine application will try Memcached first, and then on a cache miss, access Cloud Datastore. This strategy radically improves performance and reduces costs. Cloud Datastore automatically handles sharding and replication, providing you with a highly available and durable database that scales automatically to handle your application's load. Cloud Datastore provides a myriad of capabilities such as ACID transactions, SQL-like queries, indexes, and much more. While the Cloud Datastore interface has many of these same features as a traditional database, as a NoSQL database, it differs from them in the way that it describes relationships between data objects. In Cloud Datastore, a category of object is known as a kind. An object is an entity. Individual data for an object is a property and a unique ID for an object is a key, whereas in a relational database, these would be table, row, field, and primary key respectively. Built into Datastore is synchronous replication over a wide geographic area. When you first use Cloud Datastore, you must choose a location where the project's data is stored. To reduce latency and increase availability, store your data close to the users and services that need it. There are two types of locations where you can store data using Cloud Datastore, Multi-Regional locations and Regional locations. Multi-Regional locations provide multi-regional redundancy with high availability. Regional locations provide lower write latency and colocation with other GCP resources that your application may use. Both of these options enable high availability with slightly different SLAs for monthly up-time percentage. To summarize, let's look at this decision tree to help you determine if Cloud Datastore is the right storage service for your data. If your schema might change and you need an adaptable database, you need to scale to 0, or you want low maintenance overhead scaling up to terabytes, consider using Cloud Datastore. Also, if you don't require transactional consistency, you might want to consider Cloud Bigtable depending on the cost or size. I'm going to cover Cloud Bigtable after the next lab.

Lab: Datastore (Overview and Objectives)
Lab: Datastore (Review)
In this lab, you created a Cloud Datastore database. You populated the database with data entities and ran both query by kind and query by GQL queries. You then enabled the Cloud Datastore Admin console so that you could see how to clean up and remove test data. Datastore is an extremely flexible and scalable data storage service. It's used in many applications where scalable growth and consistent performance are required.

Cloud Bigtable
Cloud Bigtable is Google's NoSQL big data database service. Cloud Bigtable is a sparsely populated table that can scale to billions of rows and thousands of columns, allowing you to store terabytes or even petabytes of data. A single value in each row is indexed, and this value is known as a row key. Cloud Bigtable is ideal for storing very large amounts of single keyed data with very low latency. It supports high read and write throughput at low latency, so it's a great choice for both operational and analytical applications including IoT, user analytics, and financial data analysis. Cloud Bigtable is actually the same database that powers many of Google's core services including Search, Analytics, Maps, and Gmail. In short, Cloud Bigtable is a fully managed, NoSQL database with petabyte scale and very low latency. It seamlessly scales for throughput and it learns to adjust for specific access patterns. There are different ways to interact with Cloud Bigtable. Cloud Bigtable is exposed to applications through multiple client libraries including a supported extension to Apache HBase library. Cloud Bigtable also excels as a storage engine for batch MapReduce operations, steam processing/analytics, and machine learning applications. Cloud Bigtable's powerful back-end servers offer several key advantages over a self-managed HBase installation. From a scalability perspective, a self-managed HBase installation has a design bottleneck that limits the performance after a certain query per second rate is reached. Cloud Bigtable does not have this bottleneck, and so you can scale your cluster up to handle more queries by increasing your machine count. Also, Cloud Bigtable handles administration tasks like upgrades and restarts transparently and can resize clusters without downtime. Cloud Bigtable stores data in massively scalable tables, each of which is a sorted key-value map as the one on this slide. The table is composed of rows, each of which typically describes a single entity, and columns, which contain individual values for each row. Each row is indexed by a single row key and columns that are related to one another are typically grouped together into a column family. The example here has a follows column family that represents a hypothetical social network in which the United States presidents are following each other. Also, Cloud Bigtables are sparse. If a cell does not contain any data, it does not take up any space. This diagram shows a simplified version of Cloud Bigtable's overall architecture. It illustrates that processing, which is done through a front-end server pool and nodes is handled separately from the storage. A Cloud Bigtable table is sharded into blocks of contiguous rows called tablets to help balance the workload of queries. Tablets are similar to HBase regions. Tablets are stored on Colossus, which is Google's file system in SSTable format. An SSTable provides a persistent ordered immutable map from keys to values where both keys and values are arbitrary byte strings. As I mentioned earlier, Cloud Bigtable learns to adjust to specific access patterns. If a certain Bigtable node is frequently accessing a certain subset of data, Cloud Bigtable will update the indexes so that the other nodes can distribute that workload evenly as shown here. That throughput scales linearly, so for every single node that you add, you're going to see a linear scale of throughput performance up to hundreds of nodes. In summary, if you need to store more than 1 TB of structured data, have very high volumes of writes, need read/write latency of less than 10 ms along with strong consistency, or need a storage service that is compatible with the HBase API, consider using Cloud Bigtable. If you don't need any of these and are looking for a storage service that scales down well, consider using Cloud Datastore. Speaking of scaling, the smallest Cloud Bigtable cluster you can create has 3 nodes and can handle 30, 000 operations per second. Keep in mind that you pay for those nodes while they're operational, whether your application is using them or not.

Module Review
In this module, we covered the different data storage services that GCP offers. Specifically, you learned about Cloud Storage, a fully managed object store; Cloud SQL, a fully managed MySQL or PostgreSQL database service; Cloud Spanner, a relational database service with transactional consistency, global scale, and high availability; Cloud Datastore, a fully managed NoSQL document database; and finally, Cloud Bigtable, a fully managed NoSQL wide column database. From an infrastructure perspective, the goal was to understand what services are available and how they're used in different circumstances. Defining a complete data strategy is beyond the scope of this course; however, Google offers courses on data engineering and machine learning on GCP that cover data strategy.

Resource Management
Module Overview (Intro)
Hello and welcome. I'm Philipp Maier, a course developer with Google Club Platform, and this is module 3, Resource Management. Resources in GCP are billable, so managing them means cost control. There are several methods in place for controlling access to the resources, and there are quotas that limit consumption. In most cases, the default quotas can be raised on request, but having them in place provides a checkpoint or a chance to make sure that this is really the resource that you want to intend to consume in greater quantity. In this module, we will build on what we learned in the Cloud IAM module. First, we will provide an overview of the Cloud Resource Manager. Then, we will go into quotas, labels, and names. Next, we will cover billing to help you set budgets and alerts. To round up your learning experience, you will get to examine billing data with BigQuery in a lab.

Cloud Resource Manager
The Cloud Resource Manager lets you hierarchically manage resources by project, folder, and organization. This should sound familiar, as we covered it in the Cloud IAM module. Let me refresh your memory. Policies contain a set of roles and members, and policies are set on resources. These resources inherit policies from their parent, as we can see on the left. Therefore, resource policies are a union of parent and resource. Also, keep in mind that if a parent policy is less restrictive, it overrides the more restrictive resource policy. While IAM policies are inherited top to bottom, billing is accumulated from the bottom up, as we can see on the right. Resource consumption is measured in quantities like rate of use or time, number of items, or feature use. As a resource belongs to only one project, a project accumulates the consumption of all of its resources. Each project is associated with one billing account, meaning that an organization contains all billing accounts. Let's explore organizations, projects, and resources more. Just to reiterate, an organization node is the root node for all Google Cloud Platform resources. This diagram shows an example where we have an individual, Bob, who is in control of the organizational domain through the Organization Admin role. Bob has delegated privileges and access to individual projects to Alice by making her a project creator. Since a project accumulates the consumption of all of its resources, it can be used to track resources and quota usage. Specifically, projects let you enable billing, manage permissions and credentials, and enable service and APIs. To interact with Cloud Platform resources, you must provide the identifying project information for every request. A project can be identified by the project name, which is a human-readable way to identify your projects, but it isn't used by any Google APIs. There's also a project number, which is automatically generated by the server and assigned to your project. And, there is a project ID, which is a unique ID that is generated from your project name. You can find these three identifying attributes on the dashboard of your GCP console, or by querying the Cloud Resource Manager API. Finally, let's talk about the resource hierarchy. From a physical organization standpoint, resources are categorized as global, regional, or zonal. Let's look at some examples. Images, snapshots, and networks are global resources, external IP addresses are regional resources, and instances and disks are zonal resources. However, no matter what type of resource, each resource is organized into a project. This enables each project to have its own billing and reporting.

Quotas
Now that we know that a project accumulates the consumption of all its resources, let's talk about quotas. All resources in GCP are subject to project quotas or limits. These typically fall into one of these three categories shown here. How many resources you can create per project, for example, you can only have five networks per project; how quickly you can make API requests in a project, or rate limits, for example, by default you can only have 300 administrative requests per minute when using Cloud Spanner API. There are also regional quotas. For example, by default, you can only have 24 CPUs per region. Given these quotas, you may be wondering, how do I spin up one of those 96 core VMs? As your use of Google Cloud Platform expands over time, your quotas may increase accordingly. If you expect a notable upcoming increase in usage, you can proactively request quota adjustments from the Quotas page in the GCP console. This page will also show you your current quotas. Given that quotas can be changed, why do they exist? Project quotas prevent runaway consumption in case of an error or malicious attack. For example, imagine you accidentally create 100 instead of 10 Compute Engine instances using gcloud. Quotas also prevent billing spikes or surprises. Now, quotas are related to billing, but we will go through how to set up budgets and alerts later, which will really help you manage billing. Finally, quotas force sizing consideration and periodic review. For example, do you really need a 96-core instance, or can you go with a smaller and cheaper alternative? It is also important to mention that quotas are the maximum amount of resource you can create for that resource type as long as those resources are available. Quotas do not guarantee that resources will be available at all times. For example, if a region is out of local SSDs, you cannot create local SSDs in that region, even if you still have a quota for local SSDs.

Labels and Names
Projects and folders provide levels of segregation for resources. But what if you want more granularity? That's where labels and names come in. Labels are a utility for organizing GCP resources. Labels are key value pairs that you can attach to your resources, like VMs, disks, snapshots, and images. You can create and manage labels using the GCP console, gcloud, or the Resource Manager API. For example, you could create a label to define the environment of your virtual machines. Then you define the label for each of your instances to either production or test. Using this label, you could search and list all of your production resources for inventory purposes. Labels can also be used in scripts to help analyze costs or to run bulk operations on multiple resources. The labels applied to a resource must meet the requirements listed here. Each label must be a key value pair. Label keys and label values can contain lowercase letters, digits, hyphens, but they must start with a letter, and they must end with a letter or digit. Keys must have a minimum length of 1 character, and a maximum length of 63 characters, and they cannot be empty. Values can be empty, and have a maximum length of 63 characters. Finally, each resource can have multiple labels, up to a maximum of 64. The screenshot on the right shows an example of four labels that are created on an instance. Besides naming requirements, there are also some best practices and common-use cases for labels. I recommend adding labels based on the team or cost center to distinguish instances owned by different teams. You can use this type of label for cost accounting or budgeting. For example, team marketing and team research. Use labels to distinguish components. For example, component:redis, component:frontend. I already mentioned the environment or stage-use case. Use labels based on the owner or a primary contact, for instance. For example, owner:gaurav, owner:opm. Finally, add labels to your resources to define their state. For example, state:inuse, state:readyfordeletion. Now, it's important to not confuse labels and tags. Labels, we just learned, are user-defined strings in key value format that I use to organize resources, and they can propagate through billing. Tags, on the other hand, are user-defined strings that are applied to instances only and are mainly used for networking, such as applying firewall rules.

Billing
Since the consumption of all resources under a project accumulates into one billing account, let's talk billing. To help you with project planning and controlling costs, you can set a budget. Setting a budget lets you track how your spend is growing towards that amount. This screenshot shows the budget creation interface. It allows you to either set a budget on a billing account or a project. You can set the budget at a specific amount, or match it to the previous month's spent. Once you have determined your budget amount, you can set budget alerts. These alerts send emails to billing admins after spend exceeds a percent of the budget or a specified amount. In our case here, it would send an email when spending reaches 50%, 90%, and 100% of the budget amount. Please note that these alerts are based on estimated expenses, so actual expenses may be greater. Here's an example of such an email notification. The email contains the project name, the percent of the budget that was exceeded, along with the budget amount. Another way to provide billing visibility is to export billing. Here's an example of what a billing export would look like with its corresponding JSON Field, CSV Field, Data Type, and Description. You can export this to a file, or input it into a BigQuery dataset. Using BigQuery will allow you to run complex queries to gain insight into your billing consumption. You will do exactly that in the lab of this module.

Demo: Billing Administration
Next, you'll learn about the billing administrator interface by observing the common activities that a billing administrator performs. These actions cannot be performed in the Qwiklabs environment due to security restrictions. Therefore, I'm going to walk you through them as a demo. So, here I am in the GCP console. And the first thing I'm going to do is, using the Products and services menu, I'm going to navigate to Billing. The first page that I get here is the Overview page. It shows the different credits that I have, the projects that are associated, and any of the billing account administrators. So you can see here I'm using a trial account, actually, so I have 300 credits remaining. Over here, and this can be collapsed by the way with the Info panel, I can add any other billing administrators, but currently there's only me here as an administrator. We can also see all the different projects that are related to this billing account, so you see the two projects I have here. And if I click on these three dots over here, I can actually disable the billing or change this project to a different billing account. If I had multiple accounts in here, I could select the different ones to look at all my billing accounts and look at this Overview page for the different billing accounts within my organization. So next, I want to go ahead and look at the Budgets and alerts page. So, I just mentioned that we can create budgets within GCP, so let me go ahead and kind of demonstrate how you would do that. It's pretty straightforward, I just click on the Create Budget button here. I can give this budget a name, so I could say this should be my monthly spend. I can specify if this budget is set on a project or a billing account. If we can see that here, I have my two projects available for the billing account. And then I can specify the budget amount. I could either just define a value here, or if I had some spend in my previous months, I could use it as a starting point to set a budget. So if I click here, I could specify the last month's spend. But in my case, I'm actually just going to define, let's say, $500 for this month. Now using that budget amount, I can then set some alerts. So you see that automatically here we have three different alerts. There's one for 50%, one for 90%, and one for 100%. I can go in here and either change the percentage values, so I could say you know what, I actually want an alert at 25%, and that's going to change the amount, or I could do vice versa, I could change the amount in here, which is then going to change the percent. If I now save this budget, it is going to send me an email every time that one of these alerts is being reached, just like I showed you earlier in the module. The next thing I want to show you in here are transactions. So if I go over here, we can look at the transactions. Now, this would show a history of your financial transactions, any charges and payments. In my case, I don't have any. As you can see here, there are no transactions. But this is where you could go see those transactions within GCP. If you want to go further, you could actually export all of your billing information. So that is done over here under Billing export. You really have two options here, you could either directly export this to BigQuery, or you could export it as a JSON or a CSV file. If you do export it as a file, it is actually going to export it into a Cloud Storage Bucket that you can specify by clicking on Edit Settings. And then from there, you could if you want, download it, or actually import it into BigQuery, which is what you're going to do in the next lab. The other thing I want to show in here are the payment settings. So here we can see there's a payments account ID. You can give this payment account a nickname, you can actually change that. You'll see how you're currently paying, this is automatically you're being charged. There's a payments profile. And there are also all the payments users that are listed at the bottom, and you can directly, from here, manage these different payment users. Now, how are you actually going to pay? That's the last option here, the payment method. If you click on there, you'll see that there's currently a credit card link to this, and you can add any other payment method that you want. The activities that I illustrated contain no surprises. The billing administrator can set up accounts and run reports, which are ordinary tasks. But becoming familiar with the available options and seeing how these tasks are performed reduces the chances of confusion. For example, you'll know that reports can be generated in JSON or in CSV format, and you'll know that any more sophisticated processing or filtering of data occurs after the report is generated, and it's not part of the billing administrator's interface.

Lab: Examining Billing Data with BigQuery (Overview and Objectives)
Lab: Examining Billing Data with BigQuery (Review)
In this lab, you imported billing data into BigQuery that had been exported as a CSV file. You first ran a simple query on that data. Next, you accessed a shared dataset containing more than 22, 000 records of billing information. You then ran a variety of queries on that data to explore how you can use BigQuery to gain insight into your resource's billing consumption. If you use BigQuery on a regular basis, you'll start to develop your own queries for searching where in your application resources are being consumed. You can also monitor changes in the resource consumption over time. This kind of analysis is an input to capacity planning and can help you determine how to scale up your application to meet growth, or scale down your application for efficiency.

Module Review
In this module, we covered the Cloud Resource Manager and went into quotas, labels, and billing. Then, we examined billing data with BigQuery in a lab. Reporting is an important part of resource management. You can generate reports to track consumption and to establish accountability. A key principle in GCP is transparency, and that means it's straightforward to access and process consumption data, as you observed in this module.

Resource Monitoring
Module Overview (Intro)
Hello and welcome. I'm Philipp Maier, a course developer with Google Cloud Platform, and this is module 4, Resource Monitoring. The features covered in this module rely on Stackdriver, a service that provides monitoring, logging, and diagnostics for your applications. Let me start by giving you a high-level overview of Stackdriver and its features.

Stackdriver
Stackdriver dynamically discovers cloud resources and application services based on deep integration with Google Cloud Platform and Amazon Web Services. Thanks to its smart defaults, you can get up and running with core visibility into your cloud platform in less than 2 minutes. This provides you access to powerful data and analytics tools, plus collaboration with many different third-party software providers. Specifically, Stackdriver integrates five services, Monitoring, Logging, Error Reporting, Fault Tracing, and Debugging. In most other environments, these services are handled by completely different packages, or by a loosely integrated collection of software. Once you see these functions working together in a single, comprehensive, and integrated service, you'll realize how important that is to creating reliable, stable, and maintainable applications. A Stackdriver account is the root entity that holds monitoring and configuration information. This account can monitor multiple GCP projects simultaneously. One of these projects will host the Stackdriver account, as you can see in this diagram. This hosting project contains monitoring, dashboards, uptime checks, and configurations, and it provides the Stackdriver account with its name. To access an AWS account, you must configure a project in GCP to hold the AWS Connector and use Stackdriver's premium service tier. Stackdriver also supports a rich and growing ecosystem of technology partners as shown in this table. This helps expand the IT Ops security and compliance capabilities available to Google Cloud Platform customers. Now in this module, we're going to explore each of these services that Stackdriver provides, which again are Monitoring, Logging, Error Reporting, Tracing, and Debugging. You will also get to apply these services in two labs.

Monitoring
Let's start with monitoring. Stackdriver dynamically configures monitoring after resources are deployed and has intelligent defaults which allow you to easily create charts for basic monitoring activities. This allows you to monitor your platform, system, and application metrics by ingesting data such as metrics, events, and metadata. You then generate insights from this data through dashboards, charts, and alerts. For example, you can configure and measure uptime and health checks that send alerts via email. Monitoring is so important to Google, as it is at the base of site reliability. Site Reliability Engineering, or SRE, is a discipline that incorporates aspects of software engineering and applies that to operations whose goals are to create ultra-scalable and highly reliable software systems. This discipline has enabled Google to build, deploy, monitor, and maintain some of the largest software systems in the world. If you want to learn more about this, I recommend following the link in the notes of the slides to add access to a free book written by members of Google's SRE team. You can monitor platforms, specifically GCP and AWS, systems, through individual hosted probes, actual application instrumentation, and common applications such as the ones listed here. Let's take a look at uptime checks. Stackdriver can verify the availability of your service by accessing it from locations around the world. Here you can see an uptime check for 2 virtual machines, summer01 and webserver across 6 global locations. The type of uptime check can be set to be either HTTP, HTTPS, or TCP, and the resource to be checked can be an App Engine application, a Compute Engine instance, a URL of a host, or an AWS instance or load balancer. Here you can see an example of an HTTP uptime check for the summer01 resource. The resource is checked every minute with a 10 second time out. Uptime checks that do not get a response within this timed period are considered failures. So far there is 100% uptime with no outages. Stackdriver monitoring can access some metrics without the monitoring agent, including CPU utilization, some disk traffic metrics, network traffic, and uptime information. To access additional system resources and application services, you should install the monitoring agent. The monitoring agent is supported for Google Compute Engine and EC2 instances. App Engine includes built-in support for Stackdriver monitoring in the standard and flexible environments. Additionally, you can configure the monitoring agent to monitor third-party applications such as the ones listed here. However, the monitoring agent does not currently support Kubernetes Engine. The monitoring agent can be installed with these two simple commands, which you could include in your startup script. This assumes that you have a VM instance running Linux that is being monitored by a Stackdriver account, and that your instance has the proper credentials for the agent. Uptime monitoring can be based on different conditions like predefined metrics or log-based metrics. These conditions can trigger incidents that create notifications in the form of an email, SMS, webhook, or another third-party service. For example, if your CPU utilization on your VM is too high, you can get an email notification with all the necessary details. Here are some examples of standard metrics that can be used for monitoring. There are a lot of these standard metrics spanning Compute Engine, Cloud Pub/Sub, Cloud Storage, and Cloud Datastore. For more details, I recommend going to the documentation link in the notes of the slides. Here is an example of what creating an alerting policy looks like. On the left, you can see an HTTP check condition on the summer01 instance. This will send an email that is customized with the Documentation section on the right. Finally, you just provide the policy and name, and save it. Groups allow you to aggregate metrics across a set of machines. These are dynamically defined in that groups are filter driven, meaning any existing of future resources that match the criteria for the group will be automatically included. This means that you don't have to update the group of related dashboards/alerts when you add or remove resources, making groups useful for high change environments. Groups also allow you to separate your production from your development services. And you can filter Kubernetes Engine data by name and custom tags for clusters. Dashboards provide quick visualization of core metrics for insight. These dashboards are customizable and they can be auto-generated for common applications. The example on this slide shows different dashboard options including the two different color schemes that are available. If the standard metrics provided by Stackdriver monitoring do not fit your needs, you can create custom metrics. For example, imagine a game server that has a capacity of 50 users. What metric indicator might you use to trigger scaling events? From an infrastructure perspective, you might consider using CPU load or perhaps network traffic load as values that are somewhat correlated with the numbers of users. But with a custom metric, you could actually pass the current number of users directly from your application into Stackdriver. If you want to create custom metrics, check out the documentation link in the notes of the slides. Now, I mentioned earlier, that Stackdriver accounts can monitor all your GCP projects in a single account. This makes the account a single pane of glass, meaning that all Stackdriver users that have access to that account have access to all data by default. It also means that a Stackdriver role assigned to one person on one project applies equally to all projects monitored by that account. In order to give people different roles per project and to control visibility to data, consider placing the monitoring of those projects in a separate Stackdriver account. Let's wrap up this lesson on monitoring with some best practices on alerts. I recommend alerting on symptoms and not necessarily causes. For example, you want to monitor failing queries of a database, and then identify if the database is down. Next, make sure you're using multiple notification channels like email and SMS. This helps avoid a single point of failure in your alerting strategy. I also recommend to customize your alerts to the audience's needs by describing what actions need to be taken or what resources need to be examined. Finally, avoid noise as this will cause alerts to be dismissed over time. Specifically, adjust monitoring alerts so that they're actionable, and don't just set up alerts on everything possible.

Lab: Resource Monitoring with Stackdriver (Overview and Objectives)
Lab: Resource Monitoring with Stackdriver (Review)
In this lab, you got an overview of GCP Monitoring. You learned how to monitor your project, create a Stackdriver account, create alerts with multiple conditions, add charts to dashboards, create resource groups, and create uptime checks for your services. Monitoring is critical to your application's health. Stackdriver provides a rich set of features for monitoring your infrastructure, visualizing the monitoring data, and triggering alerts and events for you.

Logging
While monitoring is the basis of Stackdriver, the service also provides logging, error reporting, tracing, and debugging. Let's learn about logging. Stackdriver Logging allows you to store, search, analyze, monitor, and alert on log data and events from Google Cloud Platform and AWS. It is a fully managed service that performs at scale and can ingest application and system log data from thousands of VMs. Logging includes storage for logs, a user interface called the Log Viewer, and an API to manage logs programmatically. The service lets you read and write log entries, search and filter your logs, create log-based metrics, and export your logs to cloud storage buckets, BigQuery data sets, and Cloud Pub/Sub topics. It is a best practice to install the logging agent on all your VM instances. The logging agent can be installed with these two simple commands, which you could include in your startup script. The logging agent is supported for Google Compute Engine and EC2 instances. App Engine and Kubernetes Engine include built-in support for Stackdriver logging using their own software. I mentioned earlier that Stackdriver logging lets you export your logs. This is done by configuring sinks for exports. Stackdriver logs must have access to the resource that you're trying to export to, as we can see on this slide for cloud storage, BigQuery, and Cloud Pub/Sub. As new log entries arrive in Stackdriver logging, they are compared against the export sinks. If an entry matches a sink, it is exported to the destination. Log entries received before a sink is created will not be exported through that sink. Log entries exported to cloud storage are batched and sent out approximately every hour. Log entries exported to BigQuery and Cloud Pub/Sub, on the other hand, are streamed to those destinations immediately. So why should you export your logs to these services? Exporting logs to different storage classes of Cloud Storage lets you retain data longer, as Stackdriver Logging only retains the logs for 30 days. Exporting also lets you search and analyze logs in BigQuery, Google's data warehouse for analytics. Finally, exporting lets you create advanced visualizations with Cloud Datalab and stream logs to applications or endpoints with Cloud Pub/Sub. Let's wrap up this lesson, Logging, with some best practices. Don't use substring matches on service names or resource types. For example, searching for the text ABC will match log entries containing abc, XYABCYX, and even capital ABC. Instead, for an exact match, use the equals operator. Next, perform faster searches by searching for specific values of indexed fields. For example, specify your search for a log entry's name, resource type or resource label. Using advanced filters will result in even more effective queries. You can add filters directly from log entries. Finally, I recommend learning the advanced viewing interface. Start by setting up selections and filters in the basic interface, and then switch to the advanced interface.

Error Reporting
Let's learn about another feature of Stackdriver, Error Reporting. Stackdriver Error Reporting counts, analyzes, and aggregates the errors in your running cloud services. A centralized error management interface displays the results with sorting and filtering capabilities, and you can even set up real-time notifications when new errors are detected. Stackdriver Error Reporting is generally available for the App Engine Standard environment, and as a beta feature for the App Engine Flexible environment, Compute Engine, and AWS EC2. In terms of programming languages, the exception stack trace parser is able to process Java, Python, JavaScript, Ruby, C#, PHP, and Go.

Tracing
Tracing is another Stackdriver feature integrated into GCPN. Stackdriver Trace is a distributed tracing system that collects latency data from your applications and displays it in the GCP console. You can track how requests propagate through your application, and receive detailed near real time performance insights. Stackdriver Trace automatically analyzes all of your application's traces to generate in-depth latency reports to surface performance degradations, and can capture traces from all of your VMs, containers, or Google App Engine projects. Managing the amount of time it takes for your applications to handle incoming requests and perform operations is an important part of managing overall application performance.

Debugging
Finally, let's cover the last Stackdriver feature, which is debugging. Stackdriver Debugger is a feature of GCP that lets you inspect the state of a running application in real time without stopping or slowing it down. The users are not impacted while you capture the call stack and variables at any location in your source code. You can use it to understand the behavior of your code in production, as well as analyze its state to locate those hard to find bugs. Stackdriver Debugger can be used with both App Engine Standard and Flexible, Compute Engine, and Kubernetes Engine. The debugger supports multiple languages like Java, Python, and Go, although Go is in a beta release for Compute Engine and Kubernetes Engine as of this recording.

Lab: Error Reporting and Debugging with Stackdriver (Overview and Objectives)
Lab: Error Reporting and Debugging with Stackdriver (Review)
In this lab, you deployed an application to App Engine. Then you introduce a bug into the code, which broke the application. You used Stackdriver error reporting to identify and analyze the issue, and found the root cause using Stackdriver Debugger. You modified the code to fix the problem, and then saw the result in Stackdriver. Having all of these tools integrated into GCP allows you to focus on your code and any troubleshooting that goes with it.

Module Review
In this module, I gave you an overview of Stackdriver and its Monitoring, Logging, Error Reporting, Fault Tracing, and Debugging features. Having all of these integrated into GCP allows you to operate and maintain your applications, which is known as Site Reliability Engineering, or SRE. You can learn more about SRE and Stackdriver later in the specialization when we cover design and process.

Essential Cloud Infrastructure: Core Services Course Outro
This is Jasen Baker with the Google Cloud Platform, and thank you for completing the Essential Cloud Infrastructure Core Services course. So, what did you learn? Well, you learned about Identity Access Management, whether you created basic or curated roles. You've now learned about storage. Where does your data go? How do you monitor and manage the individual resources? You also worked on seven individual labs, learning about security and storage, setting up a managed Cloud SQL environment, so perhaps now maybe you won't set it up inside of VMs, you'll take advantage of Google's managed infrastructure. Perhaps you also started looking down the introduction for Google's NoSQL databases, examining Cloud Datastore. What about billing data? So now you've got a slight introduction to BigQuery, which is actually covered in some other courseware as well. You were also introduced to resource monitoring, error reporting, and debugging as well. And so, this all comes part of our Stackdriver platform, which encompasses monitoring, debugging, and logging, all into one individual service.

Course author
Author: Google Cloud	
Google Cloud
Google Cloud can help solve your toughest problems and grow your business. With Google Cloud, their infrastructure is your infrastructure. Their tools are your tools. And their innovations are your...

Course info
Level
Intermediate
Rating
0 stars with 4 raters
My rating
null stars

Duration
1h 53m
Updated
17 Jan 2019
Share course

