Getting Started with Augmented Reality on Android Devices
by Gianni Rosa Gallina

This course introduces Android developers to Augmented Reality concepts and applications.

This course introduces Android developers to Augmented Reality concepts and applications. AR apps are already available on smartphones and tablets, but in the continuing months and years, with the wide adoption of smart-glasses, they will redefine our lives by blending the physical world with virtual worlds and shifting applications from our device screens directly to our eyes. Starting from the theory, hands-on examples will enable students to quickly develop and deploy amazing Augmented Reality applications, either using low level algorithms or leveraging high-quality AR frameworks like Vuforia and Metaio.

Course author
Author: Gianni Rosa Gallina	
Gianni Rosa Gallina
Gianni is an R&D Senior Software Engineer in Deltatre's Innovation Lab, based in Italy, designing and prototyping next-generation solutions for sport-related experiences and services, from VR/MR...

Course info
Level
Beginner
Rating
3.8 stars with 43 raters(43)
My rating
null stars

Duration
3h 7m
Released
13 Jan 2015
Share course

Introduction: AR Fundamentals and Setup
Introduction
Hello and welcome to the course Getting Started with Augmented Reality on Android devices. I'm Gianni Rosa Gallina, a Software Architect from Italy and Windows Embedded MVP. During the last year, my daily work consisted of analyzing and developing a number of Augmented Reality projects on the Android platform, using Google Glass Android Wear devices. Used to Windows Phone and its development environment and tools, I had a big paradigm shift and learning curve, which gave me the idea to create this getting started course, for people like me, that want to start from scratch developing Augmented Reality applications. Augmented Reality offers the magic effect of blending the physical world with the virtual world and brings applications from your screen directly into your hands or in your face. Augmented Reality is redefining advertising, gaming, as well as education and industries in new ways, that just few years ago were unimaginable. It will become a technology that needs to be mastered by mobile application developers and this course enables you to practically implement sensor-based and computer vision-based Augmented Reality applications on Android. This course is targeted to people who want to write apps employing Augmented Reality for the Android platform, from smartphones to tablets, including wearable devices like Google Glass, Vuzix or Epson Moverio. It is assumed that you are familiar with the Java language, object-oriented concepts, the basics of Android platform, tools like Eclipse and ADT and app deployment. An effort has been made to ensure that even people without such experience can understand the content and code, but I'm not going deep on Android-specific stuff and samples will not present real-world architectures or best practices. It is beneficial if you have experience on working with external libraries for Android, as we make use of some third party libs. If you have already used the Android NDK, that's great, but not mandatory. While it is not an absolute requirement to have all these prerequisites, it is highly recommended. If you need to refresh something on such topics, you can have a look at other Pluralsight courses available in your library. If you prefer to read, these books are good starting points. In addition, there are two books dedicated to Augmented Reality for Android. If you need more details on the topics of this course, I suggest you to read them. If you want to develop Augmented Reality applications for Android, you can share a majority of tools with regular Android developers, the Android Development Tools Bundle. You can work on any operating system that supports Android development, so you can use Windows, Mac OS or Linux. I'll show you all the stuff using Windows 8. 1, but everything should work on the other platform as well. The Android Development Tools Bundle includes, the Eclipse IDE, the Android Developer Tools plugin for Eclipse, ADT, the Android platforms for your targeted devices. It also includes one or more Android emulators, but for this course you will absolutely need an Android device to test your apps, because many of the features used in the samples we're going to build, are not available on the emulator. Specifically, the Android device needs to have the back-facing camera, the GPS module and gyroscope, accelerometers and compass. Augmented Reality on mobile devices can be challenging as many integrated sensors have to be active during the running of applications and computationally demanding algorithms are executed. Therefore, I recommend to use them on a dual-core processor, or more, for the best AR experience. In addition, most AR applications, specifically the computer-vision based applications using Vuforia or Metaio, require enough processing power. Besides this standard package, common to many Android development environments, you will also need, a copy of the free and open-source AndAR library, which will help to implement marker recognition and tracking, a snapshot of jMonkeyEngine, version 3 or higher. JMonkeyEngine is a free game engine that brings the 3D graphics in your programs to life. It provides 3D graphics and gaming middleware that frees you from exclusively coding in low-level OpenGL, for example, by providing an asset system for importing models, predefined lighting, physics, and special effects components. Qualcomm Vuforia SDK version 3 or higher, Metaio SDK version 6 or higher. Vuforia and Metaio SDKs brings state-of-the art computer vision algorithms targeted at recognizing and tracking a wide variety of objects, including markers, image targets, 3D objects and more. While they are not needed for sensor-based AR, they allow to easily implement computer vision-based AR applications. Optionally, you will also need Cygwin and the Android Native Development Kit, even though we won't use them in our course. The NDK is a toolset for performance-critical applications, allowing parts of an application to be written in native-code languages such as C/C++. I'll just show you how to install and configure them, in case you would like to develop native Computer Vision applications for performance reasons. We'll see later in this module how to get all of them and how to setup your development machine, but now, let's have a look at how this course is structured.

Course Outline
The course will cover the fundamental topics to getting started with Augmented Reality. We start with a basic introduction to augmented reality and then move up through more complex features as we go, from theory and building blocks to practical results, leveraging a professional AR framework like Vuforia or Metaio. At the end, once we have all the basics to build our amazing apps, we'll see what's next in the near future, like 3D depth sensor integration and Virtual Reality. Let's see a more detailed structure. In a few moments, I will introduce you to Augmented Reality concepts and approaches, showing some examples of how augmented reality is currently used throughout the world and how it is changing our lives. At the end, we'll see step-by-step how to setup your development environment in terms of hardware, tools and SDKs. Next I will provide an overview of the basic building blocks to develop an AR application. We'll see how to capture and display the real world on an Android device and how to display 3D models in our applications, analyzing some approaches and requirements to overlay the virtual world over the physical world. Then, we'll have a deep look at Computer-vision AR, first using the Vuforia SDK, and then the Metaio SDK, two professional frameworks, thanks to which we'll see how to build high quality AR apps. An overview of sensor-based AR will follow, for outside scenarios using GPS, gyroscope, accelerometers and compass. We'll then see which approaches are available to make Augmented Reality applications interactive, such as virtual buttons, gesture recognition and proximity-based interaction. And finally, we'll have an overview on advanced and future topics like 3D depth sensor integration, such as Microsoft Kinect or Leap Motion and Virtual Reality with Oculus Rift. Great! There's a lot of stuff to see. Let's start!

Augmented vs. Virtual Reality
Prior to going deeper in the Augmented Reality world, AR, we need to clearly understand what AR is, and how it differs from Virtual Reality (VR). Unfortunately, as AR has become increasingly popular in the media over the last few years, several distorted notions of Augmented Reality have evolved. Anything that is somehow related to the real world and involves some computing, such as standing in front of a shop and watching 3D models wear the latest fashions, has become AR. But that is a big misunderstanding and it is often confused with Computer Graphics and Virtual Reality. Augmented reality and virtual reality are fields in which the lines of distinction are kind of blurred. To put it another way, you can think of Virtual Reality as the precursor to Augmented Reality, with some parts overlapping in both. The main difference between the two technologies is that VR does not use a camera stream. All the things displayed in VR are either animations or prerecorded bits of film. Augmented Reality, instead, offers us a new way to interact with the real world in real-time. It creates a modified version of our reality, enriched with digital information, on the screen of your desktop computer, your mobile device or on your smart-glasses. Merging and combining the virtual and the real can leverage a totally new range of user experience, going beyond what common apps are capable of. The term Augmented Reality itself contains the notion of reality. Augmenting, generally refers to the aspect of influencing one of our human sensory systems, such as vision or hearing, with additional information. This information is generally defined as digital or virtual and will be produced by a computer. The technology currently uses displays to overlay and merge the physical information with the digital information. Augmented Reality emerged from research labs a few decades ago and different definitions of AR have been produced. As more and more research fields, for example, computer vision, computer graphics, human-computer interaction, medicine, humanities, and art have investigated AR as a technology, application, or concept, multiple overlapping definitions now exist for AR. Despite being a relatively new field, there are enough AR apps available to allow us to make categories out of them. Here we take a look at what has already been implemented in the world of AR.

AR Apps Examples
There are hundreds of apps that use AR that are meant to be used by the average casual user. These apps are meant to be enjoyed and useful and most apps in this category can be implemented on the Android platform. They come in many types, for example, games, navigation apps, advertisement, or world browsers. They usually use the accelerometer, the gyroscope and the GPS to obtain location and the physical orientation of the device. Navigation apps adopt Computer Vision algorithms to recognize roads and turnings, and mark out the route with arrows. This process is not as easy as it seems, but is often done today. World browsers are probably the most complex of all the casual apps that are widely used. They need several back-end databases and also need a lot of real-time information from several sensors. After all, browsers still have to put everything together and display a set of icons on the screen. Almost every app you see on the market, whether AR or not, looks simple at first sight, but if you delve into the code and backends, you will realize that most of them are in fact, very, very complex and take a long time to create. The best examples of casual AR apps are BlippAR, LayAR or junaio. Together, these apps make use of practically everything you can use to make an AR app on the Android platform. I highly recommend that you install them and become familiar with the features of AR on Android. Uses by military and law enforcement agencies are much more complex and technologically advanced. They range from AR goggles to full simulators designed to help in training. The military, and some law enforcement agencies, have simulators that make use of AR and VR technology. For example, they use a wide screen inside a room or a vehicle on which various scenarios are presented, and the trainee must decide the best course of action. Specialized night vision goggles come with AR technology as well. These goggles display location and other information, along with trying to fill in gaps that could not be illuminated by the night vision goggles themselves. These kinds of apps are quite difficult to implement on current Android devices because of two main issues, low processing power and lack of more input devices and sensors. But that is changing, and in the near future and it will be possible to start developing more advanced application on consumer devices.

Business and Work Applications
As of late, vehicles have started implementing AR technology, where windscreens have been replaced with large, wide, and high-definition displays that shows additional useful information for the driver. These days, also AR-enabled surgeries are becoming more common. Surgeries done this way have a smaller error rate because the computer provides valuable inputs on the surgery and doctors have patient's vital signs constantly in their field of view. In several shops, AR is being tried out as a virtual trial room. The user can stand in front of a screen with a camera mounted somewhere. The user will see himself displayed on the screen and the computer will augment that item onto the user's image and display it on the screen. The user can turn to view himself from all angles. Also tourism has received some part of the AR magic as well. Around the world, organized tours now offer a head-mounted AR system that displays information about the current site and its buildings when you look at it. The same applies to museums, audio-guides are being replaced by AR-glasses, so tourists can move around the rooms and have additional information by looking at pictures and statues. Devices are being used to translate text from multiple languages all over the world or to enable live captioning or sign-language for deaf people. There are many uses for AR also in the architecture field. Displaying a virtual structure from the blueprints on the proposed site of construction, simulation of natural disaster conditions, and show how the building structure will react under that kind of pressure are just two examples. AR technology helps out a lot in factories and assembly lines, either for assembling cars, planes, mobiles, or anything else. Preprogrammed head goggles can provide step-by-step instructions on how to assemble it. AR technology has been successfully used in various educational institutes to act as add-ons to the textbook material or as a virtual, 3d textbook in itself. Normally done with head mounts, the AR experience allows the students to relive events as they are known to have happened, while never leaving their class, or giving them more engaging experiences with the subject they are studying. AR can be used in sports, for example to monitor athlete's performance or to get additional on-screen information on a field, during a race or a match. There are many, many more uses of AR that I haven't listed. They are mostly still in the designing and planning stages, but have the potential to forward AR technology to the forefront of daily gadgets. A decade ago, experienced researchers and computer vision experts would have been among the few who were able to create these types of applications. They were generally limited to demonstration prototypes or in the production of an ad hoc project running for a limited period of time. Now, developing AR experiences has become a reality for a wide range of mobile software developers. Over the last few years, we have been spectators to great progresses in computational power, the miniaturization of sensors, as well as increasingly accessible and featured Computer Vision libraries, like OpenCV, AndAR and professional frameworks like Vuforia by Qualcomm or Metaio SDK and tools. These advances allow developers to produce AR applications more easily than ever before. This already leads to an increasing number of AR applications flourishing on mobile app stores. While an enthusiastic programmer can easily stitch together some basic code snippets to create a facsimile of a basic AR application, they are generally poorly designed, with limited functionalities, and hardly reusable. To be able to create sophisticated AR applications, one has to understand what Augmented Reality truly is. In the next module, we will achieve a better understanding of AR. We will see some of the major concepts and we will then move on from these examples to the foundational software components for AR. Prior to continue, let's setup all the development tools that we will use throughout the rest of the course.

Where to Get the Software
First of all, if not already installed, we need to get the Eclipse IDE. You can get it from https://www. eclipse. org/. Click on Download, then click on Eclipse IDE for Java Developers link. At the time of this course, the latest version is Luna. Everything should work with higher versions but, just in case, please use Luna. Then download Eclipse IDE for Java Developers for your system. To run Eclipse, you also need the Java Virtual Machine and Java SDK. Go here http://www. oracle. com/technetwork/java/javase/downloads/index. html then click on the download button for the latest Java Platform. Currently, there is Java 8 update 25. Accept the license and pick the right version for your system. Just a little note here, during my work and experiments with Android NDK, I noticed that there were some strange errors at compile time using the 64-bit JDK on other Windows machines. On my Windows machine I'm running the 64bit version without issues. In case you have strange compile time errors, and you are using the 64bit JDK, try to uninstall it and reinstall the 32bit version. Next, we need to get the Android Developer Tools, or ADT. You can download it from the official site: http://developer. android. com/sdk/index. html From this page, you could get the Bundle version, which includes Eclipse, but while developing with the NDK, I found a problem with the ADT Bundle version. So, please, download the stand-alone ADT from here. Accept the license and then click on the Download button. In addition to Android Tools, we need to get the NDK. This is an optional pass, I just want to show you how to install and configure it, but in this course we won't use it in our sample applications. It could be useful in case you would like to develop high-performance Computer Vision applications using OpenCV or other native frameworks, like Vuforia. So click on NDK in the left menu and get the right version for your system. To work with the NDK and compile applications which use OpenCV stuff on the Windows platform, you also need Cygwin. So go to https://www. cygwin. com/ and download the right setup for your system. Like the NDK, this is optional, I just want to show you how to install and configure it, but in this course we will not use it. Prior to proceed, we need some more software. First, the Vuforia SDK. Vuforia is a state-of-the-art library for computer vision recognition and natural feature tracking. You can get it from https://developer. vuforia. com/. Click on Download and then, Download again. If you are not already a Vuforia Developer, you will be asked to freely register. We'll see Vuforia in details in module 3. We also need the Metaio SDK. So go to http://dev. metaio. com/, then click on Download. If you are not already a Metaio Developer, you will be asked to freely register. We'll see Metaio SDK in details in module 4. To help us with computer vision and marker recognition, we'll see how to integrate the open-source library AndAR. You can get the latest bits from https://code. google. com/p/andar/ using an SVN or Git client. I already checked it out in a dedicated folder on my machine. Finally, to make easier to show 3D content in our augmented worlds, we'll use an open-source 3D Engine, jMonkeyEngine. It is a powerful Java-based 3D game engine which comes with its own development environment, based on NetBeans. Instead of using the JME IDE, we will integrate the base libraries into our projects in Eclipse. You can get it from http://jmonkeyengine. org. Click on Download, and then pick the right version for your system. We'll see how to integrate and use it in module 3. Now that we have downloaded the required software, let's configure it.

Development Environment Setup
While the development tools can be spread throughout the system, I recommend that you use a common base directory for both the development tools and the sample code, let's call it AndroidDev. I suggest you to put everything in a folder without spaces, to avoid strange compiler errors when building your applications. First, if not already present, install the Java Virtual Machine and JDK. I'm going to install it in a dedicated folder under AndroidDev. (working) Once done, we need to configure our environment variables from Control Panel. We already have the JAVA_HOME variable correctly set and the JDKROOT variable to point to the same path. Also, add it to the system PATH. Now, you are ready to run eclipse. Let's unzip it in an "eclipse" folder under the AndroidDev. Once done, launch eclipse. Choose an appropriate folder for your workspace. I put it under AndroidDev\Workspace_Pluralsight folder. Close eclipse. Now we can install the Android Developer Tools. Run the installer and wait for its completion. (working) Once finished, it will open the Android SDK Manager. From here, choose the following items, API21, API 20, API 19, Android 4. 0, Android 3. 0. It will take some time to download and install all the stuff. Meanwhile, add the Android Platform root folder to the system path, PATH: D:\AndroidDev\android-sdk\platform-tools. Once finished, open eclipse. Under the Window menu, you should see the Android SDK manager and Virtual Device Manager items. Now, let's install Cygwin and the Android NDK. As I already said, this is optional, I just want to show you how to install and configure it, but in this course we will not use it. First, run the cygwin setup. We'll install just the minimum stuff needed to work with OpenCV or Vuforia. Search for gcc and select core and g++. Then filter for make and select cmake and make, and all their dependencies. It will take some time to download and install all the stuff. Meanwhile, add the Cygwin path to the system path variable. And then create the CYGWIN variable to point to the same. Once finished, install the Android NDK using the installer. Once done, create the NDKROOT variable to point to the NDK root folder. Launch eclipse, and then open the Preferences dialog, from Windows -> Preferences. Under the Android node, you can find the NDK. Select it, and in the textbox on the right, configure the path to the NDK root folder, as did with the system variable. We finished to configure our system. Now we are ready to go on in our Augmented Reality adventure.

Summary
Right, we've completed the first module. In this first part we have seen, who the course is for and what technical knowledge is needed to proficiently follow it. The course is targeted to Android developers who want to write apps employing Augmented Reality, with a good knowledge about the Java language, object-oriented concepts, the basics of Android platform, tools like Eclipse and ADT and app deployment. If you lack of any of them, to get the best from this course you can follow other courses available in the Pluralsight catalog, dedicated to the specific technologies used here. Then we clarified the difference between Augmented Reality and Virtual Reality. Augmented reality and virtual reality are fields in which the lines of distinction are kind of blurred. The main difference between the two technologies is that VR does not use a camera stream and all the things displayed in VR are either animations or prerecorded bits of film. Augmented Reality, instead, offers us a new way to interact with the real world in real-time. It creates a modified version of our reality, enriched with digital information. We've had an overview of what has already been implemented in the world of AR, from casual user apps like games, navigation tool, advertisement and world browser to military applications, and potential uses of AR at work, in factories, in education and sport. Finally, we've seen which development environment is required to follow the cours, a Windows, Mac Os or Linux machine, the Android Development Tools, an Android device, and some additional libraries for Augmented Reality. We've also seen where to get the software and how to configure the system step-by-step. Let's move to module 2, where we'll have a look at the building blocks to develop an AR application.

AR Building Blocks
Augmented Reality Definition
Hi, I'm Gianni Rosa Gallina. Welcome to the second module of the course Getting Started with Augmented Reality on Android devices. By now, you have a basic idea of what augmented reality is, what is being done with it around the world, and what you can do with it on an Android device. This module will launch you into the world of AR on Android and teach you the basics of it. In the previous module we've seen a possible definition of the term Augmented Reality, which contains itself the notion of reality. Augmenting generally refers to the aspect of influencing one of your human sensory systems, such as vision or hearing, with additional information. This information is generally defined as digital or virtual and will be produced by a computer. The technology currently uses displays to overlay and merge the physical information with the digital information. There are other type of augmented reality, such as augmented hearing, where modified headphones or earphones equipped with microphones are able to mix sound from your surroundings in real-time with sound generated by a computer. In this course, as already said, we will mainly look at visual augmentation and related technologies.

Display Technologies for AR
Let's start from Displays. The TV screen at home is the ideal device to perceive virtual content, streaming from broadcasts or played from your Blue-Ray. Unfortunately, most common TV screens are not able to capture the real world and augment it. An Augmented Reality display needs to simultaneously show the real and virtual worlds. Currently there are two display technologies for AR applications, one is the Optical see-through technology. The idea is to still see the real world through a semitransparent screen and project some virtual content on the screen itself. The merging of the real and virtual worlds does not happen on the computer screen, but directly on the retina of your eye. This technology is the one you can find in the current and upcoming generation of head-mounted displays. The other major trend in AR display is the video see-through technology. You can imagine perceiving the world not directly, but through a video on a monitor. The video image is mixed with the virtual content and sent back to some standard display, such as your desktop screen or your mobile phone. In this course, we will work on Android devices with back-camera and, therefore, we'll discuss only VST systems. With a display in your hands, either OST or VST, you are already able to superimpose things from the real world, as you can see in TV shows, sports, or advertisements with text and graphics overlaid on the screen. However, any virtual content will remain fixed in its position on the screen. With this overlay being static, your AR display will act as a head-up display, or HUD, but it won't really be an Augmented Reality experience. Google Glass is an example of an HUD. While it uses a semitransparent screen, like an OST, the digital content remains in a static position. You could use it for AR applications, but the resulting experience would be very poor. You would obtain better results on smart-glasses like the ones produced by Epson or Vuzix.

Registration and Tracking
Actually, a good AR experience requires more information about real and virtual content. It needs to know where things are in space, registration, and follow where they are moving, tracking. Registration is basically the idea of aligning virtual and real content in the same space, while tracking is maintaining the alignment while the objects, or the camera itself, change position. Probably, without even knowing it, you have already seen computer vision-based registration. If you are into sports, you will notice that 2D or 3D graphics are superimposed onto scenes of the physical world quite often. For example, in soccer, players, balls or field areas are often highlighted with colored markers. The same happens in movies with lots of cinematic effects, where real and virtual elements are seamlessly blended. For example, fake explosions, fake backgrounds or fake characters. In the same way as AR, the movie industry has to deal with the registration between digital and physical content, relying on analyzing the recorded image to recover tracking and camera information. However, AR differs from those effects as it is based on all of the following aspects. It's in 3D, in the old days, movies were edited manually to merge virtual visual effects with real content. Nowadays, more complex techniques support merging digital 3D content, such as characters, monsters or explosions, with the video image, and it's called match moving. AR is inherently always doing that in a 3D space. The registration happens in real time, in a movie, everything is prerecorded and generated in a studio, you just play the media. In AR, everything is in real time, so your application needs to merge, at each instant, reality and virtuality. It's interactive. In a movie, you only look passively at the scene from where it has been shot. In AR, you can actively move around, forward, and backward and turn. Building a rich AR application needs interaction between environments, otherwise you end up with pretty, 3D graphics that can turn boring quite fast. AR interaction refers to selecting and manipulating digital and physical objects and navigating in the augmented scene. Rich AR applications allow you to use objects which can be on your table, to move some virtual characters or use your hands to select some floating virtual objects while walking. In a following module we will discuss mobile-AR interactions. We will look at how some of the standard mobile interaction techniques can also be applied to AR. We will also dig into specific techniques involving the manipulation of the real world.

Mobile AR
In the previous parts, we discussed what AR is and elaborated on display, registration, and interaction. Some of the notions in this course can also be applied to any AR development, but we will specifically look at mobile AR. Mobile AR refers to any transportable, wearable AR system that can be used indoors and outdoors. In this course, we will look at mobile AR with the most popular connotation used today, using handheld mobile devices, such as smartphones, tablets and smart-glasses. With the current generation of devices, two major approaches to the AR system can be realized. These systems are characterized by their specific registration techniques and their interaction range. They both enable a different range of applications. The first type of system is called sensor-based AR and generally is referred to as outdoor AR system. Sensor-based AR uses the location and orientation sensor from a mobile, combining both delivers the global position of the user in the physical world. The location sensor is mainly supported by a Global Position System receiver, which is present on most smartphones nowadays. The position reported by the GPS module can be both inaccurate and updated slower than you move around. This can result in a lag, that is, when you do a fast movement, virtual elements seem to float behind. There are several possible orientation sensors available on handheld devices, such as accelerometers, gyroscopes and compass. The measured position and orientation of your handheld device provides tracking information, which is used for registering virtual objects on the physical scene. One of the most popular types of AR applications with sensor-based systems are AR browsers, which visualize Points of Interests around you. You can try some of the most popular products such as Layar, Wikitude or Junaio for an example of this type. The advantage of this technique is that sensors are working on a general scale around the world, in practically any physical outdoor position, either if you are in the middle of the desert or in a city. One of the limitations of such systems is their inability to poorly work inside, or not working at all, or in any occluded area, when there's no line-of-sight with the sky, such as in forests or in cities with high buildings all around. We will discuss more about this type of mobile AR system in a dedicated following module. The other popular type of AR system is computer vision-based AR. The idea here is to leverage the power of the built-in camera for more than capturing and displaying the physical world, as done in sensor-based AR. This technology generally operates with image processing and computer vision algorithms that analyze the image to detect any object visible from the camera. This analysis can provide information about the position of different objects and, therefore, the user. The advantage is that things seem to be perfectly aligned. The current technology allows you to recognize different types of Planar or 3D content, such as a specifically designed marker, in this case we have marker-based tracking, or more natural content, called marker-less tracking. One of the disadvantages is that vision-based AR is heavy in processing and can drain the battery really rapidly. Recent generations of smartphones are more adapted to handle this type of problem, being optimized for energy consumption, but keep in mind this issue when designing your applications. We will discuss more about this type of AR system later in this module and in the next ones.

AR Software Architecture
As in the development of any other software, some well-known concepts of software engineering can be applied in developing an AR application. In a moment, I'll show you the typical structural components of an AR application and later we'll see the typical control flow. An AR application can be structured in three layers, the application layer, the AR layer, and the OS/Third Party layer. The application layer corresponds to the domain logic of your application. For example, if you want to develop an AR game, anything related to managing the game assets such as characters, scenes, objects, or the game logic, will be implemented in this specific layer. The AR layer corresponds to the concepts we've previously described. Each of the AR elements that I've presented (display, tracking, registration and interaction) can be seen, in terms of software, as a modular component or a service of the AR layer. For example, AndAR, Vuforia or Metaio can be considered part of this layer. The OS/Third Party layer corresponds to existing tools and libraries which don't provide any AR functionalities, but will enable the AR layer. For example, the Display module for an AR mobile application will communicate with the OS layer to access the camera to create a view of the physical world. On Android, the Google Android API is part of this layer. Some additional libraries, such as jMonkeyEngine, which handle the 3D graphics, can be placed in this layer too. In the rest of the course, we will see how to implement or integrate the different modules of the AR layer, which also involves communication with the OS/Third Party layer. With the architecture and components in mind, we can now look at how information flows in a typical AR application. Over the last years, developers have converged toward a well-used method of combining these components using a similar order of execution, the AR control flow, or, better, the AR control loop. This schema has to be read from the bottom up. It shows the main sequence of tasks of an AR application, which is repeated in a loop indefinitely. First, the camera is accessed and the current frame is retrieved and shown. Then, depending on the AR type, either sensors data is retrieved or the camera frame is processed with Computer Vision algorithms, to find and track objects. Immediately after, virtual objects are transformed and moved in the virtual world to match the real world. Finally, user interactions and gestures are processed, for further changing virtual objects or performing domain-specific tasks. It can be seen as the typical AR main loop, where each activity corresponds to the same module I've presented before. In parallel, we must add the domain-specific logic and other OS-related activities.

Camera Device
Now we'll focus on making an app that contains the main part of any advanced AR app, the camera stream. We'll learn how to give our applications the view of the real world. To understand this concept, let's have a look at the camera application we have installed on our mobile. What we can see on the viewfinder of the application is a real-time video stream captured by the camera and displayed on the screen. If we move the device around while running the application, it seems like we are seeing the real world "through" the device. Actually, the camera seems to act like the eye of the device, perceiving the environment around us. This process is also used for mobile AR development to create a view of the real world. It's the concept of see-through video that I introduced previously. The display of the real world requires two main steps, capturing an image from the camera displaying this image on the screen. This process is generally repeated in an infinite loop, creating the real-time aspect of the view of the physical world. We will see how to implement both of these steps using two different graphics libraries. A low-level one, using the built-in Android library with a custom OpenGL renderer, and a high-end one, leveraging jMonkeyEngine. While the Android library allows us to quickly display the camera image, it is not designed to combine it with 3D graphics, which we want to augment on the video stream. Therefore, we need a way to combine the frame from the camera with 3D content. Prior to see some code, let's try to understand the camera device. Nowadays cameras on mobile devices share many characteristics with digital cameras. They generally support two operative modes, the still image mode, which is an instantaneous, singular capture of an image, or the video mode, which is a continuous, real-time capture of images. Video and image modes differ in terms of capabilities. An image capture usually has a higher resolution than video. While modern smartphones can easily achieve 12 megapixel in the still image mode, the video mode is usually limited to 1080p, about 2 megapixels. In AR, normally a much lower resolution, like VGA, is used, for efficiency reasons. Also, the video is not stored anywhere, it's just displayed on the screen. This mode has a special name in the Android API, the preview mode. While popular camera apps use only the preview mode for capturing a video or an image, it is the basis for the view of the real world in AR applications. Some of the common parameters of the preview mode are, resolution, it is the size of the captured image, which can be displayed on the screen. Resolution is defined in pixels in terms of width and height of the image. The ratio between them is called the aspect ratio, which gives a sense of how square an image is, such as 1:1, 4:3, or 16:9. Frame rate (FPS), it defines how fast an image can be captured. The higher the resolution, the lower will be the frame rate, which means an application might look prettier if things do not move fast in the image, but will run more slowly. In contrast, an application may run fast but the image will look pixelated. A good compromise has to be found, depending on the application. White balance, it determines what will be the white color on an image, mainly dependent on the environment light. If the white balance is not set properly, the appearance of digital models overlaid on the video image will not match and the AR experience will be diminished. Focus, it defines which part of the image will appear sharp and which part will not be easily discernible, out of focus. Like any other camera, mobile cameras usually support autofocus mode. If possible, it should be turned off. Pixel format, the captured image is converted to a specific image format, where the color and luminance of each pixel is stored under a specific format. The pixel format not only defines the type of color channels, such as RGB versus YCbCr, but also the storage size of each component, for example, 5, 8 or 16 bits. Some popular pixel formats are RGB888, RGB565, or YCbCr422. Cameras on mobile devices use compressed image formats and typically do not offer the same performance as high-end desktop webcams, combining video image often in RGB565 with 3D rendered content using RGB8888 can lead to color differences between them. There are other properties of the camera that usually are not available through the API, but are important to be considered during the development of an AR application. They are exposure time, aperture, and field of view. I will only discuss one of them here, the field of view. The field of view corresponds to how much the camera sees from the real world, such as how much your eyes can see from left to right and top to bottom. For comparison, the human vision is around 120 degrees with a binocular vision. The field of view is measured in degrees, and varies largely between cameras, from 15 to 60 degrees. The larger the field of view is, the more it will capture the view of the real world and the better will be the experience. The field of view is dependent on the hardware characteristics of the camera, like the sensor size and the focal length of the lens. Estimating this field of view is important and can be done with additional tools, like we'll see in this module.

Accessing the Camera in Android
To start, we will create a simple camera activity to get to know the principles of camera access in Android. While there are convenient Android applications that provide quick means for snapping a picture or recording a video through Android intents, we will get our hands dirty and use the Android camera API to get a customized camera access for our first application. I will guide you, step-by-step, in creating your first app showing a live camera preview. This will include, creating an Eclipse project, requesting relevant permissions in the Android Manifest file creating a SurfaceView to be able to capture the preview frames of the camera, creating an activity that displays the camera preview frames, and setting camera parameters. You can find all the source code in the course companion files. Let's start Eclipse and create a new project. Go to File | New | Android Application Project. In the configuration dialog box, fill in the appropriate fields, like I'm doing here. ViewWorldSample as the app name, com. arandroidcourse as the namespace. Set API level to 19. Stuff should work on previous Android 4. x API level, but I'm going to do all the samples on Android 4. 4 KitKat. No theme. Then, click on two more dialog boxes by accepting the default values. Then, in the Create Activity dialog box, select the Create Activity checkbox and the BlankActivity option. In the following New Blank Activity dialog, leave everything as default. Finally, click on the Finish button and your project should be created and be visible in the project explorer. For every AR application we will create, we will use the camera. With the Android API, you explicitly need to allow camera access in the Android manifest declaration of your application. In the top-level folder of your ViewWorldSample project, open the AndroidManifest. xml file in the text view. Then add the following permission. Besides this permission, the application also needs to at least declare the use of camera features. Since we want to run the AR application in fullscreen mode, for better immersion, add the following option into the activity tag. We will force our application to landscape mode and that we will handle certain config changes ourselves. In its most basic form, our Activity class takes care of setting up the Camera instance. As a class member, you need to declare an instance of a Camera class. Be sure to import Camera from the hardware package, not the graphics package, because that is a different Camera class. For an Augmented Reality application, we want to display a stream of live images from the back-facing camera on the screen. In a standard application, acquiring the video and displaying the video are two independent procedures. With the Android API, we explicitly need to have a separate SurfaceView to display the camera stream as well. The SurfaceView class is a dedicated drawing area that you can embed into your application class and implement a SurfaceHolder. Callback interface. Let's create a new class and call it PreviewSurface. It generates from SurfaceView. We have to make it implements the SurfaceHolder. Callback. This interface is used to react to any events related to the surface, such as the creation, change, and destruction of the surface itself. Let's fix the errors. We need a constructor and the methods required by the interface. (working) Accessing the mobile camera is done through the Camera class. In the constructor, we pass the Android Camera instance defined previously. We need to store this instance somewhere, so let's add a private field. We then need to configure this PreviewSurface as the manager of the camera, so we have to define a SurfaceHolder and assign it in the constructor. In the surfaceChanged method, we take care of passing an initialized SurfaceHolder instance, that is the instance that holds the display surface, and starting the preview stream of the camera, which we later want to display and process in our own application. These calls need to be surrounded by a try/catch block, in case of runtime problems. (working) The stopping of the camera preview stream is important as well. The inherited methods, surfaceCreated() and surfaceDestroyed(), remain empty. There's nothing really ground-breaking in that code. Going back to MainActivity, let's setup the UI. We do that in the onResume method. Instead of using a normal layout such as LinearLayout or RelativeLayout, we simply set a SurfaceView as content, with its height and width attributes to set to allow it to fill the entire available screen. First, we need to get the camera instance and then initialize it. Here, I'm using two helper methods, getCameraInstance and initializeCameraParameters. I'm just copying them to avoid errors and then I'll explain later. We also need to define the m_Preview field, which holds a PreviewSurface instance. Going back to the helper methods, the first, just ask the Camera API for the rear camera instance. The initializeCameraParameters, instead, configures the Camera stream, in order to retrieve a 6 40 by 4 80 frame. If the current device does not support that resolution, it will retrieve the closest one to the desired, using another helper method that retrieves all the supported resolution and look for the one requested. Returning to the onResume method, in case of camera hardware problem or when the camera is missing, we can show another view instead of the PreviewSurface. We can add it in the onResume method, instead of the onCreate, as usually done in the blank template. We can optionally enable, or disable, the menu, depending on the application. Finally, we need to disable the default layout in the onCreate method. Prior to test the application, we need to correctly dispose the camera when the application is paused or closed. So, add the onPause method, which stops the preview stream and release the camera. To test your application, you can do the same with any other project. Please connect your testing device to your computer via a USB cable. In Eclipse, right-click on your project folder, ViewWorldSample, and in the pop-up menu go to Run As, 1 Android Application. You should now be able to see the live camera view on your mobile screen as soon as the application is deployed and started.

Virtual Content Overlay
In the previous example, you got a glimpse of how you can access the Android camera with the API available in Android. As mentioned before, augmented reality is the overlaying of data that is related to the direct or indirect camera preview being displayed. In the majority of AR apps, the camera preview is first scanned for markers or other features, like text, pictures or 3D objects. For now, please take for granted the concepts of marker and tracking. We'll explore them in the next module. It is enough to know that markers helps a lot Computer Vision algorithms and generally they are defined in square shape and black and white. Simplifying the tracking, that is, their analysis and recognition in the physical world. Think of them as barcodes or QR codes in 3D, able to give precise information about their position and orientation in the real world, from the point of view of the camera. In certain kind of apps no scanning is done, instead characters, buttons, text, and so on are overlaid on the preview, like a HUD. To achieve that, we need another technique to overlay the virtual content over the video stream. There are different ways to do that. In this module I'll show you a low-level approach, overlaying standard UI elements such as textbox, buttons, labels directly on the camera preview surface, combined with low-level OpenGL graphics and an open-source AR library, AndAR, for marker tracking. Of course, you can use any other existing AR library, or write yours. This is just an example to show you where to start. Prior to start from the low-level approach, let's have a brief overview about 3D rendering.

3D Rendering Concepts
Representing and rendering virtual 3D content operates in the same way as when you take a picture with a digital camera. First, you check the subjects with your eyes and, after that, you look at them through the viewfinder of the camera. Only then you will take the picture. These three different steps are the same with virtual 3D content. The physical camera is replaced by a virtual camera to render the scene. The virtual camera can be seen as a digital representation of a real camera and can be configured in a similar way, it has a position, a field of view, and so on. With virtual 3D content, you manipulate a digital representation of a geometrical 3D scene, which we call the virtual world. The fundamental steps for rendering a scene using computer graphics are configuring the virtual scene, configuring the virtual camera, and rendering the 3D scene from the virtual camera point-of-view. As we do real-time rendering for AR, these steps are repeated in a loop and objects or the camera can be moved at each time frame, typically 25 or 30 times per second. A virtual camera for 3D graphics rendering is generally represented by two main sets of parameters, the extrinsic and intrinsic parameters. The extrinsic parameters define the location of the camera in the virtual world. The transformation from the world coordinate system to the camera coordinate system and vice versa. Intrinsic parameters define the projective properties of the camera. Every camera has its own parameters and it is important to know them. They can be represented with different data structures, but usually they are stored in matrices. There are different computational models for representing a virtual camera and it parameters, and we will see the most popular one, the pinhole camera model. The pinhole camera model is a simplified model of a physical camera, where you consider that there is only a single point, the pinhole, where light enters your camera image. With this assumption, computer vision researchers simplify the description of the intrinsic parameters as, focal length of lens. This together with the size of the camera center determines the field of view of the camera. It is the extent of the object space the camera can see and is represented in radians or degrees. It can be determined for the horizontal, vertical, and diagonal direction of the camera sensor. Image center, this accommodates any displacement of the sensor from the center position. Skew factor, this is used for non-square pixels. For Augmented Reality applications, virtual camera parameters should be configured to match those of a real camera, especially the field of view and the position. If you don't know the intrinsic parameters, you can use some default values or values you can find in the Internet for your device, to increase the realism of a scene on a specific device. You actually need to perform some measurements, called Camera Calibration. And the calibration should be performed for each device. In our examples, such precision is not required, but keep in mind that for optimal results you need to match the field of view of your virtual and physical cameras as much as possible. Without entering in these details, there are tools available on the Internet that helps you in the Camera Calibration. One of the most used tool is GML C++ Camera Calibration Toolbox, you can find it here: http://graphics. cs. msu. ru/en/node/909. While positioning objects in a scene, or the camera in a scene, we need a way of representing the location and the orientation of objects as functions of each other. To do so, we generally use some spatial representation of the scene based on geometric mathematical models. The most common approach is to use Euclidian geometry and coordinate systems. A coordinate system defines a method of referencing an object, or point, in a space using a numerical representation to define this position, the coordinates. Everything in a scene can be defined in a coordinate system, and coordinate systems can be related to each other using transformations. Transformations and coordinates are stored in vectors and matrices. The most common coordinate systems are World, Camera and Local. World Coordinate System is the ground where you reference everything. Camera Coordinate System is placed in the world coordinate system and used to render your scene seen from this specific viewpoint. Local Coordinate Systems are, for example, object coordinate systems, used to represent the 3D points of the objects. Traditionally, you use the geometric center of an object to define its local coordinate system. Great, after this short overview about 3D graphic concepts, we are ready to continue our sample application.

Overlays and AndAR
This is what we're going to achieve in the next minutes. (showing images on the screen) Returning on our ViewWorldSample project, we need to apply some changes in order to be able to overlay standard widgets. First of all, we need to configure the main activity layout, to allow for widget overlays. Earlier, the entire layout was a single custom SurfaceView, In Android, layouts can be defined either using XML or by code. Depending on the needs and the control you have over third party libraries, you could define the overlays directly in XML. In this sample, we'll integrate the AndAR library which, as we'll see in a moment, defines its layout by code. So, if we want to add our overlays, we need to define them by code too. An alternative, you can re-implement values so that AR and OpenGL stuff in a custom activity and define the layout as you prefer. In this example, we will be adding various TextViews to display information from detected markers. So before we get to the layout editing, we need to add some string resources to the project's strings. xml file. These strings will provide the labels for some of the TextViews and a message to be shown when the AR library will found more than one marker in the view. In addition we define an emptyString to be used as place-holder. We can also delete the "hello_world" string, auto-generated by the template. And also, we need to fix the main_activity layout file, which we won't use, to remove the error. Next, we need to add AndAR library to our project. You should have downloaded it already. If not, go back to the last part of Module 1 for details. We'll use the latest sources, so Import the AndAR project in the workspace. It may complain about a missing Android target. I've already fixed it, but if this happen right click on the project and then open the Properties dialog. Go to Android and select Android 4. 4. 2. Then click OK. After that, open the ViewWorldSample project properties. Select Android and then click Add in the Library section. Choose the AndAR project. Once done, click OK. If you were not able to download and build AndAR from sources, you can find the JAR file in the course companion files. Now, we are ready to integrate the library. We could proceed in two ways, integrate the marker and OpenGL functionalities in our existing MainActivity or, and it's what we're going to do now, extending an AndARActivity, which already manages everything Augmented Reality related, like opening the camera, detecting the markers and displaying the video stream. We've seen how to integrate the camera preview stream in our application. The AndARActivity does exactly the same and, in addition, prepares the OpenGL rendering environment and the AR stuff. So, let's make our MainActivity extending an AndARActivity. We need to fix the import. And we also need to implement the UncaughtException method. Then, change protected to public, for onCreate method. As I already said, AndARActivity does not integrate an easy way to show Android overlays. The first thing to do is change the onCreate method in order to not load the layout from XML, but using the one defined from the AndARActivity. We've already disabled it at the end of the last sample. We add our overlays using a custom method, I called CreateLayout. Let's define it. I already prepared the desired layout programmatically. If you don't want to type it, get it from the course companion files. Let's fix the imports and create the missing fields for our UI overlays. We must also remove all the camera related code and the PreviewSurface class. (working) Now, we can initialize the AndAR engine, add an ARToolkit field and fix the imports. Next, in the onCreate Method, after the custom layout, we create a reference to a CustomRenderer, which we'll see in a moment, and set it as non-AR renderer. AndAR will use it to draw on the screen our 3d models or other stuff. Now the core AR part. In order to detect markers, we have to register one or more ARObject to an instance of ARToolkit. In this sample, we register three markers and their related 3D objects. Add three private fields, m_customObject1, 2, and 3. In this example, to maintain OpenGL stuff simple, they are cubes in different colors, which will be augmented over each marker. Registration requires the file name of a pattern file in its constructor and this file must be located in the assets folder of the Eclipse project. Now, let's implement the 3D object. You can find the Cube class in the course companion files. It must inherit from ARObject. In order to draw a custom object, the draw method has to be overridden. Before this method is invoked, a transformation matrix will have been applied already. This means the object will be aligned to the marker, without any further steps. This method will not be invoked, instead, if the marker belonging to the object is not completely visible. Pattern files can be created by a tool called MK PATT, which is part of the AR toolkit. They are used to distinguish the different markers, markers can be black and white or color, but there are a few limitations. The marker must be square in shape, its borders must be contrast well, and the border must be a solid color. If you want more information go to this AndAR wiki page, you will find all the instructions to create your own markers. Alternative, you can use this free online tool which does the same. I've already created three markers for the course. You can find them in the course companion files. Copy them in the projects assets folder. We're almost ready. In addition to the PATT files, AndAR requires an additional camera_para. dat file, used for its internal algorithms. You can find it in the AndAR project assets folder. Next, we need to define the CustomRenderer class. You can find it in the course companion files. It must implement the OpenGLRenderer interface. In the variable declarations, we specify the different types of lighting. The setupEnv is called before we display any of the cubes. It sets up the lighting and other OpenGL-specific stuff. The initGL() method is called once when the Surface is created. If we launch the application now, it should already work. If the camera sees one or more markers, AndAR will render the relative 3D cube on top of it! (showing files) There's one last thing we need to do, update our overlay TextViews. In order to do that, we can implement an update loop which checks for the marker visibility status. The status is updated by AndAR accordingly and we can show some information about the tracked markers on top of the 3d objects and camera stream. The update loop can be implemented in different ways. For simplicity, we add a loop thread that sleeps for 30 milliseconds and then launch an internal runnable to update the UI from the secondary thread. In this runnable we check for marker visibility and, if visible, we show its name and 3d position relative to the camera. If more than one marker is tracked, we show their names and a message. If you don't want to type all the code, get it from the companion files. If we launch the application now, it should track the markers, show the 3D cubes on top of them and update the overlay labels accordingly. This brings us to the end of the coding for this app. As you can see, adding AR to an application using a third party library is not so hard. The most difficult stuff is the 3D rendering. In the example, we've used a simple static cube model, but if you want to render more complex objects or play animations, using the low-level OpenGL can be overkill. In the next module, we'll see a better solution for 3d rendering, leveraging jMonkeyEngine.

Summary
In this module, we've seen the foundational background of AR. I've presented some of the main concepts of AR, such as sensory augmentation, dedicated display technology, real-time spatial registration of physical and digital information, and interaction with the content. We've then seen the difference between computer vision-based and sensor-based AR systems, the two major trends of architecture on mobile devices. The basic software architecture blocks of an AR application have also been described and will be used as a model for the rest of this course. I also introduced you to the concept of 3D rendering and 3D overlay for Augmented Reality. I described what a virtual camera is and its characteristics and remarked the importance of intrinsic camera parameters for accurate Augmented Reality. Finally we've also developed a first, basic and functional AR application. In the next modules, we will improve the 3D rendering system using the open-source jMonkeyEngine, and we'll see in details the concepts of Computer Vision-based AR with some practical examples using Vuforia SDK and Metaio.

AR with Qualcomm Vuforia SDK
Module Overview
Hi, I'm Gianni Rosa Gallina. Welcome to the third module of the course Getting Started with Augmented Reality on Android devices. Now that we have a view of the physical world on our displays and we know the basic concepts about 3D rendering and overlaying real and virtual content using a low-level approach, our next goal is to perform superimposition in a more structured way, using a 3D Engine, in order to create high quality overlays. Next, we will advance our understanding of Computer Vision-based AR and will explore the state-of-the-art of mobile Computer Vision AR, using the Vuforia SDK and its features. Let's begin with some code, by creating a new sample application.

Camera with jMonkeyEngine
Let's open Eclipse and open the project you can find in the companion app, in the before folder. From now on, we'll have a look at existing projects and I will describe you the most important parts. If you want, you can have a look at the companion files for the full implementation. I've already setup the manifest and other options for our augmented reality needs. They are identical to the previous sample app. Have a look at the previous module if you want to review them. In the preceding example app, we've seen how to access the Android camera with a low-level graphics library and how AndAR can help us a little, integrating low-level 3d rendering and Augmented Reality stuff. As I said, there is a better way to overlay the virtual 3d content over the video view, by using a managed 3D graphics library based on a scenegraph model. Now, we're going to integrate jMonkeyEngine and see how it can be used to display the video. Differently from our previous example, the camera view will be integrated in the 3D scene. Later, we'll add a 3d model. Just keep in mind that jMonkeyEngine is a very powerful and capable engine. In this example we're seeing just a very little part of it. If you want more information, please refer to the official wiki, where you can find tutorials, samples and documentation. This is what we're going to achieve in the next minutes. We'll have the camera stream as in the first sample, but now rendered in a 3D scene. Right, let's continue with our development. First, we need to add jMonkeyEngine library to our project. You should have downloaded it already. If not, go back to the last part of Module 1 for details. Using Eclipse, we only need the jar files of jMonkey. The easiest way to obtain them is to install the SDK into the development folder we created in module 1. Then, back in Eclipse, right-click on the AR project and go to Build Path, Add External JARs In the JAR selection dialog, browse to AndroidDev/jmonkeyplatform/jmonkeyplatform/libs. Please note that jmonkeyengine is composed by many libraries but, for the samples app we're building, we only use a few of them. Select the following JARs in the libs directory, android, blender, core, jog, and plugins. In case you need other features, add the other required libs in your project. When done, click Open. The use of jMonkey in Android is similar to what we've done for AndAR library, excluded the camera management, which, instead, is the same as the very first sample we've seen in module 2. You know that an Android activity is the main entry point to create an application. However, jmonkeyengine is a platform-independent game engine that runs on many platforms where Java is supported. The creators of jMonkey wanted to make the process of integrating the engine as easy as possible. Therefore, in Android, they explicitly differentiated between the jMonkey application, which does the 3D rendering of the scene, which can be shared with other platforms, and the Android specific parts in the jMonkey activity, which sets up the environment to allow the jMonkey application to run. They obtained this result by creating a specific class called AndroidHarness, which does all the stuff required to prepare the Android activity properly. For example, it maps touch events on the device screen to mouse events in the jMonkey application, catches unhandled exceptions and manages activity lifecycle. So, the first thing we need is an Android activity derived from the AndroidHarness class. We already have the MainActivity, I made it extending the AndroidHarness class. Also, here I've already added all the camera related code from the first sample project. Just like the MainActivity class in the previous sample, it holds instances of the Camera and PreviewSurface classes. In contrast, it also holds an instance of the actual jMonkey application responsible for rendering the 3D scene. It is defined in the AndroidHarness parent class. We haven't yet provided an instance of this class but only its fully qualified name. The instance of the class will be constructed at runtime through reflection by the AndroidHarness class. During runtime, we have access to the actual instance by casting the jMonkey application class, which AndroidHarness stores in its "app" variable, to our specific class. As mentioned before, differently from the previous example, we will not directly render the SurfaceView class. Instead, we will copy the preview images from the camera in each frame. To prepare for this, we define the preparePreviewCallbackBuffer method, to be called in the onResume method after creating the camera and setting its parameters. It allocates buffers to copy the camera images and forwarding it to jMonkeyEngine. jMonkey works with RGB565 images. If your camera does not support RGB565, it may deliver the frame in the Luminance and Chrominance format which you have to convert to the RGB565 format. To do that, a color space conversion method is needed, which is very common in image processing. Without entering in the details, you can find an implementation With this method in the sample project. So, for the conversion, the previewWidth, previewHeight, and pixel format info are queried by calling the getParameters method of the camera instance in the preparePreviewCallbackBuffer method and determine the size of some byte arrays holding the image data. A jMonkey image will be passed to the jMonkey application, which is constructed from a Java ByteBuffer class, which itself just wraps the RGB565 byte array. Having prepared the image buffers, we now need to access the content of the actual image. In Android, we can do this by implementing the Camera. PreviewCallback interface. In the onPreviewFrame method of this object, we get access to the actual camera image stored as a byte array. The setVideoBackgroundTexture method of the ViewWorldJME class simply copies the incoming data into a local image object. Finally, we register the implementation of the Camera. PreviewCallback interface in the onSurfaceChanged method of the PreviewSurface class. In order to achieve this, we have to change a little the existing implementation of the PreviewSurface class. First, add a new constructor parameter, in order to pass the callback method, and then assign the method to the camera instance. Prior to run the new application, we need to define the jMonkey application. As said few moments ago, the jMonkey application is the place where the actual rendering of the scene takes place. It should not concern itself with the details of the Android system, which were described earlier. jMonkey provides a convenient way to initialize an application with many default settings. All we have to do is inherit from the SimpleApplication class, initialize our custom variables in simpleInitApp, and eventually update them before a new frame is rendered in the simpleUpdate method. For our purpose of rendering the camera background, we will create a custom ViewPort, and a virtual Camera for rendering the observed scene. While the virtual camera determines how the 3D graphics are projected on a 2D image plane, the viewport defines the mapping of this image plane to a part of the actual window in which the application runs, or the whole screen of the smartphone, if the app runs in full screen mode. It determines the portion of the application window in which graphics are rendered. Multiple viewports can be stacked and can cover the same or different screen areas. One is associated with the camera rendering the background video and one is used with a camera rendering the 3D objects. Typically, these viewports cover the whole screen. The viewport size is not defined in pixels but it's unitless and is defined from 0 to 1 for the width and height to be able to easily adapt to changing window sizes. One camera is associated with one viewport at a time. The common method to display the video in a scene graph such as jMonkey is to use the video image as a texture, which is placed on a planar object, usually a quadrilateral mesh. Let's have a more detailed look at this essential method for setting up our scenegraph for the rendering of the video background. Later, we'll add a second viewport for our 3D content. We do all this stuff in the initVideoBackground method. First, we create a quad shape and assign it to a jMonkey Geometry object. To assure correct mapping between the screen and the camera, we scale and position the geometry according to the dimensions of the device's screen. We then assign a material to the quad and also create a texture for it. Since we are doing 3D rendering, we need to define the camera looking at this quad. As we want the camera to only see the quad placed in front of the camera without distortion, we create a custom viewport and an orthographic camera. This orthographic camera has no perspective transformations. Orthographic projection, on the left, and perspective projection, on the right, define how the a 3D object is projected on a 2D image plane. Remember that for the video background we need to use an orthographic camera to avoid perspective transformations of the video image. However, this perspective is crucial for getting a proper visual impression of our 3D objects. Now, we have our camera looking at the textured quad rendered fullscreen. All that is left to do is update the texture of the quad each time a new video frame is available from the camera. We will do this in the simpleUpdate method, which is called regularly by the jMonkey rendering engine. You may have noted the usage of the conditional test on the FrameAvailable variable. This is because the scenegraph renders its content with a different refresh rate, potentially up to 60 fps, on a modern hardware, compared to what a mobile camera can normally deliver, typically 25 or 30 fps. We use the FrameAvailable flag to only update the texture if and when a new image becomes available. So this is it. With all these steps implemented, now you can compile and run your application and you should get a result similar to mine. (working) Great. Now it's time to add the 3D content.

Adding a 3D Model
You certainly want 3D objects to appear bigger as the camera moves closer to them and smaller as it moves away. So how do we go along? Right, we just add a second camera, this time a perspective one, and an associated viewport that also covers the whole application window. jMonkey uses a right-handed coordinate system, adopting the OpenGL convention, x on the right-hand side, y upwards, and z towards you. There are other engines or frameworks that, instead, adopt a left-handed coordinate system, with z pointing towards the screen. When working with computer graphics, this is the first thing to keep in mind. In the simpleInitApp startup method, we now explicitly differentiate between the initialization of the scene geometry. For video background we use initVideoBackground, while for the 3D foreground scene we use the initForegroundScene, and the associated cameras and viewports. Only when we first add the camera and viewport for the video background and later add the foreground camera and viewport, can we ensure that our 3D objects are rendered on top of the video background. Otherwise, you would only see the video background. We will now add our first 3D model into the scene using initForegroundScene. A convenient feature of jMonkey is that it supports the loading of external assets, for example, Wavefront files obj files or Ogre3D files. mesh and scene, including animations. We will load and animate <<<Oto>>>, a sample asset that you can find in the jMonkey samples. This method loads a model relative to the project's Asset folder. If you want to load other models, you can place them here. Once loaded, the model can be scaled, translated, rotated and then, it is added to the scenegraph root node. To Make the model visible, a light must be added in the scene. In this case, we're adding a directional light pointing from the top front onto the model. For the animation, access the "Walk" animation sequence stored in the model. In order to do this, our class needs to implement the AnimEventListener interface and we need to use an AnimControl instance to access the animation sequence in the model. Finally, the "Walk" sequence is assigned to an AnimChannel instance, tell it to loop the animation with the desired speed. Great, now we have loaded our first 3D model, but we still need to display it on the screen. This is what we're going to do next in initForegroundCamera. It takes care of setting up the perspective camera and the associated viewport for the 3D model. As the perspective camera is characterized by the spatial extent of the object space it can see, we pass the vertical angle of view stored in mForegroundCamFOVY to the method. It then attaches the root node of our scene containing the 3D model to the foreground viewport. While we could just copy some standard parameters from the default camera, similar to what we did with the video background camera, it is good to know which steps are required to initialize a new camera. After creating a perspective camera initialized with the window width and height, both the location and the rotation of the camera are defined. As said before, jMonkey uses a right-handed coordinate system, and our camera is configured to look along the negative z axis into the origin. In addition, we set the vertical angle of the view passed to setFrustumPerspective to 60 degrees, which corresponds to the field of view obtained calibrating my Nexus 5 camera. If you haven't calibrated your camera yet, you can use a sort of default value like 30 degrees, which corresponds approximately with a field of view that appears natural to a human. Keep in mind that in a real project, this value should be changed to the value obtained from a camera calibration operation on the specific device you are using. Afterwards, we set up the viewport, as we did for the video background camera. In addition, we tell the viewport to delete its depth buffer, but retain the color and stencil buffers with setClearFlags method. We do this to ensure that our 3D models are always rendered in front of the plane holding the video texture, no matter if they are actually before or behind that quad in object space. We do not clear the color buffer as, otherwise, the video background, which is previously rendered into the color buffer, would be deleted and we would only see the background color of the foreground viewport, blue in this case. If you run your application now, you should be able to see a walking <<<Oto>>> in front of your video background. Fantastic, but as you can see there is a problem. If you move your device around, the video background changes as expected, while the 3D model stay in place. This is not exactly what happens in augmented reality. The virtual content should stay at the same place relative to the physical world, while you move around it, not remaining fixed on your screen. To achieve that, we need to implement registration & tracking, as we saw quickly in the previous module, where AndAR library helped us to register and track some markers. In this module we'll see how to do that using the Vuforia SDK. Let's go deep in Computer Vision-based AR and let's see what Vuforia can do for us.

Computer Vision-based AR
So far, we have used the camera of the mobile phone exclusively for rendering the view of the real world as the background for 3D models. Computer vision-based AR goes a step further and processes each frame to look for familiar patterns, or image features, in the camera image. In a typical computer vision-based AR application, planar objects such as frame markers or natural feature tracking targets are used to position the camera in a local coordinate system, as we've defined in the previous module. These tracking targets allow for more precise and stable overlay of virtual content in this local coordinate frame. Obtaining the tracking information allows the updating of information about the virtual camera in the 3D rendering engine and automatically provides us with registration. In order to successfully implement computer vision-based AR, we need to understand which physical objects can be used to track the camera. Currently there are two major approaches to do this, frame markers or natural feature tracking targets. In the early days of mobile Augmented Reality, it was of paramount importance to use computationally efficient algorithms. Computer vision algorithms are traditionally demanding as they generally rely on image analysis, complex geometric algorithms and mathematical transformation, summing to a large number of operations that should take place at every time frame. Typically, to keep a constant frame rate of 30 FPS, all the operations have the completed in only 33 ms! Therefore, one of the first approaches to computer vision-based AR was to use relatively simple types of objects, which could be detected with computationally low-demanding algorithms, such as Fiducial markers. These markers are generally only defined at a grayscale level, simplifying their analysis and recognition in a traditional physical world, think about barcodes or QR codes but in a 3D space. Let's see the typical workflow for detecting markers in a frame. An image is acquired by the camera. After, it is converted to a grayscale image and then, a threshold operation is applied, obtaining a purely black and white bitmap. The next step, rectangle detection, searches for edges in this simplified image, which is then followed by a process of detecting closed-contour and potentially parallelogram shapes. Further steps are taken to ensure that the detected contour is really a parallelogram, that is, it has exactly four points and a pair of parallel lines. Once the shape is confirmed, the content of the marker is analyzed. A pattern, usually a binary pattern, is extracted within the border of the marker, to identify the marker itself. This is important to be able to overlay different virtual content on different markers. This is what we have done in the previous module, with AndAR. The last step, the pose estimation, computes the pose of the camera, that is the translation and rotation of the camera in the local coordinate system of the marker, or vice-versa. In practice, this is not a one-time computation, but rather, an iterative process in which the initial pose gets refined several times to obtain more accurate results. In order to reliably estimate the camera pose, the length of at least one side of the marker has to be known to the system. This is typically done through a configuration step when a marker description is loaded. If you remember, in the init phase of the AndAR sample we registered three markers, for exactly this purpose. Otherwise, the system could not tell reliably whether a small marker is near or a large marker is far away, due to the effects of perspective projection. While frame markers can be used to efficiently track the camera pose for many applications, developers and users would like less obtrusive objects to track. You can achieve this by employing more advanced algorithms, which requires more computational power. Nowadays, this is less and less a problem, with the current powerful mobile devices available on the market. The general idea of natural feature tracking is to use a number of local points on a target to compute the camera pose. These points are called feature points, or interest points. The challenge is that these points have to be reliable, robustly detected, and tracked. This is achieved with advanced computer vision algorithms to detect and describe the local neighborhood of a feature point. Feature points have sharp and crisp details, such as corners. For example, circles or straight lines don't have sharp features and are not suitable for feature points. Many feature points can be found on well-textured images, such as these sample images, which we'll use throughout our examples. These images can be found with the official Vuforia and Metaio Samples. Keep in mind that for the best results, feature points cannot be well identified on images with homogenous color regions or soft edges, such as a blue sky or a white wall.

Qualcomm Vuforia SDK
Vuforia is an industry-leading mobile vision platform developed by Qualcomm. The library is free for use both in commercial and non-commercial projects and is currently available on both iOS and Android platforms, with native SDKs or through a Unity 3D extension. As you can easily image, performance is improved on mobile devices equipped with Qualcomm chipsets. An important thing you need to know is that the library uses the Android NDK for its integration as it's being developed in C++. This is mainly due to the gains of high-performance computation for computer vision and image analysis with C++ rather than doing it in Java. A Java wrapper for application development and integration is included, but if you want, you can develop directly using the native library and integrate Vuforia by yourself through JNI. A Vuforia SDK-based AR application uses the display of the mobile device as a "magic lens" or looking glass into an augmented world where the real and virtual worlds appear to coexist. The application renders the live camera preview image on the display to represent a view of the physical world. Virtual 3D objects are then superimposed on the live camera preview and they appear to be tightly coupled in the real world. The library supports frame marker and natural feature target tracking, as well as other techniques, multi-target, which are combinations of multiple targets. 3D objects like Vehicles, action figures or other rigid toys with sufficient visual details, cylinders, like bottles, cans or mugs, boxes, with flat sides and sufficient visual details. It also offers text recognition, with support for English words from a standard database of ~100, 000 words or a custom vocabulary defined by the developer. Other features you can find built-in in the SDK, basic rendering functions such as camera management, video playback and 3D rendering with OpenGL, math library for linear algebra and matrix/vector transformations, occlusion management. Vuforia is able to detect and track targets even when they're partially hidden behind everyday barriers, like your hands or to display graphics as if they appear inside physical objects. Interaction capabilities, like virtual buttons. An interesting feature is the Extended Tracking, which is a capability that delivers a continuous visual experience even when the target is out of view so users have the freedom to follow game play over large areas and visualize large objects such as architectural models, vehicles and furniture. Another feature is Smart Terrain, a capability that enables a new level of immersion in augmented reality experiences. Smart Terrain enables you to reconstruct and augment your physical environment, to create new kinds of gaming and visualization applications. This diagram provides an overview of the application development process with the Vuforia platform. The platform consists of the Vuforia Engine, inside the SDK, and the Target Management System hosted on the developer portal. In the specific the Target Manager, which is an online tool for creating and managing both Device Databases and Cloud Databases. We'll see in a moment how to create our database. And here is an overview of the overall SDK architecture. From an application point of view, it offers a state object to the developer, which contains information about recognized targets as well as the camera frame. Using this state, it is possible to update the app logic and then rendering the scene. A typical Vuforia SDK-based AR application is composed of the following core components. The camera component ensures that every preview frame is captured and passed efficiently to the tracker. The developer only has to initialize the camera to start and stop capturing. The camera frame is automatically delivered in a device-dependent image format and size. Pixel Format Converter, it converts from the camera format, for example Luminance and Chrominance, to a format suitable for OpenGL rendering. For example the RGB565 format we've seen in our previous samples, and for tracking internally. This conversion also includes downsampling to have the camera image in different resolutions available in the converted frame stack. The tracker component contains the computer vision algorithms that detect and track real-world objects in camera video frames. Based on the camera image, different algorithms take care of detecting new targets or markers and evaluating virtual buttons. The results are stored in a state object that is used by the video background renderer and can be accessed from application code. For each processed frame, the state object is updated and the applications render method is called. The video background renderer module renders the camera image stored in the state object. The performance of the background video rendering is optimized for specific devices. Device databases are created using the online Target Manager. The downloaded device target database assets contain an XML configuration file that allows the developer to configure certain trackable features and a binary file that contains the trackable database. These assets are compiled by the application developer into the app installer package and used at runtime by the Vuforia SDK. Cloud databases can be created using the Target Manager or using the Vuforia Web Services API. Targets are queried at runtime by the application using the cloud recognition feature that performs a visual search in the cloud using sent camera images. A fundamentally different supported approach is the user-defined targets feature. Rather than preparing targets outside of the developer, this feature allows for creating targets on-the-fly from the current camera image. A builder component is called to trigger the creation of a new user-target. The returned target is cached, but retained only for a given AR session. Word targets, as we've seen, Vuforia SDK can recognize words and track them similarly to other types of targets, with two available recognition modes, Words and Characters. When using the Words mode, a word is recognized (and then tracked) if it belongs to a given Word List, which can be stored on the device and loaded by the application at runtime. Word lists can also be extended with additional words, and specific word filters can be loaded and applied via the SDK API. I won't get into too much of details on all the features, as a great list of tutorials, samples and full documentation is available on the official site: http://developer. vuforia. com/. To use Vuforia, you generally need to follow these three steps, train and create your target or markers integrate the library in your application, deploy your application. In this module, I'll show you how to start with a Vuforia sample and how to integrate jMonkeyEngine, for better 3D rendering management, as we've seen previously. After that, you will be able to get the official samples and the documentation to explore by yourself all the other amazing features that Vuforia offers. In the next part, I'll introduce you to creating and training your targets.

Target Manager
To use the Vuforia SDK with natural feature tracking targets, first you need to create them. In the current version of the library (3. 0), you can create your target in two ways, online, when the application is running, or offline, predefining them before deploying your application. I'll show you how to proceed for offline definition. First go to the Vuforia developer website https://developer. vuforia. com. The first thing you need to do is to log in to the website to access the tool for creating your target. You should be already registered, it was a required step in order to download the SDK, as we've seen in module 1. After login, you can click on Target Manager, the training program to create targets. The target manager is organized in databases, which can correspond to your projects, and within a database, you can create a list of targets. Databases can be local to the app or hosted in the cloud. By hosting the database in the cloud, you can manage over 1 million targets, and usually this is best for catalog publishers, retailers or advertisers. The Cloud Recognition Service is an enterprise class solution with various pricing plans based on usage. Usage is determined by the total number of image recognitions per month an app performs and is counted when a target is matched. Cloud Recognition Service is available at no charge while developing, but remember that prior to launch your app, you will need to select a paid plan. Here, I'll show you how to create a local database, which can hold up to 100 predefined target images. They can be activated and deactivated programmatically and new databases can be downloaded on the device. So, let's create our first database. Click on Create Database, and enter VuforiaJME. Your database should appear in your Device Databases list. Select it to get to the following page. Click on Add New Target to create the first target. A dialog box will appear with different text fields to complete. First you need to pick up a name for your target; in our case, we will call it VuforiaJMETarget. Vuforia allows you to create different types of targets as follows. With single Image you create only one planar surface and use only one image. The target is generally used for printing on a page, part of a magazine, and so on. With cube, you define multiple surfaces with multiple pictures, which will be used to track a 3D cube. This can be used for games, packaging, and so on. Cuboid, is a variation of the cube type, with non-square faces. Finally cylinder like bottles, cans or mugs, select Single Image target type. You need to select an image which will be used for the target. As an example, you can select the CustomPatternVuforia. jpg image, which is available in the course companion files. The target dimension defines a relative scale for your marker. The unit is not defined as it corresponds to the size of your virtual object. A good tip is to consider that everything is in centimeters or millimeters, which is generally the size of your physical marker. For example, print on an A4 or letter page. In this case, I enter the dimension in centimeters. Finally, to validate your configuration, click on Add, and wait as the image is being processed. When the processing is over, you should get this screen. The stars notify you of the quality of the target for tracking. This example has four stars, which means it will work well. Our last step is now to export the created target. So select the target and click on Download Selected Targets. On the dialog box that appears, choose SDK for export and VuforiaJME for our database name, and save. When they're loaded unzip the compressed file. You will see two files, a. dat file and a. xml file. Both files are used for operating the Vuforia tracking at runtime. The. dat file specifies the feature points from your image and the. xml file is a configuration file. Sometimes you may want to change the size of your marker or do some basic editing without having to restart or do the training, you can modify it directly on your XML file. So now we are ready with our target for implementing our first Vuforia project.

Vuforia and jMonkeyEngine
Now, let's see how to integrate Vuforia with jMonkeyEngine. We will use a natural feature-tracking target for this purpose, so open the VuforiaWithJME project in your Eclipse to start. This application is based on the previous example we've seen at the beginning of the module and integrates some parts of the official Vuforia ImageTarget sample. I'll show you in details how I integrated Vuforia stuff from the official samples, but we won't build the app step by step, it would require too much time. You will find the full app in the course companion files. You can use this sample as reference to use other Vuforia features in your own apps. Just follow the same steps to pick the stuff you want to use from the official samples. First, we need to add the Vuforia library to our project. You should have downloaded it already, if not, go back to the last part of Module 1 for details. We need the Vuforia. jar file and the Vuforia native library. The easiest way to obtain them is to extract the SDK into the development folder we created in module 1. In addition, we need to copy the native library in the project's libs folder. Go to AndroidDev/vuforia-sdk-android-3-0-9/build/lib. Copy this folder, which contains the native library, and paste in the project's libs folder. While we are still in Explorer, copy the Target Database files we created in the previous part in the project's assets folder. Then, back in Eclipse, hit F5 to refresh the project. Now, right-click on the AR project and go to Build Path | Add External JARs. In the JAR selection dialog, browse to AndroidDev/vuforia-sdk-android-3-0-9/build/java/vuforia, select the Vuforia. jar file. And when done, click Open. Right. Here we are. As you can already observe, there are two main changes compared to our previous project. The camera preview class is gone, as well as all the camera management stuff in the MainActivity. This is due to the way Vuforia manages the camera. Vuforia uses its own camera management integrated in the library. Therefore, we'll need to query the video frame through the Vuforia library to display it in our scenegraph, using the same method we've seen at the beginning of the module. Let's remove all the stuff related to the camera management. (working) Another difference is that the MainActivity, in addition to extend the AndroidHarness, implements the IVuforiaApplicationControl interface. I've taken the interface as is from the SampleApplicationControl interface, which you can find in the official Vuforia samples. The purpose of this interface is to define some common methods which help to initialize and manage the Vuforia engine and its internal dataflow, like loading and initializing the AR engine itself and setup trackers. We'll see in a moment how they are implemented. Going back to the MainActivity, we have to add a reference to a VuforiaApplicationSession. Also this class, like the interface we've just seen, is inspired to the SampleApplicationSession class, which you can find in the official Vuforia samples. We can create an instance of the ApplicationSession in the onCreate method. The initialization steps require some time and the app screen will stay black. To give a better user experience, a progress ring is shown, until the 3D scene is ready. In the official samples, all the rendering is made using an OpenGL surface accessed by Vuforia itself. Here, instead, we're going to integrate jMonkey, which handles the OpenGL stuff by its own. So, if you compare this implementation of ApplicationSession and Activity classes with the ones you can find in the samples, you will see some slightly differences related to OpenGL and rendering. Created the Session, we call its initAR method, which, as you can imagine, initializes the Vuforia Engine and the rendering system. Vuforia initialization is done in a background task, in order to avoid blocking the UI thread. Please note that this task must be created and invoked on the UI thread and it can be executed only once, as done here. This initialization task calls the Vuforia init method, waiting for its completion. (working) When done, and no errors occured, it initializes the trackers, by calling the doInitTrackers method, which is implemented in the MainActivity, as part of the IVuforiaApplicationControl interface. Going back to the MainActivity, and scrolling down to the end of the file, we can see a possible implementation. The most part of it is derived from the official sample, so you can easily find yourself home when looking at the samples and figuring how they work. Here is where you can adapt your application to use other Vuforia features. In this sample, I'm using the Image Tracker, but as we've seen, Vuforia offers a lot of other AR features. Have a look at the other official Vuforia samples for more details and working templates. Returning on the Session, once the trackers have been initialized, it is time to load trackers data. Also this operation is performed on a background task, to avoid blocking the UI thread. Trackers data are loaded in the doLoadTrackersData method, which is implemented, again, in the MainActivity, as part of the IVuforiaApplicationControl interface. Let's go to see it. Here is another place where you can adapt your application to use other Vuforia features. In this sample, tracker's data is loaded from the local storage, using the XML file that describes our image targets, called Dataset Configuration. Previously, we created the dataset using the online Target Manager, but you can also create and add targets at runtime, using dedicated Vuforia APIs. Refer to the online documentation for more information. You can load up to a hundred trackers data in this way, enabling the recognition of up to a hundred image targets. Keep in mind, though, that the engine will track only up to five targets simultaneously, and that performance will vary depending on the processor load and GPU. In order to do so, you have to instruct Vuforia. You need to set the MAX_SIMULTANEOUS_IMAGE_TARGETS hint, as you can see here. I leave it disabled because in this sample we won't manage multiple targets. Differently from the official ImageTarget sample, here we support a single dataset, loaded at runtime. Have a look to the official sample to see how to handle multiple datasets. Also, if you want to track more than a hundred targets or if you need dynamics datasets, you can use the Cloud Recognition engine. Refer to the online documentation for more information. In addition, to improve the tracking, we're using the Extended Tracking feature, which enables an app to have a continuous experience whether or not the target remains visible in the camera field of view. This feature allows the tracking a degree of persistence once a target has been detected. As the target goes out of view, Vuforia uses other information from the environment to infer the target position by visually tracking the environment. Vuforia builds a map around the target specifically for this purpose and assumes that both the environment and target are largely static. You could also enable the persistent extended tracking, which would make the engine work better when multiple targets are enabled. For more information, please refer to Vuforia documentation. Returning on the Session, when the dataset has been loaded without errors, it is now possible to register the Vuforia Update callback, which is called by the engine on every update cycle. Here, the session itself implements this callback method, via the QCAR_onUpdate. Internally, this method, calls the onQCARUpdate method which is implemented in the MainActivity, as part of the IVuforiaApplicationControl interface. In this sample, we use this method to retrieve the camera frame from Vuforia and update the background texture in our scenegraph, as we did from the camera preview stream in the previous sample. Here, it's a little bit simpler, because we can obtain the image directly in the desired RGB565 format, without any conversion. If you want to see other uses of this callback, have a look at other samples, such as the Cloud Recognition, the UserDefined Targets or the Virtual Buttons one. Back in the Session, after the callback registration, we call theonInitARDone method, implemented in the MainActivity, as part of the IVuforiaApplicationControl interface. This method, if everything went fine, starts the AR engine, by calling the startAR method on the VuforiaApplicationSession interface. This method will initialize and start the camera device and the preview stream, similarly to what we did manually in the previous sample application. In addition, it will setup some calibration data calculated by Vuforia and then will start the trackers. Finally, it tries to enable, if supported, the continuous autofocus mode on the camera. To complete our overview of the IVuforiaApplicationControl implementation, we have other few methodsm which help in stopping and unloading Vuforia resources, when terminating the application. Have a look yourself at the implementations, they are trivial and do nothing special. Fine. We've reached a good point in our integration of Vuforia. In the official samples, all the rendering is made using directly an OpenGL surface. Here, instead, we need to integrate jMonkey, which handles the OpenGL stuff by its own. Let's modify our jMonkey application in order to do so. First, I created two new fields, to store a reference to our foreground and background cameras and changed their initialization accordingly. Then in the ForegroundCamera Method and the m_ForegroundCamera method, this is needed, because now we need to update its position and orientation using the data arriving from Vuforia. In addition, we can use Vuforia to retrieve camera calibration and perspective settings, without the need to do it by ourself, as in the previous sample. Camera position and orientation data is retrieved and used in the simpleUpdate method. Here, I added a call to the new method UpdateVuforiaTracking. This method is completely different from the renderFrame method in the ImageTargetRenderer, which you can find in the ImageTarget official sample. The original renderFrame method, given one or more tracked targets, display a 3D object on the target using OpenGL. Here, instead, we first hide all our 3D objects in the scenegraph. Then, we check if there are any tracked targets. If there is one, we process it. Without entering in the details of matrices, vectors and coordinate system transformations, here we calculate the camera pose from the point of view of the tracked object, which is considered our 3D world origin. Given the pose, we then update the corresponding virtual camera we have in our jMonkey scenegraph. If you want to know more about the mathematics behind this operation, have a look at these two links from the official documentation. Just keep in mind that for working seriously with AR, a good understanding of analytic geometry, matrices and vectors is a must. There is another little addition to apply in the initForegroundScene: Models imported from Ogre, as in this case, have local up-axis set to Y, while Vuforia has local up-axis set to Z, so for a correct tracking it is required to rotate all models by 90 degrees along the X axis. In addition, a local translation and scaling may be required if models are not centered in the origin, in the exported scene or if their size is too small for the chosen image targets. For details about all these transformations, have a look to jMonkey documentation and samples. Well, that's all. We completed our basic integration of jMonkey with Vuforia. This sample is trivial, but you can move on from here and build more advanced applications, with multiple trackers and other 3D objects. Prior to launch our application, we need to apply some changes to the manifest. Let's open it. First, we need to add some permissions required by Vuforia SDK, so, even if your app doesn't use them directly, you need to add them anyway. One is the OpenGL version, here we're using OpenGL 2. 0. The others are INTERNET and ACCESS_NETWORK_STATE. Then, we add the launchMode attribute to the application and to the main activity, setting it to singleTask, which makes sure that the activity can begin only one task and the device can hold only one instance of the activity at a time. Finally, add screenSize and smallestScreenSize values to the configChanges attribute, ensuring that Android will not destroy and re-create the activity when the user flips the keyboard, changes the orientation of the device, or the actual or physical screen resolution changes, but calls the Activity's onConfigurationChanged method instead. In this sample, just landscape is supported, but following the other samples, you can easily add the support for portrait orientation. Good. Now, we can start our application. You should obtain a result similar to this one. Now, when moving around the phone or moving the image target, we have that our 3D model follows the target. Mission accomplished! Prior to continue the course with Metaio, let's review what we have seen so far.

Summary
In this module, I introduced you to computer vision-based AR. We've defined markers and natural feature tracking and I presented all the goodness that Vuforia, one of the industry-leading mobile vision platform by Qualcomm, offers. We then developed two simple applications. One to show you how to integrate the powerful and free jMonkeyEngine in an Android application, and another one, to show how to integrate the Vuforia library and combine it with jMonkeyEngine. In this last demo, you can move your device around the image target and see the virtual content from every direction. You are now ready to create natural feature tracking-based AR applications and explore all the Vuforia samples by yourself, for more advanced scenarios. In the next module, we will learn how to do a similar thing using another powerful and professional AR framework, Metaio.

AR with Metaio SDK
Introduction
Hi, I'm Gianni Rosa Gallina. Welcome to the fourth module of the course Getting Started with Augmented Reality on Android devices. In this module, we're going to have an overview of Metaio SDK, the most powerful and professional Augmented Reality Framework available today. First, we'll have a look at its features and architecture, then we'll see how to run the official Tutorial apps and, finally, we'll learn how to start from scratch the development of an AR app with Metaio. We have a lot to do. Let's start!

Metaio Solutions
Metaio is one of the worldwide leader in Augmented Reality software, research and technology. They have a solution for everyone, no matter if you are a beginner, an expert, a developer or a designer. Metaio's products allows effortless AR creation for everyone at any experience level. Probably, you've already heard, or used, Junaio, one of the first and famous Augmented Reality Browser, and it's made by Metaio. I suggest you to try Junaio, if you haven't yet. For non-programmers, they provide Metaio Creator, an easy-to-use software that allows users to create extensive AR Apps and present 3D content in new and fascinating ways. Metaio Creator supports a wide range of digital content and formats, such as 2D images, videos, audio, 3D models and animations, enabling creative people to make interesting AR apps within minutes and without development skills. We won't talk about this product in this course. Being programmers, we want to touch the code. Instead, we'll have an overview of the Metaio SDK and we'll see how to create a native Android app starting from the official Template available in the SDK. In addition to the SDK, Metaio also offers solutions for publishing the content, both online and offline. For online apps, they provide the Metaio Cloud. Using the Metaio Cloud, it is possible to manage all of the AR content and projects in a single place, making it easy to integrate updates. Apps can be deployed for free, but in this case the Metaio watermark will be shown. If you buy the license, instead, you can create custom branded apps. For offline applications, it is possible to publish apps for kiosks, desktop or mobile devices. Two kind of licenses are available, SDK Basic or Pro, based on the needed tracking types. Apps can be deployed for free as well, with the Metaio watermark, or if you buy the SDK License you can remove the watermark and publish your custom branded apps. If you are interested, have a look at the official website for more information about their offers and products. And now, let's have a look at what interest us more, the Metaio SDK.

Metaio SDK Overview
The Metaio SDK is a modular framework that offers the most advanced AR technology in the industry and an extensive toolbox with many options. It empowers users to augment their world with 2D, 3D, SLAM and sensors tracking options. 2D tracking is what you already now, markers, barcodes, QRcodes and natural features tracking. 3D tracking enables you to track any object, whether it is a toy, a car engine or features of a landscape. SLAM, which stands for Simultaneous Localization and Mapping, is a technology that learns and track the environment around you in real-time. Finally, the SDK supports GPS, accelerometer, gyroscope and compass, for non-visual tracking. Let's see its architecture. The Metaio SDK is compatible with all major platforms for software development, Android, IOS, Unity3D and Windows. The framework includes the capturing component, the sensor interface component, the rendering component, the tracking component and the Metaio SDK interface. The interface provides interaction between the application and the other four modular components. Under this setup, the details of implementations are encapsulated and the user doesn't have to worry about the details of capturing, rendering, sensors or tracking. The major functionalities are realized through simple APIs that "talk" to the other parts of the SDK, which results in the easy implementation of AR applications. The plus or specific interface of Metaio SDK can easily interact with the development environment. The combination of the Metaio SDK and platform SDK leads to AR applications. With the Metaio SDK it is also possible to create cross-platform Augmented Reality experiences with AREL. AREL, which stands for Augmented Reality Experience Language, is a JavaScript binding of the Metaio SDK's API in combination with a static XML content definition. AREL allows scripting of powerful, highly interactive Augmented Reality experiences based on common web technologies such as HTML5, XML and JavaScript. It is possible to use it for creating junaio Channels or with the Metaio SDK as well. AREL adds HTML5 overlays and easy GUI creation to each Metaio SDK project, allowing cross platform usage of your created Augmented Reality Experience. Instead of using platform specific programming languages you can use AREL, and then deploy it to your target platforms based on the Metaio SDK, obtaining a cross-platform portfolio easily, with very little effort. Of course you are still given the ability to add platform specific functionality and to access the Metaio SDK via the platform specific interface independently. In this module we will focus on native Android development. For more information about AREL, have a look at the SDK Tutorials on the Developer Portal. Before we go down to the development of AR applications on Android, it is necessary to explain the installation process, the development environment setup steps and the licensing procedures. Let's install it.

Installation
You should have downloaded it already. If not, go back to the last part of Module 1 for details. Installation of the Metaio SDK is really simple, just launch the installer and follow the installation wizard. As installation folder, I suggest you to put it in the AndroidDev\metaio folder. After the installation process has been completed, you will find Metaio SDK under the AndroidDev\Metaio\Metaio SDK 6. 0 folder. Tutorials, documentation, tools and all the libraries for the 4 supported platforms are included in the SDK. Beside this overview I'm doing now, for a more general introduction to Metaio and for tutorials on different platforms, please read the rich online documentation available in the Development Portal. There are platform-specific introductions and tutorials for iOS, Android, Windows, Unity3D and AREL. When using the tutorials, make sure to have a digital version or a printed marker prepared. These sample markers can be found in the "Printouts" folder. Regarding the development environment setup, your Eclipse and Android environment should be already configured. If not, please refer to the end of module 1 for step by step instructions. One more thing! Keep in mind that, as with AndAR and Vuforia, it is not recommended to run Metaio-based applications on an Android Emulator.

Licensing
In order to successfully run your apps, you need to register them with Metaio. App registration is free and can be done from the Developer Portal. Two important concepts to understand are Application Identifier and Application Signature. The Application Identifier, or Application ID, refers to a string that uniquely identifies the app. On Android, it is the application package name declared in the AndroidManifest. The Application Signature, instead, refers to a machine-generated string correlated to the Application Identifier. Any application that uses the Metaio SDK needs to provide an Application Signature string which has to match the Application Identifier. Please note that if the Application Identifier changes, the corresponding Application Signature has to be changed as well. You can develop and deploy apps with Metaio SDK at no charge. In this case, a small watermark will be shown at the left bottom corner of your display. If you want the watermark to be removed, you need purchase an SDK license. In the case you had already developed an app with the free SDK, you need to generate a new signature with the purchased license. Let's see how to generate a signature.

App Registration
First, login to the Metaio Developer Portal, https://my. metaio. com/. If you are not already registered, register to the Portal, it's free. Once logged in, go in My Apps section. Scroll down to the App Registration form. Define a name for the application and type the Application ID from the app's AndroidManifest. Here, I'm using this ID, which corresponds to a sample app we're going to build later. When done, click on "Register your app". This will instantly create a signature that corresponds to the Application ID. You will receive a confirmation email and you can get the Signature directly from the developer portal, by selecting the newly register app. As you can see, depending on the SDK version, you have to use the corresponding signature. Later, we'll need the v6 signature, to put into the sample application, in order to run it. So, save it in a text file somewhere on your desktop. Great, we are ready to go on with the development of AR applications on Android.

SDK Architecture
Let's have a look at which features the Metaio SDK offers us. First, it includes a built-in 3D rendering and animation engine. There is a wide range of possibilities in content creation, images, movies, and 3D animated objects. With the help of Metaio SDK, developers can easily manipulate image geometries according to the requirement of the application. AR apps can be easily made by overlying customized images onto a tracking target, which can take the form of a marker-based or a markerless pattern. Currently supported image formats are,. jpg,. png and. bmp files. A rich API for image loading and manipulation is available, for example, the resolution of the image texture can be flexible since the developer can alter the scale of the image using the SDK. Apart from the manipulation of image scale, the translation parameter can also be modified to adjust the location of the image texture against the tracking target. It is also possible to apply movie textures to a target. As simple as setting up image geometries, developers can create movie geometries and explore different configurations for movie playing with few lines of code. The SDK also offers tools to convert video files, so that they can be played smoothly in AR scenarios. To achieve a full-blown AR experience, the Metaio SDK can be fully functional in conjunction with 3D modelling and animation software. 3D content can be either created using 3D animation software or used directly off-the-shelf. As long as it is in a format that Metaio SDK supports, 3D content can be easily used in AR applications. Currently Metaio supports OBJ for static objects, MD2 or FBX for animated objects. For details about creating model files to be used with the Metaio, there are some tutorials for Blender and 3ds Max on the website. For a more complete overview of the content creation options, have a look at this free webinar. For more advanced scenarios, it is also possible to replace the built-in renderer with an alternative implementation. In this case, Metaio can be used for its tracking capabilities, leaving 3D rendering and animation to other frameworks, similarly to what we did in the previous module with jMonkeyEngine.

Camera Management
Metaio fully manages all the camera capture and calibration stuff, similarly to what we've seen with Vuforia. If you remember the basics camera concepts we've discussed in module 2, every camera is defined by a common set of parameters, like the focal length, the principal point and distortion coefficients. These parameters are called intrinsic parameters. In the context of augmented reality, the intrinsic parameters are important for the purpose of marker and feature detection, tracking and measurement. For simple applications, where tracking stability is not a primary concern, the built-in camera calibration values provided by the Metaio SDK can be accepted. However, for industrial applications, camera calibration should be taken seriously. Therefore, it is recommended to calibrate the camera on every device, prior to use the Metaio SDK for AR applications. Here, Metaio comes in help, providing Toolbox, a free application that, besides other interesting features, takes care of the camera calibration steps. For step-by-step instructions on how to calibrate your device, please refer to the online documentation. The output of this app will be an XML file, which should be loaded within your AR app for optimizing the AR algorithms. Please, keep it in mind that that one set of calibrated camera parameters only corresponds to the specific setting of that specific camera, such as resolution, shutter values, zoom, focus level. Once a setting of the camera changes, the camera parameters change with it and re-calibration may be required.

Tracking Features
The Metaio SDK provides several tracking options. Each tracking method setup is stored in an XML file, called Tracking configuration file. These files determine the tracking setup of the application at runtime and can be easily modified with a simple text editor. In general, there are two main tracking setups, marker-based and marker-less. As we've seen in the previous module, marker-based tracking uses a marker, that the camera can effectively detect. With the help of a marker, the tracking condition can be easily configured and the tracking works faster, because markers are generally easier to detect. And the design of markers can usually ensure fast alignment, which thereby allow more accurate and robust tracking conditions. With different designs of markers, Metaio SDK can configure and track more targets than with markerless tracking configurations. Markerless tracking, as we've seen, is based on natural features extraction and more advanced computer-vision techniques, like 3D Edge Detection or SLAM. In addition to computer vision-based tracking methods, Metaio SDK also offers non-optical tracking, to allow the use of other types of sensors than cameras, like accelerometer, gyroscope, GPS or compass. We haven't seen them yet in theory, but we'll do in the next module. For now, just keep in mind that Metaio SDK offers also this kind of tracking, especially for outdoor applications. The SDK has several built-in and pre-defined tracking configuration that can be directly used in your application. In a moment, we'll see an overview of all supported tracking methods. For details, settings and examples, please refer to the official documentation, tutorials and sample apps. Later, we'll see step-by-step how to get started with the sample applications. Id markers, Metaio has developed Marker Tracking with simple AR applications in mind. One can configure up to 512 different Markers with almost no performance penalty. You can download a PDF file with Markers of ID 1-512 and print the Markers you need in your scenario. LLA Markers, LLA stands for latitude, longitude and altitude and is a format for defining geo positions on the globe. LLA markers are identical to ID markers, with additional encoded information about the location. The Metaio SDK includes a QR and Barcode readers, with support for the most common formats. Picture Markers are somehow in-between ID Markers and Markerless. It is possible to use any printout or picture featuring enough visual content. The only constraint is, the reference image should have a dark border and is printed in front of a bright background. Metaio SDK currently offers two kind of marker less image tracking - a Fast and a Robust method. The Fast method works well on a wide variety of targets. It runs quite fluently on most relatively recent smart phones and it is very stable on moderately textured targets. The Robust method should be tried on targets where the Fast method did not provide satisfactory results. The Robust method fits well to highly textured targets. The tracking results obtained with the Robust method improves over time, the more and the longer you move the mobile device in front of the tracked target the better the tracking gets. The 3D maps marker-less tracking allows you to use any real world object as a tracking reference. 3D maps can be generated using the Toolbox application, available for both Android and iOS, or by code. Instant tracking can be in 2D or in 3D. In 2D, it is a technique that allows one to take a snapshot of the camera stream and use it as a marker-less image tracking reference in a bi-dimensional space. Instant 3D tracking, or SLAM, enables the creation of a 3D map of the scene, also called point cloud, and immediately use it as a tracking reference, localizing the position and attitude of the camera within such a map. 3D Marker-less Tracking based on CAD data is a tracking technology that allows a precise 3D pose localization based on a given 3D model of a part of the environment in several different ways, for example tracking the environment starting from a known object like a poster, piece of furniture or a building, or to continuously track the 3D pose of a big rigid object like a car, machine or city scenery. Now that we've seen a quick overview of what Metaio enables us to do, let's see how to try all them on your device, starting from the sample applications.

Tutorial App Overview
After getting your development environment set up, you will find the sample applications in the Metaio SDK installation folder. For each of the supported platform, you will have an Examples_SDK subfolder, like the one I'm opening here, for Android. This subfolder contains an Eclipse project called Example. To start with it, just import it in Eclipse. So, let's go in Eclipse. Right click in the package explorer panel, and click Import. Double-click Existing Android Code into Workspace, and from the new dialog, browse to the Android subfolder, under the Metaio SDK root. You'll have a number of projects. The first one, the CloudPlugin, is a Template project to use when integrating Junaio AR Channels in your application. At the moment, we don't want to do that, so, uncheck it. The second one is an app that shows you how to use a custom renderer, instead of the built-in renderer. Uncheck also this one. The third one is a sample application which implements an example for each of the supported Metaio SDK tracking features. This is the one we want to see now. Analyzing this application, and combining all the documentation you can find on the developer website, you'll be able to learn the Metaio SDK and build your own AR apps. Next, there is the Metaio SDK library. This project contains some helpful building blocks to be used in your apps. Keep it checked. The remaining two items are two Template projects. In order, they are a reference Metaio SDK-based app, and an AREL app, from which you should always start to build your own AR app with Metaio. We'll see later how to do it. So, for now, uncheck both. It is better if you mark "Copy projects into workspace" so, if you change the template, you won't change the original files for other projects. Click Finish. A part from a few warning messages, the workspace should build. If not, run a project cleanup. To use this app, you will need the sample "tracking pattern" image, you can find in the printouts folder in the SDK. This is MetaioMan and he will be your best friend in most of the marker-less tutorials. Please print the reference image, either in color or grayscale, and have it available next to you. Of course you can just simply open the PDF file on your screen, and use it in digital form. Let's run the app. Right click on the project, run as, android application. This is what you should obtain. You can see a list of tutorials, from the basics to more advanced features. By selecting a tutorial, you'll navigate to a description page, from which you can choose between the native SDK implementation or the cross-platform AREL version. Each tutorial corresponds to a dedicated page in the online documentation. You can open it using the Open button. For further details or explanations, please refer to that. If you tap on Start, a new activity will be launched, showing an example and basic configuration of the feature. Feel free to explore and try all the samples. They are a very good source of information for learning the Metaio SDK. Now, let's go back to Eclipse. I'll show you the internals of the Example project. It is a standard Android app, composed by a number of Activities. Following the Metaio guidelines, the Example app uses some building blocks you can find in the referenced MetaioSDK library. Everything starts from the MainActivity, which shows the list of Tutorials. Each tutorial is then implemented in its own Activity, as you can see here. To each Activity, corresponds a view, which you can find in the layout folder. In most activity, the view contains nothing special, usually a Close Button or other functional buttons, depending on the implemented feature. AREL versions of the tutorials, being HTML based, are shown using the ARELViewActivity, which renders the content using a standard WebView. Let's open the HelloWorld Tutorial, the one we've seen a moment ago. As I was saying, it follows the Metaio guidelines and inherits from the base class ARViewActivity. We'll see in the next sample what it does. The core of the activity is the overridden loadContents method. Here, the first thing is loading the Tracking Configuration XML File from the assets folder. For each tutorial, you have a specific configuration file and one or more additional assets, depending on the tracking feature being showcased. You can also find the AREL implementation. In the HelloWorld Tutorial, for example, we have a Markerless Image Tracking, based on the MetaioMan reference. When the target pattern is found in the view, a MetaioMan model, here in MD2 format, is shown on top of it. This is the code that loads and setups the 3D model, also defining it size in the 3D world. As you can see, Metaio APIs are easy to use and all the complexity of AR Tracking and Rendering is hidden in the SDK itself. There are other helpers methods, but in this sample are not used. The other Tutorials are structured in a similar way. Feel free to explore and analyze all the samples, to learn the Metaio SDK and its powerful features.

Develop Using the Template Project
Prior to conclude this module of the course, I want to show you how to start from scratch a new application, following the Metaio guidelines. So, go back in the Metaio SDK installation folder. Copy the Android Template into your project folder. (working) Then, rename it, here, I call it MetaioSample. Prior to import it in Eclipse, edit the. project file and rename the project to MetaioSample here as well. Then, in Eclipse, import the existing project. We have to do some other changes in the packages and files as well, let's start from the package. Rename it to the name you prefer. Here I call it as the other samples we made so far, com. arandroidcourse. metaiosample. Also, change it in the manifest file. Then, open the strings. xml file and update the app_name value to your application name, MetaioSample. OK, as you can see, there are some compiler errors. To fix most of them, we need to import another existing project from the SDK, the MetaioSDK itself. So, let's import it. Go into the path where we installed the Metaio SDK. It is better if you mark "Copy projects into workspace" so, if you change the template, you won't change the original files for other projects. Click Finish. Now, we need to fix our project to include the metaioSDK. Open the project properties, then go to Android and then click on Add. Select the MetaioSDK and then OK. There are still some errors, to fix them, we need to manually rename the failing imports, in order to get the correct classes. So, in the MainActivity, rename com. metaio. Template to com. arandroidcourse. metaiosample. Do the same in the Template class. Now, the project should build without errors. Prior to be able to launch it, we need to fix two other issues, introduced when we copied the template project to another folder and renamed it. Open the project properties again. Go to resource, linked resources, linked resources tab and edit the item, in order to point to the correct location of the Metaio SDK root. (working) We also need to update the signature. xml file in our resources as well. So, open it. Here, we have the metaioSDKsignature value, which was valid for the Template project. If you remember, we need to change it to our own signature, that we created some time ago from the Metaio Developer Portal. So, let's retrieve it and edit the value directly in the XML file. Now, if you try to launch it on your device as is, it will just work. And if you look at the earth pattern, you should see a 3D model of the Earth in the right position. You can find the earth pattern in the printouts folder. Fantastic! Try to touch it. It will expand and shows some labels around. Now, we are ready to build our brand new AR application with Metaio! Let's see how the Template project is structured. Let's start from the MainActivity. It is a standard Android activity, which does just one thing at start. It enables Metaio Debug Logging and then, launches a background task to extract zipped asset files, if any. Everything is just a few lines of code, thanks to the Metaio's Assets Manager. Let's have a look at the assets, in the Assets folder. The template project includes a sample animated 3D Earth model that, when touched, explodes in layers, showing some descriptive labels. Zipped files are optional, in fact the Assets Manager is able to load supported models, animation and texture directly. But if you have zipped files, remember to call the ExtractAllAssets prior to use them. In the folder, you can also find the earthTarget, which is the image target used to show the 3D earth, and the Tracking Configuration XML file. In your application, here is where you would put your assets and your tracking configuration. Refer to the official tutorial apps for basic configuration and sample XML and asset files. As you can see, there's also an AREL folder. In this sample we're not using AREL, but if you want, you can start from this full AREL template to build your cross-platform AR application. The AREL template does the same exact thing we've seen in the native Android application. Once the assets have been extracted, the AR activity is started. In this case, it is the Template activity, but here you can call yours. The Template activity extends the ARViewActivity, from the MetaioSDK. If you remember, this is very similar to what we did in our other samples, with AndAR, Vuforia or jMonkeyEngine. The activity dedicated to show AR content extends or implements another class or interface, which handles low-level stuff for us. As we've seen, Metaio is a very powerful library that let us implement AR applications, also complex ones, easily. The parent ARViewActivity handles for us the Metaio Engine native components and initialization, 3D rendering, UI overlay, display orientation, camera management, user interaction and the activity life-cycle. We have its full source and, if needed, we could change it as we need or remove stuff we don't use. For example, we could disable the internal 3D rendering engine and use the one we like. If you want to do that, have a look at the CustomRenderer Sample, where it removes the built-in render system and manages OpenGL directly. You will notice that is very similar to our jMonkeyEngine Integration in Vuforia or the one we did with AndAR. Once you have understood the core AR concepts, all AR applications are similar. Returning to the Template class, it implements the specific AR application, like we did in our previous example with jMonkeyEngine, where we had the jMonkey Application instance. Here, we have some 3D models and a boolean flag, to keep track of the Earth expanded status. In addition, we have two callbacks. One is called by the MetaioSDK, which can be used to do custom stuff when specific low-level events occur, like SDK Initialization Ready, Animation started or ended, new camera frame available, tracking status changed, and so on. In the template, this hook just does nothing a part from logging the event, but you can easily imagine how powerful it can be in your own solution. The other callback is called by the VisualSearch engine, and it can be used to do custom stuff when search results are retrieved or their status changed. In this sample, we're not using Visual Search, but you can have a look at how this feature works in the official VisualSearch Tutorial. The most interesting method is the loadcontents. It is called by the parent class during initialization, and it is the place where you will setup the tracking configuration and load your 3D assets, also defining their appearance and their place in the virtual world. Another important method is the onGeometryTouched. Here, you can define the user interaction with 3D augmented objects. In this sample, it tracks whether the 3D earth has been touched. In case it was closed, the Open animation is started. Otherwise, if it was opened, the Close animation is played. That's all! Using this template as is, you are already able to create a fully working AR application without much efforts, compared to other frameworks, thanks to the powerful Metaio SDK. In most cases, just changing the assets and the tracking configuration file will fulfil the 50% of your apps. The rest... is up to you and your imagination! How cool is the Metaio SDK? Eh? In less than 10 minutes we have a basic working AR app. As already said, feel free to explore the official Metaio Samples and documentation, to better understand how all the Metaio features work and when you should use them to implement your AR applications.

Summary
Great, we've completed our quick overview about the Metaio SDK. In this Getting Started course, it would have been impossible to cover all the tracking features and algorithms that Metaio offers. We just started scraping its surface. We've seen what Metaio is and which components it is made of, like the built-it render system, camera management and the Toolbox app, all the tracking features and the support for non-visual sensors like gyroscope, accelerometer, GPS and compass. We then had a quick view to the official Tutorial app and how it is structured, so you can now explore all the samples by yourself. Finally, we've learned how to develop a new AR app starting from the provided Template project. At this point, you should have all the required fundamentals to understand computer vision-based AR concepts, and you can know decide which way to choose. Having more low-level control with AndAR or Vuforia, whether using OpenGL directly or a 3D engine like jMonkeyEngine or, take the easier road by adopting Metaio. You have the knowledge, you have the power to choose. Let's go and build your own Augmented Reality app!

Sensor-based AR
Introduction
Hi, I'm Gianni Rosa Gallina. Welcome to the fifth module of the course Getting Started with Augmented Reality on Android. In the previous modules we've seen in details Computer Vision-based AR concepts, techniques and frameworks. In this module, we will now look at another way to implement AR, especially for outdoor scenarios. We'll see how to obtain registration and tracking between digital content and the physical world using the sensors available nowadays on our mobile devices, GPS, accelerometer, gyroscope and compass. I will then show you how to develop the basic building blocks for one of most common AR scenario using sensor-based tracking, AR Browsers, like Junaio or Layar.

Global Tracking with GPS
Global tracking refers to perform tracking in a global reference frame, the world coordinate system, which can encompass the whole earth. We will first look at the position aspect in general, and then the location sensors available on an Android device. We will learn how to retrieve information from them using the Android API and will integrate its position data into the sample application we built at the beginning of module 3. Keep in mind that, to create a fully-immersive AR application, you ideally need to know where the device is, where the body of the user in reference to the device is, and where the eyes of the user in reference to the body are. This approach is being explored right now, especially with Smart Glasses and Head Mounted Displays. For that, you need to track the head of the user, the body of the user, and have all the transformations between them, obtained from a calibration process. With mobile AR on tablet and phones, the realism we can obtain is far from that. So, from now on, when I talk about tracking the position of the user, to know where he is located in the real world, actually I'm referring to track the position and orientation of the device in the world. How can we retrieve the position of a device in a global coordinate system? Certainly you have used a GPS for car navigation or when going out for running. GPS is one example of a common technology used for global tracking, in reference to an Earth coordinate system. Nowadays, most mobile phones are equipped with a GPS sensor, so it seems an ideal technology for global tracking in AR. GPS is the American version of a global navigation satellite system, but it's not the only one. There's also a Russian system, GLONASS, and Galileo, the European version, which will be available in the next years. The technology relies on a constellation of geo-referenced satellites, which can give your position anywhere around the planet using geographic coordinates. At any time, a minimum of nine satellites can be viewed from the ground. To obtain a fix, a receiver must communicate with a minimum of four satellites. The satellites send three types of information to the receiver, the time of broadcast, the orbital location of that particular satellite, and the rough locations of all the other satellites. This data is then fed into some algorithms for finding the actual location, calculated using trigonometry. For common AR applications relying on GPS, we have two things to consider, digital content location and the device location. If both of them are located in the same coordinate system, in reference to Earth, we will be able to know how they are in reference to each other. With that information, we can model the position of the 3D content in the user coordinate system and update it, with each location update from the sensor. As a result, if we move closer to an object, the object will appear closer and bigger, reproducing the behavior we have in the physical world.

GPS on Android DEMO
Let's open a modified version of the ViewWorldJME project we built at the beginning of module 3. You can find it in the course companion files for this module. I added AR tracking using GPS. Later in the module, we'll also see how it is possible to integrate other sensors for a better user experience. The Google Android API offers access to GPS through the Location Manager service. The Location Manager can provide GPS data, but it can also use WiFi network or cellphone network, to pinpoint the location and give a rough estimation of it. First, we need to modify our Android manifest to allow access to the GPS sensor. There are different quality modes regarding GPS, that differs about the quality of estimated location. Here, we authorize all of them. To use the Location Manager, we need a LocationListener. So, in the MainActivity, I created a nested LocationListener class and added a reference to a LocationManager. Inside the LocationListener, we need to override different callback functions. The onLocationChanged callback is the one which is called for any changes in the user's location. The location parameter contains both the measured latitude and longitude in degrees. To pass the converted data to our jMonkeyEngine app, we will use the same principle as before, call a method in our jmonkeyapplication using that location as parameter. So, setUserLocation will be called each time there is an update of the location of the user, and the new value will be available to our application. Next, in the onResume method, we need to access the location manager service and register our location listener to it, using the requestLocationUpdates function. The parameters of requestLocationUpdates are the types of provider we want to use, the update frequency in milliseconds, the change position threshold in meters, and the listener. The GPS is one of the most battery-intensive parts of the Android system and could drain out a fully charged battery in a few hours. This is why we need to release the GPS in the onPause() method. In the jmonkeyengine class, we need to define some new fields. We also need to define the setUserLocation method, which is called by the LocationListener callback. A small issue we have with the GPS, is that latitude and longitude coordinates are not the best representation for 3D graphics. As we've seen, we use a Cartesian coordinate system to place virtual content in a scene, defined in terms of X, Y, and Z coordinates. This way, we need a way to transform latitude and longitude coordinates to X, Y and Z. There are different techniques to do so, I will use the conversion algorithm you can find on Wikipedia. Converting lat and long data to an Earth-Centered system, the World Geodetic System 84. Now we have m_CurrentPosition available in Earth-Centered format in our app class. Each time a user's location changes, the onLocationChange method and setUserLocation will be called and we will get an updated value of m_CurrentPosition. Then, we need a way to correlate the user position to the position of a geo-referenced digital content, for example, a Point of Interest. The method to use is to reference the content locally, from the current position. In a real AR application, you may have some 3D content positioned around your current position. In this sample, I've decided that Oto is our Point of Interest, and that he is positioned near our current position. In the setUserLocation method, the first time a valid GPS location is retrieved, we set the 3D model location to the current location plus a few meters. For referencing this location, we need to use an additional coordinate system, the East-North-Up system. And, for each Point of Interest, we compute its location from the current device position. The position of our 3D model in latitude-longitude format is first converted to the Earth-Centered format. Then, using the current GPS location, the position of the model is transformed in a local coordinate system. Each time the user moves, the GPS position will be updated, transformed to Earth-Centered format, and the local position of the content will be updated too, which will trigger a different 3D rendering. Without entering the mathematical details, in the code you can find you have permitted to do this transformations, this one and this other one. (working) The only remaining part is to update the position of the model using the new local position. We do that in the simpleUpdate method, similarly to what we already do for the background camera frame. That's it! Our GPS-based tracking is implemented. Showing what has been done. You can notice an issue with this solution, the GPS tracking is only considering the position of the device, but not its orientation. If, for example, we rotates the phone, nothing will happen, because changes are effective only if we're moving. To fix that, we need to detect changes in the rotation, and this is a job for inertial sensors.

Inertial Sensors
In the current generation of mobile phones, three types of sensors are available and useful for orientation, accelerometers, magnetometers or compass, and gyroscopes. Let's see them one by one. Accelerometers. These sensors detect the proper acceleration of your phone, also called g-force acceleration. A phone is generally equipped with multi-axis model to deliver acceleration in the 3 axes roll, pitch and yaw, or tilt. They were the first sensors made available on mobile phones and are used for sensor-based games, being cheap to produce. They are, however, rather inaccurate and the measured data is really noisy. Magnetometers, they can detect the earth's magnetic field and act like a compass. Ideally, you can get the north direction with them by measuring the magnetic field in three dimensions and know where your phone points. The challenge with magnetometers is that they can easily be distracted by metallic objects around them, such as a watch on the user's wrist, and then indicate a wrong north direction. Gyroscopes, yhey detect the orientation by measuring the angular velocity, using the Coriolis Effect. The ones used in current phones are MEMS, multi-axis miniature mechanical system, based on a vibrating mechanism. They are more accurate than the previous sensors, but their main issue is the drift. The accuracy decreases over time and after a short period, your measure becomes really inaccurate. The effect of inaccuracy, noise and drift induces the AR content to jump or move randomly, without moving the phone, or it may lead to placing the content in the wrong orientation. To address these limitations, we can combine measurements of each of them, using the sensor fusion technique, improving the overall rotation you can get with them. There are various methods for fusing the sensors, but we won't see them. If you are interested, search for sensor fusion on the Internet, there are a number of implementations around. I can suggest you the one proposed by Paul Lawitzki, released under the MIT License. You can find it here: http://www. thousand-thoughts. com/2012/03/android-sensor-fusion-tutorial/ And now, let's go back to code.

Sensors on Android DEMO
With the data from the inertial sensors we've just seen and a bit of physics and trigonometry, we can get the orientation of the phone. Prior to start, it is important to consider that the natural orientation of the device, which defines the coordinate system for motion sensors, is not the same for all devices. If your device is, by default, in the portrait mode and you change it to landscape mode, the coordinate systems will be rotated. In my example, I set the device orientation to landscape explicitly. Deploy your application on your device using this default orientation mode. You may need to rotate your device around to see the 3D model moving on the display. Also, you currently can't test sensor code on the emulator because the emulator cannot emulate sensors. You must test your sensor code on a physical device. There are, however, sensor simulators that you can use to simulate sensor output. Sensor access with Android API is made through the SensorManager, and uses a SensorListener to retrieve measurements. The SensorManager doesn't give you access only to the inertial sensors, but to all the sensors present on the device. Sensors are divided in three categories in the Android API, motion sensors, environmental sensors and position sensors. Environmental sensors are ambient air temperature, pressure, illumination and humidity. The accelerometer and the gyroscope are defined as motion sensors, while the magnetometer is defined as a position sensor. The Android API also implements some software sensors, which combine the values of these different sensors to provide you with motion and orientation information. If you like physics and geometry, you will be disappointed to know that Android does all the calculations for us. Sensor availability varies from device to device, and it can also vary between Android versions. This is because the Android sensors have been introduced over the course of several platform releases. Please refer to the Android documentation for more information about them. As we did before, we define a SensorManager class and we will add a Sensor class for each of these motion sensors. We also need to define a SensorListener, which will handle any sensor changes from the motion sensors. The listener overrides two callbacks, the onAccuracyChanged and onSensorChanged. The onSensorChanged channel will be called for any changes in the sensors registered to the SensorManager. The type of sensor is identified by event. sensor. getType(). For each type of sensor, you can use the generated measurement to compute the new orientation of the device. The orientation delivered by this sensor needs to be mapped to match the reference system of the virtual camera. We do that in the jMonkey application, through the setRotation method. The setRotation method takes the sensor values from the Android activity and maps them to the camera coordinate system. Finally, it multiplies the current rotation values with the initial camera orientation. To handle this operation, we also need some new fields. The variable mInitialCamRotation holds the initial camera orientation, mRotXYZ holds the sensor orientation mapped to the camera coordinate system, and mCurrentCamRotation stores the final camera rotation, which is obtained by multiplying mInitialCamRotation with mRotXYZ. Finally, we need to use this rotation value for the virtual camera, in the same way we did for the GPS in simpleUpdate. Going back to the SensorListener, we are done. Now, we need to query SensorManager to get the sensor service and initialize the available sensors. We do that in the onResume method of the MainActivity. For initializing the sensors, there are some new helper methods, initSensorsSample and initializeSensor. initializeSensor creates an instance of Sensor and register the listener with the specific type of sensor passed in argument. For the same reasons of battery-drain of the GPS, we must unregister the listener when the application ends. We are now ready to run the application. You should obtain something like this. Moving around, the model will move accordingly, and rotating the device will now produce the expected results.

Summary
We've finished this little overview about sensor-based AR. In this module I introduced you to the basic building blocks for tracking the device location in a global reference frame, using the GPS, and dynamically determining the device orientation using the inertial sensors available on current devices. Accelerometers, magnetometers, or compass and gyroscopes. We've then seen how use the sensors in Android, extending the sample application we built at the beginning of module 3. We can now use these techniques to develop AR Browsers, retrieving information about Point of Interests around the user and integrate them into an Augmented Reality view. In the next module we will see how to make Augmented Reality applications more interactive.

AR User Interactions
Introduction
Hi, I'm Gianni Rosa Gallina. Welcome to the sixth module of the course Getting Started with Augmented Reality on Android. Throughout the course, we've seen the core concepts of creating Augmented Reality applications using the two most common AR approaches, sensor-based and computer vision-based AR. Now you should know how to overlay digital content over a view of the physical world, supporting registration and tracking of a target or locating and orienting in outdoor scenarios. In this module we are going to see how to enable user interactions with the virtual content, analyzing some of the commonly used AR interaction techniques, ray picking, proximity-based interaction and virtual buttons.

Interaction Techniques
User interaction is a major component in the development of an application. Here you're focusing on user 3D interaction with 3D content, thus the following are three main interaction techniques that can be developed, Navigation, Manipulation and System Control. Navigation, that is moving around a scene and selecting a specific point of view. In AR, this navigation is done by moving physically, such as walking around a table, an object or along the street, and can be complemented with additional virtual functions like a map view, navigation paths or virtual control pads. Manipulation, it consists of selecting, moving or modifying objects. In AR, this can be done both on physical and virtual things, through ray picking or newer interaction techniques, like tangible user interfaces. New techniques are usually implemented combining Computer Vision with additional sensors, like infrared or depth-sensors. We'll have a look at them in the last module. System control, that is adapting the parameters of the application, including rendering, polling processes and application content. In AR, it corresponds to adjust tracking parameters or other visualization techniques, such as presenting the distance to a point of interest in an AR Browser, highlighting the location of a target, identifying a marker, and so on. 3D interaction on desktop computers usually involves a limited number of devices, like keyboard, mouse, joystick or gamepad. On smartphones, tablets or smart glasses, instead, interaction is mainly driven by touch or other built-in sensors. A large set of interaction techniques can be used for either 2D or 3D interactions. Starting from an interaction input, like the x and y of the screen point where the user touched or a tap or swipe gesture, it is possible to develop different interaction techniques, such as ray picking, navigation, virtual buttons and so on. Let's see in more details how we can implement them.

Ray Picking
The concept of ray picking is to use a virtual ray, which has origin from the device, and projecting it from the display screen to the augmented environment, detecting if it hits any virtual object along the way. When a hit is found on some object, it is then possible to consider that object as picked, or selected, and the user can start to interact or manipulate it. Do you remember the last sample project we built in Module 4, starting from the Metaio Template project? We had a user interaction with the geometries in the virtual world, a model of the Earth, in that case, obtained by touching the screen of the device. That's a sort of ray-picking. In a moment, I will show you how to implement a similar interaction using jMonkeyEngine. If you wish to see another example, using Metaio, have a look at their Dynamic Model tutorial. Let's go in Eclipse and import the VuforiaWithJMERayPicking project. You can find it in the course companion files. It is based on the same code of the VuforiaWithJME sample, we've seen in module 3, and the ray-picking is taken from the official jMonkeyEngine tutorial Hello Picking. You can find it here: http://jmonkeyengine. org/wiki/doku. php/jme3:beginner:hello_picking. The first thing to do is to import the packages necessary for ray picking in the jMonkey application class. To be able to pick an object, we need to declare some global fields. The next step is to add a listener to the class. In jMonkeyEngine, event management and listeners are organized into two parts, controlled by using the InputManager class. One is the Trigger mapping, using this you can associate the device input with a trigger name, for example, clicking with the mouse can be associated with Press, Push, or Move, and so on. The other one is the Listener mapping, using this you can associate a trigger name with a specific listener. They can be either ActionListeners, used for discrete events such as "button pressed" event or AnalogListeners, used for continuous events, such as the value of a gamepad movement. We are mapping the mouse left-click, which is mapped to a touch action on the mobile device by jMonkeyEngine, to the trigger named "Picked". This trigger name is associated with the ActionListener event listener that I've called actionListener. The action listener is the place where we perform the ray picking; so, on a touchscreen device, by touching the display, we will activate the actionListener using the trigger "Picked". The next step is to define the list of objects that can be potentially hit by a ray. A good way to do that is to group them under a specific group node, containing all the "pickable" virtual objects. We can do that in the initForegroundScene method, where before we defined our virtual scene with oto. In this sample, instead, we are creating a cube object and place it under a group node called "pickables". Now we have touch mapping and a group of objects that can be hit. We only need to implement the listener. The way to ray cast in jMonkey is similar to what is done in other engines. We use the hit coordinates, in screen reference system, map them in world coordinates using the camera, create a ray, and run a hit. In this example, we use the AR camera, which is updated by Vuforia. But please note that the code is the same as for any another virtual game, except that camera position is updated by the tracker instead of the user. The listener's code looks like this. We create a Ray object and run a hitting test, by calling the collideWith method, for our list of objects that can be hit, pickables. The results of the collision will be stored in a CollisionResults object. With the collision result, you can do user interactions and manipulate it. Here I'm just changing the color of the cube, from green, when it is not picked, to blue, when it is picked. You can extend the object to support further manipulation. For example, when an object is hit and picked, you can detect sliding touch motions and move the object, or you can detect a zoom gesture and scale it, or use the hit as a trigger for starting an animation, as you can see in Metaio Dynamic Model tutorial. And now, let's deploy and run the example. You should get a result similar to this one. If we look at the target, the green cube appears. If then we touch the object on the screen, it changes color! (working) This ends this first overview about ray picking.

Proximity Interaction
Another type of interaction in AR is using the relative position between the camera and a physical or virtual object. If you place a target on a table and you move around it to look at a virtual object from different point of views, you can also use that to create interactions. The idea is simple, we can detect any change in spatial transformations between the camera, on a moving device, and a fixed target, and then trigger an event. For example, we can detect if the camera is under or above a specific angle, whether it is looking at the target from a far or near point, and so on. In a moment, we will implement a proximity technique that uses the distance between the AR camera and a Vuforia Image Target. Also this app is based on the VuforiaWithJME sample we built in module 3. So, open the VuforiaWithJMEProximity project in Eclipse. You can find it in the course companion files. The first thing to do is to add some new imports. We also need to declare some global fields. First, we create three objects, a cube, a sphere, and a torus, which will be shown on the tracking target, depending on the distance of the camera. We do that in the initForegroundScene method. The simple way to change which object is displayed is to add or remove objects that shouldn't be displayed at a certain distance. We do that in the SimpleUpdate method. To implement the proximity technique, we query the location of the camera, with GetLocation, and then, from that location, we can compute the distance to some objects or just the target. The distance to the target is, by definition, similar to the distance of the camera, expressed as a 3D vector. So, we can do our evaluation and show or hide the corresponding objects, by defining three ranges. If Camera distance is far, shows the cube, if Camera distance is between a minimum and a maximum, shows the sphere, if we are very near, shows the torus. We can try to run the sample app and vary the distance of the device from the tracking target. This will affect the object which is presented. A cube will appear when we are far, a torus will be shown when we are close, and a sphere when we are in a medium range. Great! This ends this overview about proximity interactions.

Virtual Buttons
For the final technique I want to show you for implementing AR interactions, we'll see a Vuforia sample, Virtual Buttons. As said in Module 3, you can download the official samples from the developer portal. Also, you will need the Wood Target file, you can always download from there. Virtual buttons are developer-defined rectangular regions on image targets which, when touched or occluded in the camera view, can trigger an event. Virtual buttons can be used to implement events such as a button press or to detect if specific areas of the image target are covered by an object. Virtual buttons are supported only for image targets created with the Target Manager and downloaded as device target database. Cloud Targets and User-Defined Targets do not support virtual buttons. Also, virtual buttons are evaluated only if the button area is in the camera view and the camera is steady. Remember that virtual buttons does not work when the camera moves fast. The sample app renders a 3D teapot that changes color when one of the virtual buttons is triggered. If you double tap, you can access some options, such as toggling specific virtual buttons. Let's see it in action. If the target image is in the camera view, we have a teapot and 4 buttons. If one of the buttons is occluded with an object or with an hand, the teapot changes color. Let's see how it is implemented. Open the samples in Eclipse. All the code related to the virtual buttons is in a dedicated package. You have already seen how Vuforia apps are structured in Module 3, where we implemented a sample app based on the Image Target tutorial. The Virtual Button is based on the Image Target, so, here, I'll just show you the differences. In the VirtualButtons activity, we have some fields needed to define the buttons and the different textures to apply to the teapot when a button is triggered. In the onCreate method, we have a loadTexture method that initialize the various texture for the teapot. The definition of the virtual buttons is done in the onQCARUpdate method. If you remember, this method is a callback called by the Vuforia Engine, which you can use to handle the tracking results or do other things in Vuforia, as we do here. The sample app enables the user to dynamically enable or disable the virtual buttons at runtime, using a standard Android menu for setting a bitmask. If the user changed the configuration, a flag is set. To change the tracker configuration, we first need to stop it. Then, according to the bitmask, each button is defined, using the toggleVirtualButton method. As said, a Virtual button is a rectangular region on an image target. So, to define it, we give it a name and set the size and position of the region on a specific target, referred to the center of the image, using the createVirtualButton method. Also, you can set the tracking sensitivity and its enabled/disabled status. If we want to disable a virtual button, we can destroy it using the destroyVirtualButton method. Virtual Button triggering is evaluated in the rendering cycle. So, let's open the VirtualButtonRenderer class. And go to the RenderFrame method. Here, we retrieve the tracking results from Vuforia. If a target is found, then, virtual buttons are checked. For each defined button, a corresponding rectangle is drawn on the image frame. And, if a button is pressed, the corresponding texture is selected, in order to draw the teapot in a different color according to the occluded virtual button. The rest of the rendering function draws the teapot in the right position, on the tracked target image. As you can see, thanks to Vuforia using virtual buttons is very easy. Just define the rectangle regions on a target image and then use the tracking results to know if a button has been pressed or not, executing your specific application code.

Summary
We've finished this overview about user interaction in Augmented Reality applications. There are many other techniques that can be applied and implemented, but here I just wanted to give you the basics idea and a starting point, from where to continue and explore by yourself more advanced techniques. I've introduced you to the typical AR interaction techniques, suitable for a wide variety of AR applications. Then we've seen in details. Ray-picking, which allows you to select 3D objects by touching the display, just like you would with 2D selection using the mouse. Proximity-based techniques, that allow you to experiment with the distance and orientation of the device to trigger application events. Finally, we've seen how Vuforia makes easy to interact with targets as they were virtual buttons. These techniques should serve as building blocks for you to create your own interaction methods, targeted to specific application scenarios. In the last module of the course, I will introduce some more advanced techniques and we'll see what can be done in the near future with upcoming new technologies and sensors.

Advanced Topics and the Future
Introduction
Hi, I'm Gianni Rosa Gallina. Welcome to the last module of the course Getting Started with Augmented Reality on Android. As you could see from the previous modules, Augmented Reality is a big and cross-technology topic. Discussing everything in details would have been impossible in a Getting Started course. In this module, I'll introduce you to the next steps and to the upcoming technologies that will enable richer and richer Augmented User experiences and applications, that right now cannot be implemented yet, due to limitations and availability in hardware, sensors and algorithms.

Advanced Interactions
In the previous module, we've seen some basic interaction techniques, that included ray picking with touch interaction, camera to target proximity or target occlusions to implement virtual buttons. Besides these techniques, there are a large number of other ways that can be used in Augmented Reality. One standard technique, that you already seen in mobile games, is implementing virtual control pads. As a mobile device limits access to additional control devices, such as gamepads or joysticks, we can emulate their behavior using touch. We can display a virtual controller on the device screen and interpret the touches in specific areas as being equivalent to controlling a control pad. Another technique, that is becoming popular in Augmented Reality, is Tangible User Interface, or TUI. We've already see two types of TUI, when we implemented the proximity sample and the virtual buttons one. The idea of a TUI is to use one or more physical object as support to interaction. It is possible to classify TUIs in four categories, local interactions, global interactions, touch interactions and mid-air interactions. Local interactions are, for example, using two targets for interaction. Similar to the way we detected the distance between the camera and a target, it is possible to extend the idea to two or more targets. You can detect whether two targets are close to each other, aligned in the same direction, and trigger events accordingly. For example, it would be possible to use this kind of interaction for card-based games, where you want cards to interact with each other, or games that include puzzles where users need to combine different pieces together. Global interactions extend the concept of local interactions, where one of the targets becomes special. In this type, we define a target as being the reference target, and all the other targets refer to it. To implement it, we have to compute the local transformation of all the other targets in respect to the reference target. In this case the reference target becomes the origin. Once done, it's really easy to place targets on or near the reference target, somehow defining some kind of interaction plane and performing a range of different types of interaction with it. This kind of interaction is the one you can obtain using Vuforia Smart Terrain or Metaio 3D Maps and Instant Tracking. Given that, tangible interactions can also be defined as interaction behind the screen, while virtual control pads can be considered interactions in front of the screen. Another way to classify interactions with a mobile device, which brings us to the third category, is touch interaction on the target. We've seen that Vuforia lets you implement virtual buttons. But you can extend the concept to a more general definition, where you define specific areas on a target that can be used as controllers, like buttons, sliders or dials, which users can control with their hands or fingers. Having this kind of interactions with phones or tablets is a bit tricky, considering the user experience that derives holding a device with the hands and use them to interact with other physical objects at the same time. But with smart glasses or head-mounted displays, having both hands free, this should be considered an interesting interaction mode. There are other techniques that are currently investigated in research laboratories and some are already starting to be available to developers and early adopters. They will become available in the future generation of mobile Augmented Reality and Virtual Reality, so you should already start to think about them. For example, early this year, Metaio shown an early prototype of it Thermal Touch technology which, combining an infrared and a standard camera, is able to track the heat signature left by a person's finger when touching a surface. Once the technology will be available on everyday smart devices, it will be the next version of the virtual buttons on target we can have today. Another trend is towards 3D gesture interactions, or mid-air interactions, as they are also called. Rather than touching the screen or touching the target, you can imagine making gestures between the device and the target. 3D gestures have a lot of challenges such as recognizing the hand, the fingers, the gesture, physical engagement that can result in fatigue, and so on. Do not forget, we can also interact with our voice and ears, issuing vocal commands to the devices and receiving audible feedbacks, either in form of sounds or machine-generated voice. In the near future, these types of interaction, which are already popular on home gaming systems like Nintendo Wii or Microsoft Xbox with Kinect, will be available on mobile devices equipped with 3D sensors as well. Let's see which future technologies are becoming available today and what can be done.

Future Sensors and Technologies
Talking about 3D gesture interactions, you have probably heard or seen the Leap Motion Controller. The Leap Motion controller is a small USB peripheral device which is designed to be placed on a physical desktop, facing upward. Using two monochromatic IR cameras and three infrared LEDs, the device observes a roughly hemispherical area, to a distance of about 1 meter or 3 feet. The LEDs generate a 3D pattern of dots of IR light and the cameras generate almost 300 frames per second of reflected data, which is then analyzed on the host computer, deriving highly accurate 3D models of the hands, fingers included, integrated with position and orientation data. The Leap Motion Controller senses how you naturally move your hands and lets you use your computer in a whole new way. Point, wave, reach, grab, pick something up and move it. You can navigate a website, using pinch-to-zoom gestures on maps, use fingers or tools for high-precision drawing and 3d manipulation. Of course, you can also play games in more engaging ways. It is available since 2013, and recently, version 2 software has been made available to developers. The latest version, besides more robust hand and finger models, features new tracking capabilities for mounting the controller on head-mounted displays, especially those for Virtual Reality, like the Oculus Rift. Leap Motion has an App Store where apps made by developers can be downloaded, both free or paid. Leap Motion has also partnered with Asus and Hewlett Packard to embed it technology within other devices, including keyboards and laptops. If you are interested in more information, have a look at the official website. The smaller observation area and higher resolution of the Leap Motion differentiates the product from the Microsoft Kinect, which is designed for full-body tracking in a bigger space, like a living room. Kinect, is a line of motion sensing input devices by Microsoft for Xbox 360 and Xbox One gaming consoles. Based around a webcam-style add-on peripheral, it enables users to control and interact with their device without the need for a controller, through a natural user interface using body, gestures and voice commands. Kinect sensor is a horizontal bar designed to be positioned above or below a video display. The first-generation Kinect was first introduced in November 2010 for Xbox 360 consoles, followed by a dedicated Windows version at the beginning of 2012, for commercial applications. The device features an RGB camera, a depth sensor and a multi-array microphone, running proprietary software, which provides full-body 3D motion capture, facial and voice recognition capabilities. At the end of last year, the second-generation Kinect has been released, bundled with the new Xbox One console. And few months ago, also a new version for Windows has been made available. The Kinect v2 uses a wide-angle time-of-flight camera, and processes 2 gigabits of data per second to read its environment. The new Kinect has greater accuracy with three times the fidelity over its predecessor and can track without visible light by using an active infrared sensor. It has a 60% wider field of vision that can detect a user up to 3 feet from the sensor, compared to six feet for the original Kinect, and can track up to 6 skeletons at once. It can also detect a player's heart rate, facial expression and the position and orientation of 25 individual joints, including thumbs. Kinect's microphones are used to provide voice commands for actions such as navigation and in-game controls. Numerous researches, companies and indie developers are researching possible applications of Kinect, that go beyond the system's intended purpose of playing games. Just to name a few, for example, Kinect can be used for 3d scanning. Thanks to the Kinect Fusion project, users can use an off-the-shelf Kinect camera as a 3D scanner for producing rapidly high-quality 3D scans of small or large objects. The software is available in the Kinect For Windows SDK. Medical is another interesting field of application. For example, users can be given exercises that will improve their motor functions. Their activities will be monitored with Kinect's scanning ability, and a program that helps keep track of their progress. This allows the patients to recover from home under private care or with family, instead of hospital environments. Surgeons in the middle of a surgery may need certain information about their patient and this requires them to interact with non-sterile surfaces which may be detrimental to the surgery. Kinect has been integrated in a system where they can use gestures to manipulate images, from inside the operating room. Not only this does minimize problems from information transfer, it also saves time by giving surgeons access to the data they need as soon as possible. Other uses have integrated Kinect in retail stores. For example, the company I work for implemented a solution for engaging and profiling users in showrooms or exhibitions. Others implemented Virtual Dressing Rooms, where users can virtually try on clothes and see how they appear in a kind of Virtual Mirror. There are many many other possible applications. Kinect is a very interesting sensor. If you are interested, have a look at the official website. Similar to Kinect, but designed for mobile devices, there is the Structure sensor. Mounted on a mobile device or an head-mounted display, it can be used to give apps a spatial knowledge, that currently they haven't. At Structure Sensor core there's a tiny infrared sensor made by PrimeSense, the same Israeli company that designed the sensors for the first-generation Kinect and that was acquired by Apple last year. Using proprietary algorithms, data from the sensors is turned into three dimensional models. Currently officially supported for iOS, with a full-featured SDK, it can also be used on other platforms through the OpenNI drivers. You may use that data for 3d scanning, hands tracking, spatial reconstruction, Augmented Reality games or apps, and so on. If you are interested, have a look at their website here. Remaining in the field of depth sensors for desktop and laptop computers, there is the Intel RealSense 3D camera. The hardware is made by Creative and it is integrated with a bunch of technologies and software from Intel, Nuance and Metaio, bundled in the RealSense SDK, currently available on Windows. Like the Kinect sensor v2, it features an 1080p RGB Camera, a 2-elements microphone array, and an infrared depth-sensor. In the near future, the new cameras will be integrated within notebooks, tablets, and all-in-one devices, probably residing on top where actual webcams are now. Currently, for developers, it is possible to buy an external USB dev kit. As we've seen, this kind of device can be used to develop Natural User Interfaces, with gestures recognition, hands tracking, facial expressions, and vocal commands. Mounted on a mobile device or a head-mounted display, it can be used to give apps a spatial knowledge, that currently they haven't. If you are interested, you can get more information at the official Intel website. I just analyzed a few of the currently available technologies that will change the way we interact with computers and mobile devices in the future, in particular, those technologies that I'm currently using in my daily work. Of course, the outer world is vary, and there are plenty of other technologies, sensors, wearables and applications that I'm not aware of, or that it would be impossible to talk about in a Getting Started course like this.

Summary
We've reached the end of the course. In this module, I shown you what can be done to go beyond the standard Augmented Reality interactions, by introducing some techniques that you can use to create advanced AR experiences, like virtual control pads, Tangible User Interfaces. Which are characterized by the use of one or more physical object as support to interaction, and mid-air interactions, where users, possibly hands-free, interact with virtual objects with 3D gestures using their hands and body. Also, we've seen some of the upcoming technologies that will enable richer and richer Augmented User Experiences and applications, that right now cannot be implemented yet, due to limitations and availability in hardware, sensors and algorithms. This module concludes your introduction to the world of Augmented Reality development for Android. Thank you for following me until the end. I hope you are now ready to continue your adventure and advance to new and more interesting levels of Augmented Reality application development. Please, feel free to contact me for more information or just to have a chat about possible new applications or for collaborations in developing your projects and transforming imagination in reality.

Course author
Author: Gianni Rosa Gallina	
Gianni Rosa Gallina
Gianni is an R&D Senior Software Engineer in Deltatre's Innovation Lab, based in Italy, designing and prototyping next-generation solutions for sport-related experiences and services, from VR/MR...

Course info
Level
Beginner
Rating
3.8 stars with 43 raters(43)
My rating
null stars

Duration
3h 7m
Released
13 Jan 2015
Share course

