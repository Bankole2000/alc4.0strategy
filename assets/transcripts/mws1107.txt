Website Performance
by Kyle Simpson

With the rise of mobile networks and devices, website performance is now absolutely critical to building good websites and web applications. Kyle dives deep into everything web performance from resource loading, to thread, animations, JSON,...

With the rise of mobile networks and devices, website performance is now absolutely critical to building good websites and web applications. Kyle dives deep into everything web performance from resource loading, to thread, animations, JSON, minification, image optimization and more!

Course author
Author: Kyle Simpson	
Kyle Simpson
Kyle is a freelance developer based in Austin, TX. He runs several open-source projects (such as LabJS), writes books, and speaks at meetups and conferences.

Course info
Level
Intermediate
Rating
4.5 stars with 135 raters(135)
My rating
null stars

Duration
5h 3m
Released
29 Mar 2013
Share course

Introduction
Introduction
Well, thank you for having me up here. I come to you from a much sunnier and warmer Austin, Texas. Truth be told I actually don't really mind the cold that much. I'm a little bit more of a cold-natured person, so I tolerate the heat in Austin and this was a nice breath of fresh air literally to come up here. But this is first time in over a decade I've been to Minnesota, so I'm glad to be back, and I'm thrilled and honored to be part of the workshop series. Today's talk is going to be on Website Performance. You could also call it Webapp Performance. I'm just going to call it Web Performance because really what we want to do is become better adjusted to the mindset of coding things on our websites, our web apps, web applications, even just simple blogs. We want to make sure that we have the mindset of performance orientation as we develop things. One of the problems with performance optimization is it's often a last thought. It's like the last item on the checklist before somebody goes to production and there are a number of decisions that you make throughout the architecture and throughout the deployment processes and things like that for your application that make it very difficult for you to, at the very last minute, just optimize things. So you tend to have this very narrow view of what's the worst pain point, let me just fix that, so my boss doesn't get mad at me. And what we wanted to do is take a step back with this workshop series. I'm thrilled that we're doing this as part of a bigger workshop series on front-end mastery and I don't consider myself to be a front-end master, but what I hope is that this is part of your step to becoming a front-end master is understanding how to go about performance. So just a couple of quick backdrop things that Mark mentioned on me, I do have HTML5 Cookbook. I've got several copies of that and I'll be trying to give out some of those to good questions or creative answers as we go throughout the day. And if you don't get one and you don't have one of the million HTML5 books that are already out there and you're interested, this is a discount code. It's 50% off the eBook copy and 40% off the print, I think. So if you're interested in that, avail yourself of the O'Reilly discount code. I've got a number of open source projects that's part of, like Mark said, part of the reason why I'm part of this series, how he found out about me. LABjs is a dynamic script loader. It came out several years ago. It kind of got popular on some big sites and I have stopped active development on LABjs, just maintenance now, but it's interesting that in the last few months, for some reason, there's been a major uptick and interest about script loaders, in particular, LABjs, so I see on my GitHub repo that just more and more people starring it and forking the repo and things, so that's a good sign I guess that people are catching on, even if they are three years behind the curve. So another project that I've got out that I'm very passionate about right now is called grips. That is a templating engine. Yeah, yeah, I know, there's a million of those templating engines and I've done yet another one. I won't belabor any point on grips other than to say the reason why I designed it in the way that I did and I encourage you to take a look at it, there's an online demo that you can check out. All of my stuff, by the way, it's all in MIT licensed, it's all on GitHub. If you can find anything about me, it's up there. Grips is remarkable not for what it can do, but for what it cannot do. It is a restrained templating engine that does not allow you to do full programming inside of your templates. It gives you just enough logic to get the job done and it doesn't let you do the stuff that you really shouldn't be doing in the templating engine, and in fact, it spends most of its effort trying to prevent you from doing that stuff. Some people like that concept, some people really, really hate having handcuffs, but I have found that the easiest path in templating is usually to just put your code right in the template, and when your boss is breathing down your neck, you usually take the shortest path. So this is a tool that helps you, what I think, develop better and more robust templates. Take a look at that one. Another project I've got is called h5ive. This is a much newer and an earlier project. I'm really passionate about HTML5 and JavaScript. I've bet my entire career on it, but I believe very strongly that we are writing potentially dangerous production code right now because we're writing directly against APIs that are constantly buggy and constantly changing. Some people take the approach to say I'll just not work with these APIs until they're 10 years further removed. Other people choose to work on the cutting edge. Somewhere in between is the proper and responsible approach. H5ive is a set of thin, fa√ßade, wrappers around all of the HTML5 APIs that allows you to have an insulation, a buffer zone, to some of that wild, wild west, if you will, of the changing APIs. So you write your production code against these thin wrapper APIs and then if something changes, you just have to update the wrapper and not all your production code. Lastly, I've got an open source game that I've done some talks on. I am not at all a game designer and so this is not to show my remarkable game design, game mechanic skills, nor is it at all to show off my UI skills. We Puzzle It! was it's a multiplayer game where you move---you cut up an image into tiles and then people compete against each other to rearrange the tiles in the proper order. So it's not terribly interesting in terms of gameplay, but the reason I built it and the reason I open sourced it was to show there's probably 8 to 10 different advanced HTML5 JavaScript APIs that are in play. Things like request animation frame, things like HTML5 drag and drop, and app cash, and local storage, and web workers, and WebSockets, and all these things all in one project, as opposed to what happens a lot these days. When you want to go learn about these things, you read these small, tiny, disparate snippets in some book, like the one I wrote, or some blog post or something and it gives you no context about those APIs. This project was designed to solve that problem, shows you the real code in place. Now, I'm not necessarily saying that my code is awesome, in fact, there are a lot of problems with my code because it's not using any of the H5ive stuff yet, but I do encourage you to take a look if you want to see real code in action woven together, you can take a look, and that's also on our repo. I'm also a pretty commonly I go to do a lot of conference talks. Lots of times my conference talks do come out of passionate things that I'm working on in terms of projects, so I mentioned H5ive. That's the stop using native HTML5 talk. There are videos and slide decks of these talks out online, so if you're interested in hearing more about the stuff that I'm passionate about or if you just need some light bedtime reading, you can take a look at some of those projects. My newest one is browser versions are dead, and again, I won't belabor that point, but just to say I believe our industry is being held back by some lies that were told to us 10-15 years ago, lies like the fact that a browser version matters, when in reality, a browser version is just an arbitrary marketing label and the more important lie is that websites and web applications were always supposed to work and look and feel exactly the same in all browsers. That's not actually how the industry works, but we've been told that lie and it's part of what creates this problem of oh I can't get it to work in I. E. 6, I spent so much time on backward compatibility. So this talk is to try to take a refreshing look at that and say how should our industry be approaching this topic. So, if you're interested in those, I encourage you to take a look. Several of those are up on Speaker Deck, some of the older ones are up on SlideShare.

Make Performance a Priority
Why Web Performance?
Okay, so today we are going to talk about performance and I want to---I'm pretty proud of myself that it's the first time that I've ever been able to incorporate Latin phrases into my slides. So I was researching ways to describe this and these phrases came up. Caveat lector describes beware the reader. So my encouragement to you is to beware of the things that we talk about today and use them in a responsible way. In no way, shape, or form should you hear me saying that you should go and micro-optimize and focus only on optimization because optimization is only part of the bigger story. So I'm going to focus entirely on optimization today, but that doesn't mean that you should focus entirely on optimization as part of your job. Part of your job should be to understand that if you can weave these practices into your natural development practices, it doesn't have to become that last item on the checklist. It's taken care of as you go, and that's what I hope that you guys get out of the workshop today. Caveat utilitor is beware the user. If you are using this in a localhost environment and you're testing on a Mac with 16 GB of RAM and crazy fast CPU, it's quite likely that you may look at one of these exercises and it may not do any jittering or it may not have any performance problems at all and that's part of the problem with our industry is that we have this selection bias towards hardware and network, although, today we may network issues, but in general, we have this selection bias towards high-capacity devices, and we think that's the way the rest of the world works, so I do want to caution you that if you don't, as you're working through an exercise today, if you don't see it doing something that logic tells you that it should be doing or that is the spirit of the exercise, perhaps ratch it up, change the code to ratch it up how many times it's doing something or whatever, but just understand that the principle remains whether or not on your device it takes a higher number for something to happen, the same principle would happen to somebody with a lower device. Okay, so why web performance? What is the big deal about web performance? Don't we just have plugins in our web servers that kind of automatically take care of this stuff? Why is this something that we should actually spend mental CPU cycles on? Well first of all, this started several years ago with a man named Steve Souders. He's often referred to as the beginning of this web performance optimization industry and he stated what is now become to be called the performance golden rule that 80 to 90% of all front-end performance problems of all time is spent in the frontend, so if you're going to optimize, start there. That is the framework for how the web optimization sub industry, if you will, came to be. What he noticed was that when you look at how quickly, how a web page loads and all of its resources, sub-resources load, this is often---this is called a waterfall chart, and essentially, you'll see an HTML page is usually the first item and then you'll see some images and some CSS and scripts and what you notice in a waterfall chart is the wider that it gets, the more you get this sort of vertical striation where you have several things happening, but then a bunch of stuff has to wait before more stuff can happen, that's what, that's the visual depiction of slower loading on a web page, and so Steve noticed that if I go to a website and it takes 8 seconds for that website to load, he noticed that 80 to 90% of that 8 second wait time was happening with the resources actually loading and the only other 10 to 15 to 20% is happening on the back-end. This actually was a flip of what most people thought. Most people thought 5-10 years ago that the slowness of the web would be mostly due to poor application architecture, slow database access, poor web, you know, not enough web servers, not enough concurrent connections, not enough RAM, things like that. So it sort of flipped on its head the paradigm of what web performance was about because up to that point, web performance was the job of a system administrator and what we have now today as a result of Steve's work, he worked at Yahoo! and he's now chief performance engineer for Google, as a result of that work what we have now is the mindset that actually most of that burden rests on our shoulders as front-end developers. We are the people that touch that code. We're the people that intimately know how it works. Shouldn't we be the ones to optimize it? Would your system administrator who's really good at tuning the internals of Apache, would he be properly able to optimize the file sizes of your images or is that better your job to understand those things. And even if it's not directly your responsibility, can you influence the people among your team whose responsibility it should be. That's why web performance matters because the ball's back in our court.

Performance and User Experience
Web performance is one of the sort of unsung hero problems of user experience and the door can swing both ways. You can have great user experience because of great performance. You can also have terrible user experience often related to terrible performance. When you think about when a user comes to a website or a web application for the first time, the very first few seconds of what they see on the application is going to give them an indelible impression about the quality of that software that they're dealing with and I call it software because that is what this, we are creating software regardless of how it's deployed, regardless of whether it was a mobile download or it's being delivered live over the web. It's software and when you fire up a new piece of software on your machine, you get those same first impressions about whether or not that software is good and part of that impression is, man, Photoshop takes like 18 seconds to load on my Mac. What on earth could it possibly be doing loading that much stuff? Why does it take that long? Couldn't they figure out a different way? Why do I have to wait so long? The same things happen, although, not quite so verbally. They're more subverbally, but the same things happen for users including your grandma when she's surfing the web. They ask the question why are things going so slowly, or more accurately, they say yeah, I like to go to this site because this site is much quicker than that site and they're making these very subtle comparisons between the performance. That's part of why this matters is because this is all about the user. When you create content for whatever reason, it all comes down to can a user consume it in a useful way, and if not, what was the point? If you develop an application that has great content, but nobody tolerates how long it takes to see the content, it's like a tree falling in the woods. What's the value? So here's an example illustration of how performance plays out to an end user. This is a little bit small to see, but I have used a site called webpagetest. org, which I'll show you in a moment, I've used the site to do a comparison between CNN and Reuters. How many of you have used either of those two sites? Or both? Okay, so you're roughly familiar these are news sites. Now honestly, I'll tell you that I expected for Reuters to blow CNN away in terms of performance and the reason for that is because I often have heard and I've often done myself used CNN as an example of poor performance, so I expected, I picked Reuters just out of thin air, I expected oh, I'll pick a good website like that and I expected CNN to do terribly. It didn't perform as well, but it did much better than I expected. Here's an example showing, and I've cut off the sides of the strip, this actually went from the 0 second mark to about the 12 or 14 second mark, I've cut off the end of the strip just to start with over here on the left-hand side, you'll see at the two second mark is the first moment when anything visually appeared on Reuters when you were trying to load it and this was some software on webpagetest. org that creates these comparisons for you. It's highly interesting and useful for you. But you see that it went for two seconds without showing anything. CNN went for a full extra second before it showed the first thing, but you'll notice that as soon as CNN showed something, it seems to have quite a bit more content visible than Reuters does at the three second mark. And then Reuters catches up a half second later and then as you go on you can start to see and the interesting thing about webpagetest. org is it has this measurement that it calls visual completeness, that's part of the mental perception that people have about your site and how quickly it seems to be loading. You can have two different sites that take exactly the same amount of time to load, but the way, the manner in which they load, the order that things load, the visual completeness of where something loads greatly impacts their perception and there have been many studies, I won't cite a lot of particular statistics, but there have been many studies to suggest that users are more interested, or at least, as interested in the perceived performance of your site or application as the actual performance. So that's kind of interesting that we actually have both of those things to take into account. This is all about user experience as I said. So you can see here that CNN is a little bit lagging behind in terms of the perceived performance of Reuters, maybe that's one of the reasons it's on my hate list. Here's another way of looking at this data and this is actually more useful. This shows that visual completeness metric for both sites as time goes on from the 0 second mark to the 16 second mark. You'll notice the blue line, which is Reuters, you'll notice that it is higher, that is more visually complete throughout the entire length of the curve. So in their estimation based upon you know they have some algorithms for how much whitespace is being filled up and things like that and it's not a perfect algorithm, but it's a good indicator. And you'll notice, based upon their algorithms, Reuters, for the most part, starts out about the same as CNN, although a little earlier than CNN, but then there's this big gap where Reuters has a whole bunch more information on the screen that CNN takes a little while to get to. So just from that observation, we can see the user experience during the load time, during that painful 16 seconds that somebody may wait for a site, which nobody waits that long, right, but during that painful, you know, let's call it 10 seconds, right, because at the 10 second mark most everything's loaded and we're probably using the site at that point. This 16 second mark is when they determined that everything had fully completely loaded, all images were there and they had stopped downloading things, so it's sort of an arbitrary point out in the future. But I'm going to mark on top of this chart some additional helpful information that I correlated from some other data and you guys can go and try this yourselves on webpagetest. org. This blue line represents the document readyState. It approximates the document readyState because they didn't actually test for document ready, they tested for document complete, which is roughly the same thing, should be within a few 100 milliseconds of each other. So you see at Reuters it was just over the 7 second mark, around the 7 and a half second mark that Reuters considered itself to be document complete. And notice what had happened. They were almost entirely visually complete at that point. They'd spent another crazy 11 or so seconds loading some other stuff, who knows what, but it doesn't appear to have been all that important because they were 97-98% visually complete by the point that document ready happened. Now why is document ready? We'll come back to that later. Why is that important? Because that's the point at which a user is able to meaningfully interact with the page. Once the browser knows everything it needs to know about the document, they can scroll around, and click on links, and move things around. Sometimes they can do so a little bit earlier, but document ready is the standard point at which we know the browser is finished parsing everything that it needs to parse and it can let the user do whatever they want, even if the images still take a while to load. So going, as I said, going back to that perceived performance metric, it's highly important to optimize document ready. Anybody want to take a guess where CNN's document ready was? Fourteen or 15? Fourteen or 15. Those are good guesses. Anybody else? Is it 16? It was at the 10 second mark, which I find very interesting, that's part of the reason why I put this on this on this slide. Notice what happened to CNN after they got to document ready. There's at the maybe let's call it 80% visually complete mark. That means that CNN did perhaps, in this case, a slightly better job of deferring some resources until after document ready to load and to finish loading. So in a sense, they flushed their document a little bit earlier than they would have otherwise because as many of you said, you would have thought that maybe they flush their document or their document was finished over here, but they flushed it much earlier and they allowed the browser to continue to load things, maybe those were advertising banners or other things like that. Certainly, at the point that document ready was happening was much quicker in the lifetime of CNN, even though absolutely the time was further away, but it was much quicker in the lifetime of CNN than comparatively speaking here. So you see we actually have two different messages, sort of conflicting messages that we're getting from this data and that speaks to why this is a discipline, why this is something you have to work towards and master as you progress. This is a maturity state that you go through, rather than just simply checking off a list, okay, I know it's all about document ready and that's the only thing I need to know. Every site and application will have a different way that this curve looks and you are in large control of how that curve looks based upon the choices that you make and we'll talk about many of those choices today. But I wanted to illustrate before we got into the nitty gritty of what the details are why is this so important because there is a big difference between the user experiences of these sites. There were choices made. Perhaps Reuters chose that it was more important to get the content there quicker even if the document state had to wait a little longer, maybe they made that trade-off. And maybe CNN, I don't know the developers, but maybe CNN said it's a little bit more important for us to get the document ready to happen earlier and it's okay if some of that content, which is less important, takes a little bit longer to show up. You see they made trade-off performances, trade-off decisions in web performance and that's part of both the art and the science of web optimization. I just had a quick question. Sure. Totally aside how they develop that, but I see that the visual progress goes backwards at some points. Isn't that interesting? Have you noticed anything visually happening with the site, maybe have a conspeculative as to what happened there? I did not detect those things and I saw the same thing and I tried, particularly the CNN one more than the Reuters one. I tried to see---do I ever see it do that. I don't think it is what you think it is or what most of us would think. I don't think it's actually that stuff started hiding. What I think it is just jitters in this algorithm. I think it's an imperfect algorithm, but I actually don't know. I think it's a good point. We can't take this as a direct science. This is an indicator of where problems can be, so you're really more looking at the shape of the curve rather than the actually metrics of the curve. Yeah. Doesn't your script from the slide back indicate what that was, like when the image showed up or the space for the image showed up pushing the text down. The granularity of the film strip is only a half second. But isn't that perhaps where the curve drops because there's more whitespace visible at that moment. That is quite possible. That it is something I had not considered, that is quite possible. He's asking is it that it got less visually complete because it put an empty item in place that took up some space because an image was about to come in and it pushed some stuff down. That's quite plausible as an explanation. It was not visible to me when I looked through the filmstrip and I didn't notice any obvious things like that happening, but that's part of the reason why software like this helps us dig into data that we wouldn't necessarily see directly, but that's a great question. And by the way, I'm really glad that you guys are asking questions. There will be a lot of interaction as we go throughout today. This is sort of laying the foundation for what we're talking about and this is more "lecture style. " I've got a lot of slides and lot of information, but there really will be a lot of time for us to talk and I really want for you to ask questions at any point. We've got a lot of time built in and I'm in no particular rush to try to rush through stuff, although, getting to happy hour is nice, but I'm in no particular rush. It's early in the morning and we've got plenty to go through, so please do ask those questions. I thrive on that feedback. That's how I'll know if I'm going too slow or too fast or if I'm not covering enough detail. Yeah. Just a curious question about do you know how in general they measure visual progress? Like is this when the mark up is there and that when the images are there and that kind of stuff--- They're taking screenshots every tenth of a second. Okay. And they're averaging screenshots together. Okay, so it's based on change mostly. Purely visual progress. It's something a little bit akin to diffing, but in a visual sense. Right. But that's an algorithm that's sort of newer. This has been around I think they've had this for about six to nine months, I think, this feature, so they're working on it and improving it. Again, they're assigning actual metric numbers. If you go and test out your own sites here, you can get actual metric numbers and they can give you an average to the industry and you can start to make some decisions that way, but this is a new and early enough concept that I'm not sure I would necessarily suggest that. Is it based on the entire page versus just the view port? I think it's based only on the view port. Okay. There's some point at which I don't know how much that height is but it's based on that. By the way, webpagetest. org is substantially open source software, so many of those questions could actually be solved by going and taking a look at their software. Great questions. Anyone else? Who was it who asked the question about the dip in the curve? Was that you? You. Give the first book out for that was a great question. Alright. There'll be more of those as we go. This is another way of visualizing. This is called whichloadsfaster. It doesn't give you near as much detail into the metrics, but it just gives you a very simple if you and a competitor, between the two of you, which one loads faster and by how much. So this is kind of a fun site to play around with. This is a good example of something that you can pull up in a meeting with your boss when he's asking well why does it even matter if we spend time on performance. Well did you know that we're 35% slower than our nearest competitor? Show him that visually real quick. It takes a couple of seconds. It makes a big impact in terms of wetting people's appetites that there is more to be gained here. I find this actually surprising because I feel like Amazon is actually a much more visually dense site than Shopzilla. I don't know if you guys use those sites, but I chose these two sites because I thought, hmm, I think Shopzilla will load quicker and I was surprised to see that Amazon loaded much quicker. Didn't dig much beyond under the covers on this particular one, just to illustrate, but you can check out which loads faster and try it out yourself. It's kind of fun to sort of have them go head to head and sometimes you do it a couple times and it changes the results.

How We Get Web Performance
Okay, so this is not about jQuery, this is about money. Money is an important thing and there have been, again, I won't bore you with a lot of specific statistics, but there have been a lot of studies done by some major players in the web industry, players like Google, and Yahoo!, Amazon, things like that, and these companies have determined that even a tenth of a second in the length of how long their site loads can cost them tens of millions of dollars. Now if you scale that down because obviously not all of us are working on sites that bring in hundreds of millions of dollars of revenue, and so maybe they're in some rarified air and maybe some things are different, if you scale that down, there are a number of companies in a more modest space that have found similar results not to the same drastic extent, but they found 5-10% difference in online revenue on ecommerce sites as a result of paying attention closely to performance. So if your site has anything to do with your company making money, maybe you should focus on performance, and if not, maybe Mark will give you a refund for today's workshop. I also think that web performance is about quality. I think creating quality code, one of the metrics of quality code is performant code. Now I'm not universal in that opinion. There are a lot of people that think that performance of code is a different concern and a different discipline, but I believe that performant code is more high-quality code and I do believe there is a way to balance high-performant code with highly-maintainable and highly-readable code. Many people will tell you that those two are disjoint that you can't have both, you have to pick one, and I admit that there are some trade-offs. You can take the syntactic sugar in JavaScript of the for each operator that gives you a nice iteration of an array, compare that to a standard for loop. Unquestionably, the standard for loop is less graceful, certainly more performant, but I would push back and say the for loop is very descriptive of what it's doing. So it still meets what I would consider a test for maintainable code and it certainly did for dozens of years in any other language, so I'm not sure why we came along in the JavaScript world and said all of a sudden, that's no longer maintainable code. So just take that into account when you start to think about your trade-offs in performance when you start to say, well we can't write performant code because it'll automatically be unmaintainable. I consider that to be FUD. I don't think that's true. Alright, so we've talked a little bit about the why of web performance, let's talk more about the how. Let's talk more about how we're actually going to approach optimizing performance. The web is made up, this web discipline, this web optimization discipline, is made up of two sides of what I would call the same coin, measurement and perception. We talked about that earlier. There is a science behind things, which is measurement of the actual statistics and there is an art behind things, which is the user experience and somehow, some way we as web professionals must learn to marry those two, we must learn to balance those two sides. We have to measure what we're doing so that we know whether we're making progress, we'll get to that in a moment, but we also have to take into account that if our site loads really, really quick, but our users hate the experience of the way our site loads, we have not actually won. You may have ticked off on a spreadsheet for your boss that you have improved the web performance by dropping your load time from 8 seconds to 7 seconds, but if instead there's a whole bunch of jarring, moving going on and the user is visually confused by what's happening or if instead the way you got about that was you loaded all the ads first and then the content later, which some sites have done. It's kind of bizarre, but some sites have done that. Now the user hates you more because you stuck unimportant content there faster. So we have to learn to balance these two things. It's incredibly important. Web optimization is all about improving the efficiency and efficiency, again, has two sides that we have to balance. Efficiency has speed. There is the raw speed at which something happens and we will talk not only about the speed of something loading, but also the speed of the website or the web application as it continues to run because users generally do not come to your site, spend 3 seconds for it to load, and then leave. Usually, they're there for a while, so the performance not only matters during load time, which is highly important, but it also matters as things continue to go on, does your site degrade? Or does it continue to be useful and snappy and performant for them. So there is speed, but there's also memory. And a lot of times people don't think about memory consumption when they think about web applications, but there's memory on both sides. There's memory in terms of what you're doing on your server because servers have fixed memory, and there's memory, and in many cases, limited memory on say mobile devices. So memory matters a big amount. What we mean by efficiency is how well is it doing its job with what it takes to do its job. If we were to try to sort of abstract that into some unit, a unit of work by a unit of resource and what's that ratio? How efficient is it in getting its job done? It's not always about the absolute number. It's not always about the absolute did it load at 6 seconds. It's how much bandwidth did it take to load at 6 seconds? Did it allow other stuff to happen at the time or did it completely hog up the entire pipe? Those are questions that we have as web professionals have to ask. So we're going for efficiency, and again, this is both a science and an art. And there is no one-size-fits-all answer to these questions. These questions are deeply complicated. So as the adage goes, I don't want to give you a fish today, I want to teach you to fish, so that you can think critically about your applications based on these questions. And I encourage you to go back to this slide deck later and really chew on some of the stuff I'm talking about, chew on those choices between them and where you're going to make those decisions and have those conversations with your development teams, with your clients. The decisions that you make there do matter. So performance has two sides to it or two approaches. We need to benchmark what current performance is. For that, we often use a whole suite of tools, many of which we'll mention today. But we also need to optimize, we don't want to just stop. We want to first find out where we are and then we want to make sure we get better. Why do I mention benchmarking? Does that seem like that's something that's so commonplace or so obvious? In fact, in reality it's not so obvious. Many times as a web performance consultant, I could go into a company, and I know many of my colleagues have done the same thing, you go into a company to try to consult with them about optimization, and they say well we did a whole bunch of stuff and things got better, but we still need to do more and you start digging into that statement we did a whole bunch of stuff. What did you do? Well you know we tried to combine some CSS or we maybe reduce the number of images that we were loading or we did some---maybe they even did some more sophisticated stuff like they installed some plugin in Apache that tried to do some stuff. And then you ask the other side of the question, well how much did it improve? Not really sure I can quantify that. If you cannot quantify how much improvement both in actual measurement and in user experience on your site, if you cannot quantify that, guess how far your budget for performance optimization is going to stretch with your boss. Not very far. You have to be able to quantify. It is the most important step to develop a good benchmark of where you currently stand. If you get nothing else out of this section of the talk, please understand that it is highly important that you not just guess at where you are and you not just say well it feels a little bit better. Please get actual numbers and both actual numbers from the measurements of how fast things happen, how much memory takes, and also do user studies. I know that sounds a little bit overblown for many people, but sit real users down in front of your application and do user studies with them based on nothing else than how well they feel your site or app performs and judge those scores every time. I promise it will make this effort stretch a lot further.

Focus on the Critical Path
How many of you have heard this quote before? Some of you are smiling. I have a love/hate relationship with this quote as you'll see. How many of you think you know what Donald means with this quote? In your own words, how would you describe more completely what he's saying when he says premature optimization is the root of all evil. Yeah. Be sure to identify the key areas that are actually problems rather than trying to focus on optimizing everything because you'll spend all your time optimizing something that might not actually need optimizing when you have bigger concern areas that should have been looked at. Good. So to state it a little bit more succinctly, he said be sure make sure you understand what really needs to be optimized, so you're not wasting effort. That's a great way to state it, but I would challenge you to say, how do you know? How do you know? How do you know what actually matters before you try to fix it? Benchmarks. That's one answer, benchmarking it, sure. But you benchmark the overall performance of your site or application, how do you know whether really what's slowing you down is that you've got bigger images than you should have, or what's really slowing you down is you're loading too many JavaScript files, or what's really slowing you down is that your templating is not efficiently caching on your server, or any a number of dozens of other features or factors in the pipeline of your application. How do you know what the outset what work matters? Experience. Experience. Benchmark with more details. Let me give you some context because I think it will back up your broader interpretation of this quote, but let me give you some context the surrounding quote. I've left this part out, but the surrounding quote of what Donald said. Programmers waste enormous amounts of time worrying about the speed of noncritical parts of their programs. We should forget about small efficiencies, say about 97% of the time. Yet we should not pass up opportunities in that critical 3%. You see what I was asking about before and what is very blatantly clear here is we have a choice in software development and in optimization to decide what is critical and what is non-critical. And in Donald's view we can say easily that 97% of the code is non-critical, so we don't have to worry about it. Awesome. Therein lies the road though because figuring out where you should spend that effort on those 3% highly determinate of how successful you'll be at optimizing. It is possible for you to spend all your time majoring on the minor, all of your time going through and wasting effort, as Donald says, on the 97%. It is possible that you don't even realize that there's something that's in the 3%. Something that seems completely unrelated and yet is entirely the reason why something is going slow in a critical path of your software. There are no real good answers here and the reason I bring up this quote and the reason I push back on this quote is because I want you to understand that this is not a platitude that you can apply to software development. This is often what happens in software development and I have been on the receiving end of this many, many times, when you identify something that can be optimized and somebody doesn't like how ugly that makes the code, what do they say, that's premature optimization. Stop doing premature optimization, that's what they say. You see this is used as a platitude to shut you down when you're optimizing things, but it's done because you don't have metrics to why is this important. And we'll talk about how to get credence for why you work on what you work on. But I do want to push back a little bit because there is a prevailing sense that performance optimization is a bad word, that it is the root of all evil, and in fact, it is the root of all success, it's just done wrongly that it's the root of all evil. And he does speak to that here, but we often don't hear the full context, we just hear that nice quip. So optimization is about noncritical versus critical, highly important that we figure that out. Part of what I hope that you get from today is better ways to decide that, so that you don't waste your effort. We could restate that then more clearly and succinctly, that non-critical path optimization is the root of all evil. And I've said that the way we make those decisions it was said that it's about experience, I would say it's about maturity. Maturity in the way that you look at the discipline of optimization, the better that you weave that into what you do every day, the less of a chance you will have an app about ready to go to production or worse already in production and the 3% is killing you and you have no clue where that 3% is, the less chance that scenario happens, and that scenario happens most of the time. That scenario is the reason why that quote gets thrown around so quickly is because people wasted time during development focusing on things they shouldn't worry about and when it got to production, it was still really bad performance, and so we got no bang for our buck. It's highly important that you be mature about how you approach things. So I will restate Donald's quote and you'll see this later. Immature optimization is the root of all evil. Mature optimization is the path to success, but immature optimization, that is inexperienced, that is na√Øve optimization will lead you down the wrong path. So mature optimization is the name of the game and I've written an article a year and a half or so ago for Microsoft about this, again, once you get the slides, you can grab those URLs if you're interested. I focused almost exclusively on JavaScript because this was for their JavaScript site scriptjunkie, but many of the same concepts that you're hearing today are reinforced in that article, so if you want to hear more about that or if you just want to read more about places where you disagree with me, that's okay. So in the field of studying how users feel about performance, there's a bunch of studies that have been done and some studies going back as far as 30 years about how users feel about the responsiveness of an application. When they click to do something or they want to do something, how do they feel about the performance? And there were some studies done again that sort of correlated across several studies, but there was a common conventional wisdom that got us to the point that said 100 milliseconds, that is one-tenth of a second, is the mark at which most users below that mark that is things happening faster than that, most users can't distinguish the difference. If you click on a button and within 100milliseconds, that button responds and does something visual, most users will not be able to distinguish between that and instantaneous response. That's what these results tell us. So oftentimes, people in the performance industry, especially in the front-end performance industry, will tell you if you don't have something that is taking more than 100 milliseconds, it's not in the critical path. Hmm. I wonder if you agree with that. Let's go further. So the 1000 milliseconds is the full second is how long a user, remember under 100 milliseconds they won't detect it. Over 100 milliseconds, most human beings will be able to determine that there was some sort of delay and there is a spectrum up to about 1000 milliseconds that the user will tolerate that delay before they feel like it moved too slowly. So we see this spectrum, 100 milliseconds they can't even tell the difference, it's indistinguishable from instantaneous, 1000 milliseconds they know that it delayed, they know that button did not react right away, but they were generally okay with it. I understand I click the button, you've got to send something to the moon and back, that's cool. It responded relatively quickly. That's the general gist that you get from these user studies. That's actually in computing terms a broad range. A broad range of behavior falls underneath the 1000 milliseconds down to the 100 milliseconds. There's a whole bunch of stuff that we can think about when we broaden our horizon to this part of the spectrum. So I will push back on you and I will say, if you naively say that an individual operation never takes more than 100 milliseconds; therefore, it is obviously and without question not in the critical path and therefore, needs no attention, I would push back on you that you may be correct, you may not. We'll see later today. But as often quoted as a response to those who say well it's not in the critical path, it's less than a 100 milliseconds, well what about 2 operations back to back that both take 50 or more milliseconds. Well now all of a sudden we've got more than a 100 milliseconds and this is the death by a thousand paper cuts. So the question then becomes do operations in your site and in your application, do they compose in that way, do they go serialing? One happens for 50 milliseconds, another one happens for 50 milliseconds, another one happens for 50 milliseconds, and all three of those operations happen before a user gets a response, boom, you've got 150 milliseconds, they can see that difference. Sometimes that's true. Sometimes operations and behaviors do compose that way, sometimes not. The criticism of the death by a thousand paper cuts, meaning am I going to have a 1000 tenth of a millisecond things happen all serially one right after the other to get me above the 100 millisecond mark or not. The criticism of that particular approach is to say well that never happens in reality. We never compose those things in reality and I would say there is wisdom there and understanding that not always do we see them happening one right after the other, but sometimes it happens. For instance, when you have an animation loop and you've got some code in your animation loop, in the individual frame of your animation loop is never going to be more than 100 milliseconds, should never be, but you're going to do a 1000 frames of update to move a box across the screen. If that box takes a second and a half to get there instead of a second, will that user perceive that things went slower than they should have. If in the Chrome browser it moves in one second and in the Firefox browser it takes a second and a half, will they perceive that, and the answer is yes, they will perceive the difference in a 100 milliseconds, in this case, 500 milliseconds. They can perceive that difference. So it's more complex the question of whether or not something's in the critical path. I would say this, as a general rule of thumb, inefficiency is systemic. I've seen a lot of code. I've written a lot of code both good and bad, hopefully more good than bad, but one thing that I believe very strongly about all the code that I've seen and definitely all the code that I've written is that I tend to either be in the mindset of performance optimization and a lot of the code that I write is by default performant or I tend to be in the opposite side of the approach, which is that I don't care about it at all. I'm just doing quick demo code and just writing stuff and I don't care, I'll just query the DOM 100 times because it doesn't matter and I just want to get stuff done quick. Any one of those individual operations across all those lines that I wrote never would cross that 100 millisecond threshold, but if 90% of the lines of code that I wrote each one of those lines was inefficient. That code is systemically inefficient and I think this is true of a lot of development teams because I think a lot of people do not have that by default mentality, how can I write this performantly, how can I make those balances work out.

The Total Cost of Ownership
I would say I wanted to actually someday I want to write the book called the myth of the refactor because I'm sure that you guys can relate to the idea that says just do it now, we'll come back and fix it later. Many of you giggled, so I know you can relate to that. And I have seen by and large that the refactor that gets promised, never happens. I have seen by and large that the myth of the refactor is actually true. Oftentimes you get one chance to make that code right because budgets, and time constraints, and further features will never let you take the proper amount of time to go back and fix it. That's how you get to systemically inefficient code, little by little, cut by cut. You're not going to die by a 1000 paper cuts, but you'll die by the one paper cut that's left unattended in the wrong place. The total cost of ownership is what I'm talking about here. The total cost and this is my theory, my opinion, the total cost of ownership of non-performant code is higher than for performant code. Why do I phrase it like that? Because you are going to have to go back to your boss and convince him on Monday why you spent your money well to come to this workshop and he's going to think in terms like this, he's going to think in terms of amortizing out that pain now in the short term or dealing with it in the long-term and if you say to him, sure it may be quicker for us to get the software out by the deadline, but in the long-term, it will cost us X% more. If you can be mature about that conversation with your boss, that's something he'll understand. He won't understand why the for loop is not sufficient for you and why you have to figure out a different way to get it in less than linear time, but he will understand that if you make that choice and a 1000 other choices like that, you're development team will end up spending more time writing code, and fixing bugs, and responding to complaints about the speed and performance over the long-term. He will understand it in terms like that. So the total cost of ownership is higher. What I've been getting at this whole time is to have a mindset that it's performant by default. Rather than quick by default, quick to write, easy to write, easy to understand, easy to code review, what if our primary metric was that it was performant by default and we scaled back on the performance in cases where code maintainability was more important. That's the reverse of what generally happens. Generally, what happens is code reviews force us to have really nice, clean, elegant, maintainable code and we only optimize when it is so painful that we can't do anything, but optimize. Many, many times have I seen code that I predicted would perform badly and people said no, don't worry about it, and then it went to production and it performed badly, and now it's a crisis. As soon as a big paying client says this is too slow, you are slower than your competitor, you must fix it, what's your boss going to do? Walk over to your desk and say fix it, make it fast, make it happen now. I can't have that client going to one of our competitors. You know what just happened? You just ratcheted up significantly the risk of that refactor. You have to go refactor that code and you don't have time to do it right, so you're going to have to make a whole bunch of guesses as to the way to do the performance optimization. You're going to have to guess where that 3% is, you're going to have to guess about that code. You're probably not going to have time to write regression tests so that you make sure that performance doesn't fall back again because your boss wants it done right now. He wants it hot fixed and maybe we'll get to come back and do things, but most likely not. That's why the total cost of ownership is so much higher and that's why if we think performant by default we can avoid those problems. The dirty little secret of front-end performance, we're at a front-end workshop, but there's a dirty little secret in what Steve said earlier when he said that 80 to 90% of the performance time is spent in the front-end, so start there for optimizing. The dirty secret is that most of front-end optimization actually happens in what I like to call the middle-end. I coined that term a few years ago. People thought I was being cute or silly, but it turns out that it's a really important concept to think about software architecture this way. We'll get more into middle-end as we go, but I call the middle-end things like templating, URL routing, headers, caching, AJAX, stuff like that. Every software application out there, every web software application out there has a middle-end, most of you don't call it a middle-end, and most of you have nobody who knows how to optimize the middle-end, but it exists. And most of the time, these pieces are just strewn carelessly throughout your back-end code without any care or thought or concern. So I submit to you that to be a good web optimization industry professional, you're going to have to get much more familiar with the middle-end. So our agenda for the rest of the work shop is we'll talk first about the middle-end, several items in the middle-end. We'll talk about YSlow, we'll talk about resources like CSS, JavaScript, images. We'll talk a little bit about architecture and also about communication that is your data communication between client and server. I had initially intended for those two modules to be a strong amount of emphasis, as we're going about it, I believe that we probably will put a little less emphasis there because it's so hard in a one-day workshop to really dive into that. That deserves a full-week workshop. But I do want to introduce you to some of the concepts there. And then we'll jump more into what you would traditionally call the front-end. We'll talk about resource loading. We'll talk about removing abstractions. We'll talk about moving from JavaScript to CSS specifically in the realm of animations, moving things around. We'll talk about the UI thread and garbage collection. Those may be terms that you're familiar with and they may be very foreign terms to you, but they're critically important to optimizing your front-end. And lastly, we'll spend some time talking about jsPerf. JsPerf is a performance benchmarking website for JavaScript snippets and we're going to go through jsPerf and we'll actually go through a slide deck that I was part of a panel a couple of times called JavaScript Performance Myth Busters where we'll actually take some very common things that people think about JavaScript, they'll say it's faster to do array. Join than it is to concatenate strings and we'll actually test that and see whether that's true. We'll either confirm or bust some JavaScript myths. So we'll go through that as the end of the day. We're going to take a little bit of a break now. I hope that you guys got those performance exercises because I really do want for you to go through that code. Sometimes you go to a workshop and they say go through the code and most people just surf online, I really do hope that you participate in those, so please do and go and get those. I mention Node. js here not because I necessarily expect for you to go install it, but you do need to have the ability for some of those exercises to run a local web host server on your system. Your Macs come with Python built-in in Mac OS and you can just do python -m SimpleHTTPServer, look it up on Google if you don't know about that. It's a great, easy simple way to do it. If you don't have that, you can try a lightweight server like Node. js with something like the Express Framework to serve up some files. You can download an Apache for your Windows box, however you want, but during this break, please do make sure that you have your performance exercises in hand and that you're ready to run through those as we jump in. So we'll take a little bit of a break while we go through that and if you have any questions, please feel free. I found that some of those exercise folders are empty. Are they from a different branch there? What's the deal? Nope. If you read the readme, there will be instructions. Some of them have files that you'll work on and some of them will have instructions that you do things elsewhere. One of them is intentionally empty. I was going to get to this later. Exercise three was the one we were going to do on architecture and system communication because that's so much more heavily reliant upon having a server, and I can't necessarily know that everyone's going to have their own server and server architecture to work with. I've omitted that exercise and we're going to look through some code that I did a few years ago in that space, so that exercise is intentionally blank, but the other six have stuff. Exercise one is also empty in my folder. It has a readme in it. Got me there, alright. Alright. You can look ahead to those exercises if you're interested, but I'll explain the context of the exercises as we go along, so it may be more effective to just wait for that.

The Middle-End: YSlow
YSlow Rules
Alright, we'll go ahead and jump back in. As I mentioned before our quick little break, the agenda today we'll start talking in the middle-end, so we're going to talk about YSlow, which is going to bring us to some really interesting points that you may have already heard. You've probably read blogs and posts by Steve Souders and others, so if some of this feels a little bit like review, I apologize. I hope that there will be some details in here that I'm able to add that will keep that part not just review. Okay. Firstly, if you do not have the YSlow plugin installed on a browser, highly encourage you to do so at least for this class. If you want to uninstall it right afterwards, that's great. YSlow is one of the first browser plugins that came out that allowed people sort of a very easy almost automated way to test the performance of the loading of their page and it would give you all kinds of statistics about how your resources were loading and in what order and it actually had a set of rules that were applied that evolved over time. Those rules started out as what I would call the big 14. Steve Souders, again kind of the father of this industry, he working at Yahoo! codified a set of rules for optimizing the front-end, and it's not Steve's fault, but he didn't realize that he was actually talking about the middle-end at the time, but we're going to go through those rules nonetheless. So the first one that he said was to have fewer HTTP requests. This is probably if you haven't heard of any other performance optimization rules or guidelines, this is probably the one that you're most familiar with. It is definitely the most cited and makes a lot of sense that it's at the top of most people's rulesets. What this is essentially getting at is that if you have a page that consists of a single request for the HTML, and then you have 10 or 15 images that are loading in at various times and you've got 4 or 5 scripts and a few style sheets here and there and maybe you download a flash video and some other bad stuff like that, all of a sudden you start to add up those resources and one or two would not be such a problem, but you start to add up all those resource requests and you see very quickly that your performance, that waterfall diagram that we talked about earlier in the intro, that waterfall diagram is starting to get really stretched out wide or even if it's not getting so wide, it's just really, really long and you're taking up tons of bandwidth and that story is really bad for the mobile world. You certainly want to be more careful. I have worked on sites as a result of work that I've been at, you know, jobs and employment that I've had, I've worked on sites that you go to a single page, I won't name them, but it is a major toy manufacturer's website. You go to their website, you go to one of their product pages, and they load well over 250 resources just for the one page and on a fast internet connection at work, if you watch that waterfall diagram, it takes well over 50 seconds, 5-0 seconds for all those resources to download. Can you imagine how long that takes on grandma's dial-up modem or DSL? Forever. Now luckily, probably a lot of that stuff isn't all that important. Grandma just wants the picture to know that it's the red toy, so she can buy it for you. But truth be told, they really are doing a very poor job of making sure that the Elmo page loads quickly. So fewer HTTP requests speaks to the idea how can we get the same content and information down there, but require less requests to do so? There's a variety of ways that we can do that. We can combine images, we'll talk about that, we can combine scripts, we can combine CSS files, we can simply just request less, or importantly, we can request less during page load time because these rules are mostly focused on what happens during page load time, we don't have to load everything during page load time. That's one way to reduce the requests in that critical pipeline of time simply not request that stuff. Have your sites designed and your loading technologies designed in such a way that the less critical stuff happens later. On some of my sites like my blog and things like that, I have some scripts that I just don't care about as much, like for instance, Google Analytics. I like to have analytics, not all that important to me. If I miss a few clicks here and there, no big deal. So I load Google Analytics a half second after the page loads because I don't want there to be any way, shape, or form that Google Analytics is either slowing down my page, or worst yet, being a single point of failure where if Google happens to be down, which has happened, now all of a sudden my page isn't loading. So I wait for a half second after document ready and then I request that file, and if I lose people in that first half second, oh well. Now that's the choice that I make on my site, but other people can make other choices. But one way for you to reduce request is just simply reduce the number of requests during page load. Next we have use a CDN. Pretty straightforward. Any resources that you have that are static, that is that they are not often changing, or more correctly, that they do not require dynamic processing to create. So an example of a not static resource would be you know back in the day in the 1990s they had those little image counters, you guys remember those in like 1991, everybody remember the image counter, they required a script to generate that image on the fly. That's not a static resource because it requires a script and there are many others that are more current and recent examples than that, but the point being if you have static resources that do not require a server to figure out what the contents of it is like when you build, have a build step, and you build all of your CSS and you put it all together in one file, that's now a static resource. It's not going to change until the next time you do a build, so you can stick that in a version file upon a CDN. Why are CDNs important? Basically, CDNs are important for two major reasons. First of all, probably most importantly, those CDNs have more points of presence than you do on the whole. You typically will have as a business one web server or one web server farm in one location in the world, you'll have one data center. Most CDNs, if they're good CDNs, have multiple locations spread throughout the world, so they've got a server in Asia and one down in Australia. How many of you know that Australia has a very thin pipe that all of their traffic goes through and it goes into, I believe, Taiwan and that's where they get their internet. If that goes down, the entire continent of Australia is offline. So people put CDNs in Australia, so that all of that traffic didn't have to go over that small little pipe. It's very important that you understand you can't probably do that yourself. You're probably not going to go down to Australia and open up your own little data center, but the CDNs have. So that's the major most important reason for a CDN. The second reason, which I wish were more important, it's not as important, but the concept of shared caching. If I reference the jQuery file from the Google location and a thousand other sites that people visit also reference that file, in general, that person will have that file more likely in their cache more often and won't have to re-download that file. So the concept of shared caching is important and that's why use CDN is number 2. Number 3, expires or cache-control headers. What this essentially means is when you have a resource, there is an implicit amount of time that resource will live, will be valid for according to the browser, so you want to minimize the amount of incorrectness or disparity between those two. If you intend for a resource to only be valid for a day, but you send out an expires header that says it's valid for a week, there's a disparity there, vice versa, if you want it to live for a really long time and you send out no expires header at all, the browser will not cache it and every single time somebody loads the page, they'll have to get that resource every single time. So you do want to make sure that you're sending out proper headers that correspond to your intent for a particular piece of content and that is not the same for all your content. You have stable content like jQuery. It's never going to change 1. 8. 2, that version, never ever going to change. That file name, you know, it's never going to change. But then you have less stable content like your user experience or your little weather widget that you tweak on your site or whatever, that stuff changes every day, right? Those two do not need to have the same expiration headers. You need to have a much longer expiration header or cache control is another way of doing it for some proxies. You need to have longer headers or longer expirations for your more longer lived, more stable, less volatile content. Gzipping, this is compression of your text resources as they go out over the wire, very, very important. There's a whole lot of FUD that is Fear, Uncertainty, and Doubt, if those of you who don't know what I mean by FUD, there's a whole bunch of FUD about Gzip that was around for a really long time and part of the blame goes to Apache for some of their default configurations around this stuff dealing with IE6 and SSL and some other cases, and it led to this prevailing thought like Gzip is awesome, but we can't use it, and I'm here to tell you Gzip is awesome and you can use it and you should be using it and who cares if somebody on Netscape 4. 72 can't get your Gzipped content because the massive savings that you get will far outweigh the lost revenue from that one guy on Netscape, I promise you. Gzip works so well that many of the times that people try to outsmart Gzip, and I'm one of them, I've tried to do this, I've tried to create algorithms where I rearrange my code in special ways and try to just squeak out a few more bytes. I was a nerd about that for a while and every once in a while I'd beat Gzip, but most of the time Gzip was better than anything that we can come up with. It's so good and it's been getting better and it's battle-hardened, I mean it's an awesome algorithm, so you shouldn't try to fight against it, you should embrace Gzip and use that. And in some cases, you can compress up to almost 50, 60, 70% of the size of a file. When you look at something like jQuery that is 80 or 90k, the actual file size, and it Gzips down to 32k, imagine how much savings that is just by having Gzip turned on.

YSlow Rules, Continued
Style sheets at the top. Had some conversations during the break about these rules, style sheets at the top and scripts at the bottom. Many of you have probably heard that concept, especially with scripts, you probably heard just put your scripts at the bottom and you've solved all your woes. I wish that were true. It's one of the rules, but not the only one. Style sheets at the top because the browser will often block the rendering of content until it knows about all the style sheets, so it doesn't paint things incorrectly and have to redo that work. So the earlier we get it, the better. Let me just say, let me go back to that, oops, sorry, let me go back to style sheets at the top, scripts at the bottom. There's a common misconception that if you put the scripts at the bottom, you solve all your performance woes. You have not changed anything about the nature of how the browser treats that download. The only thing that you have done is by the placement in your markup because browsers parse markup top-down, the only thing that you've done is given a priority indication to the browser of which resources you think are most important. So when you frame the question that way, let me ask you the question on your site, is JavaScript the least important thing on your page? Some of you would say yes, others of you that work on something like Gmail would say hell no, it's not the least important, it's the most important. So you can't take these as one-size-fits-all rules. You are prioritizing where in that limited number of pipes of bandwidth that your browser has, you're prioritizing where in that list you want to put the emphasis first. That's what you're doing by placing in markup. It's common that people conflate scripts at the bottom with a different concept, which is called waiting for the document to be ready because they want to modify the document or attach event handlers or do some other things. They think if I put the script at the bottom of the page, surely the document's done, so when my code runs, everything's cool. That's not actually true. In reality, the way the algorithm works, it's not necessarily true that the browser is done with the document, done with the body when your script runs. In most cases, it probably is true. But if you're relying upon that assumption, you're making a faulty assumption. There are ways to do document-ready detection, some of them are good, some of them suck, but there's ways to do that, don't try to conflate that with where you put your scripts in your document. CSS expressions, you want to avoid those. For a long time, that was popular in the IE-specific world, now it's definitely fallen out of favor, so this isn't a huge one, you almost never see those, but every once in a while, you'll see one of those. People try to solve position-fixed with CSS expressions. General ideas to avoid CSS expressions, it'd be better for that position-fixed behavior to just not work on the site, than for you to have that slowing down the really, really slow archaic IE6 because the thing is it works, but it works really slowly. It recalculates that thing hundreds of times a second. It's constantly chewing up your UI thread, which we'll get to later. Externalize your JavaScript and your CSS, put them in external files. Pretty straightforward. The reason for that is caching. If your JavaScript is entirely in line with your HTML document, and by the way, most HTML documents on the web are not cached because most of them are server-generated with PHP or. NET or whatever and most people forget to put expiration headers on their markup, and in some cases, they don't want it at all because it's always going to be a dynamically-generated page. So if you inline your code in one of those types of pages, which most pages are, your code never gets cached. That does not mean that there's no use for an inline script snippet or maybe an inline style block, it just means that in general, you should not have most of your code or even a large chunk of your code inlined, most of it should be externalized. That rule is not entirely true in the mobile world. There are some benefits to inlining things and reducing the number of requests further, so the mobile world does mess with that mindset a little bit, but in general, externalizing is good. Notice it's number 8 on a list of 14, so it's not the most important, but it's important. Fewer DNS lookups, this goes kind of in hand with fewer HTTP requests. The theory that most people had was well the browser's got a limit of how many connections it'll make, but it's limited to a hostname, so I'll just create more host names, and therefore, I'll get more parallel requests. Awesome, except not because everything still funnels down through one internet connection pipe. It's not like you magically created new internet connections. We can't magically create new internet here just because we reference a different hostname. So you are kind of getting around those limits, but the reason those browsers put those limits in place was for good reason because they're trying to create a same balance between device usage. It's much more important now with devices and battery-powered CPU usage, all those things. Sure you might be able to get your page to load a second faster, but if you zapped up 20% more battery usage during that, is that a good thing? I know. But fewer DNS lookups says lets pay attention to not do too many of those, maybe a couple of extra ones are okay, but if you start referencing 8, 10, 15 different domains and you're doing a lookup DNS-wise every single time, you're adding to the latency of that page every single time. So we want to minimize the number of DNS lookups. Browsers are starting to help a little bit with this. They're doing some pre-caching of DNS lookups and things, so it's becoming less important, but still important not to just willy-nilly use different domain names. Minifying JavaScript and CSS. I think this one should be higher on the list than on the original list it was lower, it's now higher. Removing all the whitespace and comments from your code can reduce the size of your code by as much as often 10, 15, 20%, and even with Gzip in play, that's still a major savings. It'd be nice if Gzip algorithm specifically discarded comments for you, but it doesn't happen. Avoiding redirects. This is a common problem, it's not a terrible problem, it's a common problem that adds an extra couple 100 milliseconds on the front-end of every single request. When you go to cnn. com, you type in cnn. com like I do, it redirects you to www. cnn. com. What happened was a roundtrip to the server and the server responded and said cnn. com is not the canonical location for this website, tell the user to change the address and re-request. That's what happened in a redirect, so two requests instead of one. At least it didn't download all the resources and stuff, but two roundtrips to the server, and by the way, sometimes, crazy as it seems, sometimes those are different DNS responses. Cnn. com and www. cnn. com happen to reference the same server, but that's not always true. Yeah. Is that even with like Apache rewrites or---? Still a roundtrip to the server for it to ask the question, I'm wanting cnn. com and I get a response back that says no, no soup for you, you can't have cnn. com, ask for www. cnn. com, still the same concept. So the simple solution to that is don't do that kind of stuff or don't publish links that are going to require redirects. Have the simple link that people go to be the one that's going to serve up the page directly. I understand why people do redirects, they want to consolidate cookie domains and other stuff like that, just don't have that propagation of all those different domains, just the simple basic domain is good enough. Avoiding duplicate scripts, this sounds like a stupid rule, but it's actually on there for a reason because more often than not, people request jQuery four or five times. How does this happen? Because the CMS has different plugins and each one of those plugins requires jQuery and it probably requires a different version of jQuery, so you end up requesting you know multiple versions. That sounds nuts, but that is very, very common on the web. I think this rule should be higher. Yeah. How do you recommend dealing with, like I've worked on sites where there's a lot of tracking scripts, external tracking scripts all over the place, what's the---? Yeah, so that's about, that's a tough---the question was what do you do with sites that use a bunch of different tracking scripts all over the place and that's creating all those extra requests. That's a problem and that's one of the reasons why I deferred Google Analytics because I don't want all that crap to slow down more important resources for my page. Oftentimes, that is sacrosanct. If you try to convince your boss or somebody at your company like in the Marketing department to muck with that at all, they'll shoot you down. So your mileage may vary with that, but I would tell people if you're going to do all of that stuff, if you're going to have core metrics and Google Analytics and 14 other ones of those scripts in there, at least let us load it dynamically later, so that we're not slowing, not taking away all those valuable bandwidth resources. Is it best to consolidate all of the JavaScript scripts into one global file so that you're just caching it one time in the browser? That is one---we're going to get to that later, but that is one of the most common assumptions that people make from that rule number one. Rule number 1 says reduce HTTP requests. I can't possibly reduce it more than getting it down to one, can I? I'm not necessarily convinced that is a good one-size-fits-all type of rule. It is certainly true that 1 would be better than 10, but I'm not sure we should stop there, and we'll get to that later. Great questions. Alright, 13, ETags, these are hexadecimal codes that are generated, which are supposed to uniquely represent sort of a fingerprint for a particular resource. We'll get to this in just a second in this module, but essentially, ETags and expiration headers come under the concept of conditional loading, so if I have a fingerprint for a file, I can ask the server, here's the fingerprint that I have for the server, is the current fingerprint of that file different? If it is, send me the file, otherwise, tell me that the file hasn't changed and we're cool. That's what ETags are for. So it's sort of in place of checking expiration dates is checking actual file fingerprint contents, kind of like MD5, but better. Cacheable AJAX, and by the way, the reason he mentions ETags, you do want to have ETags turned on as part of your strategy for optimizing performance. You want to either have expiration or ETags, but there was a problem for a while that people didn't realize that the default configuration for Apache when you turned on ETags, part of the component that it would use in Apache for generating that ID was the file node ID, which is different for each server in a web server farm. So the exact same file would get a different ETag from one of your three web server, from each of your three web servers depending on who got it from where. So the problem with default ETag configuration is that it's not doing really what we want because it's taking into account stuff that is not part of the uniqueness test. That same file is propagated across a web server farm, it should get the same ETag. It's a simple fix to change the configuration, but by and large, whatever Apache ships is their default configuration is what most people use with Apache, so it was so important that he actually created a rule for it that said, don't use the default ones, either turn them off entirely, or at least, customize them so it's not doing stuff like that. And then Cacheable AJAX, much more important now. It was at the bottom of the list several years ago, much more important now. If you are requesting resources like a RESTful interface, if you're requesting resources, you can add an additional layer of expiration on top of those things like a list of the newest books. That AJAX can be cached for four hours or whatever your particular thing, you don't have to re-request that from the database every single time, so AJAX can be cacheable just like XML files and CSS and JavaScript. Okay, so that's the 14, there's more, which we'll come to later, but something that you may or may not have noticed, it may have been a question, some of the rules on this list are contradictory. These two for instance. Reduce your requests, but make more requests. Hmm. How do we deal with that? These two, use a CDN, but make fewer DNS lookups. Hmm. And these two. I marked these two not necessarily because you can't use the two at the same time, but because you shouldn't use the same two at the same time. So if both rule---if it's telling you to do both in your rules, you're creating extra headers. I've written a whole article, which I'll link to in a little bit, a whole article about header optimization and header bloat for a web performance calendar a couple years ago and looking---if you've inspected the amount of headers that come back, there is some much overhead in an HTTP request and a response, there's so much extra stuff that's being stuffed down and one clear thing is that you do not need both ETags and expiration headers. They both serve the same goal, which is conditional loading, only send me the file if it's changed. Reasons why you might choose one or the other are more gray, there's more complexity there. But whatever which one you choose or whatever which one you are more preferable to, just pick one, don't do both.

Beyond the Big 14
Alright, so let's jump into the fewer HTTP requests. One way to do so I mentioned was to combine this images. This technique is called image spriting. This is an example, I don't think this is still the case, but this is an example from Amazon where they had a bunch of images combined together in a single image file and then you use background positioning to position the background image in a negative location so that only what's showing inside of the image is what you want, so you know if you're trying to show a different play button, you just move the back, change the background position of the image and you only show the different play button or whatever. It's the technique with image spriting. Concatenating your JavaScript and your CSS. That's a very important thing. It's not necessarily a one-size-fits-all that you go down to one file, but it's certainly true that 1 file is better than 10 files. This is an example of some code. I just pulled some random code together and concatenated it. One thing I do want to point out because it sets us up for a discussion later about minification is that this code's not minified. We see full variable names and full function names in this code. So there's still a whole lot of space that could be taken out of this code, even though we've squished everything together and taken out whitespace, we still haven't done everything. But this is important. Oftentimes this sort of stuff happens automatically as part of your build process, you don't have to manually do this. Mod_deflate. This is how we setup Gzipping on the server and this is how you configure it in Apache. This is actually the default configuration that I've pulled out of my Debian install of Apache, so I didn't have to do anything special. You notice they have a few things in here about MSIE 6, probably not useful or necessary, but the default configuration just works. This is a page that you can actually take a website and ask it, am I properly doing GZip both on the markup, as well as my content and this is very simple, you just put in an address and it tests it. Here it tells me, yes, I have GZip. The size of my markup right here was 25, 601 bytes, but what was actually sent over the wire was 6, 945 bytes. It's 25% the size. Huge difference that just have GZip turned on made, 72. 9%. This screenshot from a little while ago I mentioned that article, here's the link to that article about bloated request headers. This was taken in IE's Developer Tools, which is why it looks a little less sexy, but the concept is the same. We're looking at some response headers in this case, and what I wanted you to note here was we have both an expires header and an ETag header, and the reason I want to point this out is that's coming from w3. org, the site that handles the standards for the web and they're sending out both headers completely unnecessarily. This is much more widespread than you think and you think oh, what's a few bytes here and there. A few bytes here and there is not the problem. The fact is you've got hundreds, thousands of these going out, that stuff is adding up, and truth be told, there are other headers that are much more important like reducing your cookie headers because they can be you know several K just for the cookie header. But if you are adding on an extra 10 to 15 characters for all these additional headers, and I've gone to some sites where there's 40, 50, 100 headers. It's kind of crazy. But if you're adding on all those extra bytes, you're more likely to start creating the need for more packets. More packets means more TCP overhead. So you're adding on all this extra weight and for the most part, for absolutely no reason whatsoever. You just need to tune and configure what headers are going out. There's lots of articles out there that explain how to do so, mine is one of them. So I mentioned that there was more beyond the big 14. There's a whole new set of rules and they've been codified into those plugins that they're testing for stuff, in fact, some of the performance monitoring systems out there, they'll actually will---they have hundreds of rules that they'll apply to your site automatically. You can test the site for free using a couple of different services, we'll talk about those later, a couple of different services you just submit it for free and they'll run 4 or 500 rules against your site and give you a report about all that stuff. It's incredible the kind of information you can get, but they're checking for all kinds of stuff like, are you using document. Write in your page or other things like that, all kinds of things that do affect the performance. Yahoo's got probably a list of about 25 or 30 that they've sort of codified. Here's a couple of them. We talked about reducing the cookie size. This is actually 1675 bytes of cookie completely from Google Analytics, all that UTMA, UTMZ stuff that's all from Google Analytics, that's how they track uniqueness of people. They attach it to the main domain of your page, which means every single resource on your page that loads off of that domain sends 1675 extra bytes for every single request every single time. Cookies are a massive problem in terms of web performance and we haven't thought about them in that perspective very much at all. We need to use different domains. We need to cut down on the size of our cookies, reduce them altogether if possible. I mentioned using different domains. You can have cookie-free domains that this is a common practice where you'll have a separate domain just for loading all your images in CSS, and the key thing that you need to understand, there's nothing special about the domain that makes it cookie-free, except that you never load any single resource ever through that domain that can set a cookie. That's the important part. There's nothing special about the setup of that domain. There's nothing special about the server. You have an additional domain like i. mydomain. com for your images. Again, not going crazy with too many, but you have an additional domain that you set up and that domain is only for resources that do not set cookies and you need to go in and make sure no cookies are ever set, enforce that by your configuration and that cookie will cookie-free, except there's a problem. and the problem is I said just use like i. mydomain. com, the problem is that most cookies are set globally, so they're sent not only for the base domain, but for every subdomain. So now to get a cookie-free domain, you actually have to have a physically different base domain, which sucks, but it's going to drastically reduce the amount of cookies that are sent for all your resource loading. If you're using a CDN it will generally take care of this, but here's the dirty secret, a lot of CDNs send out all these crappy headers. I think it's disgusting how they should be the ones at the forefront of helping us with performance optimization and they're just sending all kinds of stuff that they don't need to send there. Alright, we talked about spriting a minute ago, but you can actually optimize your sprites. There's a whole bunch of stuff. I encourage you to read up more on that article about optimizing of sprites, but did you know that if you horizontally orient your sprites, like you have a very not, you know a very short sprite, but it's very wide, that's more efficient for the browser to parse than a vertically-oriented CSS sprite. Why? There are a couple of different reasons the way they're parsing algorithm for the images works or whatever, but optimizing sprites can make a big difference and not just necessarily in terms of size, but in terms of amount of CPU usage that is required by the browser and the device to render that image. Another example is it's very common for people to think, well I'll just take all of my images and stick them in one giant sprite and then I download only one image, that's cool, right? It's terrible for memory performance because you have that giant image in memory on a mobile device chewing up huge megabytes and megabytes worth of memory, so you want to be intelligent about spriting. We'll talk in just a moment about a bunch of tools that help you with this stuff. Another one of the rules is img src= empty string. You would be surprised how often this happens on the web that people have empty source and you would think that browsers would be intelligent enough to just ignore this. Nope. Browsers, most of the browsers until just recently fired off a request to the server when they came across a tag like this. Same is true with a link tag that has an href that's empty. How does this happen? CMSs that are dropping invariables into these things and for whatever conditional reason that variable is empty so what gets written out to your markup is an empty image tag. You probably never even worry about it because you don't even see the image, so you don't know it's there, it's silently killing your performance. So we've talked about the YSlow plugin, let's get much more specific about it because we're coming to our first exercise with the YSlow plugin. I hope you have that installed. This is an example of what it looks like. It gives you a report and it grades your site A, B, C, D, E, or F, and then it gives you a numerical grade as well, which sort of roughly corresponds to what you see in school. A couple of things to note. Just because you get an A on some item here, doesn't mean that you're perfect because sometimes you can have a problem in one of those categories, but you're at 91, which is still an A in that category. You do need to look at all the categories. Don't just think, oh, I got all As, I'm cool. You do need to look at these categories and see what it's reporting to you. In some cases, especially in this exercise, I found this frustrating creating the exercise because I was trying to create a page that would fail like most of these rules and there was a bunch of rules that it was not failing that I thought it should be failing. Part of that could be due to I'm using a newer version of Chrome, so there may be a problem between the plugin. Part of it may be that the plugin is doing a lot of really intelligent stuff and I was loading only off of localhost, so it may be saying, well, I don't need to apply those rules in localhost. Remember I said, caveat utilitor. Understand that you know user beware here. Just because it doesn't tell you that it's wrong, if you know that there's a rule that says you're using too many scripts, just because it doesn't show up on the report, doesn't mean that it's not a problem, but if it shows up on the report, there's a pretty good chance it's a problem.

Exercise 1
So we're going to take a moment. It may not take five minutes, it may take a little bit longer. I'm going to be available for questions if somebody needs me to come over or I can answer them sort of globally, but I'd like you to go through the exercise. It essentially tells you to open up a couple of pages using YSlow and grade those pages, take a look at what problems there are, maybe jot some things down. In these exercises, when I tell you to jot something down, or I tell you to note something, I know most people kind of look at that and say oh that's great, but I'm not actually going to do it. I would encourage you to actually have a text pad open and write some stuff down because you'll want to go back and look at some of this stuff later after the workshop, so you can get some insights into those notes as you take them. But go ahead and take a few minutes to do that exercise and we'll come back in just a moment. If anybody's tested a major site, just shout out what sites are you testing. New York Times. New York Times. Google. Google. YouTube. YouTube. Craig's List. Craig's List. That's a good one. I bet you're getting some good information there. Alright, well I think we'll jump back into it, unless anybody wants to yell at me and tell me to stop. There'll be plenty of breaks, so don't worry.

The Middle-End: Resources
Images
Okay. Next on the agenda, we're going to talk about resources specifically, CSS, JavaScript, and images, and one of the longer exercises that we will do will be at the end of this module, and based upon our timing, and I'm not sure exactly when our lunch will start, but we may finish this module and go to lunch, and then do the architecture and communication sort of after lunch pretty quickly. That was the one I mentioned that ended up not being quite as flushed out because of the nature of the workshop and being only one day, so if we cut some on that one, that's not a big deal. It was intended to have that be before lunch, but it's okay if we stop after resources. But we'll just jump right in and see where we get. Okay so, this is a chart I shamelessly stole it directly from one of Pingdom's blog posts, so I didn't create this beautiful chart. I'm not good at creating beautiful charts. I see presentations all over the world from people that just create these awesome-looking charts. I've never been able to create anything that looks good. There's an example at the end of today where I created one chart and you'll see how horrible it looks. I'm just not good at it. But this is an interesting chart. This is comparing November 2010 to November 2011 and it's comparing the different types of resources that happen on the page, both the total size. We see that from 2010 to 2011 it jumped from 626 K to 784 K. That's a pretty substantial jump and the total size that people are stuffing down. Also what the hell is taking 728 K? I don't know what sites are doing. I believe, by the way, this is based on like the top 20, 000 or 200, 000 sites, something like that. This is data that's coming from the HTTP archive. If you haven't checked it out, it's fascinating. Every few months they're running across like 2-4 million, something like that, sites on the web collecting all kinds of performance statistics about the web and archiving them forever, so we actually can go back and find out what's happening with the web like stuff like this. We'll see HTML there went from 31 to 36. I point that out because we'll come back to specifically that one in a little bit. But JavaScript is actually, it doesn't seem like it, but JavaScript is actually the biggest performance jump here. JavaScript, I think, is like a 40 something percent, 43%, or something like that jump from 2010 to 2011. That was a big alarm when people saw it as when it was presented as a percentage rather than as this chart shows jumps out at us. Why are we sending so much more JavaScript with sites? Why aren't other sorts of resources growing? But JavaScript is growing much quicker. So all this is to say that the size of what we send down matters and it's going in the wrong direction. Even though we're optimizing stuff, we're still growing. It's kind of like the national debt. Even though you pay down stuff, it still grows, same with resource sizes. Sorry, I didn't mean to get political there. Alright, I am trademarking because as of this morning, nobody had ever written the word leanificationizing, so I'm trademarking that word. Attribute it to me. Ironic, huh? Image shrinking. It's amazing how much extra data completely unnecessarily we send in our images. If you haven't looked at this area, you would be shocked. These two images are shown at almost exactly their native resolution. I sized them down just a little bit to fit nicely on the slide, but they're almost they're native resolution and I'll go ahead and show you that the first one, if you can't read that, that's 102k for that JPEG. Then it was run through a very simple JPEG image optimizer stripped out a bunch of excessive header data, brought down the quality level down to like 30% or something like that. I forget exactly what the settings were. But anybody want to take a guess what the image size of the file on the right is? Twenty five K. Thirty. Between 25 and 30 K. First off, does anybody notice any appreciable difference in the quality between the two images? Would you be embarrassed to have the one on the right on your site because oh my goodness I can't believe I have a lower quality image on my site. If they were blowing it up, maybe but---. If they were blowing it up. Twenty K, a fifth the size, less than a fifth the size. All they did didn't change anything about the dimensions, obviously, appreciably didn't change anything about the visual depiction of this image, but reduced 82 K off the size of what's getting sent. That's the kind of thing that we're talking about. You can run an image optimizer script and there are different ones for different file types, but you can run this automated as part of your build process and never think about this again and drastically reduce the amount of stuff that you're sending down the wire. Very straightforward stuff. I've linked here to several of them. There's an online one called Smushit by the guys at Yahoo!, Nick Lasacis at Yahoo! did that one. It's kind of an online tool that you can use. There's downloadable and command line versions of a lot of these tools. Imageoptimizer. net is another one and then pngcrush is specifically a tool that you run on your system, you can download it and it's a CLI tool that optimizes your png files. You would be surprised. Most people think that png files are highly optimized already, but there's tons and tons of ways, it's fascinating if you look at that format, by the way, there's all these different ways that they can use different algorithms to calculate rows of image data and most of the image editor programs don't take advantage of any of that stuff, but an optimizer can come along and say I can do the same row of data in a third of the amount of bytes by using xyz algorithm or whatever. They do all these crazy internal things inside the image, changes the image data not at all. Literally, 0 change to the image data. This is lossless compression that's happening on these images and the JPEG one it obviously did technically reduce some data, but not by any amount that we can see. But in png, you know, it's lossless compression that's happening. There was a question here. Yeah. There used to be a practice of puzzling pieces, you know, downloading multiple pieces of one image and then---I never understood how that was an advantage if your---because for one you're more requests, so why was that even a popular practice---? Yeah, so one of the articles I linked to earlier actually talked about that, one of the YSlow articles talked about that specifically. There was a practice for a while about saying well what we have is we need things to seem like they're loading quicker. So what we'll do is split up a bunch, you know, a big image into a bunch of tiles and then we'll download these things and that's one of the main uses for table layout for a while is, and I did this too, you know, you'd have a big image heavy site that, you know, had a big heaver and you'd slice up your thing into those images. Part of the reason that people did that is because they wanted for the site to appear to be loading quicker because if these two images in theory can load in parallel, then in theory, you know, one will start to render and then another one, so at least we're showing them something, right? That was the idea. It was a terrible idea. It was a massive explosion in the number of HTTP requests, a massive explosion in memory usage, but back then none of us paid any attention to that stuff. Only now are we paying attention to it and realizing, wow, that was a bad idea. It's a good thing we did away with tables, so we don't do that anymore, but---. What about interlacing the image? I haven't done a lot of studies specifically on that. A lot of these optimizers just automatically figure that stuff out, and so I don't even worry too much about those kinds of details, but I haven't paid much attention to how---I know that that's like intersplicing the rows, so it can look like it's loading progressively fast or whatever. I don't know that that actually changes the size, but it might change the size of the file or something. Good questions. You get a book. Ah, I was going to ask you that too. Can you pass that book over to him? Thanks. Alright, image spriting, I talked about this earlier. There are a bunch of tools that will automatically do this. Some of them are pretty incredible. If we have some time, maybe over the lunch break or something, we can open up these and play around with them. In the interest of time, I'll just keep going, but jot down some of these URLs. Spriteme. org is a really cool site. Literally, like you hand the site a URL that's just it just works the way however it does with all its images in a CSS or whatever, it parses your CSS, looks at all of your images, figures out which one should go into a sprite, creates the sprite, and creates the new CSS for you with all the background positioning, and then just dumps it back to you in a zip file. It's unbelievable, like you literally don't even have to think about this stuff. It'll just sprite for you and then it will show you like the savings in terms of file size and download and all of that stuff. So if you're not taking advantage of those types of tools and one of the reasons why you're not doing spriting is because you're not---you don't want to do this stuff, it sounds too hard, you have no excuse, these tools make it stupid easy. Yeah. You say they work by actually changing your CSS for you. Do they work with the CSS new language like Sass and Compass? I'm not aware, that's a great question, I'm not aware of any sites that do the optimization and rewrite it out as far as LESS or Sass. What they do is take the compiled version, the live version of your site, and say how did you deliver it to me and how would I have modified it, so that may be a little bit difficult to sort of reverse that back into your LESS or Sass or Compass or whatever, but it sounds like a really good useful tool that somebody should build if it's not there. Maybe that's a future open source project for you to build. Alright, so next is the concept of inlining images. Now again, there's a balance here. Remember I said before those Latin phrases, you want to make sure reader beware here. There's a balance here because when you an inline an image, you do in fact reduce the amount of external requests that are happening, and in some cases, that's a big win for you, but you've also now tied the cache ability of that image to the cache ability of your CSS file. In some cases, that makes sense, in some cases, that's a terrible idea. In some cases, your CSS is going to change far more frequently than your images are and you're going to cause a lot of extra download, so do be careful about that technique. But inlining images, again, there's some tools that do this automatically. I've linked to several of these here that will automatically, you know, grab this. The first two tools create the data URIs for you. The last tool takes your site and automatically inlines all your images into your CSS and creates the CSS for you, so if you don't want to do any of the work of even figuring out the CSS. But that's an example of what the markup looks like. You have a data URI, data:, and then a mine type, and then base 64, and then just thousands of columns of crap. That's your base 64 encoded image data and all modern browsers are capable of showing these way back even pre-dating IE6, so inline imaging is perfectly safe and even on some of the older IE6, and 5, and 4 there's MHTML or something like that. There's a standard that works even in the older IEs, but nobody cares about that.

Minification
We talked earlier about concatenation, let's also talk specifically about minification. There are minifiers for JavaScript. This site right here compressorrater, I've used this site for years. If you go to this site, and I just have it bookmarked because I could never remember that weird URL, I never understood why he did that, but if you go to that site, you paste in a JavaScript File, it will run your script through four or five different script minifiers like the UE compressor, and the Dojo ShrinkSafe compressor, and several other ones, it'll run it through with all the different combinations that those tools take and with Gzip and without Gzip and with compression, no compression, all that, and give you a table listing and order it by which one gave you the best results. It's kind of cool because sometimes UE compressor gets it a little bit better and sometimes ShrinkSafe gets it a little bit better or whatever. I found that most of the time the Dean Edwards compressor did the best for most of my scripts, but that is without the base 62 encoding. If you know what I'm talking about, great, if you don't, don't worry about it, but compressorrater will show you with and without base 62, always avoid the base 62. But anyway, I used that for years and then just recently I switched over to using Uglify. Uglify. js is a project that's been out now for a couple of years. It's doing consistently better on all of my scripts than any of those other tools were, so it's now surpassed the standard scripts that's why it's my go-to tool. It also has a Node. js module, which I work a lot in the Node. js world, so I use it to sort inline now. But either one of those gives you great results. You end up minifying your scripts by a ton. Uglify. js does some really advanced stuff. It actually rewrites your code. It doesn't just shrink variable names, but it'll like replace semicolons with commas and do some crazy stuff, but I've never had a problem with it, so it's pretty good. I know other people talk about the Google Closure Compiler. I tried Google Closure Compiler when it first came out and of the three scripts that I tried it on, two of them broke with Google Closure Compiler and I never looked back at that one. I'm sure they've fixed bugs by then, so I just don't endorse it because I don't use it, but it's probably really good, probably similar to Uglify. js. Then down here at the bottom, these bottom links, this bottom link is a CSS compressor, that's also really important. It has a variety of tools that you can use for CSS compression. In CSS, you can't shrink down like property names like you can in JavaScript, but you can at least strip out comments in whitespace, which makes a big difference. And finally, at the bottom, these two links that I've highlighted here in this golden rod, I think is the name of the color, jsbeautifier. org and cssbeautify. org, they actually go the reverse, they can't fully go the reverse, but they take minified output and they put back in all the nice pretty spacing and indentation. So if you have a minified piece of JavaScript, and by the way, some of the developer tools like in Chrome, they do this too, but if you have a piece of minified JavaScript and you can't make heads or tails of it, but you need to understand, look at it, or whatever, I leave jsbeautifier open as an open tab in browser, I use it all the time to look at my minified code in a nicer, friendlier format, at least with spacing. I'm seeing q and rx as variable names, but at least I can see the structure of the program better than when it's all on one line. So those are some tools that'll help you out in that respect. Concatenation. This tool was pretty cool, but it hurt my feelings because I gave it my blog, which I thought I had, at least you know, I don't do much on my blog these days because I'm busy writing code, back in the day, I fancied myself as pretty good at optimizing stuff in WordPress, it's a WordPress blog, and now it appears that the updates to WordPress have completely broken everything I did because it's telling me that I could save 71% on the size of my bandwidth for my site. This is a very cool site. Again, you just drop in your URL and it looks at all your stuff and figures out if you're making big mistakes. So if you just want a broad 20, 000 ft view of what should I look at, this is a good place to start. It's called zBugs. com. And by the way, they have those download links, so they don't just tell you what's wrong, they give you the optimized JavaScript, the optimized CSS if you want to download that, at least start and then feed it into this gentleman's wonderful tool that he's going to build for us, create LESS out of it. Alright, as if I haven't given you enough links, I do want for you all to go to this page, obviously you're not going to type in this link, so search for Robert Nyman concatenating, minifying in Google and find this article. He wrote it back in 2010. It is still an awesome article. Some links are dead, but this is like probably a 100 or more links in this article that he put to various tools for all these different things that we've been talking about. So rather than me just dumping all those links in here and pointing to his article, again beware some of the links are dead, but there's some awesome stuff there. There's stuff where there's online tools that sort of help you nice, quick and easy identifying problems or fixing quick fix problems, and then there's tools that you can integrate into your build process to fix stuff as you go. So look up again Robert Nyman, N-Y-M-A-N. He's a brilliant guy, by the way. He works for Mozilla. And take a look at that blog. He talks about concatenating, minifying tools, different development environments. He's got tools for PHP and Java and all the back-ends in there so.

Exercise 2
Okay. So as I mentioned, we do have an exercise. I don't necessarily think that exercise 2 will take you 20 minutes, but I did want to give plenty of a block of time because you'll be doing some stuff with some tools and this may or may not be that easy for you. The exercise builds upon exercise one in that after you finish, first, I had mentioned you get a benchmark, so the exercise tells you first take a benchmark using YSlow, and then after you've gotten your benchmark using YSlow, try to fix some of the problems. Some of the problems, you're probably smart enough to just straight up look at the markup, you say man, he did some really stupid stuff with extra script tags and link tags. This was that late at night a few nights ago, I was trying to create this thing and I kept trying to break stuff, so that I could get more rules broken in the YSlow plugin, so you'll see some crazy stuff in there. I don't necessarily expect for you to fix all of those problems, it's not a coding exercise per say, but I do want you to get familiar with using the YSlow plugin to diagnose problems. If it tells you there's too many script files, go and look at how many script files I have, and consider what your options are in terms of concatenation and minification. If it tells you that you are repeating your link tags too many times, look in the markup and remove the duplication. If it tells you they're in the wrong place, you know, fix that. So take a look at those tools. We'll break here for a little bit and there is a natural break after this exercise as well. We'll probably---I'm going to look for some guidance on whether or not lunch is going to be ready, but I'm guessing that we'll fold in that break directly into our lunch break, otherwise, we'll come back and talk about a few things before that. Otherwise, we're going to work on this stuff. If you have questions, which many of you may have questions how to use some of these tools or whatever, please just grab me and we'll go for the exercise. I'm surprised I'm not getting any questions in the exercises so far. Either that means you're not doing the exercises or it means I'm so good at writing instructions that it's fail proof. Which one is it? The second. Okay. I hope so. I get really curious about the intent of some of it like when you find the duplicate style sheets, I wonder well do I preserve the cascade order or do I assume that the subsequent ones are wrong and should be deleted. Am I trying to preserve the actual look of the current page? In almost all cases, there are a few minor exceptions, if you see two link tags to the same style sheet, removing one of them should not change anything about the behavior of the page. There's a couple of very minor exceptions, but for the most part, things like if it's appending content and stuff, but for the most part, removing duplicate references should be okay. However, when you combine files, make sure you keep order, and especially if you're moving stuff from inline style tags into a style sheet, make sure you preserve the intended order. On duplicates, shouldn't we always remove the first one because the second one overwrites something that's in this, so the third one would overwrite something in the second one in the previous one. My guess---. Yeah, he's got a good point that in general, unless you're intending to move stuff around, which is the spirit of this exercise also, move stuff around, but in general, you can construct a page in such a way that you have a style tag somewhere in the body and then you have a link tag right after it, and so if you're going to remove duplicates, you should remove the earlier ones rather than the later ones, so leave the last version of one, but then if you're going to go ahead and reorder stuff, it probably doesn't matter which one you're deleting, but yes, that's a very good point that you do need to make sure you keep it in the relative order to other style sheets or style blocks. I'd give you another book, but you already got one. Okay, here's how I'm going to give away the third book right before our lunch. Who got the highest YSlow score on the exercise so far? What's your numerical score so far? I got 85. Eighty-five, okay. Ninety-two. Huh? Ninety-two is pretty high. Anybody got higher than 92 yet? Now I'm getting you interested in the exercise. People are furiously like, oh let me fix this, I need a book. I should have brought one book for each exercise to keep people motivated. I'm going to continue to let that upload in the background, so that I'm not standing too much in your way of lunch. I just want to go over a couple of things. Hopefully, you read through those hints. I realize especially with the lower internet, you may not have been able to avail yourself of all of those tools. This is why I encourage you to take these exercises home and go back through them when you've got the opportunity to test out more of those tools and see if you can get even more help with this stuff. So that's one important thing. Another thing, I mention here, but I'm guessing that some of you might have skipped over it, make sure that when you make changes like this, just like you would in development, make sure that you're not making breaking changes because a faster broken site is not necessarily better than a slower correct site. Now this site is not correct in the sense that I intentionally broke it, but you want to make sure that you're not just saying, oh, I moved stuff around and I got a higher score, but now the paragraph is green when it was supposed to be blue or whatever. I see a few nodding heads, like oops. I mention here in point number 7 don't ignore a rule grade of A, there's a couple of them on there, I think, where it was reporting A on a rule score, even though it had items below it for you to address. Maybe that's a less important thing for you to do to come back to you know later after you've addressed your F score, but it does mean you should be aware of all the problems and not just ignore them. I'll open up the file and just point out a couple of things just to make sure that everybody was catching them. First of all, obviously, there was a bunch of images that could have taken advantage of spriting like those images that were three of a kind and then four of a kind, those could have taken advantage of spriting, so you should be doing that. There were several CSS files and I had a whole bunch of junk stuff that I'd dumped in just so we had bigger extra files, so a lot of that stuff is totally unnecessary. In the real world, you would remove unnecessary stuff, but I'm asking you to pretend like that stuff's necessary. Something that I'm not sure if any of you caught, the jQuery that I included here is the full unminified with all the comments version of it, so it's massively bigger than it needs to be. Did anybody catch that? Okay. Could of easily done with some minification, or in this case, you could have just gone and redownloaded the proper version from jQuery and you'd be surprised how many times people drop in the development version into their production code and never think about, never give it a second glance and have no idea just how gigantic this file actually is. To be fair, shouldn't you just point to the CDN version anyway? In general, yes. Very true. Okay. Point out a couple other things here. Did any of you catch that some of these JPEGs were rather large? I intentionally made them rather large. That one's 209 K and then this one is 359 K. There's a whole bunch of extra stuff in those that can be stripped out by those image optimizers. Let's take a look at the index. html file. You see off the top of the bat you see some script tags at the top, you see a style block, and some more script tags. In general, they tell us to move the script tags to the bottom, so preserving the order we would want to collect all the script tags that we find and move those script tags to the bottom. Then we're going to want to start concatenating those files together. In the case of jQuery, I probably would not include jQuery together with the rest of the files, instead, as mentioned, I would reference a CDN location. Markup wasn't the main point of this particular exercise. Some of you may have had qualms about my markup and that's okay if you did, but there's some link tags interspersed throughout here. In general, it's probably a bad idea to design sites where you're relying upon the presence of a style tag at some given point of your code, but that's common in CMSs for you to drop stuff in sort of conditionally like that. Here in the case of those two images, I was using standard image with source attributes. There was a rule that I'm not sure how many of you caught, but those images were actually of much bigger dimension and I'm them with the attribute. Did any of you catch that on those YSlow rules don't scale down images? Okay, good. Done a whole bunch of extra link tags. I just was dropping stuff in, trying to get one of those scores to go on up. For some reason it never would ping for me. Hopefully, some of you saw some of the scores go up, but a couple of them they were just persistently not complaining, even though I know they should. Yeah. Can you speak a little bit to like the retina display and how that maybe should _____ image sizing and loading the two different versions of the image. Yeah, it's a great question. The question is about responsive website design with retina displays. How do we respond by serving higher resolution images? I'm not an expert on this field. I don't do a lot of traditional web front-end design work where I would be dealing with that kind of thing, so I'm not going to comment, I'm not an expert in that realm. I have studied the way those techniques work and I think that the new markup elements that they're talking about, the picture element or whatever are the solution to it. There was recent article that was going around that was talking about this technique of taking an image and blowing it up to 5 or 10 times its normal dimension size and then scaling it back down, and in some cases, they were able to get a smaller image, even though it was much bigger because they would drop the quality all the way down to 0, but when it was scaled way, way back down, you couldn't tell the differences, and so it's kind of this crazy technique like look you get responsive images with lower file sizes, awesome, until somebody realized that it takes up 4 times as much memory, especially on mobile devices. That's a terrible idea, don't do that. Well a lot of people are doing maybe only 2, 1. 5 or 2 times the dimensions and then having a compression rate about 70%, so a higher compression, bigger file dimension size, but the file size ends up being comparable or slightly smaller and the memory consumption isn't absurd because you don't have 5 times the dimensions, you only have 1. 5 or 2x. Two X is still a four X usage because it's tiny. Yeah, so 1. 5 is what I guess quite a few people I've been talking to have been using. I would recommend against it, but obviously some people are finding value in that technique, but in general, I would say this is a little bit more of a design problem if your site simply can't function well because you're not serving a high definition image, then serve a high definition image, and just pay the bandwidth. But I would say in almost all cases, a high definition version of an image is a far less importance than rounded corners and gradients and it should be treated as the very lowest and last step of a progressive enhancement technique. It should be loaded dynamically later, rather than stuffing that into the normal pipeline or something like that, so I would in almost all cases I would serve a lower definition version of that image and swap it out later with a dynamic loading technique or something like that. I would not force a gigantically bigger file down, either file size or memory usage, I wouldn't do that just because people have these high definition screens, so it's just kind of---it's a balance game like we've talked about. That was a good question. Alright, I think I've mentioned everything that I wanted to mention. Did anybody have any other thoughts or questions, things that surprised them about exercise 2 with testing your YSlow? It was really weird to see a style sheet at the bottom and not think that like those inline styles weren't supposed to be the right style. Like because you always think of like inline if you see inline style like that would be the last thing that's applied, but when those ones were at the bottom, those were really confusing at first. Okay. Alright, I did that on purpose, but I'm glad you were able to ferret through that. The reason I did that is because there are a lot of sites that are put together with CMSs where the final markup that goes out is a Frankenstein collection of a whole bunch of snippets from different places and this sort of thing is very common, you drop in a plugin for some you know third-party widget button and it just includes its little link tag along with its markup or whatever. It's very common to see this sort of crazy markup and when you come along and address it, the eventual address that you're going to have to do is fix the systemic problem, which is that your CMS is not post processing your markup. But just in terms of improving stuff to see where is my 3%, you're going to go through that code and have to make decisions like this, like you know do I need to care about that style tag or is it no longer relevant or whatever. Alright. We will without any further ado, we're going to break for lunch right now. We'll come back and spend just a few minutes after the break talking real quickly through some architecture communication stuff. We won't belabor that point too much and we'll try to jump into front-end after the lunch, so yep, so that's it. Let's do lunch.

The Middle-End: Architecture & Communication
Understanding the Middle-end
Okay. We'll jump back in. Here after this lunch break, I mentioned before that we're going to go back and quickly review those modules that we were kind of skipping over, so I will cover some of that stuff fairly quickly, but if you have questions, please feel free, you know I'm not saying we can't talk about that stuff, and we can certainly come back at the end of the workshop if anybody wants to dig back into that. It may be just boring stuff that we should have skipped over anyway. But before we go back into that, I just want to remind you guys of this quote because it really is the frame by which this whole workshop is built. So keep that in mind that we're really going for trying to up the level of maturity in our understanding and experience of how to make these difficult decisions. There are no one-size-fits-all type choices in this world. Okay, so we'll jump into some discussion of architecture for a moment. How many of you have done any sort of back-end architecture of your web applications in any language or platform? Have any of you worked in that area? Okay. About half of you have done so. How many of you were deep back in like databases and stuff like that? Okay. So there's a pretty fair mix. And then how many of you spend most of your time just purely in the client, just purely like front side stuff? Okay. Again, about the same half of you. The other half of you is still asleep after lunch, that's cool. Alright, so I'm going to share with you some ideas that I have about architecture that go back to that concept of the middle-end. I'm not talking about what happens in the back-end, even though this kind of stuff often will happen on the server, I'm not talking about what happens on the back-end, and so to describe what I mean by that, let me just say that when we talk about that middle-end, what we're talking about is a layer that exists whether you call it that or not and whether it's well-formed or not, there is a layer that exists between what's happening purely in the front-end, purely in your browser, and what's happening purely in your back-end, things like connections to databases, and all of that stuff. So the way I choose to look at the world is a very clear delineation between front-end, middle-end, and back-end, and that's not just a word game, that helps us to categorize things in a useful place. And so when I talk about I'm not dealing with the back-end, the reason why the workshop is only dealing with the middle-end and the front-end is because the back-end actually doesn't matter that much. We saw from the performance golden rule that it's only 10 or 20% of what's going on anyway and in reality, I would never be able to convince you, you should choose Rails, you should choose PHP, you should choose Java, those decisions are much bigger and more complex than we can deal with or often any sort of influence of them; however, I don't think that means we have to relegate the decisions about the middle-end to whatever our back-end developers have chosen. So I suggest this sort of alternate architecture called middle-end architecture. There is a domain called middleend. com, it's got the two Es, so middleend. com. I think that was in one of my slides, maybe not, but middleend. com, it redirects to a bunch of blog posts that I've written on this topic, so if you're more interested in sort of my crazy view on this whole web architecture thing, essentially the idea that I advance is that your back-end can be whatever you want it to be. It can be. NET, Rails, Java, Node. js, it doesn't matter, it can be whatever you want it to be. But all of this stuff that we call middle-end, things like templating and URL routing, data validation, data formatting, headers, caching, all of those things are things that are more associated with your front-end than with your back-end, so they need to be pulled up out of the guts of your back-end and put into a well-defined middle-end layer. And of all the developers on the team that could handle your middle-end, I think your front-end developers, those with front-end skillsets, hopefully most of you, I think you're more well-suited for managing the middle-end than are your back-end developers. So the architecture that I suggest here is that your back-end you can start to think of as a black box. It has an API on it and inside of that black box, it manages state. It is a StateMachine. It manages state through sessions, it manages state through saving stuff to the database, it manages state through however you want to do it and it can use whatever kind of architecture you feel like using. I don't like MVC and there's a bunch of reasons why I don't like MVC, mostly because it turns out in practice to not work that well, but if you like MVC, go for it, use MVC in your back-end. But what you expose on top of your back-end is a stateful API that responds with JSON data and you have a contract between your back-end and the lower half of your middle-end, if you will, the place where the two connect, you have a contract about what sorts of data get exchanged. So the source of interactions that would happen there is the middle-end would be fielding some request or trying to respond to something to the user and he would say hey back-end, whatever state we're in, here's what URL somebody clicked on, you tell me what to do. Or maybe even more generically, the middle-end would simply say hey back-end, I know we're in state ABC, the user's now requested state DEF, you handle that request and the back-end would be responsible for deciding if that was a valid state change, if there's an error that needs to be thrown because they haven't provided enough data, but the primary job of that back-end would be to abstract away all of that decision-making, all of the true back-end business logic about state management, about you know data management, and storage, and all of that stuff to abstract all of that away and have a very simple contract that he speaks to the middle-end with. If you're capable of turning your back-end into essentially a headless, stateful API like that, then you can work in this world that I'm talking about with a middle-end because it frees you up from having to care whatsoever in your back-end about how things are going to be presented. What you present is data and somebody else takes care of figuring out how that data needs to get transformed into something that users can see. That transformation may be as simple as exposing a web API because your service has a web API and that's okay, but that API is a middle-end API, not a back-end API. That's one of the conflation points that I think is troublesome is when people start creating back-end APIs and then they start trying to create their front-end to use those, but then they have to have some private APIs because there's some things they need to do that others can't and all that kind of craziness. So that all goes away when you have a back-end API that's very clearly defined as simply state change management with data back and forth and there's a well-defined contract that happens between the code and the middle-end that's accepting and interchanging that data and the back-end. What that means is that the back-end developers can do whatever they want to do with that data contract and the front-end developers don't have to care at all. What that also means is that front-end developers are completely insulated from any changes to the way the data works in the back-end. When you change from. NET to Ruby, it should not, in theory, affect your front-end at all because all that needs to change is the adapting that's happening in the middle-end. Again, I understand that this a highly complex topic. I just wanted to sort of lay that groundwork for what I wanted to talk about here. The middle-end is a buffer, if you will, between the front-end and the back-end and they never directly talk, they talk through contracts and it keeps the front-end side of the world working efficiently, and the back-end side of the world working efficiently, and it provides the grease for those two to happen at the same time. So when you do that, one of the offshoots is that you've now exposed all of that stuff that used to be really, really difficult for you to do, you've exposed it into a well-defined layer called the middle-end. I don't know if any of you have ever tried to go into the guts of your particular application server and manage how it sends out different headers depending upon different file types and things like that, but sometimes, oftentimes, that can be quite a nightmare and that's a symptom of what I'm talking about with application architecture that's not well architected, you don't have control over that and as a front-end developer, I think you should have control over that because otherwise how are you going to do any of the things that we've talked about today. In an exercise, you know, it's fun to play around with stuff, but in the real world, in your real application, if you don't have control over this stuff, how will you make it better? Are you going to rely on trying to convince a back-end developer that they should change some configurations for you? How about rethinking that architecture and it's sort of a freeing thing because back-end developers no longer have to care about the crappy world of cross the main JavaScript and markup and all of that other junk, they get to be data nerds, which is generally what they're good at. And front-end developers don't have to care. I worked at a job one time that I walked in and they had hired me as a front-end developer to do HTML, and CSS, and primarily to do JavaScript, and the very first day I walked in, this is literally a true story, I walked in and said okay, to build this widget thing, you know the first thing I'm going to need is to be able to generate an HTML Page, put some CSS and script in it, so you know where do I go? Is there like a public HTML folder, you know, where am I going to stick my file? And the guy looked at me and said well, actually it's not quite so easy and they were a Grails shop, which is Groovy on Rails, kind of a Java bastardization, I guess. I didn't work there long, so I don't care. But he literally handed me a 400-page Groovy book and he said you're going to need to read a few chapters of this book to figure out how to create an HTML Page. That is the epitome of what I think is wrong with our architecture and it plays out in various ways in different companies, but the fact that a front-end developer cannot just create a file and serve it up to the web means the architecture is more complicated than it should be, plain and simple. So with that as the backdrop, let's just briefly talk about some architectural ideas.

Introduction to Architecture
First of all, I mentioned before, you remember this graph that we showed earlier, now I've zoomed in on the HTML and you see that 31 to 36, if we were taking this graph at face value we'd say well markup really hasn't grown that much, markup's not really that much of a problem, but what I want you to look at is the amount because what we're going to talk about now is the concept of single page architecture, which I think is a very powerful architecture, but it can be done wrongly cross-reference what Twitter has gone through the last several years, they can't seem to figure it out, but it's a very powerful technique and one of the biggest reasons why it's a powerful technique talking at a web performance meet up, would you like to load 36 K 15 times? Or would you like to load it once? Most of you would say once, I hope. Now, sometimes the markup changes you know dynamically and I understand that's a drastically simplified view of the world, but wouldn't it be nice if you only ever had to load your markup once. That's the ideal that we want to be shooting for. So that's really the reason why I get excited about single page apps. It's not even as much a user experience thing like oh the browser doesn't have to refresh, that's cool. But for me, I get excited about it from the performance perspective because there's far less transferring over the wire when done correctly. So what we can do inside of our application architecture is say we serve up an initial request, this is sort of a hybrid view, it's not all client-side, not all server-side, it's somewhere in the middle, sort of the Goldilocks if you will, somewhere in the middle is the right mix for our application. We can serve up some stuff that we built on the server, but then once we have a fully-built app, fully-initialized app inside of the browser, now we don't have to re-roundtrip to the browser and ask the browser to recreate a whole new page for us when all that changed is just a simple snippet. Think about the Facebook comment mechanism for a moment because that was going to be the example we were going to go through here was creating sort of a commenting mechanism, that's a good example of this use case that I'm talking about because the markup for creating a comment, when you think about it conceptually, the markup for creating comment is fairly simple and straightforward and when you have a list of comments, you've got a bunch of repetition of that same markup. So you have a choice the way you architect things to either make a roundtrip to the server and ask for the whole comment list, or worse, the whole page to be re-rendered or you can choose to say, I have the data that needs to go into the markup and I have the markup, so I just have a very simple templating task to regenerate just a little item and add it to my DOM. That's a slightly more evolved or a slightly more structured way of doing what we've been doing for quite a while. DHTML from 15 years ago taught us that we could change the page on the fly. Well let's be a little bit more structured and a little bit more rational about it and let's say we compose our page as a variety of essentially template snippets, so when our page changes, when there's some data that changes, generally speaking, there's just a single snippet that needs to be updated. So if we have one of those JavaScript-based templating engines of which there are many to choose from, when you have a JavaScript templating engine, you can invoke that engine and say here's some data, give me the new markup snippet or you can write a whole bunch of complex JavaScript to go and stick your data into the existing places and update it like live data bindings do, but I'm going for the more simple approach. I just replace some markup with a new element. It's pretty straightforward. But what I really didn't want to do is I really didn't want to make a roundtrip to the server and ask for that whole new page again considering the new data. So when you think about what Facebook does, when you type in a comment, Facebook immediately renders that comment, or in most cases they do, there are a few cases I've found where they're not actually doing this, but in most cases, they immediately render that comment into the comment list, and then in the background, they've sent that data off to the server, and they're expecting to hear back a response that the server said yes, I got the data, everything's cool, there's no problems with that data, and assuming that things work well and they get an okay back, everything's cool because I've already got it rendered the way it would have been had I done a full page refresh. I've already got it rendered, I don't need to change anything. Only in those exception cases, which hopefully are fewer than not, only in the exceptions cases do I need to undo what I just did. So an architectural approach that says performance first or performance by default says let's do the least amount of work that's necessary to get something visible to the screen and we'll only do more work when forced to by an exception case. There are a lot of, like I mentioned I kind of picked on Twitter, you know, there's a lot of this back and forth about, you know, they had this metric they called time to tweet, like how long did it take before somebody could see the first tweet, and so they swung the pendulum all one way and they said let's do all JavaScript-based rendering, send nothing out from the server, and now they've swung the pendulum all the way back and now they're doing everything on the server with roundtrips for the page. And Facebook has had similar crazy swings of their technology. I think the hybrid approach is a better approach. I think it allows the server to make the decisions about where stuff gets rendered and what markup gets rendered based upon performance conditions. For instance, you could have an application that conditionally decides where to template stuff based upon the performance characteristics of the bandwidth and the device in play. So on a smaller and less-capable device, you may offload more of the templating work to the server and in a more capable device like a 16 GB RAM Mac device you're going to force more of that work down the wire and say device take care of it. That's the type of flexibility for performance optimization that most architectures we currently deal with don't let us do and my view of the world is, let's change the way we think about architecture which gives us all these extra options.

Data Validation
So I've been, I should've advanced this, but I've been talking about the concept of single-page apps. I'm highly in favor of these things, but I'm not in favor of all client-side or all server, it's got to be somewhere in the middle. Data validation is another example of the argument for middle-end architecture. How many of you work in a non-JavaScript back-end? PHP, Java,. NET, whatever, almost all of you. So in a non-JavaScript back-end, you write some data validation rules, stateless data validation rules like is the email address well-formed, that crazy long RFC RegEx, right, that everybody uses or you know did they type in enough characters for a first and last name, did they make sure to include their middle initial, those kinds of things. You write that in Ruby or Java or whatever and then you turn around and rewrite it in JavaScript because for performance reasons, we want to make sure that we can respond to the user that perceptive performance, we want to respond to the users as quickly as possible. We don't want to necessarily wait for a roundtrip to the server to hear back the answer that says the email address is not well-formed. Why don't we do that like while they're typing? That'd be cool, right? So we end up writing our data validation rules, our stateless data validation rules twice or you get really cute, you write it in some intermediary DSL and you compile it to each language or some silly thing like that, but the bottom line is that code ends up existing in two different places. One of the maxims of computer science back from my you know days in the Computer Science program, one of the maxims of computer science is that any time there's more than one copy of something, one copy is always wrong. So anytime there's more than one copy of your data validation rules, there's a really good chance that they're not in sync. There's a really good chance that when you change it in once place, it's not being validated in the other place or vice versa. What if we wrote our data validation rules once in one language that can run in both places? We use the exact same code, not a duplicate copy of the code, not a pre-compiled, cross-compiled version of the code, but the exact same code in both places. A middle-end that is realized using server-side JavaScript gives us that capability. Now this is when I was talking about our application before, I didn't necessarily say that the middle-end had to be JavaScript. There's an implication there that I think it's a really good choice. Data validation is one of those further arguments for why using server-side JavaScript as your middle-end layer makes a lot of sense. It's really, really good at high-performance, you know, fielding requests and stuff like that. It's probably better at it than your. NET or Java is, maybe, but it's probably better at that. So there's a lot of reasons there, but there's also the reason that if we use the native language that can run in both places, which is JavaScript, we get to do code reuse in a really good and powerful and useful way. So I advocate very strongly that instead of having no validation on the server, I mean on the client, instead of having no validation on the server and just trusting the client, which would be crazy, and instead of writing it twice, which ends up wasting our time and taking our focus off the stuff we should be focusing on like optimizing performance, the compromise is use JavaScript on the server and write your data validation rules once. Now remember, I said stateless data validation rules. Stateful data validation rules, like is it a unique email address, that cannot be done in the client-side because your database doesn't live in the client-side. It requires a roundtrip and it requires more than just your middle-end because your middle-end is stateless. So you're going to have to talk eventually down the stack into the back-end, but you're going to ask the back-end black box, that abstracted API that just deals with state management, hey black box, I've got an email, is that cool? And he's going to say yes or no. And then back up the line, you can respond very quickly to the user. You didn't have to get into any of this interaction where the back-end had to re-render the page or any of that other stuff. You just simply asked a question about data and the back-end did what it's good at. It asked a question about data and state and it gave you a good answer. That's the simplified view of the world that I really wish we could push toward and I know it's really hard in practice, but I think if you start to consider the value of inserting a JavaScript-based on your server middle-end layer and asking your back-end to stop mucking so much with what's going on in the front-end, I think the world would get a lot simpler and I think you'll have a better time of implementing these performance rules that we've talked about. Okay, real briefly we'll just cover this. You guys know---I'm sorry. Yeah. I was wondering if you had any opinions on the Microsoft MVC data annotation-based validation. Not familiar with it. How does it work? Basically, you can markup your model and C-chart plan and then you just kind of allow the framework to use jQuery validations to validate it for you. You don't actually need to write any additional code, I mean you might if you add some custom stuff, but I was just wondering if you were---. So it's code generation essentially? I believe that's how it works, yeah. Not a fan of code generation, and you know, I could spend a whole day explaining all the intricate reasons for it, but I don't use Coffee script either. I write my own JavaScript. So the easiest and simplest answer is I don't want another tool to generate the code that I'm relying on to work properly. I want to write that code and I want to be able to optimize that code myself without having to fiddle around with some workflow that's always regenerating my JavaScript. So the middle layer for with the JavaScript, would that be---is that what Node. js is? So Node. js is a very popular server-side JavaScript environment. What I'm suggesting is certainly a far cry smaller and simpler than what Node. js was originally intended to do. It's an application platform just like Java. It was intended to take over the entire back-end, do everything, and it's really good at it and really, really performant. I'm suggesting a comprise approach instead of telling people go back to your boss on Monday morning and tell him you're going to rewrite the entire application in JavaScript, I'm suggesting instead go back to your boss Monday morning and say how about your back-end guys never have to deal with another line of cross-browser markup ever again. I think maybe those types of conversations have a little bit more of a chance of succeeding. So I'm suggesting that we insert a server-side JavaScript layer of which Node. js is a great choice and could serve the job well; it is not the only choice. If you're in a Java world, you're probably better off choosing something like Rhino because Rhino already talks really well to Java and it's getting better. And there are others that I heard rumor that the new Java 7 is coming out with a new set of server-side JavaScript offerings or whatever. So Node. js by no means is the only option, but it is a really good option and given the popularity of that and the momentum of that, it certainly would be a good place to start, even if all that was prototyping and then you decided to go with some other server-side JavaScript environment for real. But Node. js is a great place to start there. But if your back-end is already Java, Rhino would be a good---. Rhino's a great choice because it is written in Java and you can use native Java objects inside of JavaScript, not necessarily that this architecture I'm suggesting would benefit from that because I would de-emphasize that and more emphasize that sort of JSON you know detached API approach, but certainly if you're in the Java world, it makes a lot more sense to go with what's really natural, which would be Rhino. Yeah. Are there any JavaScript libraries that exist where it'll automatically can include that will automatically validate whether or not your email is correct just based on the HTML5 input of email or something? Hmm. I am sure there are some, but I don't have any that I can recall off detail. Maybe in a break, if you remind me, I'll try to do a Google search and see if I can turn some stuff up, but you do bring up a good point that the regular expression form of validation in markup is a really common thing that's happening in HTML5 and I'll just take a real quick diversion into that. I think that's a violation of separation of concerns to put your validation into your markup. I think that's a code thing. So I don't like the approach of marking up my markup with something that the browser will take and use that to decide stuff. I think that belongs in code. Okay. Great questions. Anybody else on architecture?

JSON, Ajax, & Web Sockets
JSON. The only thing I really want to mention specifically about JSON is I have seen from several companies and it seems like I'm picking on companies like Twitter a lot. I apologize if you guys are---, I like Twitter a lot too, but the world of JSON is not so clear cut in terms of what you send over the wire. I think we tend to have this perspective that well it's magical, right, JSON it solved all the problems that XML gave us and so now that I have a JSON transfer between browser and server, I don't ever have to worry about it again. I just stick all of my data on JSON and everything's cool. That's not necessarily true because the performance of your application is not just about what happens at page load, like we said, it's a user experience over time, and if you're AJAX requests are constantly going as quick as they should be because you're stuffing tons and tons of data into every JSON request, you are violating the same sorts of concerns that we spend so much time and effort worrying about with page load time making every single button that you click to respond to the user taking 900 milliseconds, closer to the upper limit of what they tolerate than to the lower limit of what they tolerate. So it's important to understand that JSON is not some magical black box that we just stuff stuff into and we don't worry about it, JSON the structure of what we transfer in our data is very much important and Twitter is one of those really bad offenders, in my opinion, because if you've ever taken a look at the response from a Twitter API request, I've sniffed on when I'm working on even on their WebClient and looking at what they're transferring back and forth and if you look at, for instance, just the basic timeline API that they have, I didn't put the code up here because you wouldn't have been able to understand all of it, but the way that they do things is they repeat data in every single entry. Like they repeat all these different pieces of data like if you've got a single tweet that was re-tweeted by somebody, in every one of those markings for where they've re-tweeted it, they repeat all those user data over, and over, and over again. Now I'm sure from an API consumer perspective having my data all in one location is awesome. I'm sure it makes it easier. I don't write JavaScript against their API, but I'm sure it makes it easier for people and I'm sure it's probably even easier on the back-end for them to not have to think about stuff, but their API requests, I did a test, this was a while ago, so I'm sure the data is out whack, but I did a test where I took one of their JSON structures from one of their timeline API response and I just simply went through and removed all the duplicate stuff, things that we know we can do if you have a reference to a user object and you know all the information about the user, now all you need is a user ID and you can reference it from a JavaScript object. I did stuff like that and I was able to reduce the size of their JSON response by 72%. Seventy two percent bloat just because maybe it's easier for an API user to use, maybe it's easier for their end API back-end developers to just throw out a bunch of duplicate data and not have to worry about it, but this is the sort of decision that falls exactly in line with what I was talking about that bad performance is systemic. When you aren't thinking about performance constantly, you end up making bad decisions at every layer of your stack and this is another one of those places where bad decisions happen. So think about the types of data that you're sending over the JSON wire, think about how slow that's making it, we talked about caching, AJAX, and JSON, things like that. That's really important. There's actually a, I should have put the link in, I didn't, but I'll try to tweet it out, there was an article recently that was talking about a group that went through, they were trying to transfer a bunch of data over JSON and they went way beyond what I'm talking about. They didn't just deduplicate data, they came up with their own sort of almost DSLish-type language inside of JSON where they were packing variables into ordered arrays and things like that, so they were removing all those references to property names and they got a 60% decrease on their JSON, which was already highly optimized and they were able to get a massive amount of data packed high, you know, tightly-packed into their JSON. So think about JSON as a tool that you have a wide variety of options over. It's not just your overly repetitive key value pair store that you think about conceptually. What goes over the wire needs to be as small as reasonable for you to do. Next, we'll talk just real briefly about AJAX and WebSockets. So for a decade or more, we've had the paradigm that the way we communicate between client and server without a page refresh is the AJAX request that's a full request response cycle at the HTTP layer. It opens up a new connection on the server and, therefore, it's taking up extra resources, it's taking up extra HTTP packets over the wire, and extra server resources, and extra connection resources in your browser, and on your limited mobile devices. So in all places of the stack, the AJAX request is not necessarily heavy, but heavier because it has to do that entire stack. By contrast, we have this new technology called WebSockets and you've probably heard about it, but I just want to explain a little bit about why WebSockets are important. WebSockets create one initial HTTP request response cycle. There's a full request response cycle with all the headers and it's fully safe to go through all the proxies. As soon as they have established a good, solid connection, a handshake between client and server, the client that decides he wants a socket connection sends a request over that open HTTP socket at this point, he sends a request and says I'd like to upgrade. We've been talking AJAX here for a couple of milliseconds, now I'd like to talk in the WebSocket protocol and the server says cool, I know about the WebSocket protocol, we agree on versions, they do some secure handshaking stuff, and magically, now what we have is a persistent socket between the two, which doesn't require any of that HTTP overhead to communicate. That's why I'm talking about it from a performance perspective. By contrast, there are 1684, I think, bytes that happen in an HTTP request header, those are overhead bytes, in addition to the data, 1684 bytes goes over the wire for the HTTP request response handshake. I think I have that number correct. By contrast, a WebSocket packet has 8 bytes of overhead for each packet. Massively smaller. So we do pay that penalty, if you will, for that initial request response handshake, but once we have that established socket, we have extremely lightweight communication between client and server and it stays around for us, so none of that overhead of the server having to open up a new connection for you and allocate a process and all of that other stuff, it's a persistent socket, so we know we can talk to it. In addition, it gives us two-way communication, which AJAX doesn't give us. There's ways for years that people came up with long-polling and these other things to sort of fake their two-way communication. The two-way communication is incredibly important in the new class of applications we're seeing today like chat applications and email applications with push notifications, gaming, all of those things, they're all taking advantage of the fact that when something happens on the server, we need to notify everybody who's listening. We need to push out a piece of information to the client and we don't want to wait for that client to ask for it, let's just push it now. So socket communication is often referred to as real time communication. It's important to understand that it's not truly real time, it's just much, much, much, much lower latency than a traditional AJAX request. As opposed to an AJAX request, which may have anywhere from a 100 to 250 milliseconds of roundtrip latency time, a traditional socket on a pretty decent connection can be in the tens of milliseconds for that whole roundtrip. That means you can get very, very close to real time, not exactly 0 millisecond, but very close and good enough for most of the things that we like to do like gaming. So hopefully that gives you some perspective on architecture and communication and why those things are useful. I was going to do, as I said before, I was going to do an exercise about this, I think there's some other things that we're going to cover later today that are more important, so unless any of you have more questions or you want to dive more deeply into it, we can certainly come back to it in a break or at the end of the day. This is a website that I've got up and I built this years ago and it's completely a demo. Do not actually use the site for what it seems. It's a URL shortener. Don't use it because I take it down most of the time. I put it back up, after having been down for six months, I put it back up just for the purposes of this class. But I wrote this several years ago. The code for it is open source and this was back when I was exploring the concept of middle-end architecture like I was just discussing where I had middle-end implemented in JavaScript, and in this case, the back-end was implemented in PHP and I actually at the time I used the server-side JavaScript that I used, I wrote a server-side JavaScript engine at about the same time that Node. js was being written by Ryan Dahl, I was writing one because I wanted to use JavaScript on the server and I was choosing to serve a different use case than he chose to serve. He went the asynchronous route, the highly-parallelized and highly-performant route and I went the synchronous route because I wanted to be able to easily use JavaScript like call out to JavaScript from inside of PHP or call out to JavaScript from inside of Java or something like that and do so in a synchronous way. Instead of dealing with all the overhead of creating an HTTP request and talking to a different socket like you would with Node, I wanted to have a synchronous per request JavaScript model, so I built one, and I called it BikeChain. I don't recommend you use BikeChain now. It's been rough, basically an abandoned project, I mean it worked as far as it worked, but I saw the signs of the times, everybody went with Node. js, so I stopped putting that much effort in. But I did build a full environment for that at the same time that Node. js was being built and shortie. me is built on that. So you can take advantage if you want to take a look, if you look at on my GitHub you can see the code. If you're interested at all in seeing how that JavaScript plays and some things, it's three years old, that's not necessarily the newest state of the art thinking, but there's some code out there that talks about this interplay that I was dealing with middle-end architecture, and you can see how I was approaching it back then.

The Front-End: Resource Loading
Preloading Images
Okay, so now let's jump into front-end. This second half of the workshop, if you will, where we're going to talk about front-end, we're going to go more in-depth into code, and I hope a little bit quicker. So, if some of you feel like we've been belaboring some points, I've been trying to lay some good foundation work, but we're going to go much quicker through some of this stuff, so we'll jump right into a discussion about resource loading. We have several options when we're talking about resource loading, and what I mean by resource loading is resources like JavaScript and CSS, to a lesser extent images. Image preloading was a big thing back in Macromedia's heyday. I don't know if you guys ever used Macromedia's auto-generated JavaScript, the MM_preloadImages or whatever. Back in the day that was state of the art. We don't do as much of that image preloading stuff, but in some cases you still do that, and we talked earlier about Retina images, and that might be one of those cases where you might preload a Retina image for use later or something like that. But the concept behind preloading is essentially taking advantage of the browser cache, or just taking advantage of the browser memory to say I know that I'm going to need a resource later. While I've got you on the line, web server, why don't you just go ahead and give me that resource now? And I'll cache it, and I'll keep it in memory, and I'll have it for when I actually do need it. That's essentially the communication that happens, and the server says okay, I didn't think you needed it, but that's cool, and he gives it to him. So, preloading is powerful, but you do need to understand the implications of preloading as they compare to what we've been talking about with performance because the spirit of preloading is to say when somebody gets later to that resource we can have it for them really quick, and that's a really good thing. When somebody asks for a new page and you've preloaded all the resources and that page pops up just like that, that's a great thing for performance, and the user experience, the perceived performance will be much higher. The problem is that preloading is often realized as stuffing a whole bunch of stuff down the pipe at initial page loading time. So what you're essentially saying is this future stuff that somebody might need is as important or more important than the stuff that's on the page that they're currently looking at, which, when you think about it like that, is sort of a bizarre perspective to take. It's sort of bizarre to say they're loading the login page, but I don't care as much about the performance of the login page as I care about the page after the login page, so I'll make the login page slower by stuffing a bunch of stuff down the wire right now. That's basically what happens with preloading. It's a powerful technique, and I'm not saying you don't want to use it, but you do need to do so in a very restrained way, and you need to think about am I preloading stuff at a critical moment of the stack, or am I preloading when things are more idle, which will bring us to a technique we'll talk about in just a second. But all preloading does not have to happen at the same time as your main requested resources are happening. You can more lazily preload stuff later. If that's a really important thing to make sure that next page comes up quick, that's fine. You generally have anywhere from 3 to 20 seconds of idle time that a user's sitting there doing nothing but typing. It's plenty of time to go and request some resources and preload those. You don't have to do so in that first critical few seconds while they're coming to the page. So there are a couple of different techniques for preloading. One is a more recent addition to the HTML spec. There on line one you see link rel=prefetch. What this essentially says to the browser is hey browser, here's a resource that the author of this page considers to be very important. It would probably be a really good idea for you to download that as quickly as you possibly could and have it ready because they're going to need it, and it's big. That's essentially what that hint is saying. But notice it is a hint. The browser does not have to listen to that hint, and the browser does not have to load it at the time that you say to load it. It can decide to do so at any point that it feels is right, or at no point at all. It is a hint for preloading. And this is actually I think a good style for preloading because it gives control from the developer who might choose to be overzealous back to the browser that often will know better how it's managing its network connection resources and resources in memory and all of that stuff, and it can say I know you've asked me to preload that, but I've already preloaded a whole bunch of other stuff, and we're just about full of memory, so I'm not going to preload that one. I have to tell you that I as a developer would like to have that help because I don't know how much memory the browser's using. I don't know if you guys have ever had a browser crash on a mobile device, but it's not a pretty event. And it happens. The browsers just flat-out die when they run out of memory, so I want to trust that mobile browser especially to manage things a whole lot better than I think I can manage them. I don't know how much memory it takes. I'm greedy. I just want as many resources there as quickly as possible. I would like for the browser to help me be a little bit more sane about that, so I like that feature in theory. In practice there are a few problems, but that's one of the ways you can do it. The more old-school approach is creating. For images, you can create an image element and reference an image. Technically, you can actually reference any sort of file this way, although there's some weirdness, quirks, cross-browsers, so it's usually best to only do images that way. But in both cases, what these things are relying upon is load something, and primarily load it into the cache, which should raise a red flag to you that says --- or at least an orange flag to you that says what if that thing's not cacheable? What if they're not sending out proper expiration or caching headers? Well then you can end up shooting yourself in the foot badly, and I see a lot of sites that do this. This is a big problem that sites try to preload stuff, but then they don't send any caching headers along with it, so they actually end up reloading stuff more than they needed to. So if you're going to do preloading or any of this sort of optimistic loading or dynamic loading, if you're going to do any of that, please make sure you understand how to do caching headers right, and do so. Take advantage of them. They're one of the most powerful parts of our platform, and we oftentimes don't pay much attention to caching headers. It's sort of something like that Apache admin will take care of that for me. That's a really important thing that you need to be taking care of. Yeah. Isn't using CSS a better way, a more modern way of doing this? In what way? Well, as far as hiding images and then using when you needed them kind of like a spray. So, if you're asking can I preload an image by referencing it in CSS, is that what you're asking? Right. Okay, so one of the problems with that is that you don't have nearly as much control over when the browser is going to parse and interpret what happens inside of a CSS file. You reference it in your page, and the browser will do it as soon as it can and as soon as it thinks it should, which means that it might start to request that image during your critical path downloading. This sort of technique with new image allows you to choose at a later time, like half a second after load, have you choose on demand when you'd like that download to start. Link rel prefetch, again, is just sort of a hint to the browser, and it'll do it whenever it thinks it should. And you can use both of these techniques and others in your pages to do preloading. Just make sure you keep that sanity check on how that will affect your performance. With the prefetch, is there a way for your JavaScript to know if the resource has been successfully prefetched? With link rel prefetch? Correct. Nope. Okay. There is not. By definition they didn't put that event on there. That's a little bit frustrating, but, in fact, we don't have a standard well-defined load event for any sort of link, even style sheets, so we definitely don't have them for prefetch. It's kind of a bummer. Yeah. I've seen the HTML5 Boilerplate uses a similar technique for actually for DNS prefetching. Any experience with that, and does it seem to help? I don't think you should do DNS prefetching that way. There's a different technique for DNS prefetching that several of the browsers, like Chrome I know for sure, Chrome will automatically do DNS pre-caching by looking ahead in the markup, and any host name that it sees or domain name that it sees that it doesn't yet know about, it'll automatically go and fetch that. So even if it sees it in an href tag, and it thinks you might actually click on that link to go there, it'll pre-cache that DNS lookup so that when you click on it it'll happen quicker. That's DNS pre-caching. The browsers do that unless you tell them not to. There's actually a meta tag header that you can send that says don't do that. Why would you do that? Well, in some cases you actually have pages that have references to a whole bunch of different domain names like that link that we talked about with Robert Nyman that's referencing hundreds of links and they're all on different domains. Boy, that's a lot of DNS prefetching that's happening in that case, and most of it, frankly, is probably wasted, and the browser has no way to know which one you're going to click on or not, so in some of those cases you may want to tell the browser don't do some DNS pre-caching or prefetching for me. But I wouldn't use link rel prefetch for that. I would use the technique that's already being used that's already sort of built into the way the browsers are doing it. So, you're saying really don't do anything; just let the browsers optimize that? I think the browsers are better at it than we are in code, and I don't often say that. Oftentimes I do like to empower developers, but in this case I think the browsers are better at that stuff.

Lazy Loading
Alright, next we'll talk about lazy loading. I sort of referenced this earlier when I said so preloading is a little bit more like let's do something sort of early in the critical path, and let's do it way ahead of time in anticipation of somebody needing something. The other end of the spectrum, lazy loading, which can also be on-demand loading, or in some respects postloading, those techniques are the opposite end of the spectrum. They wait until you're absolutely certain that somebody needs something before they download it. For example, you can have a page that has a calendar widget in it, but until the user clicks on the calendar widget, you don't download any of the code to handle the calendar widget. Would you do that? Well, there are some cases for it. Steve Souders came out with a ControlJS library that was sort of advocating that approach saying absolutely minimum loading, don't load anything, don't load stuff until people click on it. Of course if you wait to load until they click on it, there's a much better chance that you're going to cross the 100 ms or even the 1000 ms thresholds that the users are going to tolerate, but you did save a whole bunch of code downloading that there is a chance that person may never use because what if it's behind a tab and only 10% of your users ever even click to that tab? So, you can start to make some choices about well I don't want to download those resources until they're actually necessary. You might, for instance, say if I've got a tab set, and I've got some controls in one of my tabs, download all of the information for those controls as soon as you open the tab. Don't wait for them to click the control so there's sort of some middle ground there. But lazy loading, postloading, on-demand loading, these are all techniques that say let's respond to loading requests only when absolutely necessary, or at least let's be more intelligent about when we load requests. Let's not waste a whole bunch of time on the way or waste a bunch of memory, fill up the cache with a bunch of stuff that we think the user might need. These are two opposite ends of the spectrum, and both are valid. Both are part of your arsenal for loading resources, but you do need to understand how to take advantage of them. Can I comment on that? Dustin Diaz, he's kind of a prolific developer from Twitter, he talked about --- he's no longer with Twitter anymore --- yeah, obvious, whatever, but he said Twitter used to be --- it took longer to load the initial page, but then everything else you did was fast, and he was saying now everything loads fast, but when you're trying to do other things so many things are lazy loaded that other things become really slow --- now stuff is slow --- and in his opinion it was a worse experience to continue to only fetch these things when you take the actions versus a little bit extra initial payload, so that's just illustrating the tradeoffs there. Definitely. If you're optimizing for page load only, you can get yourself into other problems. That's definitely true. I think that reinforces a point that I was trying to make earlier, which is that because this is a tradeoff there is no one right answer here, but the way a user feels about your site is probably more important, probably more important than those mere statistics that say well we're loading in one second versus two seconds or whatever. A user often will spend more time on your site than that critical three to five seconds of page load time. So even though it's really important that you give them a good first impression, it's a bad tradeoff if you give them a really great first impression and a terrible impression for the rest of the time they're on your site because there's a much less chance they'll ever come back. If it loads really quick, but then they feel like it's slow and sluggish all the time ---. Like every once in a while I feel that way about the Gmail interface when I use their web interface. That thing loads pretty quick when you consider how much they're loading, how much logic. They're at like 2 MB worth of JavaScript, and it loads pretty darn quick. And in some cases that interface works really fast, but every once in a while, man it's like dang I click on a link, and it takes forever to open up an email. And so I've become so used to things being fast that when it's slow for them, I hate it, right? And that's the same kind of thing. You want to avoid people getting into those pain points, so that's a great point Mark. So, lazy loading, on-demand loading, postloading, here's an example with script files, and this code may be a little bit hard to read in the back, but you can grab it. This is not terribly rocket science code. This sort of boilerplate dynamic script loading has been around for years. Millions of people have written libraries around this stuff. But it boils down to creating a script element as we do there on line five, creating a script element dynamically, setting its source, and then appending it to the document. And then additionally if we want to listen for whether or not the thing finished loading, we can listen to the onload event in some browsers, and in other browsers we need to listen for the readyState and the onreadystatechange, but in either case we'll get notified that the script had finished loading. And by the way, even though it says onload, there's something true about it that you maybe know sort of instinctually, but have never heard somebody actually say out loud. Onload doesn't mean on finished loading. Onload means on finished processing and executing and already done. It means ondone. So, there's a whole world --- I've worked in the LABjs script loader, which we're going to talk about here in a moment --- there's a whole world of this script loading where we'd like to have the opportunity to load something but not execute it and all of this stuff. And the naming of this event is really unfortunate. There's no way to change it now because it's been around for 15 years or more, but it's not actually onload. It's ondone execute is what that really means. We don't have any control over loading things only and not executing them, except we do have that control in Internet Explorer strangely enough, and we've had that since IE4. So, the next thing that I want to talk about is parallel loading. And by the way, on that previous snippet, let me just go back, on this previous snippet, so what you would do here, the reason why you would do this is this is how you would say I don't need to load this script until they've activated a widget. You're going to do a very similar technique for I don't need to load that CSS file. You can do a similar technique, although not as complicated, if I don't need to load that image until they open the tag. So I was showing JavaScript here, but this is really just sort of a pattern. You create an element that references that just the way that element would've been in the markup. You create that dynamically and add it to the document, and in most cases it has the same effect as if it had been in the markup, but you get to control when that request starts, or roughly control.

Parallel Loading
Parallel loading is an additional step on top of what we just talked about, and this is the world that I've lived in for a number of years, writing script loader like LABjs. Parallel loading says I don't just have one script that I need to load; I've got many scripts that I need to load. For instance, I may decide that all of the scripts that I'm loading at the beginning of my page I want to do so dynamically. We'll talk about why you would do that in just a moment. But parallel loading is essentially about enforcing execution order, preserving execution order because for more than 15 years now we've had the script tag in markup, and we know that if script tag A and script tag B are in markup, we know that A will run before B, and so there are millions and millions and tens of millions of scripts on the web that are written based upon assuming execution order. It's how we compose our modules and things like that. Now there are better ways, in some respects, to do that, using module loaders like AMD and some of those others. There are more sophisticated ways of doing that now, but most of the web relies upon execution order. One of the dirty secrets, unfortunately, of that previous technique, if you were to ask for two different scripts to execute, or to download, like you had a widget that needed two JavaScript files, if you just ask for those the normal way just creating script elements, the browser's going to download both of them in parallel, but it's going to run whichever one finishes loading first. It doesn't enforce or preserve execution order at all by default. And actually that's my fault. I actually got that change to happen in the spec a couple of years ago. So if you don't like that part, you can a little bit blame me, but I also fixed it. So, here's an example of how we could load several scripts and make sure that they are preserved in order. And the key line, the only line that you actually need to pay attention to is this line because that's my contribution to the HTML spec. I got scr. async added. Well, scr. async was already there; it was there in markup. I got it added into the JavaScript world. And notice I'm setting something back to false, which is a little bit strange because in most cases in JavaScript things default to false until you set them to true. What I got changed was that when you dynamically request a script file from JavaScript it defaults to true because that's the behavior of it. It's async; it's going to happen in an ASAP execution order. That was what was already happening in several browsers, and we just codified that that's what happens by default everywhere, and that's now true of all browsers. IE10 finally got it; everybody's got it now, so all the modern browsers have it that when you request files they run as quickly as possible, ASAP execution order. However, if you don't want that, if you want them to load in parallel, but you still want it to preserve execution order just like it did in markup, you set async back to false. That's all you've got to do. If you set async to false on a script element that you've dynamically created, it puts it into a different queue, an ordered queue, as opposed to just a pool of scripts that are loading that'll execute as fast as possible. So if all your scripts have async set equal to false, they'll all download in parallel, but they'll execute in order. There is one foot-gun to understand about this. That means that if you are loading four scripts, and the second of those four scripts never finishes running, the other three may load, but they will never run because the browser is waiting in that queue for the second one to finish before it goes onto the third and fourth, which is not the same as what happens in markup. You can argue that this is either a good or a bad thing, but it's a reality that that's the way this works, so it's unfortunate that we don't really have full control to create like separate queues and like sandbox stuff, can't do any of that. You can just opt in to ordered execution or not by choosing async equals false. I mentioned many times LABjs is a script loader. LABjs is a wrapper around this sort of logic, but it actually handles all of the corner cases of preserving execution order of multiple parallel downloaded scripts in all the browsers all the way back into like the IE6 and Netscape 7 kind of days. It handles those other corner cases, which this doesn't. So, if you only cared about the modern browsers, this is all you'd have to do. If you really want to do dynamic parallel loading with execution order preservation, you need a script loader, and I know of a good one. So you can check out LABjs if you're interested in that. It's really tiny; it's like 2k. But I've sort of assumed that we all agree that dynamic script loading is useful. Why dynamic script loading? Why is that useful? Especially, why am I talking about it so much in a performance workshop? This is our old friend, the waterfall diagram. The top one is using script elements in markup, the exact same scripts, and the bottom one is using dynamic loading techniques using LABjs. You'll notice that their shape of that waterfall diagram is roughly the same. There's a slight difference in terms of speed, but that was actually --- I tested them a bunch of times, and that was just sort of network variation. That wasn't actually that this one reliably took longer. So you could roughly say that they took about the same amount of time, and they had about the same shape, so why am I showing you this? If the modern browsers are capable of doing all this parallel loading of script elements and they preserve execution order by default in markup, why would anybody ever need something like LABjs? Can anybody spot what's different between the two diagrams? Onready. What do you mean onready? When the DOMs ready. The DOM ready. Where do we see that? The blue line. You sir get a book. Alright. Can somebody pass that over to him? The line that he's talking about. This line right here represents document ready, the DOMContentLoaded event. That thing I talked about that was so important to optimize for for perception, look at the difference in the location of the blue line here versus here. It's six-and-a-half seconds worth difference. The difference is that browsers have to assume that a script element will include that bane of my existence, document. write, because if they receive a document. write inside of the script tag, the whole game changes because you may write out actual new markup, may completely change the way it should interpret the rest of the page, so it cannot fire the document ready event until after it's finished loading the scripts when it sees something in markup. The sort of unspoken, unwritten assumption here is that you would never dynamically load a script that had document. write in it, which is unfortunate because a lot of people do try to do that, and they end up crashing the browser or overriding the page. If document. write runs after the page is finished loading, it wipes everything out. So it's really unsafe to dynamically load scripts that have document. write, so get rid of document. write, stop using it. Any of your vendors that are still using it, tell them that you refuse to work with them anymore if they continue to use it. It's a terrible technology, had its use for a while, we now have plenty of better options. This goes for --- I'm literally serious. I don't care if all of your revenue from your company comes from one advertising partner and they refuse to stay on document. write. You'd be better giving them the finger and telling them I'm going with somebody who knows web technology than sticking with document. write. It's that bad. It leads to this kind of terrible performance. You guys might think that I'm crazy, but this is the kind of thing that plagues the web, terrible performance like that because we have to make these assumptions about document. write. Dynamic loading, with the assumption that there is no document. write, gives us the same loading profile, but a vastly quicker document ready event. Yeah? So working with --- I love RequireJS AMD and all the new stuff that can do nice stuff like this, but with legacy code bases I know we have some document. write instances for script tags, so I would be happy to replace them with like your async false, but what is the cross-browser support for that like? For async false? Yes. Or LABjs? Async false in particular. So async false, you still have exactly the same problem, by the way. You can't use document. write inside of any kind of dynamically requested script no matter what flag you set on it, but async false in particular, I have a blog post where I put this exactly, but I'm trying to recall from memory, it's in IE10, Opera 12, Firefox 4, Chrome, I want to say 12, and Safari 5. So, it's very modern, but it doesn't go all the way back to like the IE6 days and stuff. That's why you still need something like a LABjs that takes care of the other corner cases.

The Front-End: Abstractions & Animation
OO is Slower
So, next we'll talk about removing abstractions, and at the end of this one we'll actually have an exercise. So if you're interested in getting into code, we're about to get to some code. So, I'm going to go out on a limb and state an opinion here that may be quite controversial. I do not like object-oriented code, and I especially do not like object-oriented code in the context of JavaScript in the browser. The reason I don't like object-oriented code is because it doesn't work the same as object-oriented code in a compiled language. It is a live interpreted link of inheritance, rather than a compiled away link of inheritance. And that may not seem like that matters, but it's a massive difference in terms of the implementation details and how things run. I also think that in general when you think about the world in a purely object-oriented way, you tend to create more abstraction than is necessary hoping that someday it'll be necessary, and so you end up creating more function calls, more layers of abstraction, which tend to slow things down, so I tend to take the approach that I move from less abstraction to more abstraction as necessary rather than the other way around. That's my stated bias, and I've got plenty of performance tests to tell you and prove to you that object-oriented code will run slower than non-object-oriented code. That's not to say that the maintenance benefits of object-oriented code don't matter. It's just simply to say that when you are deciding I've got a critical path of code, and we've talked a lot today about ways to think about that stuff, ways to find where those critical paths are, if you find some code that's in a critical path that's that important 3% that you should pay attention to, you really need to seriously consider not having object-oriented code in that path because it will perform slower. Another problem that I have with object-oriented code is you oftentimes don't build into the interface the assumption that things need to happen in batch. I'll tell a story from a while ago, one of my first jobs that I had in my web development career. I was developing a timesheet and payroll management application for a biotech company, 400 employees. They, for whatever reason, didn't want to buy their own. They hired me to build it, so I spent a good year designing and building from scratch this system, including building out all the server hardware and all of that stuff and training the employees, and we eventually launched this application. But about three weeks before the launch, and let me back up actually and give you a little bit more background. So, this was a PHP application, and it used some JavaScript as well. I developed this really --- I was right out of school, so I was fresh with all the object-oriented concepts in my head, and I'm at a job that I can write object-oriented code. This is awesome! So I approached the timesheet model from an object-oriented approach, and I said well, what we have is that an individual employee, which is an object, an employee is an object, and that employee has a collection of timesheets that has been associated with them, and for each timesheet there's a collection of weeks, generally about two weeks, for each week there's a collection of days, for each day there's a collection of tasks that person did in a day. This is the traditional way that you would object- oriented model that particular problem domain. So, I built out that object-oriented structure both in PHP and in JavaScript since it was a web-based timesheet system, and I had great things like I had a little method that I could call like total time, and I could call that on a timesheet object. And he would loop over all the weeks in him and ask them for call getTime on the week, and the week would internally loop over all of his days, and the day would internally loop over all of his tasks, and then you could call that across employees and had all this great object-oriented awesomeness in it, and it worked really well as an application. We were about to launch. Three weeks before we went to go launch, the payroll department came to me and said okay, everything looks good, we need to get some data out, so we need for you to create some simple reporting things. And I was like that's cool. I've got all those methods built. All I have to do is instantiate all 400 timesheet objects and call the method getTime, and then I can drill in and subtotal and task and stuff. I've got all those methods already built. It's cool; I'll do it. So I wrote the reporting code, and we fired up the report the first time to calculate all the time that was spent in that two week period, and it took four-and-a-half minutes to run that report, four-and-a-half minutes to instantiate object-oriented code. It wasn't doing anything like batching through log files or anything. This stuff was in a database, and the database query was quick. It took four-and-a-half minutes in PHP to instantiate all those objects and make all those nested chain calls into the sub-objects and total everything up. I mean, I thought my career was over at that point. The main goal was to help support the payroll department, and I thought I had done exactly what they taught me in computer science school, which is object-oriented is the way to go. You model the problem domain exactly the way it should be, and the code just works. And it does, except when you start talking about batch operations. Similar questions arise in backend development when you start modeling ORM layers on top of databases. ORM layers are great for individual record access, and they generally suck for batch access. Same problems exist. So, one of the reasons why I dislike object-oriented code, especially in a dynamically interpreted environment, is that it tends to lead us down a path that the code is really nice and graceful, but it hides away from us that most critical part, how fast will it run? And we have no control over those details. We simply call a top-level method, and we hope that it's doing its job, but in reality it's not because in reality it's got a whole bunch of extra work to do that we can't even see from that top layer. It hides away a really important detail, how fast will it run. That's the reason why I'm bringing this up in this workshop. And again, I understand why object-oriented code is so important. I just think that it doesn't belong in interpreted environments, at least to the level that we take. I use some of the concepts like encapsulation of modules, hiding private data members. I use some of those concepts a lot. I don't like to use inheritance; I don't like to use lots of layers of abstraction.

Exercise 4
So, with that said, we've got a project, exercise 4. I've allotted 30 minutes. Again, I don't think that it'll necessarily take that long. I'm going to spend a significant amount of time with that code up and talking, but you're welcome to jump in to the exercise if you see fit. I've created some object-oriented JavaScript code that's modeling an address book example, which has a list of people, and people has a list of addresses associated with them. And I'm going to ask you to take a look at the code, and I'm going to ask you if you want to do some performance testing on that code. We'll talk more about best practices for performance testing, but I really want you to get familiar with this is the sort of JavaScript code that's very common, and I'm not entirely sure that it's going to be very performant code when it's rolled out in the real world. The instructions say to examine the code, identify which classes have composition, which is mixins, like this object contains several instances of that. That's composition mixin. Another pattern is inheritance. Do any of the objects have inheritance? You'll see both of those in there. So identify those, kind of make some notes of inside of the code where is that, and then which ones have substantial, some substantive object-oriented behavior to them and which ones are abstractions simply because they're abstractions because there's some of that in there. And then once you've kind of identified some of that, I'm going to ask you to try to modify some of this code to remove some of these abstractions, to remove some of these problems, and then finally to talk about how might bulk operations be handled. So as we go down in the instructions, we talk about things like loading in a big list of addresses. Do you want to load that in one at a time calling a method one at a time for each address, or would it be nice if your interface for the address book had a bulk load to it that you could pass in an array and it could pop it all in all at once? That's the sort of thing that we were talking about before, and it matters when you design APIs how you design them. If you don't design them for batch operations, later on down the road people will find that your object- oriented code all of the sudden is really slow. So let me open up this code, and I'll kind of orient you guys a little bit to the code. In some respects I'm using some traditional concepts in JavaScript, object-oriented. There's this slight variation on the pattern that I use that you're probably not familiar with. When I do object-oriented coding, the most important thing, or the thing I find most valuable, is modularization encapsulation of data. That is hiding private data. The problem with traditional prototype-based inheritance is that everything's a public property, so I have a slight variation on how I do things that allows me to use a closure around an object to get some private data variable usage, but still get that sort of prototypal inheritance behavior that we seem to be so addicted to. So, it's not terribly important to understand all the intricacies of this pattern, but just understand that here instead of creating iterator directly as a function that then you set prototypes on, I have a function that is going to get executed because when a function's executed I'll create a private closure scope so I can have these private data members like index inside. Here is actually where I start doing the thing that looks more like traditional JavaScript code where I have a function and I set some stuff on the prototype, and then I instantiate that thing at the end, and I return it. So, I know that might just look like a slightly weird version of JavaScript oriented, but it's for the purpose of giving me private data members as opposed to public data members. Performs exactly the same, just has closure involved in it. Here's the Person class. You'll notice all the Person class has is two data members that we pass into the constructor, and it stores them on the instance, and then in this case I just have a serialization, a two-string serialization to print it out. I might've been able to come up with some other things, like I could've put accessor methods. That's common in object-oriented. We hide our data members, and we only give you methods to set and get your things. I could've done that. But that seems like overkill here, right, because this is just data, and once I've created a Person object shouldn't I be able to freely change the string if I find out that their name is different or something? Sure. So this Person object is just a thin veneer, if you will, for two data members and a little serialization that tells us how to make it a string. Address is exactly the same way. It has four, but it's exactly the same concept. AddressList. This is where things get a little bit more interesting. You saw up here I had this generic abstract Iterator class. This is my version of Iterator. It's certainly not as powerful as some of the more standard iterators, but my version of Iterator is an abstract one that you inherit, and it's simply a wrapper around incrementing an integer. But I'm hiding away those details because I might choose to model this in an entirely different way because that's what object-oriented lets us do, right? So this Iterator hides that detail, and it simply returns us the output, which is what our next index should be. So AddressList is going to inherit Iterator; it's going to extend Iterator. So, again, there are a few things in here that mostly have to do with the way I'm managing the private scoping on AddressList, but here's the stuff that you're probably accustomed to seeing. I'm extending the prototype, or redefining the current method, which I inherited the abstract version, I'm redefining it, and I'm calling my super, I'm calling my parent one, and then doing something useful with it like referencing my private data member addresses. Same thing with next. So the way you iterate over an AddressList, you can reset the Iterator, that's one of the ones up here, resetIterator sets us back to the beginning, and you can say current, which is whatever current one we're on, and you can say next, which is going to go ahead and move the pointer and then return that one. So, here we have current address; here we have next address. Pretty standard stuff. This is what you'd see for the most part in an object-oriented computer science textbook. Here I have addAddress. So, makes a lot of sense that once I have an AddressList I might get some new addresses, somebody may have entered something into a form or there may have been a data update for my model in the backend so I need to add in an address. And in this case I'm giving you a nice helpful thing that you can add a label to your address, so you can call it your work address or your home address, but pass in a label, and you pass in an address object, and we'll store that in our addresses private data member. We know it's private because I'm not doing this. there. I'm keeping a public property that tells me how many addresses I have just for convenience sake. That could've been a member, a method function as well, but didn't go that route. There's my iterate over myself function, so I'm calling this. reset operator, this. current, and this. next, so I'm calling it on myself to go ahead and get my addresses out. I could have looped over the address array because that's a private data member, but that's not object-oriented. The object-oriented approach says if I'm an iterator, iterate over myself. So I'm eating my own dog food here, and I'm iterating over myself in the same way that somebody publicly would iterate over themselves over the object. AddressBookEntry. It's the same as Person and Address except I add in a couple of things, but I don't have any private data variables here, so it's a public data one. AddAddress. So on the AddressBookEntry you can have more than one address. You can have a work address, have a home address, etc. You'll notice that all it does is call the addresses list version of that method. And then my serialization. Finally, the main chunk, the AddressBook class. AddressBook. I have this nice helpful feature that if you pass in an existing instance of an address book, like we do in traditional object-oriented programming, we have the copy constructor, so I can copy all the address references from another address book into this currently new built AddressBook. How do I do so? By iterating just like we would in any object-oriented interface. Are you starting to see the trend that to eat my own dog food I'm creating a whole bunch of function calls? That's the problem with object-oriented JavaScript. In a compiled language, all of that abstraction gets compiled away. It creates roughly the same byte code as if you'd written it in the old unmaintainable looking way. But a dynamic language doesn't have that same benefit, mostly because in dynamic languages like JavaScript with prototypal inheritance, the inheritance is a live link. There's no copying of behavior from one to the other. You can copy if you want, but the default behavior is not to copy. The default behavior is to have a live link. So, I like to tell people this, and sometimes it helps, and sometimes it screws people in the head up, but JavaScript doesn't have inheritance. Regardless of what you've seen, regardless of all those books that tell you that JavaScript has inheritance, it doesn't have inheritance. What is has is behavior delegation, delegating behavior up the scope chain, in this case the scope chain being the prototype chain. That's how JavaScript prototypal inheritance works. That's how other prototypal inheritance works. You delegate behavior either to my current instance or to my parent prototypal instance or his parent or his parent. You're delegating that behavior up the chain. That's a better way of thinking about what JavaScript's doing. It's not copying like from a blueprint like they do in C++. So my AddressBook, of course it's an Iterator. I can iterate over entries in my address book with current and next, I can add an entry, and I can do a toString. So here's how I use it. Create a new AddressBook, create a new Person, a new Address, create an AddressBookEntry with my Person in it, add an address for home. In this case I work from home, so my home and my work are the same address. And then I add that entry into my AddressBook. It's all pretty straightforward. This should look familiar to you if you've dealt with object-oriented JavaScript. I'm not creating things that are totally foreign. This is the way people write code. I'm just showing you under the covers often what you don't see. Now I create another one. In this case, I'm creating one for Marc same exact way. Here I'm creating another AddressBook, and I'm copying the mybook, and I print some stuff out, and then I also add it to the DOM just so you can see it outputted into the DOM, again using iteration. So, with that walkthrough of the code, I'd like you to spend a few minutes thinking about the places where object-oriented stuff is happening, where these abstractions are happening, like the iterator and stuff, and ask yourself how easy would it be to write maintainable code that didn't have that abstraction? Do I have to have this abstraction to have maintainable code? Because it's clear we could easily performance test, it's clear that this code, especially in bulk, will run much slower than to the bones just wrapping up of data. Are we really buying a lot by having this abstraction? That's where I'm really trying to get you guys to ask that question, and I know that may seem like an uncomfortable question to ask, but in the JavaScript world every single millisecond matters in performance, so ask yourself the question whether or not that inheritance pattern that you're getting here is really buying you a lot or is it hurting your performance more than it's helping? So spend some time with those exercises, and I will certainly be available to answer questions again either as the full class or individually.

JavaScript Animations
Alright, next we're going to talk about JavaScript to CSS, and specifically we're talking about moving what you traditionally do, things like animations that you traditionally handle in JavaScript, talking about some patterns, some simple patterns for how to move that into CSS. Why does that matter? Well, put simply, the performance of your animation will be better if it is offloaded to the CSS engine than if it is running in your JavaScript. There are a variety of reasons for that that we won't necessarily dive too deep into, but there's a premise that says if CSS can do something for you, don't do the hard work and do it yourself. Now, there are some cases where the animations that you need to do, or the things you need to do are just too complex, and they don't model well, and that's cool, keep doing things in JavaScript, but in the cases where you do have some stuff, there are some simple wins that you can do, and that's the patterns we're going to look at. So, first let's talk about what JavaScript animation looks like. Let's take a look at how that kind of works. So, this is just sort of a bare bones boilerplate for how you would approach JavaScript animation yourself. Now, there are some details that I left out conveniently in this little comment here on line 11. I said calculate the new x, y. That's the hard part, right? Calculating all your delta x's and y's. If you've ever done that stuff, yeah I fully understand that I left out the hard part. I get that, but this is the framework by which you would JavaScript animation. You would essentially say once I've calculated my new x, y, what I need to then do is update my element, which sets some new CSS, like my left and my top, and then set a timeout to tell me to call this thing again, and here I'm doing a calculation, 1000 divided by 60 ends up being, I don't know, whatever that ends up being, but that's how many milliseconds I would end up wanting to update to keep my frame rate going. Now, there are a variety of problems that can happen with this sort of code, which is one of the reasons why very complex animation libraries have sprung up. There are things like Scripty2 and some of the other ones. JQuery's got animate. There's a whole bunch of it's the devils in the details kinds of things that happen here that sometimes are a problem and sometimes are not, but let's just talk about what some of those problems can be. First of all, does anybody see if there's a problem with using setTimeout loop the way that we're doing it here? Anybody know of any pitfalls that might create? Yeah. It doesn't actually execute the interval that you set --- okay --- it doesn't actually fit that way because the function call can be delayed based on the event when you set up the browser. Okay, so to repeat what he said, he said, paraphrasing slightly, you tell it how many milliseconds you want it to wait, but it doesn't necessarily happen exactly at that moment. Well, in non-critical animations, the fact that some frames get dropped or some frames get delayed, it's probably not a terrible thing, but if you are doing a very important animation, like let's say you've got two different elements, and they need to both animate at the same time, and they need to stay in sync for their positioning, it's really important that your animations work frame for frame exactly the way you've planned them out. So, you're exactly right that in this case we tell setTimeout to wait for whatever length of time. That may not even be the right math, I don't write JavaScript animation, so if I'm wrong on that I apologize, but whatever that math is, that may be exactly the right number of milliseconds. We may want it to wait every 16. 7 ms or whatever, but it may not actually wait that long. It may wait 18 ms, or it might --- it never comes early, which is a good thing, but it may not happen right at that time. What the browser promises you with setTimeout is it will ask for it as quickly as possible after that time. But there's a whole bunch of other stuff, and in our next module we're actually going to talk about the UI thread. There's a whole bunch of other stuff that, as you said, can force us to not execute it exactly at that moment. Well, there's another approach that you could take. It's called setInterval. SetInterval is automatically going to happen, you hope, every 300 ms, or whatever you set your interval time to, so in a setInterval style of animation, instead of this where I wait until the end of my function to call a setTimeout, what this is susceptible to, by the way, let me back up for a moment, what this kind of code is susceptible to is what if the amount of time that it takes to do these calculations and updates is several milliseconds, 10, 15, 20 ms? Well notice we didn't take into account how long this function took to execute in this math. We just assumed it was basically 0, but in reality it took a little bit longer, so we're not taking into account the length of the execution here. The same is true of setInterval. On setInterval we tell it to happen at a certain amount of time. Now, if it's a 300 ms, you can be roughly sure that most of your code is never going to take 300 ms to execute, but what if it's every 16 ms? Pretty easy to create code that takes longer than that, so what happens with setInterval is stuff starts to pile up, so you don't skip. In this case, you can end up skipping some frames or having a lower frame rate than you want. In setInterval you wouldn't end up skipping frames. What you'd end up is having all your frames that sort of stack up behind the slower frame so your entire animation, instead of taking one second, takes a second-and-a-half. You get all those frames, but it just takes longer. So that's some problems that you can get with JavaScript-based timing. It's not a precise science. SetTimeout and setInterval are, at best, hints to the browser of when we'd like it to happen, and our best attempts at calculating that and correcting for it are still subpar. So even the most complex animation libraries are really good at what they do, and they correct for a lot of the stuff, they take care of a lot of this stuff, but they still suffer the problem that you cannot guarantee when something will happen. The best that those libraries can do is monitor when it happens and adjust the next frame delay for that so they can see when it should've happened, when did it happen, and if there was a skew adjust for that in the next delay so that we catch up. Aright, so if one frame took a little bit longer for some reason, like garbage collection, which we'll talk about, if one frame took a little bit longer, make the next delay shorter so that we get back on track, and that's some techniques that they use. So, again, I didn't show all that code, but I just wanted to talk about when we're dealing with JavaScript animations. And by the way, JavaScript animations are not just a nice to have toy. In some cases they're very important, and I understand that. Some of you may scoff and say well animations are purely a decoration thing, but there are some cases where animations are very important. For instance, the idea of taking an element and moving it from your list into the trash or whatever, it's important to reinforce that visual metaphor that the element is moving, so you want to animate that in some way that it's moving from one thing over to the trash or whatever, and if those animations are slow or not working properly or they're janky, the user is going to feel like the performance of this app is suffering. So, along comes HTML5 and says you know what, JavaScript animation, setTimeout-based animation sucks for all the reasons listed above and more, so let's give them a better interface for this. Let's give them an interface that actually attempts to really solve this problem, and this interface, the really long name is requestAnimationFrame. It's a new addition to HTML5 and to the browsers. It was actually a really powerful API, and it can solve a whole bunch of problems, so let's just --- compared to the previous code, let's take a look at what that code might look like. I'll try to flip back and forth between these two slides just to show you. You see here at the top, you see we're doing exactly the same thing; we're updating our element. Here we have a doAnimation function, and we're calling doAnimation at a certain time. The only difference here is the use of requestAnimationFrame without setTimeout and without a number. We're not having to calculate that. And the fact that I don't call doAnimation directly, I do requestAnimationFrame, this line is not strictly necessary, but it reinforces the point that I want to make about requestAnimationFrame. The purpose of this API is to say it's not actually requesting anything, which I don't like the name of the API. Maybe implementation-wise it is, I don't know, but in reality, conceptually that's not what you're doing. What you're doing is saying I've got some code that I want to run that the very next time the browser is about ready to paint an update to the screen, browsers roughly, modern browsers on modern devices, roughly paint the screen 60 times a second. They run at 60 hertz, which is about 16. 7 ms roughly. So, what we're saying is every about 16. 7 ms, when you're about to get ready to repaint the screen, I've got some stuff I want you to do before you repaint the screen; I've got some changes to make. Why is that important? I could choose to make the changes --- let's say that it's happening every 16. 7 ms, and it's going to paint it here, if I make the change here, it'll be ready for the time that it happens here, but what if I make the change right here? It's also ready. So a convenient way for us to sort of queue up behavior is to say request an animation frame the very next time you're going to paint. I'm telling you that this code is in some way, shape, or form, that's actually part of the reason why it says animation frame then is that you are sort of programmatically telling the browser in a way that it can do something useful, that your code has something to do with animation or some otherwise sensitive performance visually-related code, which is typically animation. You're saying I'm making updates that are very sensitive to the timing. I need you to make sure that you're very precise on this. So guess what? The browser, in most cases, will call requestAnimationFrame right before it's about to paint, and it will, in most cases, paint at a roughly very predictable and steady 60 frames a second. So your code, by being run inside of requestAnimationFrame, has a much, much, much better chance of running exactly when you expect it to happen. Now, there's a difference here is that you actually are running at a frame rate that you don't control. In the previous example, you got to control what your frame rate was based on your math. Here you don't control the frame rate. The browser controls the frame rate. It has a lot of things that go into that decision-making, but that's a good thing. Like I said earlier, some things the browsers are much better at, and managing frames per second I think is probably something it's better at, especially in all the different devices where maybe it needs to run in a low power mode or whatever. Another benefit, by the way, of requestAnimationFrame, we were talking about performance, and we tend to focus mostly on things like speed and memory, another version of performance, another way that performance happens, I mentioned it earlier, is efficiency. Did you know that browsers that have setTimeout loops happening in them, when that tab goes inactive in a desktop browser or even in a mobile browser, those timeouts keep happening. Sometimes they drop them to like a one second, a thousand millisecond resolution, but those timers keep happening. But guess what? Because we've told the browser that this is code that's specifically about visually updating things, the browser knows that it can just simply pause this loop, and so it stops doing any of this work when the tab is in the background or closed or something like that, or inactive, so you have vastly better efficiency on your mobile devices not taking up as much battery usage for your background web pages or apps. That's yet another way that the performance can help greatly when you use something like requestAnimationFrame. So it's kind of like an event handler, right? It's essentially saying when the next animation frame is about to happen, which is much like listening for when the next click is about to happen, I'm waiting for an animation frame, I'm binding for animation frame event. That's a way to look at it, yeah. Okay, so one thing that I want you to understand about this code though, which we'll come back to in just a second, but I do want you to see, is that we do still have a potential problem here, and that is what if this stuff still takes a really long time to figure out? What happens if the handler for the animation frame takes longer than 16. 7 ms to execute? What's the browser going to do? Anybody have a guess? Does it slow the browser down? No. It's going to skip a frame. Skips frames. It just stops dropping them, so you lose frame rate. So you do still have to worry about the performance even with requestAnimationFrame. It's not a silver bullet, but it's a lot better than setTimeout because it's a lot more precise. It's not based upon some math that you hope things will happen. It happens on a fairly predictable schedule.

CSS Transition vs. CSS Animation
Now that we've looked at the state of doing things in JavaScript, we've certainly improved the state by being able to handle things with requestAnimationFrame, we've made progress, but can we be even better about it? Can we try to use things like CSS? So, CSS has two different ways of handling movement, and that's the basic poor man's way of how I'm going to describe this, is movement of elements. There's a whole animation frame in CSS that allows you to set up like key frames and do all kinds of really incredible, awesome stuff. Way over my head. I'm not an animator, and I don't get into that kind of stuff, so I'm not going to cover that in detail, but to tell you that there is CSS animation out there that you can take advantage of. But when people say CSS animation, they don't always mean just the animation. Sometimes they mean both of these things together because the other side of the puzzle is that we have what's called transitions. What transitions are is like an automatically calculated animation. A transition says you have a starting value for a property and an ending value for a property, and I want for the CSS engine to figure out all that intermediate delta stuff. I don't want to do that in my code. That's what you have to end up doing in JavaScript is calculating all those deltas and all that stuff and figuring out if you want to have like a linear movement or an ease in or ease out. You've got to do all that stuff in JavaScript. In CSS you just simply say here's the starting value, it's got a starting value, change it to this value, and take this amount of time to do the change. That's what you say, and the browser, the CSS engine, figures it out. So, a lot of the types of animation that we typically do in JavaScript, like moving a box across the screen, can be modeled as a simple CSS transition, and that's what I mean when I say CSS animation, so we'll look at how you might go about that. So firstly we have this code, which is a simplification of the earlier code, and notice something important. On line 10 I say calculate the new x, y. I'm not calculating any delta x, y; I'm doing this calculation once. This is not inside of a loop. I just figure out, I know where it currently is, it's already got its own position, and I figure out where I want it to be, however I do that, like where you've clicked somewhere. So I figure out where I want it to be, and then I just simply update the element once. I tell the CSS engine change it to left and top. Automagically, this is going to tell the CSS engine to transition that to those values, not actually automagically because we have some CSS that tells it to do that. So I had this CSS already applied, I didn't have to reapply it each time or anything, this is some CSS that's set up that says over a half a second period of time transition the left property, and over a half a second period of time transition the top property. You can't transition all properties, but you can transition a lot of them, positioning being one of them. So, I have a very simple single line of CSS that's not dynamically calculated at all that sets up my timing, and then in JavaScript I calculate the ending point, and wherever the starting point is to the ending point, the CSS engine immediately calculates all of that stuff and does a smooth transition across it using all the best powers that the CSS engine has available to it, which is more than you have available to you in JavaScript. It's even better than requestAnimationFrame. Still not perfect. Even CSS transitions and animations, even hardware accelerated animations and transitions, which we won't get too much into, but hardware accelerated stuff, even those, can be subject to your device just choking for a moment because you can have too much going on at the CPU or at your graphics card or whatever, but we're progressively getting higher and higher up in terms of our reliability by offloading more and more of this to lower layers. By offloading it to the CSS engine, we take advantage of the best that it has capable accessible.

Exercise 5
So we've made some real improvements, and I want you guys to see these improvements in exercise 5. You'll start out with a little program that has some boxes on it that you click a box to activate it, and then you click to a new location, and it animates it. And the way the code is first written is the code doing that with JavaScript, and it actually uses JQuery's animate, which I could've used any animation engine. I didn't want to do the math myself, so I used the animate, but it moves the element. But what you'll notice is that it's doing something else while the animations are happening. It's constantly updating this box that says fetching, and it's adding stuff to the DOM. So there's some work happening in the background while our animations are happening, and this is what I want you to see. I want you to compare whether or not you see differences in the animation when you do it first the JavaScript way, and then you'll see I've already written the code. You don't have to figure out the code. I've already given you the code for the CSS animation way. You just do the commenting and uncommenting to enable it, but I want you to compare the performance of those things and see whether or not you see a difference. So we'll take a few minutes. We don't have to take a full 10 minutes on that if you guys don't need to because there's no real coding here. I just want you to see the differences in those two animation approaches. There are snacks in the back too, so there are cookies, and vegetables, and chips. I suspect all of you are amazed by my awesome skills at designing that little box moving thing, huh? I was actually writing that particular demo on the plane up here last night, and the lady next to me just out of the blue, she's like what are you doing? She's like peeking over my shoulder at my computer. She's like that looks like fun. I was like I get paid to do this kind of stuff. You'll notice in the instructions it gives you pretty straightforwardly there's code to uncomment in both the CSS and the JavaScript file, and then in the JavaScript file you've got to comment out some other code. (Waiting) Also in the instructions it talks about using your developer tools. Some of you may have those and may have a browser; others may not. This is one of those cases, again, just sort of --- you may need to come back to this exercise at a later time if you don't actually have those, but there's some really interesting things that you can see about the behavior of these two approaches to the animation when you watch the developer tools, when you watch how the properties are updated. When you watch that JavaScript is having to update the CSS every single time, whereas in the CSS animation none of those properties have to change constantly every few milliseconds, you can see that there's a lot more work happening when JavaScript has to do the work. (Waiting) Those of you who ran the first version of the JavaScript animation where that little fetching thing was sort of animating over there, if you watch it for maybe a minute or two, at least on my machine, and I still have a decently powerful machine, but I saw some jitters. Did any of you see the jitters where that thing would sometimes not be smooth in terms of its animation? If not, give it a little bit. Some of you are nodding or seeing it. Again, this somewhat depends on device environment, but you should be able to see that in some cases, every once in a while, that animation sort of jumps; it doesn't go smoothly. (Waiting) If you go to this site on a normal computer, scroll down, everything's smooth, so the Retina display. It's actually been worse in the past. Are you talking about the parallax effect on the ---? Yeah. How is that happening? Is that done with CSS? I think it's JavaScript. Yeah, requestAnimationFrame with --- I have CSS transitions --- okay --- of like some small amount of milliseconds so that it's smooth when requestAnimationFrame, so he's asking about the Frontend Masters site. If you go to frontendmasters. com and you scroll, there's that parallax in the background. I used requestAnimationFrame, and inside the requestAnimationFrame I calculate all the positions of where things are supposed to be, and then I have a CSS transition applied to all of the elements so when JavaScript updates its position it's going to smoothly transition. Even if the frame rate isn't super great, instead of doing the entire animation with JavaScript, I'm just on requestAnimationFrame updating the position, and then CSS kind of transitions between those. That's actually a really good thing. So I want to share a couple more things while you guys continue to work on this stuff. So what he's essentially saying is that he is using a transition, but instead of transitioning from like 0 to 100, he's doing 100 transitions of 0 to 1, 1 to 2, 2 to 3, 3 to 4, which works, but it is forcing the engine to do a lot more work because it can't just know that the starting and ending point is 0 and 100 and calculate a smooth thing. It's having to recalculate that every time you change the scroll, so that may be one of the reasons that is slowing down a little bit is that it's having to do more work in that style. Makes sense the way you've designed it, I'm not criticizing it, I'm just saying that's one of the implications there if you understand how if you update that transition every time, even if a transition is still happening, the browser will abort the previous one and recalculate the new one. So that brings me to the next point, which is this has actually happened real world on a lot of sites, Twitter was one of them and several others, there's a concept called debouncing. And I didn't show this code, it's not terribly complex, and there are a lot of blog posts about it. There's a concept called debouncing, which essentially says I only want for something to happen a maximum number of X times in a maximum Y amount of time. And this comes actually from the old-school keyboard days where keys on the keyboard you'd end up having repetition to the key because you'd press down the key once, but internally the mechanics of the key would send several signals to the keyboard or the typewriter or whatever, and so they came up with this concept of debouncing that said in a short span of time we only want one signal from the key, and even if we get three or four, we're only going to send one along, so you create this buffer. That's where it got the name debouncing. The reason I bring that up is to say the scroll event, which is what's he's basing the Frontend Masters site on, the scroll event can happen, it's different in different browsers, but it can happen really rapidly, and it can happen even more rapidly than you can update the CSS like in your animation frames that are updating. And so if you have logic that you hook up to a scroll event that you're calling requestAnimationFrame to do some stuff like he's saying, what you want to do is deploy that debouncing technique to say let's only fire that off maybe once every 10 ms or something. And the way you do that with requestAnimationFrame is you, in the scroll event, you call requestAnimationFrame the first time, you save the value or save some sort of Boolean property to say I've now set up a requestAnimationFrame handler, and the next time the scroll event runs, if that Boolean is still true, don't set another requestAnimationFrame because we want to debounce on the animation frame rate of the page, not more or not less. So the way we debounce there is we say only set one requestAnimationFrame handler per animation frame. Even if doScroll happens 10 times in that period of time, the first time we set a handler, the other 9 times we ignore that signal. And that's one way that you can cut down drastically on that, so that's another way that we might be able to smooth out those animations. How are you guys feeling about the demo? Anybody want to raise their hand and say I want more time? Alright, not hearing any objections. We'll regather and go on.

The Front-End: UI Thread, Garbage Collection & jsPerf
The Single Threaded Browser
Next, this is actually a fun one. I enjoy this one because this has some knowledge that you may sort of intuitively know, but maybe haven't heard a lot of direct discussion of it, and some of you maybe this is old hat, but this is some really important knowledge that you need to have. And I probably would've put this one as the very first module if it had made sense with the flow, it fit here, but this is some of the most important content of the workshop. So, I know it's getting later in the day, we're looking forward to happy hour, but make sure you tune in here for the next little bit. So, in the browser we have a single thread of execution. Those of you that work in or come from a more old-school development background, maybe you worked in C or did some systems programming or things like that, you're probably familiar with the concept of threaded programming. Java has threaded programming. This is the concept of having the CPU manage these different chains of operations of execution, and they can go in parallel and the CPU can really fast move back and forth or even it can have two threads, one on either core, so it literally can be processing two operations at exactly the same moment. Threading allows us to have more than one operation happening or being handled at the same time, and as you might guess, threaded programming is vastly more complex to do correctly and safely than is single threaded programming. So, one thing that you need to understand about JavaScript is that JavaScript is, and forever will be, single threaded. Many of you might think, well wait a second, I know that JavaScript's got a whole bunch of async stuff to it and no JS, and that's all cool, right? That gives us multi-threaded capabilities. So that brings us to one of the biggest misconceptions that people have about this, and I want to demystify those two things. There are two concepts here. There's asynchronous code, and there's parallel code. JavaScript has asynchronous code that happens on an event loop, but it does not have parallelism. It is not multi-threaded; no JS. Despite what you may think, it's not multi-threaded parallel programming. It is still a single thread, and they have shared memory and all of that stuff, which would be a huge no-no in threaded programming, so I assure you there is not threaded programming going on. You could spin up two entirely separate node instances that run on different processes, and they may very well be in different threads, but within a single execution context of JavaScript it is, and forever will be, single threaded. How do I know that it forever will be? Because the creator of JavaScript has declared JavaScript will never be multi-threaded. Now, there are some ways to take advantage in some very special scenarios, I won't deal with it much, but just to say there are some ways to take advantage in some very special scenarios of truly parallel programming, even in JavaScript. And there's a plugin, it's now actually being rolled directly into the JavaScript engine in Firefox, but it started out as a plugin, written by Intel, which I think is awesome because Intel has made some really great contributions to the software community. It's called River Trail. And what Intel came along and said is there are some highly specialized operations that by their definition are thread safe. Could we create an interface inside of JavaScript that we fired off one of those operations, and we completely hid all the details of multi-threaded programming from the program, but we still took advantage of multi-threading? For instance, a map reduce operation on a massively large binary array. Could we make a calculation across that array using eight different threads and just fire off a single callback in JavaScript when all those things finish? So there's none of the concept of threading happening, but from a single call in JavaScript could we opt in to some really, really powerful performance improvments with multi-threading? The answer turns out to be yes, and what's really cool is that we've now sort of agreed, the JavaScript community has sort of agreed, that those things need to be added, so there's a standards route that those things are happening. There's, I don't even know what the interface is called, but basically it'll be like a parallel array. You'll be able to create a data structure that by its definition opts into this special behavior, which is black box to you. You still can't deal with those threads, but you know that an operation on that data structure will happen vastly quicker because it will tap into all this power underneath the covers, so it's a nice compromise there. So it's not really multi-threaded JavaScript, but there are some cool things happening there. So, just so you understand, asynchronous, the way JavaScript handles asynchronous code, it handles it in the event loop, is it essentially it's got this single thread of execution, but it's got a whole bunch of different operations and different queues that it might choose to do, but it only will ever do one at a time. So you can think of it as this big pool of stuff out there. This is the way NodeJS works sort of, this big pool of stuff, all these different connections that are asking for stuff, and he has an ordered way of saying okay I'm going to take you, then you, then you, then you, and he does super-fast switching between all these different tasks. He still only does one task at a time, so you never have two different functions executing at the same time, that would be parallelism, but he's able to quickly switch between the different queued-up event handler tasks. What about web workers? We're going to get to that in just a moment. That's a good question. So, I just want to make sure that everybody understands the difference between async and parallel. JavaScript is async. It's awesome because it's async. That's what gives us things like NodeJS. It's not parallel. Alright, we talked about requestAnimationFrame earlier, but I want to come back to requestAnimationFrame for a moment because I want to talk about the UI thread. I mentioned that the UI thread is single threaded. One of the implications of that is that there are several different systems inside of the browser that all share the same thread meaning they can only ever have one of those things happening at a time. So what are those systems? Well, there's the CSS rendering engine, there's the DOM, there's the JavaScript engine, there's the garbage collector, and there are others. All of those different subsystems all share a single thread, and there are various reasons for that, but they all share a single thread. What that means is if at any given moment the CSS rendering engine wants to repaint a frame at that exact moment, but at that exact moment JavaScript is still executing, guess who has to wait? The CSS engine. That's why even with the best that we have to deal with with requestAnimationFrame, if we make requestAnimationFrame take too long, remember I said earlier what about if it takes too long, if we make requestAnimationFrame take too long, then we have JavaScript running while the CSS engine is trying to do stuff, and things won't happen as quickly as we want. Same goes for garbage collection, which we'll look at in a moment. If the garbage collector, which it tries to be very conservative and only run during idle times, but it if happens to be running at the exact moment when your JavaScript wants to run, guess who has to wait? Or if it happens to be running when the CSS engine has a frame it wants to repaint, guess what happens? Jerkiness. That's how we get --- that's one of the biggest causes of jerkiness in animations on pages is something like the garbage collection engine for whatever reason it spun up, maybe it didn't spin up at a great time, but it spun up, and it caused some jankiness in one of our animations. It's just a reality of the way we live. So, this module, the purpose of this module is to bring to light the concept that you have a single thread and a whole bunch of stuff that you can force to try to happen all at once, and the more you do that, the more likely you are to have jankiness in your behavior. This is an interesting view of the developer tool in Chrome. These are animation frames, and it's a little bit hard to see in this text, but over here it says 30 frames per second and 60 frames per second. So when I first saw this view I thought oh that's interesting. That's a nice visualization of here we've got a bunch of really fast frames, and then all of a sudden a slow frame, and a bunch of fast frames, and then all of a sudden a slow frame. That's the way I first interpreted this. It took me about 30 minutes of working through this stuff before I realized that that's not what this diagram shows, which is why I wanted to bring it up. This frame actually shows, interestingly, a whole bunch of really slow frames, and all of the sudden a really fast frame, then a whole bunch of really slow frames that are all at like 18 frames a second, and then a couple of really nice fast frames, and then a bunch of slow. So it actually shows the inverse of it. And I went to try to dig into what's happening, there are some interesting things that were happening there, but I just wanted to show you that there's a developer tool that'll actually show you visually what your frames per second are, and you can see, oh man, that's why I got some --- I had a really slow frame there, or in this case that's why everything is slow because all of my frames are slow, except every once in a while I get lucky. In this particular case I think it was at that exact frame, for whatever reason, there was no update to happen, so it didn't have to do anything. I think it was during an animation where I moved something that didn't need to move, so it didn't have to do anything.

Threaded JavaScript
So, the question was asked earlier about threaded JavaScript, and the truth is that we do sort of have a functionality besides what I was talking about with River Trail, we do sort of have a functionality of being able to execute JavaScript in parallel, but there's a really giant caveat. It is an entirely separate JavaScript context. They do not share anything except an asynchronous communication channel. It's exactly the same concept as your browser and your server do not run on the same computer, so your code and your other code do not run in the same thread, and the only way they can talk to each other is asynchronously. The same is true here. We have what's called web workers. Web workers are a way for you to point out a particular JavaScript file and say I want for that file to explicitly run in its own thread separate from my thread, and I want it to run as fast as possible, so give it the system resources that it needs, and because it's in its own thread, no matter how long or how slow that works, it's never going to be affected by or affect what's going on over here in my thread. Super powerful technique that allows you to take long-running JavaScript code, say a calculation that takes a while, or maybe you're doing some communication with the server and there's some heavy data processing that's going on. That sort of heavy stuff may be happening at exactly the same time that you want to update a really nice, smooth CSS animation. I had exactly this scenario in that Wii Puzzle It game. I have got all these animations going of all these different people moving their pieces all around the screen, but at the same time I've got all this massive amounts of communication happening with the server, JavaScript needing to listen to event handlers and stuff, and what I want is the nice, smooth animation with all the nice, fast server communication. How do I resolve that? I put all of my socket communication into a web worker, and it runs in its own thread, and it runs very fast, and you almost never see a hiccup between the two. The downside, if you will, is that the communication layer between the two, you send a message and listen for a message, that communication layer becomes your bottleneck. So, if you're trying to do high throughput of that data, you can see problems there. But if you're not trying to do high throughput of data, like in my case I'm receiving all of these messages from the socket, and let's say I batch up all these messages that tell me all the different pieces that moved in the last 10 ms, I batch those all up for 10 or 20 or 30 ms, and then I send one message over to the browser UI frame that says update everybody. I do that roughly every 30 or 40 ms, and I'm going to get roughly, if I've got requestAnimationFrame running on the other side, he'll hear that within at least 16 ms, so I'm going to get roughly a 30 ms latency to an update, but a nice, smooth animation at 60 frames a second. So that piece will appear to have moved maybe 30 to 50 ms slower than it appeared for this guy who moved the piece, but I still had a nice, smooth animation of the game piece. That's how powerful web workers can be, and so you want to take advantage of these where you can. They're starting to become a lot more popular, but what does that code look like? Well, this is inside of our main code inside of our main page, the type of JavaScript that you'd normally write, and you can see here that what we're doing is instantiating a worker object here on this first line, and we give it a reference to a file. That's all you have to do to create a worker is instantiate a worker object and give it a reference to a file. The JavaScript engine takes care of wiring up all of that stuff so that there's a new thread spawned and that there's a nice good asynchronous communication channel between the two. How do we opt into the communication channel? Well, on line three we have an onmessage handler. We listen for the onmessage event, we get some data from the event, and then to send messages we do postMessage. That's literally the extent of the API. There is nothing more complex than that. Listening for messages, getting messages back, or sending messages, getting messages back. Well what about inside of long. js? What does it look like inside of the Worker? Listening for messages, sending messages back. That's all it is. That's the entire API. All the rest of this stuff is me showing you that you could do a really long-running calculation here and then send back the answer after you've figured it out, and in your exercise that we're about to go through you're going to see exactly this concept of a long-running operation. How do we offload that into a web worker? How do we let it run for maybe two or three or four seconds? It could take a really long time in some cases, but we don't care because the only downside is that it takes a little while for us to get our answer back, but at least everything continues to run smoothly inside of the main browser frame. So self is defined within the web worker as a ---. Yep, self is a reference to the worker itself. That's the only sort of special syntax, if you will, is it gives you that special global. When you fire up a web worker, it fires up its own thread. How is that --- if I make an asynchronous request of a webserver from just the main thread, what's the difference? The difference is inside of the main thread when you make a request to the browser, once it leaves the JavaScript engine, now it opts into traditional system programming in terms of threaded handling, and it can actually handle multiple network requests in the deeper layers, the bowels of your operating system and your browser. Once it leaves the JavaScript engine, it's capable of being parallelized and sent out and all of that stuff muxed in with your other network communications. All of that cool stuff is happening. But as soon as that communication needs to come back, it's taking advantage of all of this nice parallelism until it gets to the JavaScript engine, and then it hits a brick wall. Then it has to wait for the JavaScript engine to have a free cycle, and the JavaScript engine won't have a free cycle until the UI thread is free. So if that request comes back while a CSS animation is happening, you can't handle that request until the animation's done. The same is true that if you --- and by animation I don't mean the full animation, I mean painting the frame, but you can't get that request until the frame finishes painting. By the same token, if it is starting to process your request, and you've got a bunch of data in that request, and it takes a few 10s or 20s ms to process all that JSON data that you just got back, and the CSS engine is saying I need to update something, hurry up, you've now blocked your CSS. By contrast, inside of the web worker, the information comes back and forth as quickly as the threaded systems can handle because there's no blocking inside of your worker, and the only delay is that there may be this communication channel, so you can be much more judicious about what you send over that communication channel than you necessarily might be with an XHR request. Okay. Does that kind of clear it up? If that's the case, then I'm wondering why we wouldn't wrap everything in web workers and then all those asynchronous requests. Right, so I'll get to your question in just a second. The caveats to web workers are that at the moment the communication channel between the browser thread and the worker is string-based, and it's copy only. So if you have large amounts of data that you're trying to send, you actually end up with two copies of that data in memory for a brief moment while one copy is in the worker and one copy is in the browser tab until a garbage collector can come along and clean up one of them. There is a solution that's been worked out to that that's called shared objects. I think that's what it's called. It's going through some spec processes right now. But there's a solution that basically says whatever kind of object I have, whether it's a string or an object with properties or an array or whatever, I can essentially hand ownership of that object over to a different thread, and as soon as I hand ownership of that object over, it's no longer accessible to my JavaScript engine. It's an instantaneous exchange of the token of who controls that object. So, I put my stuff into a shared object, and I say I'm handing it over, post messaging it over to a worker, and immediately that object no longer exists for me; it's empty for me. And so in that way it doesn't have to copy the data. It can just simply change the reference, the memory reference, if you will. I don't know how it's actually implemented. It can change the reference so that the worker immediately has access to it. So as that becomes more standard, I think that's already in Chrome, but as that becomes more standard, that will be the way that we take more advantage of workers sending data back and forth, all that kind of stuff. That make sense? Yes. The other thing to keep in mind is that the browser doesn't guarantee you a new thread. It promises very strongly a new thread, but the browser still has final say-so about how stuff gets implemented. So it should be a new thread; it might just be a new execution context in the same thread. It is possible, but in almost all cases you're going to at least get one thread. But workers can create other workers, and you can create dozens of different workers, and you can easily, if the browser just had no controls on it, you could easily exhaust the system resources by spinning up thousands of workers in a heartbeat. So the browser has to have some sanity control on it and is not going to guarantee necessarily that all of your different workers are all going to be on different threads. What it will strongly almost guarantee is that you'll at least have a different thread for your workers from your main UI frame. Okay. Yeah? I have a question just on the garbage collection. We're going to get to garbage collection in just a moment. It's its own module, but go ahead. Well it's more about this subject here. Okay. So let's say you have spun off a worker. It's doing a ton of work, which means there's a lot of garbage collection. Is that going to affect the UI? Could the garbage collection be running for another thread and cause the UI to miss a beat? That's a great question. I'm going to make an educated guess. I'm actually not 100% positive, but about 90% positive that each thread gets its own garbage collector. Separate memory. Yeah, because they are using a separate memory space, but I'm not 100% positive on that. I honestly, like the implementation details, we'll get to garbage collection in a moment, but the implementation details of garbage collectors, I've never been able to understand from a system programming aspect why the garbage collector has to run on the same thread as the UI, but I've had people assure me that it must, so they're much smarter than I am, and I take them at their word. But the people that write these browsers have thought about this before, so I think it's tied to the thread.

Dynamic Memory Allocation
So I've said we were going to talk about garbage collection. Let's talk about that for a moment. JavaScript allows you to create variables, to create elements in your DOM, and then it allows you to not delete those variables, but to simply stop referencing those variables. So let's say you have a big object, a whole bunch of entries in your address book for instance, and let's say that you've got three different references to that address book, and then two of those references go away because they go out of scope, and now you've got one reference left, and then that reference goes out of scope. What happens to the object? The object does not immediately go away. Looking at this code, we've got object with lots of data, and then in this case it's not going out of scope, but I'm setting the reference to null. I'm not setting the object to null. I'm setting the reference to null meaning nothing still pointing at the object that the obj variable used to point at. So what happens to that object? It was a physical thing. In browsers, I don't know if you guys even know this, I worked at Mozilla for a while, and I was really surprised to see when you create a JavaScript object in the JavaScript engine, like in the V8 engine or in the SpiderMonkey engine, it literally goes and creates a C++ object that represents your object, and there are a whole bunch of layers or wrapping and stuff, and it's doing memory save typing and all this other junk that's happening. There's literally like eight or nine layers of abstraction. When you create this simple little object, you think it just sits inside of your JavaScript, but it's actually being represented in the C, C++ world as its own object reference, and they do all kinds of crazy stuff with that. So, that object physically exists. It's actually taking up a measureable amount of memory, so at this point we could say maybe it's a megabyte of memory that this really big object is taking up. What happens to that megabyte of memory when I stop referencing it from JavaScript? There's obviously a whole bunch of other places that it's still referenced, like in the C++ layer. The answer to that question is typically in dynamic languages is that somebody has to come along and clean up all that memory. You dynamically allocated memory because we didn't know before the program ran that this object would need a megabyte of memory, and now that we've dynamically allocated it, once we know that no one is referencing it, we are 100% sure that no one will ever reference it ever again. It's interesting to think once I stop referencing a variable, if that's the last reference to it, there's no way to get that one back. You can create another object that looks exactly like it, but you can't get a reference to that one back. But it does not go away immediately. It's not magical. It's not like the engine automatically and magically says delete that thing. The garbage collector is a process that comes along and says here are objects that used to have references to them and no longer have references. We need to reclaim that memory, put simply. I mean I'm not going to go into all of the deep complexities of how garbage collection algorithms work, and there's like mark and pass and all these other ones, I don't even understand them, but I'm certainly not going to go into all the details, but just at that level you need to understand that garbage collection is a reality in a dynamic language. What's the implication performance-wise? Well we've talked about the fact that garbage collection is one of those tasks that runs on the UI thread. That means the more often the browser thinks that the garbage collector needs to run, the more likely it is that your code will be affected negatively performance-wise. So, the conclusion is we need to write code that creates less garbage collection need. So how do we do that? How do we write code that doesn't churn through as much of these data needs? This is an illustration. Again, it's from that same Chrome developer tool, and you're going to get an opportunity to use this one here in the exercise in just a moment. A fascinating tool. Anybody want to guess what we're seeing here in this graph? Objects being created and then forgotten. What we're seeing is over time in this particular demo, over time I'm creating more and more objects. Notice that in each one of these there could be a drop-off immediately, but there isn't. I'm creating more and more objects and letting more and more objects go, and I'm using more and more memory up to about this 3. Now 3 MB is obviously very low, so it's not like hundreds of megabytes or whatever. The garbage collector is going to kick in way before that. But this is a night I was pleasantly surprised at how easily this was to visualize because I could not have drawn a better graph for you. That's exactly what's happening is that my memory usage is growing until an arbitrary point, not anything I did, because all this code, you'll see in the demo, all this code I'm creating and dereferencing, and creating and dereferencing here. At some magical arbitrary moment that I have no control over, the UI thread says it's time to get some of that memory back, and so we see a huge drop-off in the memory, and then it grows slowly again. It's a perfect example to illustrate to you how garbage collection works.

Exercise 6
With that all in mind, with the discussion about UI threads, we're going to go into the next to last module, and we have finished the next to last module, and this exercise, again, probably won't take the full 20 minutes, and we will take a little bit of a break, there's a natural break here before we do our final part of the day, but in this example you are tasked with looking at two different files that are doing, one is illustrating the UI threading problem, like stuff happening too much on the UI thread, and the other one's tasked with watching your memory usage, like I just illustrated, and seeing if you can reduce how much that garbage collector needs to run. Everybody understand that? Again, I will be available for questions. I'm going to put the instructions up here. By virtue of the fact that there are two, these instructions are a little bit longer, but they're not more complex. This is one of the simpler labs for you to do, or exercises for you to do. In the first one, thread. html, we've got an animation that's happening. I've got an image that's animating back and forth using CSS transitions, and while that animation is happening back and forth, it's just sort bouncing back and forth, while that is happening, I'm calculating Fibonnaci numbers, and I'm doing so in a really naive way because I know there's really highly optimal loop-based ways of Fibonnaci calculation. I'm doing so in the really na√Øve way that we learned in computer science using recursive calls, which means that I can go to on my computer, about 35-38 is the Fibonnaci number that I can calculate, and beyond that I create too many function calls that it takes too long, and it starts to slow stuff down. On your system, that number may be lower or higher. It's set at 40, and it resets. So it goes from 0, every 100 ms or whatever, it calculates the next Fibonnaci number, 0, 1, 2, 3, 4 up to 40, and then it goes back to 0. So when you run it, you should see at some point it start to really slow down your animation. It becomes really obvious really quick. And then as soon as it gets past 39, it goes back to 0, and everything goes back to fast again. If you're not seeing that because you've got a really super-awesome computer, and it doesn't slow down at all for you, just change the variable MAX_FIB from 40 up to 45 or 50. Change it in small increments until you start to see it. But be careful. If you set it really high, you're going to completely crash your browser, trust me. I want you to take a look at the code. I'm saying here to take a look at the code of how I've structured the requestAnimationFrame and the setTimeout. Make sure you understand in depth why I did it that way. What would've happened if I wasn't using requestAnimationFrame there? And then the last part of this part of the exercise will be what happens when we stick that long-running code into a web worker and we post message back and forth? Anybody have any predictions on what'll happen when we do that? Nobody? Ah, that's the easy answer. He said work faster. It won't slow down. It won't slow down that thread. It'll slow down a different thread. Well, I'll let you try it. I won't spoil the surprise. I'll let you try it. Would the numbers get out of sync with the animation? They're not supposed to be in sync. There's no implication of them being in sync, but you're hinting at exactly what will happen, so you're on the right track. Will everything still slow down because it's still eating up all the memory? That's a good guess. That's not correct, but it's a good guess. You'll just have to try it. It's pretty simple. I've already put the code there. You just need to call it. In the second part of the exercise, we just deal with garbage collection. You can see that I'm creating a bunch of DOM elements and JavaScript objects. I'm keeping references to those DOM elements. That's exactly what was showing you that diagram, that graph that I showed of the gradually rising, so what I want you to do on that part of the exercise is look at that code that's creating all those references and creating those DOM elements, see if there's a way to not create so many new references. In other words, is there a way to reuse references? We'll probably go for about the next 10 minutes or so unless you guys need more time, and then at 4 we will finish up our day by going through the final module, which is on jsPerf.

Introduction jsPerf
Our last module of the day is we're going to talk about jsPerf. For those of you who don't know, jsPerf is a web-based tool for running head-to-head tests of JavaScript snippets and testing the performance. So, for instance, you might want to test the performance of the common misconception that array join is faster than string concatenation. So you would create two different tests. You can see some tests listed here. This is from a templating engine shootout that tests the performance of templating engines. There are like 40-some-odd engines here that it tests, but you could have one test that would create a string by doing a bunch of concatenation of string literals, and then another test by putting them all into an array and calling array join, and you would run those two cases head-to-head. And the way this site works, it uses a highly-sophisticated library called Benchmark. js, which is the result of a whole bunch of research about the statistics of performance and how you know exactly how to average things together. It repeats things, and corrects for itself, and looks for the plus or minus margins. It does a whole bunch of stuff. That underlying library, Benchmark. js, is what jsPerf uses. It was developed actually out of the effort of jsPerf. It was written by Jonathan Dalton and a few others who've contributed to it. So Benchmark. js as a standalone project is highly valuable. Somebody was asking me earlier, I think it was a great point, what about doing sort of automated regression testing of performance? We do automated regression testing of functionality to make sure that things don't break. What about automated performance regression testing like, for instance, against our JavaScript? Let's say we a have common API function that always gets called, and it calls a whole bunch of other stuff, and just over time we want to make sure that we're aware of the fact that thing might be slowing down if somebody's working in some other part of the code base. Well one way that you could do that, in the same way that you would do queue unit or some other sort of unit testing, is you can employ a JavaScript-based library like Benchmark. js, set up a test similar to the way jsPerf does, and run that code over and over again. Every time somebody checks in code it runs that, and then it just outputs the information, and then you store that in your logs. And if you see over time that function's slowing down, you can start to tell people you can't slow it down or whatever. Firefox, for instance. At Mozilla they have basically a crazy sophisticated system where they every time a piece of code is checked in they spin up 300 or so compilations of the engine and all these different operating systems and stuff in these virtualized server farms, and one of the set of tests that runs on every one of those platforms is a performance benchmark. So they know how fast Firefox is supposed to start up in that particular platform scenario, and when they recompile because the code has changed, they rerun that performance benchmark to see how fast did Firefox start up, and they flag warnings if there's even a millisecond slower time, and those warnings are something that somebody has to come along and say yes that's okay that that's slower now because we intended to do something extra or whatever. They take that performance incredibly seriously, so can we as web performance industry professionals, can we do the same thing? And the answer is plainly yes, and I encourage you to do so. So, jsperf. com gives you a nice sort of interface to doing things, and it's a great way to say, maybe you're having an argument on a dev team, like no it's faster this way, or no it's faster this way. Hey, let the test answer the question. The test wins. If the test tells you A is faster, it doesn't matter what your logic tells you about B. A's faster. So, I highly encourage you guys to get really, really familiar with jsPerf. We do not have anywhere near enough time to go into all the quirks of it, but I do want for you, and the purpose of this last module is for you to be familiar with jsPerf, and so we are going to have an exercise that you're going to have an opportunity to do some, but before we do that I want to get you a little bit more familiar with jsPerf. Here's another part. It shows you all the performance graphs. So what you're seeing here with all those lines is each one of the different tests in each of the different browsers, so here's Chrome 22 and Chrome 18. This is crowdsource performancing. So I'm running on Firefox 18. When my results post in, they'll average in with the Firefox 18 results. When you run it on Chrome 24, your results will average in there, and so you start to get a really long list of all the different versions of browsers that have been tested because you create a jsPerf, and then you send it to all your friends and say hey, will you just run this? And it collects all that data and keeps that data around for you using Browserscope. Browserscope, by the way, it's a phenomenal service, you should also check it out too, but Browserscope's a way for you to send in your data about performance testing or whatever, and they keep track of it. They create graphs, and they have a whole API and stuff, so this is a perfect match for something like jsPerf.

JavaScript Performance Mythbusters
To get familiar with jsPerf, I thought that it would be useful to go and take a look, we won't go through the whole thing because it was an hour presentation in and of itself, but I want to go through a few of the performance tests that we did as part of this panel that we gave. We gave this panel at South by Southwest last year down in Austin, and then we gave this panel earlier this year at the Velocity Conference, which is the big web performance conference. It's got a lot of great reviews from people, and I hope that we'll continue to be able to give new versions of this and test new things. It was a lot of fun. But I just want to go through a few of the examples. Essentially what we did is we said here is a common statement that people make as a truism about performance in JavaScript. They say that array join is faster than concat. Let's test it, let's let the test speak for itself, and then we either confirmed plausible or busted those myths just like they do on the Mythbusters show. Again, I know that that's a really long URL for you to grab. If you want to try to pull in, this is a separate slide deck, I didn't pull those slides in here, so this is a separate slide deck, but it is available if you Google search for JavaScript Performance Mythbusters. And you probably do want to look for the one that says velocity in it. So, real quickly, just before we get into these, I want to explain something about looking at jsPerf results. Let's see if I can go full screen with this to make it a little bit easier to see. When you are interpreting jsPerf results, first of all, one single test run is not enough to make your decision. More is better. The more results you have, the more browsers you have, the better. That stands to reason, but it needs to be repeated because a lot of people do kind of forget that from time to time. So, don't just create one test, run it once and be like bam, done, because weird things can happen. Also, you see vastly different results sometimes between different browsers. You can have something that's like a million operations a second in Firefox, and it's like 100, 000 operations a second in Chrome, and you're like why? Well, it's differences with the engine. Now, I don't necessarily --- I think it would be immature and micro optimization for you to start optimizing for individual browser engines unless you happen to be in one of those rare scenarios where you're working specifically only for one browser, maybe some embedded browser or something. I'm actually working on a project like that. In that case it's okay, but in most cases you should try to look for the broad trends, and if you see vastly different behavior between the two, look for a way to get those results to be the same in both browsers. And the other thing to point out here is this statement I thought that John-David Dalton, the creator of the Benchmark that I talked about, Benchmark. js, the point of this test is to show that even though there are micro differences, there is no real world performance concern for using void 0 over undefined or typeof as most of lowest ops per second are still in the millions of operations per second. So here's what I want to illustrate to you. What he's saying here, which is a really important point to understand about jsPerf, if you run a test and the operations per second, that is the number of times it can run that test in a second, if that's a low number, say one test comes out at 1000 operations a second, the other one comes out 1300 operations per second, running it at those really low speeds means that comparatively speaking, the inverse, which is that test takes a while, maybe it takes 100 ms or more if you're only getting 10, or it maybe takes 10 ms to run, that's comparatively a drastically long period of time for any JavaScript code to run, 10, 15, 100 ms. So, on those lower operations per second numbers, the results that you get are far more valid and more useful real world. When you see two different things in those lower numbers differentiating by even a small amount, that can actually make a real difference in your program's performance. However, some of the things that people test on jsPerf are like var A=1, var A="1, " which is an insanely fast operation and hundreds of millions of times a second that operation will happen, and you are going to have to have hundreds of millions of those happening in an app, which never happens, before you'd ever start to see us getting over that 100 ms or 1000 ms threshold. So, even though what I said earlier is true that sometimes and oftentimes performance is a systemic problem, you do not want to immaturely major on the minor and focus on oh, well look, this one says it's 192 million operations per second, and this one says 191 million operations per second, therefore I should go with this one. That's not a proper way to use jsPerf because statistically speaking you're never going to run into a case where that difference actually matters, whereas you would likely run into a case where the difference between 1300 and 1000 matters. So keep that in mind. That's part of using jsPerf responsibly. He also said don't forget about hardware bias. This cannot be emphasized enough. The fact that it's crowdsourced means you have no idea when it reports Firefox 18, you don't know anything about that hardware. Just because it's running slower on that guy's machine, what if he has a crappy machine? So we don't know anything about the hardware that's running, and so you have to understand that there's got to be a lot of test results to average that kind of stuff out. That being said, there's still a lot of value to jsPerf, so let's take a look at a couple of the tests. I've mentioned this one over and over again, so we'll just take a look at this one. Hopefully this will click and open for me. I'm trying to make that a little bit bigger for you to see. So this particular test, string concatenation versus array join, sets up a really long test where it's doing, as you can see, a bunch of string literals being added together, and there's a reason why, by the way, he's doing so many of them as opposed to a few. Why is that? Because he wants for this test to take a little bit longer so the numbers are lower, so you actually can see real differences. Even with that, the numbers are astronomically high on this because string operations are really, really cheap. And here he's returning an array that he eventually way down at the bottom calls array. join. So, any guesses between string concatenation and array. join? The myth is, and the cult logic for a long time was that array. join always beats out string concatenation, and the reasoning behind that is because in JavaScript, as in other dynamic languages, there's this concept of immutable strings. What that means is that a string object once created can't be changed. So when you add two strings together, it has to create a new one and copy the contents of both into the new one. That's true of a lot of these dynamic languages the way they implement, or at least it used to be true. So the theory goes that if you're not doing all of that recreation of strings, you're certainly saving time, and therefore array join should be faster. Anybody believe that test to be true, that statement to be true? In Chrome or cross-browser? In all browsers. IE6 and 7 I think join is still faster. Way faster. Okay. But in more modern browsers we think that which one is faster? Concatenate a string. Concatenation. Anybody think that they're exactly the same? No. No? I think they're about the same. You do? Pretty close. You took the bait. Somebody had to. He's going to take your book away. Alright, so this one is a test that is so vastly skewed that the charts are even hard unless you go to Browserscope and zoom in, but just take a look. The red is the string concatenation, the array join is the blue, and as I scroll down here, and some of these browsers there were errors or that's why you can't even see stuff, but in some of these browsers, 314 million operations a second to, and I probably can't even zoom in on it, but it's so small it's in the thousands. The array join is so vastly slower in these cases than string concatenation that one could generally make the argument that even if it was way, way, way slower in IE6 and 7, it makes absolutely no sense to do it because in all the other browsers it's so vastly slower the other way. So that's one way to interpret these types of results. Here's a Firefox 9 at 434 million operations a second. Here's a really tiny one where Opera 12 was only 1, 731, 000 operations a second. What's the percent difference that you've got on the test? Let me go back, and I'll --- that's a good question. This is Firefox that I'm running these in, so it's the total number is slower than it is in Chrome. Chrome does tend to run things faster, but we'll see here it's like 99%. So, it's going to tell me here that the green one was the fast one at 90 million operations a second, and there's my margin of error, plus or minus. 44%. This one was the slowest. It was 100% slower at 5000 operations a second. So, if you just modified that test there and went down to three items in the array or in the string concat, is the percent difference going to be substantially off? No. What's happened here, this test has been edited. We can see that it's been edited several times. People have been trying to --- this test is almost so vastly unfair, it's so wrong to do it one way versus the other that it's hard to glean any useful information from these charts other than to say never use array join. That's literally the only conclusion that you could get out of this, and so people have been trying to create scenarios that are a little bit more reasonable, and that's part of the reason why they created a whole bunch of strings because there are things that browsers do internally with caching of string operations and things like that, which can throw off jsPerf. The first run it might be slow, and then the browser is like oh, now I see that that function every time it's called it does this, and I can highly optimize that, and you start to see the tests getting skewed, and so there's a technique in jsPerf where you add on a random number into a string just so you can sort of bust some of that caching and get a better, clearer result. But the reality is the browser is doing that because the browser's trying to optimize things that it thinks people are doing often, which is a good and helpful thing, and we should not handcuff the browser from doing that. Let's go back to here. We'll look at maybe just another. Like you guys said, we found that it was busted, but that it was plausible in IEs less than or equal to seven. This was an interesting one, and so we'll probably do this one, and then we'll finish, and you guys can go and look at more of these, but this was an interesting one. There was an article that came out from Opera a while back, and it was saying that in browsers when you have --- so you have to understand in JavaScript you have the concept of a string literal, like the little quotes with some characters in it, and then you have a string object, which has methods on it. And in JavaScript when you have a variable that's just a string literal and you call a method on it, internally the browser has to cast that to an object before it can call the methods on it, which sounds like extra work. So, there was this article that came out from Opera that said if you are going to be frequently calling methods on your strings, like your replaces and those other ones, you should declare those as string objects rather than as string literals, the native literals, because the browser will have to do less work casting it every time, and then if you're using it as a string literal in a couple of cases, then it can downcast there, but it'll have to do a lot less work, therefore would be a lot faster, so we thought this was a perfect example of something that we should test. And the logic behind that is very sensible. Less work means it'll go faster, right? I've already betrayed the end results, but here's that article, by the way. I quoted from the article, but you can go to this jsPerf is you want and take a look at more of the article. So, what we set up was a string that was a string literal and a string that was a string object, and we called the charAt function on that string object over and over again, millions of times a second, and we said in S1's case it's going to have to convert it to an object every single time before it can call that method, so theoretically the simple string should be a lot slower. Anybody want to take a guess on what happened? They were the same. They were the same? That's a good and interesting guess. It's doing lazy caching of it. Could be doing lazy caching. The interesting thing is that the simple string was not only the same, but vastly superior to the string object, which seems quite opposite the way you would logically reason about this. Why on earth would it be much, much faster for the browser to do much, much more work, and when we presented this the first time at South by Southwest, we didn't have an answer to that question. We said I don't know why, but clearly strings are better, so clearly we've busted the myth that article came out with. Well luckily, one of the main developers for Firefox, Boris Zbarsky, was in the audience at South by, and he grabbed the microphone and stood up and said I can tell you exactly why that's true, and it's true of all the browsers. It essentially comes down to them paving the cow path. What they saw was that people were doing this sort of thing with strings, the string literals, all the time in their code. Nobody was doing the string object. They were doing the string literals and calling methods on it, so they went in and put special code in their JIT optimizer to recognize that case early and optimize for it. And so they were able to --- it's not that they made the other ones so slow. They were able to optimize this one so much better because they put special emphasis on it. That's why this one far outperforms the other even though it's completely contrary to the normal logic that we would have not understanding how the browser worked. This is an example, I think a really good example of why it's important to test stuff and not just try to reason about things. This is a very valuable tool in your toolset.

Exercise 7
We're at 4:30, and I encourage you guys to spend a couple of minutes taking a look at this example. It will not take you 15 minutes. This tells you to go to a jsPerf that I've already created a shell for. It tells you to go to that and fork your own and do a couple of tasks, so you try a couple of things. This should be pretty straightforward, but just test it out, run it a couple of times and see. So you should be able to get through this in just a couple of minutes, but if anybody, again, if anybody has questions, let me know. And after this we're going to be finishing up, so don't worry too much about the time. We're just wrapping up. Let me also say while you guys are doing that, so I'm not taking up time at the end, earlier we did a couple of different examples and labs where the performance of something, namely the object- oriented lab that we did where we were removing abstractions from object-oriented code, there was an assumption made there that there are performance benefits to removing the object-oriented code. I have tested that assumption on many occasions, and I can say with confidence that it is true. When you reduce the amount of prototype chain lookup, when you reduce the amount of function calls and nested recursion and all of that, you do drastically improve the performance significantly and in a useful way. We made an assumption with that code that we had already identified by some other means that particular piece of code was in the critical path and was something we should work on. If we had not determined that, we would not necessarily just go pecking around at our code and immaturely changing things just because we felt like it. Although I would like to remove abstractions, it's not always the best answer. It's the best answer when we need that performance and when it will help. So, a responsible way to approach things would've been to figure out that it's in your critical path, and then to use the jsPerf techniques that we just talked about to test it before and after. You must benchmark what you're doing. So I leave that, sort of as you work through jsPerf keep that in your mind, I leave that as your sort of homework from this workshop, is to go back to that example, I think it was example four of that exercise, go back to that, and try out the original code in a jsPerf versus the code that you optimized and see what the performance differences are. Alright, we'll take a couple of minutes for you guys to work through that. If anybody has any questions, let me know.

Conclusion
Conclusion
Alright, I know that some of you may still be working on that. Hopefully you got your feet wet with jsPerf, hopefully I've inspired you to do this more, and you'll get better at it as you go. You'll write bad tests, and then people will come along, and they'll edit and fork your test, and they'll fix it. It's exactly like an open source development. You write a code, and put it out in a project, and then people come along and help. The same thing happens here. That's the value of crowdsourcing this stuff, so cannot speak highly enough of the value of jsPerf as one of your most important tools. That's another tab that I just keep open persistently in my browser. So, long day, a whole bunch of stuff I threw at you. Some of it I imagine that you agreed with. There's probably some of it that maybe you're still kind of scratching your head or you're not sure you totally agree with. That's totally cool. My goal from today was not actually, even though this was a workshop about teaching you things, my goal was not to teach you a specific piece of knowledge, although I hope you came away with a lot of pieces of knowledge. My goal was really to train you to think like a web performance engineer. Whatever your job title is, it's important for you to take performance as a default measure in the way you approach your code. There's a whole bunch of different tools, a whole bunch of different techniques, and you will take some of them and leave others, and that's okay, but I believe, and I hope that if you do that in good faith, that you will produce better, more quality software that will have a lower cost of ownership and will perform better for all the users. So I leave you again with what we talked about. In middle-end we talked about YSlow, so remember. In the resources we talked about how to optimize resources. There are a bunch of tools. Talked some about architecture and communication, resource loading, preloading, post-loading. We talked about removing abstractions, moving JavaScript animations into CSS, taking care of the UI thread and the garbage collection in a responsible way, and finally we talked about the tool jsPerf and how to actually test performance of JavaScript. And I leave you with the quote that I said earlier. This is all about maturity, and so the next time that somebody tells you premature optimization is the root of all evil, rephrase that for them, share with them that this is about understanding what parts of performance optimization are valuable and what parts are sort of in that fluff area. So I appreciate very much. It was a huge honor to come and teach this workshop. I hope that it was helpful. I genuinely do mean, when I say here's my contact info, this website has all of my contact info on it, everywhere that I can be found on the web, I'm found under getify, and I genuinely do hope that you will reach out and ask questions and share feedback. I want to make this better if I can share with other people, so please do provide that feedback. Either way, you can tell me I was an idiot, and that's totally cool, but I appreciate very much you having me here. Thank you Mark for hosting me, and go out and make the web faster.

Course author
Author: Kyle Simpson	
Kyle Simpson
Kyle is a freelance developer based in Austin, TX. He runs several open-source projects (such as LabJS), writes books, and speaks at meetups and conferences.

Course info
Level
Intermediate
Rating
4.5 stars with 135 raters(135)
My rating
null stars

Duration
5h 3m
Released
29 Mar 2013
Share course