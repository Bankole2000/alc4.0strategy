Streamlining API Management Using Google Apigee
by Janani Ravi

This course is about working with Apigee, an API management platform that enables providers to design, secure, deploy, monitor, and scale APIs. Acquired by Google in 2016, Apigee is popular and works seamlessly with Google App Engine.

Monolithic architectures are passing out of vogue these days and are increasingly being replaced by more modular service-oriented architectures with specific APIs for different purposes. Consequently, APIs are becoming valuable resources, and regulating access to APIs and monetizing their usage is becoming very important. In this course, Streamlining API Management Using Google Apigee, you will gain the ability to build, deploy and fine-tune API proxies to enforce policies and regulate access to your APIs on the Google Cloud Platform. First, you will learn the powerful features and often underestimated advantages of using a fully-fledged API management platform like Apigee where you can create policies to administer quotas, authorize users, charge for the usage of your APIs, enforce limits on usage and protect against security threats. Next, you will discover how to create, deploy and undeploy API proxies using Apigee Edge. Then, you will take advantage of Preflows, Postflows, and ConditionalFlows, which are ways to specify logic that the Apigee Edge Proxy will enforce. Finally, you will explore how to integrate Apigee with Google App Engine. When you’re finished with this course, you will have the skills and knowledge of Apigee Edge needed to protect, monetize, and fine-tune your APIs.

Course author
Author: Janani Ravi	
Janani Ravi
Janani has a Masters degree from Stanford and worked for 7+ years at Google. She was one of the original engineers on Google Docs and holds 4 patents for its real-time collaborative editing...

Course info
Level
Beginner
Rating
4.9 stars with 14 raters(14)
My rating
null stars

Duration
1h 52m
Released
10 Jan 2019
Share course

Course Overview
Course Overview
Hi, my name is Janani Ravi, and welcome to this course on Streamlining API Management Using Google Apigee. A little about myself, I have a master's degree in Electrical Engineering from Stanford and have worked with companies such as Microsoft, Google, and Flipkart. At Google, I was one of the first engineers working on real-time collaborative editing in Google Docs, and I hold four patents for its underlying technologies. I currently work on my own startup, Loonycorn, a studio for high-quality video content. Monolithic architectures are passing out of vogue these days and are increasingly being replaced by more modular, service-oriented architectures with specific APIs for different purposes, which means APIs are becoming valuable resources, and regulating access to APIs and monetizing their usage is becoming very important. In this course, you will gain the ability to build, deploy, and find-tune API proxies to enforce policies and regulate access to your APIs on the Google Cloud platform. First, you will learn the powerful features and often underestimated advantages of using a fully fledged API management platform like Apigee. Using Apigee, you can create policies to administer quotas, authorize users, charge for the usage of your APIs, enforce limits on usage, and protect against security threats. You can also make these endpoints easily available for access using REST, SOAP, and various other protocols with minimal effort. Next, you will discover how to create, deploy, and undeploy API proxies using Apigee Edge. These API proxies are the primary mechanism for the checking and enforcement of policies and have complex internal architecture, including target endpoints and proxy endpoints, which are specific processing carried out with both incoming requests as well as outgoing responses. You will also learn to take advantage of preflows, postflows, and conditionalflows, which are ways to specify logic that the Apigee Edge proxy will enforce. Finally, you'll explore how to integrate Apigee with Google App Engine. When you're finished with this course, you will have the skills and knowledge of Apigee Edge needed to protect, monetize, and fine-tune your APIs.

Getting Started with Apigee
Module Overview
Hi and welcome to this course on Streamlining API Management Using Google Apigee. Apigee is a fully fledged platform devoted entirely to helping you manage the APIs that you expose to developers. You might want developers to consume your backend services. Here is a full lifecycle API management platform helping you expose these APIs in a secure way. Apigee Edge is the product that we'll use which allows us to secure, deploy, monitor, and scale the APIs that we expose to developers. When you use Apigee Edge, you'll find that you'll be able to configure and manage your APIs completely separately from the back- end service which exposes these APIs. Apigee allows you to create and deploy API proxies which act as a façade in front of your backend service and control access to your backend applications. In addition, Apigee also allows you to specify processing stages, or flows, to control the processing of the requests and responses that flow through your APIs. Apigee also allows you to create API products where you can bundle multiple APIs together, apply security policies, and monetize your APIs. Apigee also offers powerful analytics functionality which allows you to view how your APIs are performing and how the traffic to them flows.

Prerequisites and Course Outline
Before we dive into the actual course contents, here are some of the prereqs that you need to have so that you can make the most of your learning. This course assumes that you're familiar working with HTTP and HTTPS requests and you know how to hit these APIs using a command-line tool such as cURL. This course also assumes that you know how to expose services in the form of RESTful APIs and you have a basic understanding of cloud computing. We start this course off with an introduction to Apigee, which allows us to perform lifecycle management for our APIs. We'll then go on to a hands-on module where we'll see how we can deploy proxies using Apigee Edge. We'll see how we can configure API policies and conditional flows, use the test and production environments in Apigee, and create proxies using the OpenAPI Specification. We'll follow this up and see how we can build backend services on Node.js and host them on Apigee using hosted targets. We'll deploy our Node.js applications on Apigee and use custom analytics to analyze our APIs. We'll then see how we can expose our App Engine applications using API proxies on Apigee. We'll also see how we can secure access to these APIs using either API keys or OAuth. Through all of the demos of this course, we'll assume that we're engineers working at a hypothetical online retailer called SpikeySales.com. This retailer specializes in flash sales of trending products, which means they have huge spikes in traffic on sale days. SpikeySales is considering a move from their on-premise datacenter to a cloud platform, specifically the GCP. Cloud computing fits their bill perfectly. They want pay-as-you-go functionality for their resources, and they don't want to hold idle capacity during off-sale periods.

Introducing Apigee
So what exactly is Apigee all about? Now if you're developing a product or a feature, you might want to expose APIs that developers can consume. You can think of an API as a gateway to a platform or a service. It's a way for an application to invoke code or consume data from another application. When you build an API for another developer to consume, it should be easy to access. You should reuse code and other policies, and you should make sure that APIs are built for specific use cases. They're not too general purpose. APIs are an integral and significant part any developer's professional life, which is why Apigee exists. Apigee is a company that was built to allow you to manage your APIs for its entire lifecycle. It's a lifecycle API management platform and was acquired by Google in 2016. Apigee is now an integral part of the Google Cloud Platform. Within Apigee, we are focused on Apigee Edge. Apigee offers a number of other products. The Apigee Edge management platform allows developers to build and manage API proxies, where proxies are a façade in front of your API. You might choose to build your backend service on any platform, on your on-premise datacenter or any cloud computing platform. An API proxy that you configure on Apigee is a program that sits in front of your API and proxies incoming user requests to the APIs and provides many value-added features. With API proxies on Apigee, you can configure policies for quota management to control the number of requests to monetize your APIs and so on. Let's say you're a company such as Facebook, and you offer a number of APIs to developers. All of these client applications might access your backend databases from which they'll access data or services that run on your app servers. The APIs between all of these backend databases and services imply a contract between the services and the applications that consume these services. Once you've exposed these APIs and developers have used them within their products, if you're constantly adding new functionality to your services, it's possible that APIs might change, and this can be pretty painful for developers. Some of this pain can be mitigated by using a platform such as Apigee Edge. These are API proxies on Apigee which sit in front of your API. They act as a façade in front of your API. API proxies are configured on Apigee Edge, and these API proxies decouple the app- facing API that developers use from the backend services that you maintain. This way, you can deploy a new version of the backend service without changing the API, or you can change the API without deploying a new version of the backend service. Building these facades, or API proxies, is very, very easy when you use Apigee Edge. These API proxies are not just dumb request routing mechanisms. You can use these proxies to secure calls to your APIs and the data that you pass in. You can configure these proxies to use API keys or OAuth in order to control access to your APIs. Apigee also allows you to process the requests to and responses from your APIs. You can manage and throttle traffic to your APIs using quotas and REST policies. You can also use Apigee to bundle several API URLs together to form an API product. You can then set and enforce policies on this product. An API product on Apigee is simply a collection of API resources that you create, and you'll combine these resources together with a quota or a service plan, and may be protected with an API key. An API package on Apigee is a collection of collections. It's a group of API products which you'll monetize together as a bundle.

Endpoints and Flows
Before we can get hands-on with Apigee, it's important that we understand key terminology and some of the concepts that are involved. Here we'll study endpoints and flows. When we create and work with API proxies on Apigee, you'll find that there are two types of endpoints. The first of these is the ProxyEndpoint, which defines how your client applications consume your APIs. This is the endpoint that is directly accessible to the users of your API. The second kind of endpoint is the Target Endpoint, which defines how the API proxy interacts with the backend service which performs the actual processing logic. When a client or user request comes in, it will first hit the ProxyEndpoint and then move on to the TargetEndpoint. And responses will move from the TargetEndpoint towards the ProxyEndpoint. Here is a big-picture architecture overview of how API proxies are configured on Apigee. Requests from our HTTP client come in from the left-hand side of this diagram, and our backend sits on the extreme right. You can see that the request that flows in from our client app first hits the ProxyEndpoint. And after the ProxyEndpoint is when it moves on to the TargetEndpoint. Responses which flow back from the backend service flow from the TargetEndpoint toward the ProxyEndpoint, after which it flows back to the user. You can see that the components in the ProxyEndpoint are basically flows, which are the processing stages for your request and response. The flows at the ProxyEndpoint are the various processing stages for preprocessing the request before it reaches your backend service. This can perform processing of the following types, whether your apps are accessed over HTTP or HTTP, what security, quota, and other policies apply to the requests that reach your API. All of these are part of the flows of our endpoints. The ProxyEndpoint also contains stages for post-processing the response before the response returns to the application. We can go back to the main overview diagram and take a look at the TargetEndpoint. The TargetEndpoint interacts directly with the back end which exposes our service. And the individual components of our TargetEndpoints are flows which form our processing stages. The TargetEndpoint is responsible for forwarding requests to the correct back end, to the back end that we have configured. The TargetEndpoint can also be used to format the response that we receive from the backend service in the correct format for our applications. Both the Target, as well as the ProxyEndpoints are made up of flows. These are called PreFlows or PostFlows, depending on when they process your request or response. Flows in Apigee are sequential stages in the API request processing path. It controls the flow of a request with logic, conditional statements, and error handling. Within these, conditional flows are executed based on a certain condition. They won't be executed for all requests. There are four basic types of flows in Apigee, PreFlows, conditional flows, PostFlows, and PostClientFlows. Let's take a look at each of these in turn, starting with PreFlows. These are policies, or flows, that are executed before anything else happens. PreFlows can be part of the ProxyEndpoint or the TargetEndpoint. This is processing that occurs before the ProxyEndpoint is hit or the TargetEndpoint is hit. PostFlows are executed after conditional flows and PreFlows, and these are great for extracting log data and for notifications. Conditional flows are executed, as you might imagine, conditionally. They are not always executed. They're executed after the PreFlow and before the PostFlow, and only one conditional is executed per segment. The first flow that evaluates to true is what is executed in the conditional flow. The PostClientFlow is the processing stage where you typically log messages after the response has been returned to our client. It's only used in proxy endpoints.

Policies and Other Features
Policies are what we use to implement processing stages in Apigee. A policy is a module to control API behavior. It implements a specific, limited management function as a part of the request or the response flow. A policy is what applies a certain process to our requests and responses. Once you deploy an API proxy on Apigee, you'll create and associate a bunch of policies with your flows. These can be as a part of the PreFlow or the PostFlow processing. Apigee has support for many, many common built-in policies. You can enforce policies for quota management using API keys for authorization to your APIs for access control, for response caching and transformation, and a bunch of other things. When you deploy API proxies using Apigee, there are a number of useful add-on features available as well, such for security. You can configure special features to protect against cross-site injection, cross-site scripting, and cross-site request forgery attacks. You can also configure Apigee to work with spikes in user traffic that you might experience. You can use Apigee for dynamic routing, caching, and rate limits on your APIs. Apigee also makes it very easy for you to monetize your APIs. You can bundle a number of APIs together and set up an API product, you can offer flexible rate plans, you can track usage and API keys, you can offer tiered access to different bundles, and you can specify varying rate limits and pricing. Apigee also offers many useful tools for performance management. You can track active developers and apps, you can track the response time and latency of your APIs, you can track error rates, revenue metrics, traffic, and so on. In the context of Apigee, when we speak of users, we refer to developers of backend services who want to then expose APIs to consumers. Consumers are developers who access those backend services using API proxies configured on Apigee.

Organizations and Environments
In this clip, we'll briefly talk about organizations and environments in Apigee. An organization is a top-level container in Apigee Edge and contains all of the API proxies that's we'll configure. When you first create an account on Apigee, it's associated with an organization. An organization also serves as an isolated sandbox where you can deploy hosted targets such as Node.js applications. When you configure API proxies on Apigee, here is the basic structure of a URL for your API proxy. You'll see that it has the orgname as a prefix. In addition to the orgname, this URL includes the environment in which this API proxy was deployed, whether it's the test environment or the production environment. An environment in Apigee is the execution environment for the API proxies. An organization can contain multiple environments. You'll develop in the test environment, iterate on your API proxy, and once you finalize what the proxy looks like, you'll then deploy it to production. The URL for your API proxy will also include the environment right after the orgname.

Creating Cloud Functions as Backend APIs
Before we use Apigee to build our API proxies that will process the requests before they reach our backend services, we need to have a few backend services, and that's what we'll create in this first setting-up demo. We'll set up a few cloud functions on the GCP, and this will serve as the back end for our proxies. For most of our demos, we'll be on the Apigee site. The URL is a little different. Here we are on the Google web console at console.cloud .google .com, and we are currently working in the spikey-apigee project. Make sure that you have billing enabled for this project so you can use Cloud Functions and anything else that you need. Use the navigation menu which you get by clicking on the hamburger icon on the top right, and head over to APIs & Services, Library. In order to create our serverless Cloud Functions, the equivalent of Lambdas on AWS, we need to enable the Cloud Functions API. Click on the ENABLE button here, and your Cloud Functions will be enabled. Cloud Functions are serverless functions in that you don't have to provision infrastructure to use them. They're great if you want to write code which is responsive to external events, such as to HTTP calls or to some changes in some GCP service. Let's head over to the Cloud Functions page using our navigation menu and create our first function. I'm going to call this first Cloud Function get_spikey_message, accept the default values for other parameters. This is going to be triggered by HTTP calls. These HTTP calls are what API proxies built on Apigee will make to our Cloud Function. I'm going to use the inline editor in order to add code, and I'm going to write code using Python 3.7. The code for all of our Cloud Functions is going to be very straightforward. We simply have a simple function which returns a response to the client in the plain text format. Greetings from Spikey Sales! This get_spikey_message function will be executed when our Cloud Function is triggered by our HTTP. Click on Create, and create and deploy this first Cloud Function. If you click through to the Cloud Function, you'll be able to view various bits of information about this function. The Trigger tab will show you how you can invoke this function. Simply click on this HTTP URL, and you'll see Greetings from Spikey Sales! This is the URL for one of our backend services for which we'll build API proxies. Going back to our web console, activate the Cloud Shell. The Cloud Shell is an ephemeral VM on the GCP which gives you a terminal window with all of the command line utilities that you need on the GCP preinstalled. Cloud Shell makes it very easy to run commands against your GCP resources from within your browser window, and that's what we'll be using in this course. I'm going to run an export PS1 command to change the prompt of my Cloud Shell so that long commands will be visible on the same line. This is completely optional; you don't have to do this if you don't want to. We'll now use the curl utility, which comes preinstalled on Cloud Shell, in order to hit the REST API that we just created using Cloud Functions. Here is the full URL path to our get_spikey_message Cloud Function, and we're passing in content which is urlencoded content. That's the content type. And here is the greeting that we get in response. All's good here. Let's head back to our Cloud Functions page and create a new function. I'm going to call this. get_spikey_customers. Once again, we'll write some Python code for our Cloud Function, import some Python libraries that we need to set up our Cloud Function. Specifically, we'll use the dicttoxml library to return a response in the XML format. The name of this function is called get_spikey_customers. We get parameters in the request in the JSON format, which we then parse and return a response in the XML format. We simply return the same CustomerID and CustomerName information to the client in the XML format. In the requirements.txt file for this Cloud Function, make sure you specify the dicttoxml library, as well as flask, which is what we've used here. The function to execute is get_spikey_customers, and here is our Cloud Function all set up. We'll now test this Cloud Function by making a POST request using curl on the command line. Here is the URL for our function, and we pass in content in the JSON format. We pass in a CustomerName and CustomerID. Notice that the response is in XML format. We get the same CustomerID and CustomerName, but the response is in XML. Let's go back to the Cloud Functions web console and create one last function that we can use. I'm going to call this get_spikey_products. Accept default values. Once again, we'll write code in Python 3.7. Set up imports for the Python libraries that we need. This time, we're going to accept request parameters in the XML format and return a response to the client in the JSON format. We parse whatever XML data is passed to us in the request, convert it to JSON, and return it back to the client. In the requirements.txt file, you only need specify the xmltodict library. Here is the function that will be executed when this Cloud Function is triggered, get_spikey_products. Click on Create and wait for this function to be up. Once again, we'll switch tabs to the Cloud Shell command line in order to test this using a simple curl POST request. The content type that we pass in for the request is text/xml. The request that we pass in contains a product id and a corresponding name, and the response that we get from this Cloud Function is the same information in the JSON format. We now have three Cloud Functions that will serve as APIs to our hypothetical backend services, and we'll set up Apigee API proxies for them.

Creating an Apigee Account
With our backend APIs all set up, we're now ready to start working with Apigee. In this demo, we'll create an Apigee evaluation account, for which you do not need to specify a credit card number or set up any billing. Simply head over to login.apigee .com /sign_up, and get set up with your account. This will walk you through a very simple series of steps that you need to follow in order to create an account. Make sure that you have a valid email address. I'm going to use spikeysales@ loonycorn.com. Click on the Create Account button, and wait for your account to be created. They've sent us a confirmation email, and to click through to that confirmation email, you'll be able to activate your account. Once you're logged in to your Apigee account, you can start exploring what it has to offer, specifications, API proxies, the documentation portal, the community around Apigee, and so on. I'm going to close the email tab that we had opened earlier and focus only on Apigee. You can see that we are logged in as the Network Security Proxy Admin. That is our first name and last name. Apigee requires organization details. Based on the information that we specified, our organization is spikeysales. Notice the -eval indicating this is an evaluation account. The Apigee UI can be navigated easily using links on the left nav pane. The Develop tab will give you a bunch of options. Here is where you can create your first API specification using the OpenAPI Specification. The API proxies will show you what proxies you have currently set up. Apigee comes with two built-in proxies, OAuth and Hello World. We'll create our own in just a little bit. Don't worry. There are other useful tabs here. The Publish link will show you how you can publish your API products so that developers can start consuming your APIs and you can start monetizing them. Apigee offers many services. There is a tab for analytics, the different kind of analysis that you can perform on your API traffic. There is a tab for administration. There are a whole bunch of things here, and we'll be exploring them in the next few modules. And with this, we come to the very end of this introductory module on Apigee. Apigee is a full lifecycle API management platform that was bought over by Google in 2016. The Apigee Edge product allows us to secure, deploy, monitor, and scale the APIs that we expose to developers. When we use Apigee Edge, we'll create and deploy API proxies that act as a façade in front of our backend service and control access to our backend applications. In this way, we can configure our APIs separately from the actual back end. Changing an API does not require us to redeploy our backend code. In addition, we can configure processing and policies on API requests and responses. Apigee allows us to bundle APIs together and sell them as a product. It allows us to configure security, monetization, and analytics on our APIs. In the next module, we'll get hands-on with Apigee. We'll create and configure API proxies to work with our back ends. We'll also create policies and attach them to request and response flows.

Deploying Proxies with Apigee Edge
Module Overview
Hi and welcome to this module where we'll get some hands-on practice with Apigee. We'll see how we can deploy proxies using Apigee Edge. Now that we are signed in to the Apigee platform, this module is completely hands-on. We'll see how easy it is to build and deploy proxies on Apigee. Once we have the proxies configured, we'll attach specific policies to the request and response flows of these proxies. We'll add policies for rate limiting, quota management, and to transform our responses the way our clients want them. We'll also see how we can use the trace tool on Apigee to trace the path that our requests and responses take and how they're transformed along the way. We'll study how we can configure conditional flows for specific paths of our API, and we'll use these flows to help implement logic inside a proxy using JavaScript code. We'll work in the test and production environments that Apigee sets up for us by default. We'll use the test environment to test our proxies and finally deploy these proxies to production. In addition to manually configuring proxies, we'll also create a proxy from an OpenAPI Specification YAML file.

Creating an API Proxy
We are now ready to create and test our very first API proxy in our Apigee evaluation account. We start this demo off in the API proxies page, which we had explored in an earlier demo. You can see that there are two proxies already available to us, OAuth and Hello World. These are proxies that Apigee set up by default. We'll be creating our own proxy for our backend service. Click on the +Proxy button on the top right in order to get started. The first thing you have to do is choose what kind of proxy you want to set up. There are a number of options available here you can set up, a RESTful or pass- through proxy for a SOAP service, you can set one up with no target, and a number of other options that we'll explore later on. For now, we'll go with the simplest and most common proxy. That is a reverse proxy. It simply routes inbound requests to backend services. This will allow us to explore a lot of the options that we discussed earlier, flows, policies, and so on. Check Reverse proxy, click on the Next button, we'll be taken to a page which you can then use to specify the details of this proxy. The name of this proxy is the customers_cf_apiproxy. Cf refers to Cloud Functions. The URL base path for the proxy is the same as the proxy name, customers_cf_apiproxy. The existing API that this proxy will call is our get_spikey_customers Cloud Function that we had created earlier. You can provide a description if you want to. It's optional here. Click on the Next button, and you'll then be asked whether you want to secure access for users and clients. In this case, we'll keep things simple and choose Pass through, no authentication or authorization. The next page will give you an idea of what your URLs will look like. These are the URLs for the API proxies which we'll call in to our backend service. The first thing is for you to decide whether you want to expose your APIs using HTTP or HTTPS. The default is HTTP. Secure is HTTPS. You can also see what environments your proxy will be deployed to. Apigee has pre-created environments with different URLs. There is the test and prod environment that is set up for you for default. You'll use the test environment to iterate on your API proxies, and finally, when you're happy with the setup, you'll promote that proxy to the prod environment. For both our default and secure URLs, we want to use both environments, and here under HOST ALIASES, you can see what the API to our services looks like. Notice that we have the spikeysales-eval prefix for every alias. This is because we're currently using the trial account. Once you agree to a paid account, you can simply use the name of your organization as your URL prefix. Click on the Next button to continue configuring your proxy. This will take you to a page which will ask you which environment you want to deploy this proxy to. We're still testing out our proxy here. We don't want it visible to the external world, which is why we'll go with the test environment. Click on Build and Deploy, and your first proxy will be deployed to Apigee. With this out of the way, we're ready to move on to the fun stuff. Click on customers_cf_apiproxy, and this will take you to the proxy editor on Apigee. This is what we'll use to specify flows and policies that we want to attach to our proxy. Let's briefly explore some of the options that we have available here. Under the project navigation menu, you'll find options to save this current version of the API proxy as a new revision. You can also save this as a completely new API proxy, delete the current revision, and perform a number of other actions. Every version of a proxy that you deploy can have multiple revisions, and you'll see what revision you're currently working on if you take a look at the revision menu. We've just started, so we're working on Revision 1, and you can see that this revision has been deployed to the test environment. For more details on which environment this revision has been deployed to, you can take a look at the Deployment drop-down menu. You can see that the test option is clearly highlighted in green here. The very basic proxy that you just created has two deployments to the test environment. If you expand that plus button there, you can see that we have a default deployment for HTTP and a secure deployment for HTTPS. You can see that spikeysales-eval is the prefix for this URL, and at the suffix, we have customers_cf_apiproxy. That is the proxy base path that we had specified. We are currently on the overview page. If you want to make changes to your API proxy, you need to click on the DEVELOP tab off to the far right, and this is what will bring up the editor that you can use to change how this proxy behaves. You can see here that we have the ProxyEndpoint, as well as the TargetEndpoint, and within this, you can see the PreFlows and PostFlows that are applicable to this particular proxy. Remember that these are the stages that make up a proxy processing path. This is the processing logic that is applied to the request that you make to the proxy and the response that you receive from the proxy. When you make a request to the proxy, you first hit the Proxy Endpoint, and here are the PreFlows and PostFlows that are configured for the proxy endpoint. And here are the corresponding PreFlows and PostFlows for the target endpoint. The TargetEndpoint in this case is our actual backend service, the Cloud Function that we had configured on the GCP. When we are configuring the various flows for our proxy, we'll be using the UI editor a lot, but in the end, all of these flow specifications are in the form of XML, as you can see highlighted here in this editor. We'll get slowly familiar with the XML specification here. You can see that the virtual hosts available for this proxy are the secure and default hosts. These correspond to how we want to access our API using HTTP, as well as HTTPS. The processing that we'll do on the API request and on the corresponding response can be visually viewed using the screen here. Select the default Pre and PostFlows for our target endpoint, and you'll see another XML specification. You can see that we have no processing for the PreFlow, as well as the PostFlow. At the bottom here though, you can see the backend service URL that our target endpoint will hit. This is our Cloud Function. This is the development editor. If you scroll over to the right, you'll find a TRACE tab that'll allow you to trace the path of any request and response made to this API proxy. There's nothing here now. We'll be using this tab a lot in just a bit. Click on the Start Trace button up top in order to start tracing any requests that you make to your test APIs.

Attaching Policies to the API Proxy
In order to test our API proxy, we'll now move away from the Apigee editor and switch over to Cloud Shell on the GCP console, and within Cloud Shell, we'll install a few libraries. The first thing we need is the libxml2-utils library that will allow us to parse XML content. We'll run a sudo apt-get install command to install this library on our Cloud Shell VM. We'll now make our first curl request to the API proxy on Apigee. Notice the URL here, spikeysales-eval- test.apigee .net /customers_cf_apiproxy. This proxy in turn will simply call the Cloud Function that we had configured as our backend service. Our Cloud Function returns a response in the XML format, which we'll then prettyprint using xmllint library. And here is the nicely formatted response from our backend Cloud Function. It simply spit back the parameters that we had passed in in the JSON format in the XML format that is now displayed on your screen. We can now switch back to the tab window which has our Apigee editor and stop the trace session for our request. Click on the Stop Trace button, and we can now examine what the request flow was like. We can see that every component in the request and response path can be graphically visualized using the center pane. You can see that we first received a POST request from the client at the proxy base path customers_cf_apiproxy, the response was 200 OK. A bunch of details about this request is available to you. You can see that the content was application/json, the request was made using curl, and you can see the actual request content as well. And here is the XML response that was received from our backend service and passed on to the client. You can use this Next button on this trace page in order to step through the various processing that this request went through. In the visualization here, each processing stage is highlighted in pink, and all of the details are available in this lower pane here. If you're debugging the flow of a request, this graphic is very useful. Click on Next, and step through your request path. I'm going to hit Next a bunch of times, and here I am in the response path now. The response from the target server was 200 OK, and here is the content of the response. As you can see, this is pretty cool. However, the API proxy so far is pretty dumb in that it doesn't really do anything with the request and the response. We can change this by adding a policy to either the request or the response flow. We'll start off by using the +Step button here to add a policy to the response flow for this API. You can see from the proxy endpoints and target endpoints frame here that this policy will be a part of the PreFlow of the proxy endpoint. This will bring up a dialog showing you all of the built-in policies that are available. The policy that I'm going to choose here is the XML to JSON. I'm going to take the XML response that we get from the backend service and convert it to a JSON format because that's what my clients want. A name for this policy is automatically prefilled for you. Click on Add, and you can see the XML specification of this policy. This newly added XML to JSON policy is now a part of the policies list for this API proxy. You can also visually see here that it's a part of the response flow path for this particular API proxy. If you click on the ProxyEndpoint XML specification, you'll see that is policy is a part of its PreFlow. That's where we added it. Click on the Save button here to update the current revision of this API proxy. Now let's switch over to the TRACE tab and start a new trace session, and with trace on, we'll make a request to the same API using our Cloud Shell terminal window. But this time, we are going to format the response using JSON because that's what we expect the response to be, in JSON. And our policy took effect. Our XML response was converted to JSON. Back to the Apigee page. Stop the trace session, and you can see that this policy, XML to JSON, was applied to our response content. Our XML response was converted to JSON before we sent it on to our clients. Apigee allowed us to change the response based on our client needs without updating the backend service. This is a huge deal. We didn't require a new release to production of our backend.

Attaching Spike Arrest and Quota Policies
We're now ready to attach a few more policies to our API proxies. In this demo, we'll see how we can secure our proxy using a rate-limiting policy. A rate-limiting policy will allow us to limit the number of requests made to our APIs. We'll start off in the main page for API proxies on Apigee. You can see that the customers_cf_apiproxy is already available here. We're now ready to create another one. Click on the +Proxy button. We'll create a reverse proxy as before, and this time, we'll have our reverse proxy hit the products Cloud Function. We'll call this proxy the products_cf_apiproxy. This is the proxy base path, and it'll hit our Cloud Functions API called get_spikey_products. Click on the Next button once again. We'll create this API proxy with pass-through authorization. We want both the default and secure versions of this proxy, and we want both environments, prod as well as test, but we'll deploy to the test environment first. Our proxy isn't ready for production yet. Let's click through to the proxy editor and switch over to the DEVELOP tab where we can make changes to our proxy. The rate-limiting policy is best associated in the PreFlow of our proxy endpoint. We want requests to the proxy to be throttled before the proxy endpoint is reached. Select the Proxy Endpoint PreFlow and click on +Step and choose the Spike Arrest policy from this dialog. The Spike Arrest is a built-in policy available on Apigee allowing you to throttle requests. And here is the Spike Arrest policy added to our request processing path. The XML specification for this policy will give you additional details. You can see that we are currently throttled at the default rate of 30 requests per second. In order to view this policy in action, we're going to change this to throttle at a rate of 3 per minute. Update this revision by clicking on the Save button, and let's head over to the TRACE tab in order to trace requests that we make to this particular API proxy. Start the trace session here, switch over to our Cloud Shell window, and make a request to our products_cf-apiproxy. Remember that the requests to this Cloud Function should be in the XML format. It contains the product id and the name of the product. And the response from this Cloud Function is in the JSON format. Let's make another request here. Remember our throttling is 3 per minute. A third request, this is our last one, and when we try to make the fourth request within a minute, you'll find that we get an error. The error very clearly says that the fault is a Spike arrest violation. The allowed rate is 3 per minute, and we violated that. Switch back to the trace page on Apigee and stop the trace session, and you'll notice a bunch of interesting things here. You can see that we had three successful requests with 200 responses. Select one of these, and you can see the request content and the response content. You can see that these requests were successful. The fourth request here though was a 429. This was a spike arrest violation. Tracing the path of this particular request shows us that the spike arrest violation was triggered, as you can see from this exclamation point here. The details pane here at the bottom will give you the exact nature of the violation. Let's head back to the DEVELOP tab of this API proxy and add another policy in the same PreFlow path of the proxy endpoint. This time, we'll add a Quota policy. A quota is what you can use if you're packaging this API into a product and you want developers to have only a limited number of requests that they can make to their APIs before the free tier ends, or before you charge them more. Currently the Quota policy is after the spike arrest. You can drag it over to be before the spike arrest. You can see from the XML specification of the PreFlow that the Quota policy comes first, and then the spike arrest. Let's update the quota available. The default allowed count is 2000. We'll switch it over to 1, which means after making 1 request, this policy should come into play. This quota is on a permanent basis, so only 1 request per minute. Click on the Save button to update this revision, and let's start a trace session by clicking on the Start Trace Session button. We'll head over to Cloud Shell and make a request to our products_cf_apiproxy. The first request will get a successful response. The second request that we make though will be an error, and the fault here clearly says rate limit quota violation. Based on how you want your API product to be set up, you might want to use a Spike Arrest or a Quota policy to limit the number of requests to your API. And as before, you can trace the path that this request took on your Apigee page. Stop the trace session. You can see here that the first request was 200 OK; it was successful. The second was in violation of our quota.

Configuring Response Cache Policies
We're still in the DEVELOP tab of our products_cf_apiproxy. We're going to add a few more policies to this API proxy. We start off by clicking on the plus button next to Policies. We'll create a policy first and attach it later. This time, we want to add in a Response Cache. The response cache will allow us to cache the responses from our APIs for a specified time period. This can greatly improve the performance of your APIs. We've created and added the Response Cache policy. We'll now associate it with the PreFlow of our target endpoint. The Response Cache policy is listed under Policies here. Simply select it, and you can drag it to where you want it to live. This is in the Response path of our PreFlow of the target endpoint. You can see that you can create your policies first and use them in the right flow path based on your use case. Select the Response Cache policy in order to view its XML specification. Let's change the timeout to be 15 seconds. If the response from our backend API is fast changing, you might want to cache it just for a few seconds. We'll choose 15 seconds here. We'll also update our Quota and Spike Arrest policies. Our quota will be 100 per minute, and spike arrest will be 10 per minute. So far, all our proxy updates have been to the same revision. This time, we're ready to save this update as a new revision. That is Revision 2. If you take a look at the Deployment drop- down, you'll see that this Revision 2 has not been deployed to either test or prod. Before we test it out, let's deploy this revision to the test environment. Click on Deploy here, and your new revision will have been deployed. Let's head over to the TRACE tab and start the trace session in order to trace the requests that we make to this API. From the Cloud Shell terminal window, I'll use the curl command to hit the products_cf_apiproxy as many times as possible, till SpikeArrest stops me, so that is up to 10 times. You'll find me hitting the API over and over again. SpikeArrest, which allowed 10 requests per minute, has kicked in. We'll now stop our trace session and analyze the results. You can see that we made 11 requests. The last request was unsuccessful. Thanks to our response cache, what's really interesting here is the latency of each of these requests. You can see that some requests have very low latencies, they're hitting the cache, and some requests have very high latencies. We kept making requests till spike arrest came into play.

Creating Conditional Flows and Access Control Policies
So far, the flow processing that we applied, applied to all requests made to our API. This time, we'll see how we can configure conditional flows and how we can specify access control on those flows. We'll start off in the main page, which lists all of our proxies, and we'll click through to the customers_cf_apiproxy. Head over to the DEVELOP tab, and here is where we'll configure our conditional flows. We want this conditional flow to apply to the proxy endpoint, so we configure this by clicking on the plus button under Proxy Endpoints. The name of the flow is GET All Customers, and the condition type is a path condition. There are a number of different ways in which you can specify conditions in Apigee, a custom condition using a path as well as a verb, and so on. We'll stick with the path condition for now. We want to associate the condition with the /customers path on this particular API. This will create a new flow for this particular path on this API. Click on Add, and you can see that this flow is now available for you to attach policies to. Select this conditional flow for the /customers path, and click on +Step to add a processing step. We want to associate some kind of access control to the /customers path for this API, so this is the Access Control policy. And we'll edit this XML of this policy, which was a default ALLOW, to be DENY. Imagine that all other paths on this API is public, but the /customers path is private, so we explicitly deny access to this path. You can also make this condition more specific my specifying the IP addresses from which traffic is not allowed. Let's head over to a Cloud Shell terminal window and find the current IP address of this ephemeral VM. A curl request to ipinfo.io /ip will give us our current IP address. Let's configure this in the match rule of our access control. Traffic to the /customers path on this API is denied from this source address of our Cloud Shell VM. We're still on Revision 1 of this API. Let's save and update to this revision. Head over to the TRACE tab and start a new trace session for our request. We'll now head over Cloud Shell and hit our customers_cf_apiproxy, but we'll access the /customers path. And based on the access control that we've specified, we get an Access Denied response. Heading back to Apigee, we can stop the trace session and analyze the results. You can see that access control kicked in, and we got a 403 response for our request to the /customers path. 403 is the Forbidden status response, and you can see that this specific IP was what was forbidden. Let's switch back to the DEVELOP tab and update our access control policy in order to allow requests from our Cloud Shell ephemeral VM IP. We'll also tweak this policy a little bit to deny all other requests, so only requests from our Cloud Shell ephemeral VM are allowed to this particular path, the /customers path on our API. Deny all source IPs except for our Cloud Shell VM. Save this revision to our API proxy. Let's head over to Cloud Shell and hit the /customers path once again. And this time, our request will be successful. You can try the same curl request from other machines, maybe your local machine, and you'll find that it will fail. It'll only succeed from your Cloud Shell ephemeral VM that has this current IP address.

Deploying an API to Production
So far, we've been working with APIs in the test environment. We'll now see how after we've finished testing an API, we can deploy it to production. Here we are in the Develop editor of our customers_cf_apiproxy page. You can take a look at the Deployment drop- down, and you'll see that this revision is currently deployed to test. Let's switch this and deploy this API to production. Of course, this is best done when you're finished testing your API because production APIs will be visible to the public. Click on the Deploy button here to confirm deployment, and if you take a look at the Deployment drop-down now, you'll find that this revision 1 is currently both in test, as well as prod. You can always select one of these options once again to undeploy a proxy from a particular environment. If you click on the test option here, this will bring up a message to undeploy this proxy from the test environment. Let's cancel out of this. Now we know how it's done; we don't really want to undeploy this. Click on the default link under Target Endpoints, where you get the XML specification for your PreFlow and PostFlow for your target endpoints. We'll now change the endpoint of the proxy to point to a different Cloud Function. This is the get_spikey_products Cloud Function. Our customers_cf_apiproxy is pointing to a different Cloud Function for this particular revision. Once you've changed this endpoint, let's make a few more changes here, go over to the PreFlow for your proxy endpoint, and we'll add a new policy. This will be a Spike Arrest policy. We'll update the XML specification for the Spike Arrest to be 3 requests per minute. Now remember that this is the revision that is currently deployed to both the test as well as prod environments. When you click on Save, you'll get a warning dialog. Oops, you tried to deploy to prod. Is that really what you wanted to do? No, we don't want to. Let's cancel out of this dialog. We want to test these changes in the test environment first before we move it to prod, so let's save these updates as a new revision. This is now revision 2, and if you take a look at the Deployment drop-down, you can see that this revision hasn't been deployed to test or prod. Let's choose test here and deploy it to test so that we can test it out. Click on the Deploy button here, and you can see that this API proxy has now been deployed to test. If you now take a look at the Revision drop-down, you'll get a clear indicator of which revision is deployed to which environment. Revision 1 is deployed to prod, and Revision 2 has been deployed to test. This information is very useful. It's also available on the overview page. You can see here that revision 1 is in prod, 2 is in test. We'll now switch over to our Cloud Shell terminal window in order to make curl requests to our API proxy. You can see that we're now making a request to the test environment. Remember that the test environment now points to a different back end, get_spikey_products. This back end requires different input parameters, and it's not compatible with this current request, which is why we get an error in the response. But the very same request made to our API proxy in the prod back end will work just fine. We'll see that we get a valid response. Okay, this makes a good case for not deploying our current version, which is in test, to prod. We should leave the prod version as is, conduct more tests before we are ready to deploy the test version to production. Meanwhile, we can go back to the Apigee page and head over to the tab at the very right, the PERFORMANCE tab, which will allow us to view the performance of our APIs in both the test environment, as well as the prod environment. Here is the traffic per minute to our customers_cf_apiproxy. I've made a total of six requests to this test URL. Let's switch environments here and go to the prod environment, and you can see that we made a total of just two requests here. The Apigee PERFORMANCE tab gives you a quick insight into how your APIs are doing.

Writing Procedural Code to Process Requests
Apigee also gives you an option to include some code as a part of your flows, whether it's conditional flows or simple Pre or PostFlows. Here we'll see how we can include procedural code as a part of a PreFlow. We're currently in the Develop page for customer_cf_apiproxy revision 2. This is the revision that has been deployed to the test environment but not to production. At the bottom left here, go to the Scripts option and click on the plus button there to incorporate some procedural code. I'm going to create a new file, and I'm going to specify code in JavaScript. The other options that you have for this script are in Java or Python. The JavaScript code that I'm going to process my requests is going to be in Script- 1.js. I'm going to keep the script very simple. I'm going to redirect this particular API call to another URL, to the get_spikey_message Cloud Function, which returns a plain text response. You can access the context of this particular API proxy using the context variable, and you can also access certain flow variables such as the target.url. This setVariable command is what we use to rewrite the target URL that is hit by this particular API proxy. Procedural code allows you to work with requests, responses, and perform custom processing logic that is not available with the built-in policies. Once we've written out our procedural code using a script file, we can now add the script as a policy. Click on the plus button next to Policies. We're going to add a new JavaScript policy here that points to our script file, Script- 1.js. Once we've created this policy, we can now associate this policy with the PreFlow of our target endpoint. This makes sense here because we want the target endpoint that is hit by this API proxy to be changed before we actually hit our back end. So the PreFlow of the target endpoint is the correct place for this procedural code. With the target endpoint PreFlow selected, drag this JavaScript policy into the Request path of this PreFlow. With all our changes in place, let's save this as a new revision to our API proxy. This is revision 3, and we'll deploy this new revision to the test environment. The test environment is always the safest place to test things out. Choose test here, and click on Deploy. This request is interesting enough that we should trace its path as it flows through our proxy. This is the URL that we're going to hit. Click on Start Trace Session to start tracing our request. Head over to the OVERVIEW page where we find the URLs for this particular proxy. This time, we won't use curl; we'll simply click on this particular request and make a GET request from our browser. And observe that the response we get is not product or customer information. Instead, we're redirected to the get_spikey_message Cloud Function. This is thanks to the procedural code that we wrote in JavaScript. Go back to the TRACE page on Apigee and stop the trace session, and you'll be able to analyze how this URL request and response flowed. Observe that the JavaScript procedural code in the PreFlow of our target endpoint redirected us to a different backend service URL.

Deploying a Proxy Using the Open API Specification
If you've written your APIs using the OpenAPI Specification, you can directly use those to create Apigee proxies. Let's see how. If you haven't worked with OpenAPI Specification before, it's a vendor-neutral API description format, which can be specified in either JSON or YAML. If you want to learn more, you can visit the URL openapis.org. We are going to work with a standard OpenAPI Specification file that Apigee has provided to us, which is located on githubusercontent.com. This is an API specification that will hit the mocktarget.apigee .net test URL. There are a number of paths available on this API, /help, /user, /iloveapis, /IP, and so on. Let's see what this URL has. Let's hit mocktarget.apigee .net. It simply says Hello, Guest. You can test out some of the other paths as well, such help, which will list out all of the paths available on this URL. This is a great URL for you to play with Apigee because it has so many different paths, different kinds of configurations, and XML response, JSON response, and so on. Now that we've explored this, we're ready to go back to our main Proxies page. Here are the 2 proxies that we've created so far and the traffic on these 2 proxies over the last 24 hours. We're now ready to create a new proxy. Click on the +Proxy button. This will be a reverse proxy, but this time, we'll use the OpenAPI Specification. You can choose to upload the YAML specification or JSON specification from your local machine, or you can import from a URL. Here is the githubusercontent URL that we had explored earlier. Give the specification a name and hit Select. The API specification from the URL that we had given is now available on Apigee. Click on Next, and you will find that all of the details have been filled in for this page from the YAML specification, the name of the proxy, the base path and the description, as well as the URL that we want to hit on the back end. And when you move on to the next page, you'll find the number of flows that are available for this particular API. These are conditional flows which have been specified in our OpenAPI spec. Let's say we're not interested in attaching any policies to the JSON flow. We can simply unselect that conditional flow. Click on Next here, and let's move on to the authorization. Once again, we'll choose pass-through authorization. We'll choose the test as well as prod environments, and default and secure access to this URL. Select Build and Deploy and wait for this API proxy to be up. Let's head over to the DEVELOP tab and take a look at what's there. On the left here, you can view all of the paths that are available here on the proxy endpoint. These are the conditional flows picked up from the YAML specification file. We can now test out the APIs from our Cloud Shell terminal VM. Here is the mock-target-api, which returns Hello, Guest! That seems to be working fine. If you remember, we had left out the conditional flow for the /json path. If you hit this JSON path on our mock-target-api, the path exists and can be accessed. However, if you want to attach policies to this path, we need to add in the conditional flows, and that's what we'll do now. Click on the plus button for our proxy endpoint, and it'll give you the option to pick up additional conditional flows from our OpenAPI Specification. Here is the JSON flow that we had unselected earlier. We want to add that in at this point. With this conditional flow selected, we can now add a policy to this flow. We'll convert the JSON response to an XML format. Once the policy has been added, we can now attach this policy to the View JSON response conditional flow. Simply drag this policy to the response path of our View JSON response conditional flow path. So you have to add in the flow first before you can associate policies with that flow. With this flow all set up, we are now ready to save this as a new revision, and once this has been saved, we can deploy it to the test environment. Yes, indeedy, we do want to deploy revision 2 to the test environment, and we can now head over to our Cloud Shell VM and make a request to this JSON path. And thanks to the policy associated with this conditional flow, we'll get our response in XML. And on this note, we come to the very end of this module where we got a lot of hands-on practice deploying API proxies to Apigee. We saw how easy it was to build and deploy proxies, and we also saw how we could configure policies for rate limiting, quota management, and transforming our responses to the correct format. We studied how we could use conditional flows to help implement logic inside our API proxy. We also saw how we could use the test and production execution environments after we've deployed our proxy to test and iterated on it a bunch of times. That's when we move it to production. In addition to manually configuring proxies, we also saw how we could create a proxy using the OpenAPI Specification. In the next module, we'll work with hosted targets on Apigee and see how we can deploy Node.js applications to the Apigee organization environment.

Building Proxies with Node.js on Apigee
Module Overview
Hi and welcome to this module on Building Proxies with Node.js on Apigee. Apigee allows you to deploy your Node.js application develop locally onto Apigee as a hosted target. This hosted target is then hosted on the Apigee platform and can serve as a backend service for your APIs. You can then invoke these APIs using API proxies and configure these API proxies exactly like we did earlier. Hosted Targets run these Node.js applications in a native isolated sandbox inside the Apigee organization, and these are completely independent of the Apigee platform. This runtime environment hosted by Apigee is secure, scalable, and completely isolated. There are a few constraints on Node.js applications running as Hosted Targets. You don't have direct access to flow variables or to the cache. Apigee Hosted Targets give you a very easy way in which to get up and running with your Node.js applications.

Hosted Targets
At the time of this recording, there are two ways in which you can deploy your Node.js applications to Apigee. The first option is to use Hosted Targets. This is the native Node.js runtime, and there is nothing Apigee specific about it. You simply test your application locally, and it'll work in exactly the same way on Apigee as well. The second option is still available at this point in time, but it's deprecated. This is the traditional deployment option where Node.js executes directly on Apigee Edge. We'll focus our attention on deploying Node.js applications using Hosted Targets. This is the current preferred method of deploying Node.js apps. Hosted Targets allow Node.js applications to run in a native environment, and it does not depend on any Apigee-specific runtime. You test your application locally, and it'll run in exactly the same way on Apigee. Once you deploy your backend service using a Node.js application on Apigee, you can then set up an API proxy to access your backend APIs. Hosted Targets are scoped to a particular organization, the organization that is created when you set up your Apigee account. You can think of these Hosted Targets as being an isolated sandbox within your organization. You can then configure an API proxy to call into your Node.js application. You can then associate security policies, quota management policies, and anything else that you would do with any other API proxy. Developers would interact only with the API proxy, and your Node.js application can then make calls to any other backend service that you wish it to. Hosted Targets are a way for you to quickly deploy Node.js applications in a secure, scalable, and isolated environment, and configure Edge API proxies to call into these Hosted Targets. Your Node.js applications will run in a sandbox of your Apigee organization. Hosted Targets are extremely simple to configure and deploy. At this point in time, they only support Node.js though. Other runtimes are not supported. Your Node.js application running as a hosted target can integrate very closely with API proxies; however, hosted apps do not have access to the proxy runtime environment, which means flow variables that make up part of your API proxy, cache control, and other bits of information associated with the proxy is not available to the Node.js app. Think of Hosted Targets as a quick and easy way to set up a backend service for your proxies.

Implementing a Simple Node.js Application
In this demo, we'll see how we can deploy our Node.js application to Apigee Edge. This application will serve as a back end for our API proxy. All organizations on Apigee can host Node.js targets. If you want to confirm this, you can hit the Management API to view details of the organization that you have created on Apigee. This URL here will allow you to view the details for your specific organization. Here is what the page looks like. Scroll down, and you'll see a resource URL where you can click on the org_name here, replace the org_name with your organization. For us, it's spikeysales-eval. We'll specify our content in the JSON format, that's the default, click on the HTTP Basic authentication button in order to authenticate ourselves using the email that we had used to create our account, spikeysales@ loonycorn.com. Specify the password, click on Save. This is the authentication for our request. Click on Send this request, and you'll be able to view the details of your organization. Here is the response that we receive from Apigee with our organization's information. If you scroll down to the bottom here, features.isEdgeFunctionsEnabled should be set to true. This information tells us that Hosted Targets, which is what we'll use to deploy our Node.js application, is available to us because it's available on all Edge public clouds. Let's switch over to our Sublime Text code editor window in order to take a look at the Node.js application that we are going to deploy. Here is the index.js, package.json, app.yaml, and the node-hosted- express.xml files that we'll need. We'll keep our Node.js application very, very simple. It'll use Express. That is the minimalist and flexible Node.js framework for web applications. And we'll then configure an API path for /products. And this will simply extract the product ID from the request and return it in the response. And here is code to simply run this web server. You can specify the port for this server as a part an environment variable called PORT, or simply use port 9000. The package.json file on Node.js is where we specify the dependencies for this application. The only dependency here is the Express framework. The startup script for this Node.js app is the index.js file that we have just created. The app.yaml file gives us some additional specification for our Node.js app. This gives Apigee information about the Node.js runtime that we want to use when we deploy this as a hosted target, the name of our application, and the various environment variables that have to be set. The node-hosted- express.xml file gives us the XML specification of the API proxy that will be created on Apigee. You can see that the name of our API proxy is node-hosted-express, and this is revision 1. The nested Resources tag here indicates that this is a hosted target, and here are the three files that we'll host on Apigee, the app.yaml, index.js, and package.json. We'll now head over to nodejs.org in order to install the Node.js package on our local machine. This is so that we can use npm, or the node package manager, in order to download the Apigee tool that we'll use for deployment. I'm currently running on a macOS. Click on the LTS version, and that's the version I'm going to download. This is the version with long-term support. With this version downloaded, I'm going to click on the pkg file in order to install it on my machine. Simply follow through with the install steps that make sense for your machine. Click on Continue here, and wait for Node.js to be installed. Make sure you agree to this license agreement, click on Continue, and wait for the install to run through. Click on Install, and once the installation has been complete, you might be prompted for your password. You can simply close this dialog. With the install complete, I no longer need the Node.js installer; I can move it to Trash.

Deploying a Hosted Target to Apigee
I've now switched over to a terminal window on my local machine, and I'm checking what version of node is installed. Node -v will give me version 10.13 .0. You'll see that npm is also installed, and the npm version that I have is version 6.4 .1. This is what I'll use to install the apigeetool. Call sudo npm installed and specify the apigeetool which we'll use to deploy our Hosted Targets. This tool has been successfully downloaded and installed on our machine. We can now set up our hosted target, our Node.js app. Create a new directory called hosted_target and cd into that directory. This is the directory where we'll set up our app. Under the hosted-target directory, we'll have a resources directory where we'll place our JavaScript files. Here we'll create a new file called index.js using the nano editor, and we'll paste in the code that we saw earlier for index.js. Hit Ctrl+X to save this file and open up a new file package.json using the nano editor and paste in the code for package.json as well. Save this file, and set up the app.yaml file with the same code that we've seen before. With these three files created, we are now set for everything that we need under resources. Move one directory up to the hosted_targets directory and create the node_hosted_express.xml file and paste in the XML specification for our API proxy. Let's take a quick look at the structure under the resources directory. We have app.yaml, index.js, and package.json, and the node_hosted_express.xml file is one level up. We're now ready to use the apigeetool that we installed to deploy our hosted target called apigeetool deployhostedtarget, that is our command, and we need to specify the organization to which this target should be deployed. That is the spikeysales-eval. We'll deploy this proxy to the test environment specified by the -e flag, and the -n flag gives us the name of our proxy. We call this the node_webapp_apiproxy. Here is the username for our account, spikeysales@ loonycorn.com. It'll prompt you for a password, which you can now type in. And with this, you'll find that your API proxy, along with the hosted target, has been deployed. If you go back to your Apigee account and take a look at all of the proxies, at the very top you'll find the node_webapp_apiproxy that we just deployed. You can click through and take a look at the overview of this proxy. You can use the DEVELOP tab to create and attach policies to this hosted target. If you take a look at the target endpoint, you'll see that it says it's a HostedTarget. And here are the three files that make up our hosted resources. Here is the app.yaml that we had uploaded, the index.js which contains our code, and package.json which contains the dependencies. We're now ready to make a request to this API. Click on the TRACE tab, and let's start a new trace session so that we can follow this request. Switch over to your Cloud Shell VM, and make a request to this API to the /products path. We pass in the id as a query param to this URL, and this id is what is returned in the response message. Our API as a hosted target has been deployed successfully. Switch back to Apigee and stop the trace session, and you can view the path of your request and corresponding response. Once you've deployed your Node.js app as a hosted target and set up an API proxy for it, you can set up policies on this proxy exactly like you did with other API proxies. You can use the apigeetool from your command line in order to get information on your deployment. Apigeetool getlogs will stream the logs for this current deployment. Make sure you specify the --hosted_build flag to get build logs. You need the username and password for your account, and here are the logs for your build. If you take a close look at your logs, you'll find that your Node.js application has been hosted within Apigee as a Docker container, and this container is registered with the Google Container Registry. Gcr.io is the prefix. You can also stream runtime logs to your command line using the apigeetool. Make sure you specify the flag --hosted-runtime. And here are the runtime logs for our app.

Configuring Custom Analytics on API Proxies
In addition to a bunch of built-in analytics about traffic patterns to our APIs, Apigee also allows us to configure URL-specific custom analytics. Here we are in the Develop page of the API proxy that we've just deployed for the Node.js application that is a hosted target on Apigee. We'll now add a new processing step as a PreFlow for our proxy endpoint. This step will extract some variables from our request that comes into our proxy. The request to this proxy includes a product ID, and it is this product ID that we want to extract. We'll call this policy to extract variables the GetProductID policy. Add this policy as a PreFlow to our request. This XML for this policy is something that we'll update, and we'll paste in something very simple. The ID of the product for which the request is made is available as a query parameter, and we want to extract this product ID. This will allow us to analyze which products are the most queried. We'll also add in another step in the response part of our proxy endpoint, and this step will be a Statistics Collector. This will collect statistics about the product ID that we just extracted. Update the XML of the Statistics Collector to extract the ID from our query param. This is the ID that we collected in the request path. We'll now save the updates to the API proxy of our Node.js app as a new revision, this is revision 2, and we'll deploy revision 2 to the test environment. Once the UI gives us the happy message that the deployment has been successfully completed, we can switch over to the TRACE tab and start the trace session. We'll now make a number of requests to this API with different product IDs. I'm going to send three requests, two of which have the same product ID, and the third one has a different product ID. So here are the three requests, two with the same ID, and the third with a different ID. Switch back to the Apigee page, stop the trace session, and let's analyze the results. Here are the 3 requests that we made which had 200 responses. We set up a Statistics Collector for custom analysis, and this is available in the ANALYZE tab. Click on ANALYZE, and then Reports here in the screen that pops up, and let's click on the plus button to generate a custom report. This report name is Traffic Monitoring of products. We want a line chart here. The first metric that we want to track is traffic to our API, and we want to perform a sum aggregation on the traffic. We can track additional metrics in this report. Click on the +Metric button here and track the response size. We want the average response size for this API. You can then scroll down and specify the dimension for which you want the metrics to be calculated. Here are all the built-in dimensions available. You can see our custom id dimension is also available here. Choose this custom dimension and click Save in order to generate our custom report. And here is what the report looks like. You can use this drop-down here to customize the date range for this report. If you click on this, you'll get an option to specify the start date and the end date. If you hover over the pie chart on the left here, you'll see the split of product IDs for which requests have been made. We made exactly one request for the FUR-BO product ID, and we made two requests for the product ID starting with OFF-AR. Here is a line chart tracking the sum aggregation on the traffic for the various product IDs. Each line represents a request for a different product ID over time. And there is a legend off to the right showing you which line represents which product ID. And here is the line chart giving you the average response size for each of the product IDs. There are a number of options here for your charts. You can switch from the chart view to the table view and view all of this information in the form of a table, and you can click through and view individual IDs' information as well. And on this note, we come to the very end of this module where we studied Hosted Targets on Apigee. We saw that Hosted Targets allow us to deploy Node.js applications to Apigee, and the deployment was very straightforward using the apigeetool from the command line. These Node.js applications, which are Hosted Targets, can be configured as backend services for our API proxies. This app can then be invoked by API proxies, and you can specify policies for these proxies, just like you did before. Node.js applications are hosted targets. They reside and run in isolated sandbox environments within our organization. Hosted Targets are completely independent of the Apigee platform itself, and Apigee currently supports a number of versions of Node.js. There are some limitations when you're using proxies with Hosted Targets. You don't have direct access to flow variables or even the cache. However, it's a quick and easy way for you to get up and running with your backend services on Node.js. In the next module, we'll see how we can use Apigee to provide a front-end façade for our App Engine apps. We'll also see how we can secure access to our APIs using API keys and OAuth.

Using Apigee with Google App Engine
Module Overview
Hi and welcome to this module where we'll configure Apigee proxies to work with Google App Engine. App Engine is where our backend service will be hosted. App Engine is what we would use to develop hosted web applications on the GCP. If you want functionality beyond Cloud Functions but you still want to have a serverless platform-as-a-service offering, App Engine is what you would use. As a developer on App Engine, you only focus on writing code. Provisioning, versioning, scaling, security, everything is taken care of for you by the Google Cloud. Just like with other backend services, Apigee proxies can be configured to hit App Engine endpoints as the target endpoint. In this module, we'll also cover how you can configure secure access to your APIs using either an API key or OAuth.

Introducing App Engine
Apigee is often used to expose your App Engine applications using API proxies. But what exactly is App Engine? App Engine is the platform-as-a-service option that you have on the GCP to build hosted applications. So anytime you're thinking about compute choices, there are five broad choices that you have spread out across a spectrum. At the very left of the spectrum are bare metal environments. You can choose to run your applications here. At the very right end of the spectrum are serverless functions such as Cloud Functions. You can run your applications on the cloud as well. Your operating overhead for running compute really falls as you move from the left to the right end of this spectrum. Less ops means less administrative overhead. You don't really have to provision machines when you're working with serverless functions. As you move towards the left, though, you get more control over how your code runs, more control and low-level access to disks and files. Any cloud computing platform, including the GCP, will offer products along this spectrum. We have Google Compute Engine on the very left, which allows you to provision VM instances on which you app can run, and Google Cloud Functions on the very right allow you to run serverless functions. The same range of compute choices is available on any cloud platform. For example, these are the options available on AWS. AWS EC2 is the equivalent of the Google Compute Engine. This is the infrastructure as a service, or the IaaS offering on the GCP, as well as AWS. At this end is the platform-as-a-service offering. This is where you'd run hosted applications, and this is where App Engine comes in. When you develop apps using App Engine, you don't have to worry about provisioning machines at all. App Engine is Google's fully managed serverless application platform, which means all you have to do is write code. You'll write code in Java, PHP, Python, Ruby. App Engine supports a variety of programming languages. When you use App Engine, all you have to do is write code and then deploy it. You don't have to worry about provisioning machines, scaling, versioning, security, nothing. The GCP abstracts you away from all of these details. In addition, you can use Stackdriver to configure monitoring, logging, and diagnostics for your app. App Engine is a very easy way for you to get up and running with hosted applications on the Google Cloud.

Building and Deploying an App Engine Application
Let's start off by using the Google Cloud Platform to develop an App Engine application and then deploying this application, which we'll use as a backend service for our API proxy. Here we are in the Cloud Shell terminal window on the GCP web console. Let's create a new directory called the spikey_webapp and cd into this folder. We'll write the code for our App Engine application using the browser-based code editor that the GCP offers. This is code editor, and this is the explorer window within our code editor. This explorer window shows us a graphical user interface of our home directory on Cloud Shell. Within the spikey_webapp folder, we'll create a new folder and call it templates. This will hold the templates for our application. Within this templates folder, create an index.html file that will contain the HTML and CSS for our web application. We'll create another Python file called main.py, which will contain the Python code that will run our web application using Flask. Set up the import statements for the Flask libraries. Flask, if you haven't used it before, is a micro web framework within Python. It's very lightweight and easy to use. Instantiate a new Flask application. Flask uses Python annotations in order to route requests to specific pages. The root URL path, /, will be routed to our index.html file that we just created. This is our very simple App Engine app. We'll simply run this Flask application when we execute this Python code. One level up under the spikey_webapp folder, create a new file and call this the app.yaml file. This is the YAML specification for our App Engine app. This YAML fil is used to specify metadata information for our App Engine app. We use the Python environment; we use the App Engine flexible environment. App Engine offers two environments which you can use to build and execute your applications, the standard environment and the flexible environment. The flexible environment is more advanced and requires a little more configuration. We'll run our Python application using a simple, lightweight Gunicorn web server. Our app uses Python version 3. Specify the version here, and specify that we want just one instance to run this app. It's a very simple app, after all. We don't want it running on multiple VMs and taking up additional resources. We'll use a single CPU instance with this memory and disk specification as you see here on screen. Under the spikey_webapp folder, create a new requirements.txt file, where we'll specify the dependencies for our application. We just depend on Flask and Gunicorn. Let's switch over to our Cloud Shell terminal window in order to build and deploy this application. We'll perform our build and deployment using a virtual environment, which will allow us to install and set up all of our dependencies in an isolated sandbox. Create this virtual environment and then activate it using the source env/bin/activate command. Once we're within this virtual environment, we'll install the dependencies we need using a simple pip install of everything that is in requirements.txt. This was Flask and Gunicorn. Once the installation runs through, we're ready to build and deploy our application. We'll deploy our app using gcloud, which is the command line utility that Google offers to work with the GCP. Gcloud is already installed on your Cloud Shell VM. The gcloud app deploy command will build and deploy this current App Engine application in your current working directory. Confirm that yes, indeed, you want to deploy this app, and your app will be built and then deployed. At the end, once things run through successfully, you'll see a URL for your application. Our application is running at spikey- apigeeproxy.appspot .com. This is your project ID, .appspot .com. Running the glcoud app browse command will allow you to browse your App Engine app. This simply points you to the URL itself. If you go to your web console on your main dashboard page for your project, under Resources you'll be able to see the one app that we've deployed to App Engine. Click through here, and you'll get a bunch of details on this app, including monitoring information on the top right. You'll see the URL that if you click will bring up your application. Here is our puppies website that we've just deployed.

Securing Access Using API Keys
On Apigee, when you're configuring an API proxy for a particular backend service, it's possible to secure your APIs using keys. In order to give developers who are accessing your backend services secure access to these APIs, the app developer must first register a client application with Apigee. For this, you first set up a developer account and then that developer sets up a client application. When this client application is registered, your developer will receive an API key, and this API key must be included in every API request to that proxy. The API proxy will then use this API key to see whether the developer is authenticated to access this API. Now this key is in the control of Apigee and can be revoked at any point in time, which means you as the creator of the backend service that is exposed as an API can always revoke access to those APIs. Keys give you a lot of control over who is consuming your APIs. You can also configure keys which are only valid for a short time period. API proxies that are configured to be used with API keys verify the API key policy. This policy ensures that the key is valid, has not been revoked, and it matches the key for the requested resource.

Configuring a Proxy to Use an API Key
In this demo, we'll configure an API proxy to access our App Engine application that we deployed earlier. We'll protect this API using a key. We start this demo off on the Apigee API Proxies page, and we'll click on the new proxy button to create a new API proxy. We'll configure a reverse proxy as we've done before, and the next page, we specify the name of the proxy. We'll call this the gae_webapp_apiproxy. Here is the proxy base path, gae_webapp_apiproxy, and the API that it'll call will be our App Engine application that we deployed earlier. Click on the Next button. On the next page, we'll be given an option to secure access to this API proxy using a key. Select the API key, and you'll be asked if you want to publish an API product. When we were introduced to Apigee earlier on in this course, we spoke briefly about API products. This is basically a collection of API resources which can all be bundled together and associated with a quota or a service plan. This is who you might monetize your APIs. Click on the Next button. We'll go with the default environments and access, we'll have the test and the prod environment, we'll have default, as well as secure access. We'll first deploy this proxy to the test environment. Click on Build and Deploy, and then you can view the gae_webapp_apiproxy. Here is our Overview page. If you click on the DEVELOP tab, you'll see that this API proxy comes preconfigured with few policies. This is what secures our API with a key. The first policy here applies to our proxy endpoint PreFlow, and it's the Verify API Key policy. This policy will check to see whether a request to this API has been made using a valid key. If the key is not valid or if it has been revoked, the request will be rejected. It will never reach our target endpoint or our backend service. Now our target endpoint, which is our App Engine URL, doesn't really know about this API key. All of that is part of this API proxy, which means before we pass our query onto the backend service, we will remove the query parameter associated with the API key. And this remove.queryparam .apikey policy is configured to do exactly that. By the time the target endpoint for this API proxy, which is our App Engine URL, sees the request, there is no API key associated with the request. The access control using API keys is only via our Apigee proxy. So if you go to spikey_apigeeproxy.appspot .com, that is our App Engine URL directly, you will still be able to access our website. However, if you hit the URL for our proxy, you'll find that this fails. Because we don't have a valid API key in our request, we're not able to access the backend service.

Creating a Developer and an App
In order to access our API proxy, we now need an API key. And to get this API key, we need to create a developer account and an application within Apigee, and that's exactly what we'll do. Click on the publish icon to the left and choose API products under PUBLISH. This will list out all of the API products that you currently have available. We just have the single product protected by an API key. This the gae_webapp_apiproxy product. Under PUBLISH, we also have the Developers tab, which you can use to create a new developer account. Head over to Developers, click on +Developer, and let's set up a new developer named Philip Fox. This developer has the username philip. Let's give him a fictitious email address, we'll call it philip@ spikeysales.com, and click on the Create button. This will create a new developer account. And once this developer account has been set up, in order to get a key, we need to create an app associated with this developer account. Head over to the Apps link on the left, and on the Apps page, click on the create new application button on the top right. We'll call this the apikey_webapp, and the developer for this is Philip Fox. We want this application associated with Philip Fox to have a valid API key to access our App Engine product. Click on the +Product button and choose the gae_webapp_apiproxy. We've configured this spikey_webapp application in Apigee associated with Philip Fox to have access to our API product that calls into our App Engine application. Click on Save, and we have set up a new application.

Specifying API Keys Using Query Params and Headers
In this demo, we'll see how we can generate a valid API key that we can use to access our API proxy, which will then call into our App Engine URL. Here in the Apps tab under Publish is our spikey-webapp that we just created. If you click through, you'll view at the bottom that it has access to our gae_webapp_apiproxy-Product. Just above that is a Show button that will allow you to get a valid API key to access that particular product. Click on Show, and here is our API key. You can then hide it once you've copied it over. This application now has a valid API key to call into this API product, our gae_webapp_apiproxy. Let's switch to a new tab on our browser window and make a call into our API proxy, but this time, we'll pass in a valid API key. Our API key is equal to the API key that we just copied over from the previous tab. Observe that we pass in the API key in the form of a query parameter to our URL. Now that we're using a valid key, we have successfully managed to hit our App Engine app. Let's go back to our develop tab and switch over to the API Proxies page. We'll now configure our proxy to accept an API key in the form of a header, not as a query parameter. Click through to the gae_webapp_apiproxy and head over to the DEVELOP tab. If you observe the XML specification of this Verify API Key policy, you'll see that the API key is requested in the form of a query param. Let's change this to ask for the API key in the header of our request. Save the update to this particular revision, and this time, we'll use our Cloud Shell terminal window in order to make a curl request, and we'll specify the API key in the header. Notice the -H for the header. The apikey is the API key that we copied over from our app. And you'll see that our request is successful because we have a valid API key. We're still in our virtual environment. We can go ahead and deactivate this. At this point, our App Engine app has been deployed, and we've secured access to it using an API key.

OAuth for Secure Access
If you browse the web at all, chances are that you've often used OAuth to gain access to a secure resource. In this clip, we'll understand what exactly OAuth is and see how we can use OAuth to secure our APIs. OAuth refers to an authorization protocol that enables apps to access information on behalf of users without requiring users to divulge or share their username and password. OAuth is what enables your favorite grocery application to allow you to sign in using your credentials from either Google, Facebook, or some other login. Your actual credentials, that is your Google username and password, are not actually shared with your grocery application. Instead, your grocery app will initially redirect you to your Google or Facebook login, and you sign in with your security credentials there. This, in exchange, will give your grocery app an access token. You're exchanging your security credentials for an access token, and this access token becomes your card key. You'll then use these tokens and use them to access secure resources. Let's say you have a username and password that gives you secure access to a site, joe@ loonycorn.com, and pass123 is your password. Now this can be converted using OAuth to an access token. This access token is just a temporary random string of characters which can't really be guessed, and this access token will allow you to access the secure resource. You don't need to keep specifying your username and password. When you use Google credentials to sign in to your grocery app, this is OAuth. The resource owner can delegate access to a secure resource without sharing username/password information. The grocery app will not have access to your username and password on Google. Instead, it'll use the access token to verify that yes, indeed, you are authorized on this app. Here are the steps that you need to follow on Apigee in order to protect your API proxies using OAuth. You'll first create an API proxy and configure it so that it's OAuth protected. You'll then create an API product exactly like we did when we generated an API key. We'll then create a developer and an application, and we'll get credentials for the developer in that app. These credentials will not just be an API key. Instead, it'll be a username and password for that developer. We'll then exchange those credentials, that is our username and password, for an OAuth access token. This we'll do using an OAuth provider that Apigee gives us out of the box. The OAuth provider will give us an access token for our username and password, and once we have this access token, we can use this token to access the OAuth- protected API proxy. That is the original proxy that we had set up for our backend service. OAuth is very popular and widely used, though it's not without its own vulnerabilities. It works over HTTPS; it authorizes using access tokens rather than your username and password. The current version of OAuth is OAuth 2.0, and it's completely different from the previous version. OAuth 1.0 and 2.0 are not compatible with each other. If you want secure access to your API, protecting it using OAuth is a good choice. We want to avoid direct authentication, which is why we might need to use OAuth. It also avoids ever providing passwords for delegated access. So your username and password are not exposed to anyone.

Configuring an API Proxy Secured Using OAuth
In this demo, we'll see how we can secure an API on Apigee using OAuth. We'll start off in the API Proxies page for Apigee. We have a number of proxies already configured here. We'll now configure a new one. This is once again a reverse proxy, and we'll call this the customers_oauth_apiproxy, and it'll call the cloud function get_spikey_customers. This is the target endpoint for our proxy, our backend service. Click on the Next button, and we move on to the page where we click on the radio button where we use OAuth 2.0 to authorize access to our API. We'll also publish an API product. Click on the Next button and accept the defaults. We want the test as well as prod environments. We want secure as well as default access to our API. And once again, we'll work in the test environment where we can play around with our proxy. Click through to the customers_oauth_apiproxy here. This is the Overview page. Click on the DEVELOP tab where we'll view the policies that apply to this proxy. Observe that the very first policy configured in the Preflow for our proxy endpoint is the Verify OAuth v2.0 Access Token policy. This is the policy that will check to see whether our request has a valid OAuth access token. Now of course our target endpoint, which is our cloud function, knows nothing about OAuth, which means we have to remove our OAuth header authorization before we pass the request onto our cloud function, and that's what this remove-header-authorization policy does. Let's use the Cloud Shell terminal window on our GCP web console and try and hit this API proxy that has been protected using OAuth. Our content type is in the JSON format, and here is the customer name and the corresponding ID. And you can see that we don't have a valid access token. Our request failed because our access token was invalid. Now if you head back to your Apigee page and head over to API Proxies, you'll find that there is an OAuth proxy here which can be used to generate a valid OAuth token from your client credentials. This is what we'll use to get a valid access token. Every organization in Apigee comes preconfigured with this API proxy. If you take a look at the DEVELOP tab for this OAuth API proxy, you can see that the first policy is the GenerateAccessTokenClient policy. This the policy that this API proxy uses to generate access tokens for the client credentials that you specify. Let's make a small change to the XML specification for this policy. We'll change it so that any token generated using this proxy is valid only for 5 minutes. Our access token will expire in 5 minutes. Click on the Save button here and save this update to our OAuth proxy. This is the built-in proxy that we've tweaked a little bit. Head over to PUBLISH using the navigation menu, and under PUBLISH, we'll go to API Products. We've already created an API product associated with our customers_oauth_apiproxy. Click through and view the product details. We've seen this page before. There's nothing really different or new here that we haven't seen. Let's head over to Developers and create a new developer. Let's create a new developer called Lynn Smith. Her username is Lynn, and her email address is lynn@ spikeysales.com. Click on Create, and here is a developer created. We'll now head over to Apps and create an app associated with this developer. Click on the +App button here. We'll call this the list_spikey_customers_app, and this is associated with Lynn Smith. We also need to specify the products to which this app has access. Click on + Product. This app has access to our customers_oauth_apiproxy-Product. Click on the Save button here and wait for this app to be created.

Generating an Access Token and Accessing the API
Here is the app that we just created. Let's now click through and get the credentials for this app, which we can then exchange for an access token. Click on the Show button next to the Consumer Key. That will give us our username, that is the consumer key, and click on the Show button next to the Consumer Secret, and that will be our password. So this consumer key and the corresponding secret is the username and password for this product, the customers_oauth_apiproxy-Product. We now need to exchange this key in secret for an access token. We do this by making a curl request to the OAuth API proxy. Here is the secure URL to this proxy, spikeysales-eval- test.apigee .net /oauth/client_credential/accesstoken, and the grant type is client credentials. The client credentials grant type is typically used when applications request an access token to access their own resources and not on behalf of a user. In the request body, we specify the consumer key and secret associated with the API product. Here is the client ID and the corresponding client secret copied over from the previous page. The response from this API proxy will give us the OAuth access token. Copy over the access token here. This is what we'll use to access our API. Let's try accessing our API once again, our customers_oauth_apiproxy API, and this time, we'll specify a valid access token. This is the access token that we just retrieved from the previous API call that we made to the OAuth proxy. Here is the customer name and ID specified in the JSON format. You'll find that this request is successful, and we get the customer name and ID in the XML format. We'll configure the OAuth proxy on our Apigee account to have the access token be valid only for 5 minutes. So if you try the same request once again after 5 minutes, you might get a message like this, Access Token expired. In order to access our customers API once again, you'll need to request a new access token from the OAuth proxy. Specify the client ID and secret. Here is our new access token. You can now use this new access token to authorize ourselves to the customers_oauth_apiproxy. Here is our new access token. This should now be valid. And you can see from the response that yes, indeed, it is. We've successfully secured access to our API using OAuth access tokens and used a valid access token to access our API.

Performance Analytics for API Proxies
We've created a number of proxies and deployed them to the test and production environments. Let's now see how we can view analytics for our API proxies. On the Apigee page, click on the analyze option here to the left, and under ANALYZE, let's head over to the API Proxy Performance page. This will bring up a number of graphs allowing you to view the performance of your various proxies. Here is the Traffic per Hour graph. This gives us information for all of the proxies that are configured. Here are all of the requests that were successful, and here are the requests that were failures. Here is the graph showing us the average response time across all of our proxies. Some time around 2 hours ago, you can see that there was a spike in the average response time. That's when there was a spike in the number of errors as well. And here is a traffic split by individual proxy. Here we can see which proxies are popular. Here is the products_cf_apiproxy, here is the node_webapp_apiproxy, and the less popular ones are the customers_cf_apiproxy, and so on. If you scroll on below, you'll see a graph which gives us the average response time on a per-proxy basis. The node_webapp_apiproxy, which runs against our hosted target, seems to be the slowest of all. All of these are performance analytics for our test environment. You can switch the environment over, and you can view analytics for the prod environment as well. We have exactly one proxy deployed in prod. Only the customers_cf_apiproxy has been deployed to prod. Here is the traffic for that proxy and the average response time for that proxy. If you head over to the Devices page, that will give you information about the various kinds of devices that were used to access your API proxies. This is especially useful if you have APIs running that can be hit using curl, mobile apps, or your website. Once again, you can switch environments to view device usage patterns by environment. Here are the usage patterns for the test environment. If you scroll down here, you get information for traffic by platform and traffic by agent as well. This can give you information about the kind of browsers that were used to access your APIs. As you can see from the left navigation pane here, there's a lot you can view under performance analytics. I'll leave it to you to explore all of these options.

Summary and Further Study
And with this demo, we come to the end of this module where we saw how we could deploy an App Engine application and create an API proxy to access it. App Engine is the GCP solution for hosted web applications and can be considered to be the equivalent of AWS's Elastic Beanstalk. This is the platform as a service, or PaaS, offering on the GCP. Provisioning of machines, versioning, scaling, all of that is abstracted away from the developer. App Engine is often an important backend service when we use Apigee. Apigee proxies can be configured to hit App Engine endpoints. We saw how easy it was to build and deploy a simple App Engine app and configure an Apigee proxy to hit it. We also saw how we could secure APIs using an API key on Apigee. We also saw how we could secure APIs using OAuth. API keys and OAuth are implemented in Apigee using policies on the request path. With this, we come to the very end of this course. But before you head out, make sure you clean up the resources that you've used. You won't be charged for your Apigee evaluation account, but it's best to delete it anyway. Delete all of the API proxies that you've created so you don't leave anything open. Delete the Cloud Functions that we created on the GCP to use as backend services. If you're interested in continuing your learning of other technologies on the GCP, here are some courses on Pluralsight that you can watch. Architecting Event-driven Serverless Solutions Using Google Cloud Functions will give you all that you need to know about Cloud Functions on the Google Cloud. Or if you really want to learn about App Engine, Architecting Scalable Web Applications Using Google App Engine is the course that you might want to watch. And that's it for me here today. I hope you enjoyed this course. Thank you for listening.

Course author
Author: Janani Ravi	
Janani Ravi
Janani has a Masters degree from Stanford and worked for 7+ years at Google. She was one of the original engineers on Google Docs and holds 4 patents for its real-time collaborative editing...

Course info
Level
Beginner
Rating
4.9 stars with 14 raters(14)
My rating
null stars

Duration
1h 52m
Released
10 Jan 2019
Share course

